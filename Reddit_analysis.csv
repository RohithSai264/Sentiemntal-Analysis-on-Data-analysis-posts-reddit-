Top_comment,Comment_Score,Post_id,Self_text,Subreddit,Post_created,Total_comments
"I'll take advantage of this metapost to ask a metaquestion.. what do you think we should do with posts that are clearly not asking actual complete questions?  i enjoy reading and answering in this sub but tend to just downvote [this kind of thing](https://www.reddit.com/r/MLQuestions/comments/gkr03z/i_tried_making_opencv_project_for_counting_number/) because there's no actual question there.  (""how can i add more features?"" what are we supposed to say to that..)

i feel like on the one hand its good to help people learn how to formulate the question they are trying to ask, so i don't necessarily believe every question needs to be complete and specific, but somehow I feel there also needs to be a lower bound on the quality of questions here because the sub is filling up more and more with things like single sentences like ""how should i do some project that i don't know what will be...""

while downvoting is a fine solution in theory, i find that often these posts actually get upvotes and i can't help but think it's just bots, so i feel like some moderation may be necessary to clear the spam.

so, open question, what is appropriate to flag as spam, and what should be just downvoted, and what should be responded to in some kind of official ""please expand your question or it will be removed""?

edit: suddenly people answering this after 4 months, why?",6,g9q5x2,"My apologies! I got busy lately and didn't know what happened around the subreddit type and everyone was required to be approved to make a post in the subreddit. 

I have disabled this and made the subreddit public. As the number of posts are increasing in the group, I would request the readers to tag any spams whenever you see them. Thanks.",MLQuestions,2020-04-28 09:16:30,6
https://alexnisnevich.github.io/untrusted/,1,xi0zyh,,MLQuestions,2022-09-18 20:48:00,2
"For a given word, isn't the denominator in the td-idf the same for both classes, i.e. a rescaling of the counts?

As NB looks at words independently it seems like the basic count probability model should work the same as td-idf. 

td-idf renormalization is interesting/useful if you're embedding or reducing a whole document vector with multiple words at one time into some space and looking at distances.",2,xhhy3q,"I have this general understanding that for simple text classification ML models, **frequency vectors** of tokens work well with **Multinomial NB** and **binary vectors** work well with **Bernoulli NB**. Notably these two vectors have only integer components, so there's some innate sense of count. But **tf-idf vectors** are not integer, they're continuous real numbers. Now does it make sense to use them with **Multinomial NB** or **Bernoulli NB**. Or should I go with **Gaussian NB**, please provide explanation :)",MLQuestions,2022-09-18 07:21:15,2
"As it is an artwork, you might want to try using style transfer to create pseudo artwork, or using more recent model such as Stable Diffusion with training with textual inversion (see [this post](https://www.reddit.com/r/StableDiffusion/comments/xb1p5j/textual_inversion_with_automatic1111_webui/))",2,xgydnv,"So I want to train a DCGAN on a custom dataset, to output my artwork. 
How do I go about creating the data set ? Should I use image augmentation to create a large dataset because currently I have only 140 photos of the subject.",MLQuestions,2022-09-17 14:36:21,5
"I would like to add here, that I am looking for any suggestions or tips as to how I can find ""advanced"" project ideas like this, that deal with technologies that we often hear about? Also, I would like to ask, was this problem(UAVs not responding to moving threats) a common one in 2013-14, or was it covered by some select news channels/web magazines at the time?",1,xgnh3m,"Hey,

So, I've recently been getting into Machine Learning, and I've been browsing some of the best projects that people have built using Machine Learning/AI. A project that struck out to me was the winner of the Google Science Fair in 2014(13-14 years old category), called Firefly. For reference, here is the link detailing how the project works:

[https://hackaday.io/project/7139-firefly-a-low-cost-flying-robot-to-save-lives/log/23303-escaping-from-moving-threats](https://hackaday.io/project/7139-firefly-a-low-cost-flying-robot-to-save-lives/log/23303-escaping-from-moving-threats)

As you can see, the creator mentioned that he got the idea from reading the news. But, I couldn't find any news articles that talk specifically, or even mention, the problem of robots (UAVs) not being able to react to their environment. So, the question that I wanted to ask was that how could he have gotten the knowledge of the problem through reading the news, and what can I do to get my own ideas like this? (The question that I'm asking is not actually about the project, but more about how the creator got the idea and how I can get these ideas too..)",MLQuestions,2022-09-17 07:08:15,1
"try to think of a way to formulate your problem in terms of a prediction of some kind. 

* The target of your prediction is going to be what you will be computing likelihoods on. You want to compute likelihoods over day's and/or events, so that means that's what you're predicting. 

* It sounds like maybe your calender events fall into a couple of different ""event type"" categories, which makes me think we could represent each event-type as an indicator variable: 0 if the event did not happen on a given day, 1 if the event happened. 

* it then follows that we'd represent a calendar day as a vector of  length-N, where N is the number of event-type categories in you're data.

* now we need some kind of predictor variables. it's not elegant, but a really simple thing you could do would be to just take the vectors from the last K days and flatten them into a 1xK*N vector and call that your input features. Concretely, you could interpret this as predicting what events will happen tomorrow based on what events happened when during the past week (or whatever). You probably have richer features you could use at your disposal.

* The output of your model is now going to be a vector of likelihoods, one for each event type. that means that a given output doesn't need to sum to 1 and could even potentially sum to N. This is a hint for how you will probably want to construct your loss, which I'll leave as an exercise.

* You can now plug your estimated likelihood vector into that entropy formula to estimate the entropy of any particular day.

... or you could just calculate the relative frequency of a particular event type happening on any particular day and use that.",2,xgpkei,"I need to measure the entropy within a system. If anyone doesn’t know, entropy is the amount of disorder or unpredictability within a system. I would like to calculate this with the true definition of entropy which is the sum over all data points where you multiply the probability of the data point occurring by the log of that probability.  -Sum{ p(x)*log(p(x)) }

My problem has to do with calendar event data. I have a whole bunch of calendar data which is composed of calendar events. Each event contains the event’s type and the start/end time of the event. To measure the entropy correctly I would need to know a way to estimate the probability of each event or the probability of each day based on its event. These probabilities would be based on previous events that the algorithm has already seen. 

I’m wondering if anyone knows of a way to calculate the probability of these calendar events or alternatively if anyone knows of a less rigorous way to estimate the disorder/predictability within any given week or day based on the events that have made up previous days/weeks.  

Thank you for any help!",MLQuestions,2022-09-17 08:33:03,1
"When I've run into tough to debug numerical issues, I either trace through with a debugger or print out the variables and trace it all by hand.

Also, I'm not familiar enough with Rust to read the code but I'd suggest a very simple function like the sum of the inputs. Something that you know for sure the NN can learn perfectly. It also makes the math easier when you're manually checking the values.",3,xgpvhe,"[https://github.com/ph04/nn\_exp/blob/main/src/main.rs](https://github.com/ph04/nn_exp/blob/main/src/main.rs)

I wrote this program but the loss acts very weird: sometimes, it doesn't go down at all, sometimes it goes up a bit, then it starts to go down, and sometimes it just goes up. I have no clue what the problem is, this is my first time trying to implement backpropagation (the first time i did it with a simple random search, if you are interested in that check out the [article](https://medium.com/codex/but-what-is-artificial-intelligence-exactly-c1acbe3c3cd1)). This is SO hard to debug and try to understand where the problem is. I based the algorithm from [this article](http://neuralnetworksanddeeplearning.com/chap2.html#the\_backpropagation\_algorithm) (which is truly amazing by the way). Thanks for the time and help",MLQuestions,2022-09-17 08:45:31,11
"Are you interested in researching about computer vision? 

If you are extraordinarily smart and can just get a PhD, then sure? If not might want to go into something you are inherently interested in.",1,xgec77,"Hey guys , I'm an undergraduate student CSE student very much interested in pursuing a PhD in computer vision. As Im a newbie, i lack the advance knowledge about the topic though I have done courses related to it like Digital image processing, deep learning. So I thought doing a master's will help me improve my skillset and prepare me for my PhD. One has suggested to me that doing more research work getting citations is the key. But for that the knowledge shd be on a whole other level and I'm not that experienced as I did only courses.  Could you please recommend some steps to go deeper(to get the confidence that I can do a PhD in cv)  into cv from a basic level.

Thankyou",MLQuestions,2022-09-16 22:55:12,2
"so, one semi-supervised approach (there are many) is something called 'pseudo-labeling', which might be applicable here.  With pseudo-labeling, basically you train on your labeled data, and when you have a bit more confidence in it, you start to use the labels generated by your unlabeled data, and you add them into the training set.

It ... sort of works.. it's not _amazing_, but it can sometimes lead to improvements.  The general gist of it is that your now newly-labeled data has ""noisy labels"", and the noise in these labels gets kind of smoothed out.  In practice obviously this can also introduce run-away biases, so it's not a guaranteed win, but it's something relatively easy to try.

Beyond that you have to start looking more generally at semi-supervised learning, but it's kind of too wide a topic to get into in detail.  Especially with text I am not sure what the best approach is, but for example with images you can often combine some supervised learning with an autoencoder that you train on your unlabeled data.  But yeah, for text, I suggest you start doing a bit of a literature survey on what approaches are possible.  Maybe someone more knowledgeable in NLP can offer some more specific advice here.",3,xftwud,"So I am working on a binary classifier for some twitter data and I've used a few supervised ML algorithms that were trained on manually labeled dataset and managed to achieve a fairly decent precision for the existing problem. I have only really done machine learning at university, and usually all assignments just kind of ended there.

This time I want to go further and as a goal I want this classifier to be able to predict the label for any new incoming unlabelled tweet. I started reading about it and the internet recommends to use semi-supervised algorithms to use the unlabelled data during the training.

What I really don't understand is can supervised methods be used on unlabelled data at all? Would I be able to use this classifier after it was trained to load a random unlabelled tweet/put it though a set of other tweets and see what label is predicted? And if not what is the point of building those classifiers?

And as another question, would the semi-supervised algorithm be good of the job? Or is there other methods I should use?

Any reading that would help me to get a grasp of it is welcome too.

Thank you from a very confused graduate who's is not ready for the real world.",MLQuestions,2022-09-16 08:10:04,4
"This is called multi-task learning, where you want to train a single model to do multiple tasks (in your case, two). When using neural networks, the last layer will actually be composed of two different layers, each solving a different task.",4,xfl04d,"I would like to build a CNN, it will be required to output 2 sets of output. 
For example:

Given an image of a person, predict a.) the posture of the person (e.g. sitting, standing etc.) and b.) the gender of the person

Therefore the output will be a N-length vector with the first N - 1 elements belong to the binary encoding of the postures, the last element belong to the gender. 

Is it possible to combine these 2 tasks into one model? Can I apply softmax to only the first N - 1 elements and sigmoid function to the last element?",MLQuestions,2022-09-16 00:49:00,5
Check ur DM,1,xfmgti,"I have an xgboost regression model that predicts a price of a vehicle. I would like to evaluate how confident the model is by its prediction e.g. price predicted for a given vehicle is $25000 with 87% confidence level.

Did some research and apparently there are number of ways and regression seems to be quite hard and no straightforward answer. Came across MAPIE ([https://github.com/scikit-learn-contrib/MAPIE](https://github.com/scikit-learn-contrib/MAPIE)) which predicts a upper and lower range of values.. how can I then calculate the confidence for a given prediction? 

Are there any other valuable techniques out there I should give it a look?",MLQuestions,2022-09-16 02:21:13,1
Are all the images in the dataset aligned?,1,xffbnr,"here 1 means happy and 0 means sad

&#x200B;

https://preview.redd.it/nz7bprmhv4o91.png?width=1233&format=png&auto=webp&s=6adf2cfe6fd2ea5e6d4acf98bc1a80f4f5cc758c",MLQuestions,2022-09-15 19:40:35,5
"Certifications in general aren't very useful for getting jobs. I've never heard of a situation where the certification someone has made the difference of getting/not getting an interview or an offer. 

Certifications can be useful for beefing up/rounding out the 'technical skills' section on your resume, but that basically just gets you past an ATS. Recruiters pay attention to your work experience first, education second, everything else third. 

That being said, I'd recommend taking a look at certs for AWS or other cloud providers. The knowledge itself is helpful, since you'll likely be working for a company that uses the cloud (e.g. pulling data from S3 or RDS, hosting model endpoints, etc), and it may also help you communicate to recruiters/hiring managers that you have some engineering chops (which can be a weak point for some entering the field from academia with backgrounds heavy on science but light on coding).",3,xf4s10,"I have some machine learning projects on my resume but I'm worried that since my entire professional experience (2 internships) have been in automation testing no company will interview me even to see my skills let alone hire me because of that.

Are there any recommended machine learning certifications/courses that I could possibly use to push myself forward into the career I'd like? I know Google has a TensorFlow certification but I've heard that it's not very useful in getting jobs since it doesn't show mastery in the field.

Any help is appreciated!",MLQuestions,2022-09-15 11:57:43,2
i think the skorch library assumes the model object is an sklearn estimator.,1,xf5aw0,"I am trying to run a Resnet model through Skorch for classification which I found in a research paper. I am still learning the ways of Torch and Skorch, and I'm unable to find what to fix to get this to work.

ResNet class:

    class ResNet(nn.Module):
        def __init__(
                self,
                *,
                d_numerical: int,
                categories: ty.Optional[ty.List[int]],
                d_embedding: int,
                d: int,
                d_hidden_factor: float,
                n_layers: int,
                activation: str,
                normalization: str,
                hidden_dropout: float,
                residual_dropout: float,
                d_out: int,
                regression: bool,
                categorical_indicator
        ) -> None:
            super().__init__()
            #categories = None #TODO
            def make_normalization():
                return {'batchnorm': nn.BatchNorm1d, 'layernorm': nn.LayerNorm}[
                    normalization[0]
                ](d)
            self.categorical_indicator = categorical_indicator #Added
            self.regression = regression
            self.main_activation = deep.get_activation_fn(activation)
            self.last_activation = deep.get_nonglu_activation_fn(activation)
            self.residual_dropout = residual_dropout
            self.hidden_dropout = hidden_dropout
    
            d_in = d_numerical
            d_hidden = int(d * d_hidden_factor)
    
            if categories is not None:
                d_in += len(categories) * d_embedding
                category_offsets = torch.tensor([0] + categories[:-1]).cumsum(0)
                self.register_buffer('category_offsets', category_offsets)
                self.category_embeddings = nn.Embedding(int(sum(categories)), d_embedding)
                nn.init.kaiming_uniform_(self.category_embeddings.weight, a=math.sqrt(5))
                print(f'{self.category_embeddings.weight.shape}')
    
            self.first_layer = nn.Linear(d_in, d) # 1, 256
            self.layers = nn.ModuleList(
                [
                    nn.ModuleDict(
                        {
                            'norm': make_normalization(),
                            'linear0': nn.Linear(
                                d, d_hidden * (2 if activation.endswith('glu') else 1)
                            ),
                            'linear1': nn.Linear(d_hidden, d),
                        }
                    )
                    for _ in range(n_layers)
                ]
            )
            self.last_normalization = make_normalization()
            self.head = nn.Linear(d, d_out) # 256, 1
    
        def forward(self, x) -> Tensor:
            if not self.categorical_indicator is None:
                x_num = x[:, ~self.categorical_indicator].float()
                x_cat = x[:, self.categorical_indicator].long() #TODO
            else:
                x_num = x
                x_cat = None
            x = []
            if x_num is not None:
                x.append(x_num)
            if x_cat is not None:
                x.append(
                    self.category_embeddings(x_cat + self.category_offsets[None]).view(
                        x_cat.size(0), -1
                    )
                )
            x = torch.cat(x, dim=-1)
    
            x = self.first_layer(x)
            for layer in self.layers:
                layer = ty.cast(ty.Dict[str, nn.Module], layer)
                z = x
                z = layer['norm'](z)
                z = layer['linear0'](z)
                z = self.main_activation(z)
                if self.hidden_dropout:
                    z = F.dropout(z, self.hidden_dropout, self.training)
                z = layer['linear1'](z)
                if self.residual_dropout:
                    z = F.dropout(z, self.residual_dropout, self.training)
                x = x + z
            x = self.last_normalization(x)
            x = self.last_activation(x)
            x = self.head(x)
            if not self.regression:
                x = x.squeeze(-1)
            return x
    
        class InputShapeSetterResnet(skorch.callbacks.Callback):
            def __init__(self, regression=False, batch_size=None,
                         categorical_indicator=None):
                self.categorical_indicator = categorical_indicator
                self.regression = regression
                self.batch_size = batch_size
            def on_train_begin(self, net, X, y):
                print(""categorical_indicator"", self.categorical_indicator)
                if self.categorical_indicator is None:
                    d_numerical = X.shape[1]
                    categories = None
                else:
                    d_numerical = X.shape[1] - sum(self.categorical_indicator)
                    # categories = list((X[:, self.categorical_indicator].max(0) + 1).astype(int))
                    categories = [sum(self.categorical_indicator)]
                net.set_params(module__d_numerical=d_numerical,
                module__categories=categories, #FIXME #lib.get_categories(X_cat),
                module__d_out=2 if self.regression == False else 1) #FIXME#D.info['n_classes'] if D.is_multiclass else 1,
                print(""Numerical features: {}"".format(d_numerical))
                print(""Categories {}"".format(categories))

&#x200B;

Skorch Wrapper:

    def create_resnet_skorch(id, wandb_run=None, use_checkpoints=True,
                             categorical_indicator=None, **kwargs):
        print(kwargs)
        if ""verbose"" not in kwargs:
            verbose = 0
        else:
            verbose = kwargs.pop(""verbose"")
        if ""lr_scheduler"" not in kwargs:
            lr_scheduler = False
        else:
            lr_scheduler = kwargs.pop(""lr_scheduler"")
        if ""es_patience"" not in kwargs.keys():
            es_patience = 40
        else:
            es_patience = kwargs.pop('es_patience')
        if ""lr_patience"" not in kwargs.keys():
            lr_patience = 30
        else:
            lr_patience = kwargs.pop('lr_patience')
        optimizer = kwargs.pop('optimizer')
        if optimizer == ""adam"":
            optimizer = Adam
        elif optimizer == ""adamw"":
            optimizer = AdamW
        elif optimizer == ""sgd"":
            optimizer = SGD
        device = kwargs.pop('device')
        if device == ""cuda"": # ! only for CPU training, is cuda by default
            device = ""cpu""
        batch_size = kwargs.pop('batch_size')
        callbacks = [InputShapeSetterResnet(categorical_indicator=categorical_indicator),
                     EarlyStopping(monitor=""valid_loss"",
                                   patience=es_patience)] 
        callbacks.append(EpochScoring(scoring='accuracy', name='train_accuracy', on_train=True))
    
        if lr_scheduler:
            callbacks.append(LRScheduler(policy=ReduceLROnPlateau, patience=lr_patience, min_lr=2e-5,
                                         factor=0.2))  # FIXME make customizable
        if use_checkpoints:
            callbacks.append(Checkpoint(dirname=""skorch_cp"", f_params=r""params_{}.pt"".format(id), f_optimizer=None,
                                        f_criterion=None))
        if not wandb_run is None:
            callbacks.append(WandbLogger(wandb_run, save_model=False))
            callbacks.append(LearningRateLogger())
    
        if not categorical_indicator is None:
            categorical_indicator = torch.BoolTensor(categorical_indicator)
    
        mlp_skorch = NeuralNetClassifier(
            ResNet,
            # Shuffle training data on each epoch
            criterion=torch.nn.CrossEntropyLoss,
            optimizer=optimizer,
            batch_size=max(batch_size, 1),  # if batch size is float, it will be reset during fit
            iterator_train__shuffle=True,
            module__d_numerical=1,  # will be change when fitted
            module__categories=None,  # will be change when fitted
            module__d_out=1,  # idem
            module__regression=False,
            module__categorical_indicator=categorical_indicator,
            verbose=verbose,
            callbacks=callbacks,
            **kwargs
        )
    
        return mlp_skorch

Skorch Model:

    <class 'skorch.classifier.NeuralNetClassifier'>[uninitialized](
      module=<class 'tabular.bin.resnet.ResNet'>,
      module__activation=reglu,
      module__categorical_indicator=tensor([False,  True, False, False, False, False, False, False]),
      module__categories=None,
      module__d=256,
      module__d_embedding=128,
      module__d_hidden_factor=2,
      module__d_numerical=1,
      module__d_out=1,
      module__hidden_dropout=0.2,
      module__n_layers=8,
      module__normalization=['batchnorm'],
      module__regression=False,
      module__residual_dropout=0.2,
    )

&#x200B;

I have 8 columns in X for training, 1 of which is a categorical column which is to be embedding through an embedding layer in the NN. From what I've found so far, that is the root of this error since it's coming across this categorical class in execution. But in the forward method, it's supposed to have an embedding layer for the same. Any idea what changes I might need to make for the same?

Error stack:

    Traceback (most recent call last):
          File ""/test.py"", line 639, in <module>
            model.fit(X_train, y_train)
          File ""/anaconda3/lib/python3.9/site-packages/skorch/classifier.py"", line 142, in fit
            return super(NeuralNetClassifier, self).fit(X, y, **fit_params)
          File ""/anaconda3/lib/python3.9/site-packages/skorch/net.py"", line 917, in fit
            self.partial_fit(X, y, **fit_params)
          File ""/anaconda3/lib/python3.9/site-packages/skorch/net.py"", line 876, in partial_fit
            self.fit_loop(X, y, **fit_params)
          File ""/anaconda3/lib/python3.9/site-packages/skorch/net.py"", line 789, in fit_loop
            self.run_single_epoch(dataset_train, training=True, prefix=""train"",
          File ""/anaconda3/lib/python3.9/site-packages/skorch/net.py"", line 822, in run_single_epoch
            for data in self.get_iterator(dataset, training=training):
          File ""/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py"", line 521, in __next__
            data = self._next_data()
          File ""/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py"", line 561, in _next_data
            data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
          File ""/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py"", line 52, in fetch
            return self.collate_fn(data)
          File ""/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py"", line 84, in default_collate
            return [default_collate(samples) for samples in transposed]
          File ""/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py"", line 84, in <listcomp>
            return [default_collate(samples) for samples in transposed]
          File ""/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py"", line 74, in default_collate
            return {key: default_collate([d[key] for d in batch]) for key in elem}
          File ""/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py"", line 74, in <dictcomp>
            return {key: default_collate([d[key] for d in batch]) for key in elem}
          File ""/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py"", line 86, in default_collate
            raise TypeError(default_collate_err_msg_format.format(elem_type))
        TypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'pandas.core.arrays.categorical.Categorical'>",MLQuestions,2022-09-15 12:18:10,7
"it might help if you described specifically what kinds of skills you want to learn or what kind of job you are trying to apply for. The term ""data scientist/analyst"" can mean different things to different people/companies. Might also help if you describe your background a little.",1,xf38d1,"What are the best data science/ data analyst  (free if possible) courses so i can catch up to today ?

&#x200B;

Thank you!",MLQuestions,2022-09-15 10:54:31,1
"First thing that comes to mind is on the security side with anomaly detection for things like intrusion, ddos, etc.",2,xewv5x," A college student here, looking to make a minor project (of 2 months). My goal is to apply ML to the field of Networking. Are there any viable applications of ML in this field? Relatively fresh to the field of ML and Networking but since it's a college project i'm willing to invest some time in learning the essentials and studying around the concepts to get the project going.",MLQuestions,2022-09-15 06:28:18,4
"The answer to this is always domain-specific. If I trained (let's say with millions of examples), a classifier to determine whether or not a photo of a human face is smiling. I could reasonably expect that the classifier will work a year from now, as A) the definition of a smile is unlikely to change and B) the features that predict a smile are very likely to remain the same- i.e., are likely to be drawn from the same distribution that the model was originally trained on. A stretched mouth and crinkles around the eyes are still going to be good predictors.

Now lets imagine I train a fake news classifier on past data. Fake news is topical, so my classifier is likely to do very poorly a year from now due to a data shift. If I trained a classifier last year, it would predict ""the queen of England is dead"" to be fake news. A year ago that was a correct classification, today it's not.

I don't know anything about your domain, so this is for you to answer. If there's reason to expect your features will be drawn from a different distribution (e.g., all trees are taller next year or a fire reduces complexity), your model will probably have issues. If there's reason to believe that the underlying distributions of your collected data will be the same a year from now, then your model will still probably give decent results.",9,xe3aw3,"I have created a random forest model that predicts plant biomass from lidar canopy structure metrics (ie height, density, complexity, etc).

This year, I harvested plant biomass plots to be used as training/validation dataset. I am wondering if next year, I can just collect lidar data and use the random forest model I create this year to predict biomass?

Fairly new to machine learning stuff so if I am misunderstanding how it works please advise!",MLQuestions,2022-09-14 07:23:11,9
Everyone do it. There is nothing wrong with using pretrained weights.,9,xe66ok,"If I'm building a CNN of my own and in the early stages I build a model based on VGG16, save it and then fine tune it on more of my own data, how is that viewed in general?

 Is it just a sensible way to get from a to b or is it frowned upon and we should build all of the layers ourselves etc?

I'm asking myself, why wouldn't everyone just use one of these pretrained models every time as a starting point?",MLQuestions,2022-09-14 09:20:04,8
"1. Learn the fundamentals very well. Don't jump straight to DNNs or ResNets etc.
2. Most machine learning is just statistics, make sure you understand multivariable calculus, probability, and statistics very well
3. Become adept at data handling / wrangling. This is 60% of the work in an industry setting.
4. Write algorithms from scratch to make sure you understand how they work. [This course on youtube](https://www.youtube.com/watch?v=p1hGz0w_OCo) is a great start for some beginner-friendly algorithms
5. Centralize some of your best projects in high-quality, well-structed GitHub repos. Have a clear understanding of what problem you were trying to solve, your experimentation process, your final solution, and measurable outcomes.
6. Focus on what **you** want to do. There is a difference between data engineers, data scientists, and deployment specialists. Many job postings will say they want a ""full stack data scientist"" but this is frankly infeasible for 1 person in any moderately sized organization. Become very good at one thing rather than okay at the end-to-end pipeline.
7. Quality over quantity.
8. For getting an internship, learn how to present yourself well, come prepared, ask questions. Be confident about your accomplishments and forthright about your areas of needed improvement.",4,xdxxt7,"So I am new to this community, and wish to study about Machine Learning, and also land some internships and projects while at it. Would be glad to hear about how you all started as a newbie. How did you land your first project/internship? Thank you",MLQuestions,2022-09-14 03:04:06,6
"there are actually many images of traffic signs with stickers on them in the standard traffic sign datasets like [gtsrb](https://benchmark.ini.rub.de/gtsrb_dataset.html)

what you are thinking of though can also be consider part of data augmentation, dropout, and adversarial robustness.  some topics to read up on.",3,xdwsdy,"I was thinking about pixel attacks and other such things that can throw a neural network off, has anyone trained one using images full of irrelevant information? E.g. a stop sign with random rectangles stickered on it.",MLQuestions,2022-09-14 01:53:31,2
"It’s the exploration vs exploitation problem — it has a ton of literature about it, that I’m not well enough read on to summarize. 

Exploration is taking suboptimal actions in hopes of finding some set of actions that leads to a huge reward (later on). Exploitation is taking actions that give you guaranteed short term rewards.",7,xdf0l7,"Apologies in advance for the loosy goosy question. This isn't about any sort of framework or game in particular. But imagine a reinforcement learning scenario similar to StarCraft and AlphaStar.

Let's say an AI has access to many ""cheap"" options. And so it explores that space and uses them in many combinations, etc. But it also has access to an ""expensive"" option, like a nuke or a battleship. What would drive it to wait on spending gold to stop iterating on cheap options and commit to buying something expensive?

I'm weirdly assuming it would get stuck in some sort of local optimum of playing well with only cheap units. Like how a StarCraft AI trained from zero might only build SCVs and no advanced units.

Do you have to change the fitness function to reward exploration? Like reward saving money and reward buying expensive units? How do people currently solve this problem?

In OpenAI Five, one expensive encounter with a mega unit (Roshan) was worked around by having different iterations of Roshan, that are sometimes cheap and sometimes expensive, so that the AI would learn that Roshan is an option on the table. But this kind of learning only works when the expensive scenarios are configurable, and thus you're training on a much wider space where the scenario is variable in expense, versus the one canonical scenario where it is always expensive.",MLQuestions,2022-09-13 11:40:30,4
It sounds doable but kinda hard to answer without seeing any examples.  how regular to these residues appear?  am i right that you are trying to do object detection here?,1,xdj7p6,"Hi, I'm working on a project which requires us to determine certain qualities of 2-D structures of small proteins. In particular, we need to tell if all residues with particular qualities cluster in a particular area of a helix. We have potentially thousands of 2-D images of these helices, so we wanted to see if some sort of image recognition could assist us in filtering. 

The plan is to label the residues of interest with a shape and see if the program can recognize when the shapes are clustered vs. when they aren't and filter based on that. While this seems simple enough, no one involved is experienced with these sorts of programs, and I've been overwhelmed trying to figure out which service to use/how to use them so any guidance with how to get a project like this done is appreciated.",MLQuestions,2022-09-13 14:28:42,3
"You can definitely start with a U-net.  You'll have to learn a bit of PyTorch, so start by following some tutorials.  Then, I had some success using this implementation: https://github.com/mkisantal/backboned-unet/blob/master/backboned_unet/unet.py

For your masks and classes you can use your masks and assign each one an ID according to the category.  So if your mask is:

    0 0 0 0
    0 1 0 0
    0 1 1 0
    0 0 0 0

then say it is category 3, then make the data,

    0 0 0 0
    0 3 0 0
    0 3 3 0
    0 0 0 0

This should allow you to combine all your examples into a single dataset.  Then, make the U-net have the correct number of outputs and minimize the cross-entropy loss.",3,xd4arm,"Hi All,

I am new to data science and would like to know how to build a U-Net model to segment cells. I have the image jpeg files and the mask jpeg images. Also there is a csv file which gives the classes that each cell belongs to. Such which organ it came from. 
I would like to know it is possible to build a image segmentation model using U-Net or something better that U-Net please. Also utilising the classes please.

Would anyone be able to help me in this regards please. Most of the tutorials are bit advanced in my opinion for a beginner in this field.

Thanks & Best Regards 
Schroter Michael",MLQuestions,2022-09-13 03:50:39,1
"I would do some basic signal processing to extract features of interest -- amplitude, variance, running mean (low-pass filter) etc.  You could include STFT features if you think that's appropriate. Then use these as input features to a simple classifier -- linear if you can, otherwise a very small MLP or decision tree for example, and train it on continuous labels. (ie label each timestep, or small chunks of timesteps).  Then the collection of continuous labels for the snippet can be used for a final classification by some simple aggregation.

So the ""middle layer"" here on your last example would end up with: <FLAT FLAT RAMP RAMP PULSE>, and then you can aggregate this information into your superclasses.  I strongly suspect your initial classification for tiny pieces of this signal can be just a linear mapping of the amplitude, its derivative, and variance, for example.",1,xd29oi,"Hello,

I am an electrical engineer and am experienced with signal processing and programming, but not with machine learning. I would like to ask this community for some input on a problem I am working with, 

I need to classify some simple time domain signal snippets that basically come in three to four different flavors. [This](https://imgur.com/a/pWQ1cXY) is what the signals and the few different categories would look like. The phase of the snippets may change between measurements and some of the snippets may have features that combine two or more of the categories. The task is clearly not extremely complicated and it is feasible to do it with a more manual approach - I however would like the processing speed to be quite high and I imagine that machine learning could be a good option.

Any advice on how you would approach solving this problem with ML? Thank you!",MLQuestions,2022-09-13 01:47:32,1
If they are at different places I can't see how it would be a problem.  Even if they are together it might just be kind of redundant but not necessarily harmful.  Only way to know is to try I guess.,1,xczvgd," Hi everyone,

I wondered if using LayerNorm and BatchNorm together in the same network makes sense, for instance, if you were using a ResNet to extract features from an image and used a Transformer with multi-head attention as the classification head, would it make sense to use BatchNorm for ResNet layers and LayerNorm for the transformer layers?",MLQuestions,2022-09-12 23:16:30,1
"would need to see the paper to be sure, but chances are all you need to do is popoff the last layer (probably performs classifications) and use the activations that would otherwise have been fed to that last layer (i.e. the features used to perform classifications)",2,xcjsgd,"I'm a bit new to machine learning.

I've built a basic convnet and I'm interested in extracting the feature map it produces.

Is there any way to do that? I'm not interested in visualizing feature maps as images and stuff, I'm interested in extracting the feature maps as actual numbers that comes out after training the convnet.

For reference I'm following paper that trains a 1d convnet and then extracts the feature maps and passes those extracted feature maps to an LSTM. I can link the paper if that would be helpful.

sorry if this is the wrong place to ask.

Thanks!",MLQuestions,2022-09-12 11:06:32,9
"That output is the tensor description of what's happening at that point in the model, you'd need to run an input through to actually see what the value of the tensor was. You can create another keras model with your inputs and that as the output, then run the model on some input and observe the computed values.",1,xctvox,"Hello!

&#x200B;

I have a tensor that I created from getting the output off a convolutional layer of my model.

    xtttx = model.get_layer('conv1d_181').output

I tried to use the line of code to print the output of the tensor, but I'm only getting the output information of shape and data type

    y_true = tf.keras.backend.print_tensor(xtttx, message='xtttx_true = ')
    output: <KerasTensor: shape=(None, 15, 20) dtype=float32 (created by layer 'conv1d_181')>

Is there any other way to ""see the inside of a tensor"" and print out the contents of a tensor.

I'm using tensorflow and keras and python btw.

Also sorry if this is a silly question, I'm a beginner so I'm learning the ropes!

&#x200B;

Edit: 

Is using

     layer.getweights

the same as getting the output of a model at that specific layer?",MLQuestions,2022-09-12 18:12:29,1
"- How to detect, probably YOLO variant
- How to draw square, [opencv](https://stackoverflow.com/questions/23720875/how-to-draw-a-rectangle-around-a-region-of-interest-in-python)
- How to apply for more generic dataset, probably [data augmentation](https://albumentations.ai) or synthetic data",2,xclny8,"Howdy,

&#x200B;

So, I made a (hopefully good) model in python for apples; I used a dataset from Kaggle that had a bunch of pictures of apples upclose, and trained it to recognize them with accuracy of around 85%. 

&#x200B;

My question now is how do I use that model to draw squares around apples from my picture and count them, for example a box of apples?",MLQuestions,2022-09-12 12:23:17,4
"What do you mean “how different does it have to be”? Different to do what? And what do you mean no overall pattern has emerged?

The “best” ML algorithm for a task has always been task dependent. If an NB model yields the same accuracy as a transformer model on a task, then I’d be setting money on fire by deploying transformer in production.

Consequently, what’s “hot” in ML has always been constrained by data and compute. Back in the early 2000s, SVMs (proposed in ‘93) were the hottest thing and almost every ML dissertation from that era mentions SVMs/ kernels. As data and compute became more available, NNs (proposed in ‘58) became viable. Bayesian networks were hot, then they weren’t, and now they’re kind of hot again. Models come in and out of vogue- it’s important to keep a broad toolkit. We know NNs are less expressive than human neurons, that suggests there’s still a lot we haven’t discovered.

So, no, NNs are not necessarily the best for any task: in many cases they perform worse than traditional ML, especially when dealing with sparse or tabular data. I’d say deep learning models tend to perform best on unstructured data with lots of labeled training examples, but again, what’s best depends on the task and constraints.",3,xcdvow,"By this I mean, if one were to build a new machine learning algorithm (as in a new design and not a larger scale version of an existing design) how different does this design have to be from previous designs? I have the impression that what works from one design iteration to the next tends to be different enough that no overall pattern has emerged. Is this true? What is the current state of the art? 

My further impression is that neural networks are currently the main machine learning algorithms used. Is this still true? 

Thanks!",MLQuestions,2022-09-12 07:11:55,3
"just looking at your confusion matrix, it looks like your model has trouble predicting severity 0 and 1 but has not trouble predicting severities 2,3,4. I recommend you revisit the data and better understand what these severities mean and what might make those two severities so much more difficult to predict correctly. It may be the case that you don't care if the model has trouble with those two cases, and it may be the case that you don't care if the model can predict the other severities well if it does poorly on those two classes.

> - I used 7:3 ratio for train_test_split
> - I did use gridsearch to find the best parameter

how did you perform this gridsearch? If you evaluated your hyperparameters on your test set: this is a data leak and may have contributed to your confusing performance. hyperparameter selection should be treated as part of your model fitting process. the test set is there to score the entire process, which means your hyperparameter choices cannot be informed by the test set.",1,xc685o,"Dear community ,

Recently I had worked on a project regarding multiclassification heart disease severity machine learning model (0 to 4 levels of severity) . The model I had created has a decent accuracy on k-fold , classification report(f1\_score etc.) and confusion matrix the results are at below images. But when I tried to make prediction on it's own trained data , I always get the wrong prediction every single time.

&#x200B;

NOTE:

\- I used 7:3 ratio for train\_test\_split

\- I did use gridsearch to find the best parameter for the KNN algorithm used for training the model ( K-value = 1)

\- The missing values for my dataset is filled in with imputation method.

\- The imbalanced data for the dataset is handled with oversampling method .

&#x200B;

&#x200B;

[Classification report](https://preview.redd.it/jbk4lflbodn91.png?width=560&format=png&auto=webp&s=257d741a469baf9904acfdd5241432297c54a47e)

&#x200B;

[Confusion matrix](https://preview.redd.it/gkq0o8m8odn91.png?width=707&format=png&auto=webp&s=231c3e2d26270fdfaf52dfaa513189d8ff3658c7)

&#x200B;

[Evaluation with K-cross validation](https://preview.redd.it/7gqcr4hondn91.png?width=680&format=png&auto=webp&s=6dbb8a119af1ca2b38f9cb7cb2bfa16d4805e02f)

&#x200B;

&#x200B;

[on the left is the codes for predicting new data , on the right is the .csv file that shows the value of each variables of the patient along with the severity of their heart disease at the end of the column.](https://preview.redd.it/vlkk54s1pdn91.png?width=1584&format=png&auto=webp&s=71b43f4ee07123cae287f17479d1306081b9b1ba)

In this case on the above figure , the input is  :  (61,1,3,150,243,1,0,137,1,1,2,0,3)

which corresponds : age , sex , cp , trestbps , chol , fbs , restecg , thalach , exang , oldpeak , slope , ca , thal

the output should be 0 severity but instead, the model predicted 2  I know the row of data might be on the test set which has never seen before by the model .But i tried on other columns as well and it always gives the wrong prediction no matter how i tuned my hyperparameter . Tq for reading , pls help",MLQuestions,2022-09-12 00:21:39,3
Andrej Karpathy just started a youtube series. Highly recommend it [link](https://youtu.be/VMj-3S1tku0),1,xbvo73,"Guys am a computer science student i have passion foe AI and i wanna learn it on my own , can someone please tell me what courses to take and in what order so i can be able to create advance ai programs (i dont want just an introduction i wanna get as deep and as advanced as possible), thanks",MLQuestions,2022-09-11 15:24:49,2
"Try setting a seed (also sometimes called ""random state"")  when you split the data and hyperparameter tune",1,xbhp44,"Hello so I was doing kaggle advanced regression challenge for housing prices and im encountering something strange 

Every time I run the model and do RMSE, model.score and a graph of predicted values vs actual values ( divided uses train test split ) I get different score for each of these every time I run the jupyter r notebook cell

I suspect this is because every time I run it it chooses a different set of train and test split and sometimes the scores don’t change too much but sometimes they change drastically where it’s not even a good graph 

My question is why is this happening and how do you actually get a reliable model score or evaluate your model for a task like this",MLQuestions,2022-09-11 05:48:15,2
"You could try embedding the images into a smaller size using 2D convs and then using 3D conv only in a later layer.

Also, you could compare the 3D conv with some sequential idea like LSTM for the time axis.",1,xbbnv4,"I am attempting to use EfficientNet3D for a video classification task.

[https://github.com/shijianjian/EfficientNet-PyTorch-3D](https://github.com/shijianjian/EfficientNet-PyTorch-3D)

&#x200B;

I wanted to use a 3D-CNN that allowed me to input higher resolution images. I believe my project accuracy is currently suffering because my image inputs are 112x112. So much data is lost when resizing original images to 112x112, shapes and features are almost unrecognizable even by human eye.

I think that instead inputting images of a larger resolution; ex. EfficientNet 'b7': (633, 600), I would get much higher accuracy.

However I loaded up the model and when I try summary(model, input\_size=(1, 200, 200, 200)) , the model takes \~30 seconds to display torchsummary

When I tried summary(model, input\_size=(1, 333, 633, 600))  the model took >15 minutes. (I believe this could also be a memory issue, i'm not sure)Am I doing something wrong??

Has anyone else tried 3D-CNN with larger image resolutions?

Should I ditch 3D-CNNs all together and try a different route? If I cant practically input larger image resolutions then this is a dead end.",MLQuestions,2022-09-10 23:55:45,1
"I hope somebody corrects me if I am wrong but it looks to me like the model collapses.

Can you add more information about what you are trying to do and details of the dataset, model and loss?",3,xavcvn,,MLQuestions,2022-09-10 10:45:32,7
"Here are textbook recs I’ve encountered in graduate ML courses:

Stats/ Prob: « Statistical Inference » by Casella and Berger & « All of Statistics » by Wasserman

Classical Intro textbooks: « An Introduction to Statistical Learning » & « The Elements of Statistical Learning »

More advanced textbooks that cover classic ML + some deep learning: Bishop’s « Pattern Recognition and Machine Learning» & Murphy’s « Machine Learning: a probabilistic perspective »

Deep learning only: Ian Goodfellow’s « Deep Learning »",3,xao0ys,"I’m currently in the process of applying to universities in the UK and wanted to find some good books or papers I could read and put on my personal statement at pretty much an introductory level. More specifically its application in physics, or current problems with ML/AI but if you have a personal favourite I’ll be glad to check it out :)",MLQuestions,2022-09-10 05:20:11,2
Check the values before the .reshape,2,xastfq,,MLQuestions,2022-09-10 08:57:59,11
have you tried turning it off and back on again?,9,x9zoj0,,MLQuestions,2022-09-09 09:18:19,16
"This is pretty much a classic case of overfitting. How big is your dataset? Attention is All you need model is pretty big iirc, if your dataset is small this is bound to happen",1,x9zjyt,,MLQuestions,2022-09-09 09:13:19,2
"Hi, in theory yes, but RNNs have a lot of shortcomings that's why most SOTA models nowadays are Transformer-based, there are some very noticeable differences, one is speed, RNNs are not parallel in the time doman, so longer sentences the longer to compute, also after a certain number of timesteps, RNNs start to forget, hence Atttention was added to avoid this, but then people had the clever idea to build a model only based on Attention, TLDR; Attention Is All You Need,  a One-To-Many Sequence to Sequence could solve that problem that you're describing. But you need to have a fixed number of outputs.",1,xa40ik,"Hi,

I've been doing my best to get up to speed with neural networks by reading books. A lot lately but what I find lacking is the experience of building them to brings my skills up. What I am here for and I would be very thankful is if you could guide me. Could one argue that an recurrent neural network can predict any number and it's subsequent numbers accurately? Let's say I want to predict a number in-between 1 and 10. I choose the number 1. Will the RNN be able to finish predicting more given this instruction?

So 1 will give the output of 2,3,4,5,6,7,8,9,10.

So just to make this short does a RNN ""wield"" numbers and is able to give multiple outputs based on the input given? Also what kind of accuracy is there? Thanks for reading and any answers you can give.",MLQuestions,2022-09-09 12:18:46,1
"This is an extraordinarily vague question. Yes there is always a way to “export” the sufficient statistics such that you can recreate the model. For some classes of models this would just be model weights. For other classes of models we are talking about exporting all the data points or voronoi cells encoding some decision boundaries. 

How you practically do this is highly dependent on the libraries you’re using and the type of model.",4,x9t2lt,Anyway to get the hyperparameters of a saved ML model regardless of the framework used?,MLQuestions,2022-09-09 04:32:28,6
"knowledgeisle.com
https://www.knowledgeisle.com › ...PDF
Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow

Didnt read it yet, but heard good stuff about it",4,x8z0v6,"Hi! Can anyone suggest me some website/book/other sources to learn in a proper way how to write code in python for machine learning? 

I've already completed the courses from Kaggle and they were very useful. I'm searching for something like these courses: some theory on python functions and exercises to apply what you have just read. Thanks!",MLQuestions,2022-09-08 05:18:18,6
"> Is my reasoning correct?

Yes, it is correct precisely because of your reasoning. I don't think you need a reference, it simply follows from the properties.",2,x8uegw,"I am building a classification model to predict user churn, using an xgboost model. The model is calibrated well, and the results are decent. Now one of the stake holders is interested in the expected churn rate of all users as a whole - not at individual level.

A simple way to do this is, to get a probability threshold and convert the xgboost probability scores to binary predictions, then sum up all the 1s to get the expected churn rate.

I was wondering, instead what if we sum up the probability scores? I think it makes sense because at an individual user level, the prediction is nothing but the expected value of user churning (y), conditioned on their properties (x). So outcome of each user is an independent distribution and we have multiple users - basically for the n users in the dataset, we have n i.i.d distributions. So summing up the expected values of individual users should give me the expected value of the whole group of users' churn.

Another thought exercise is, lets say there's a subset of users where the xgboost output is 0.9 for each user, and the threshold is 0.5; So in the first method, we get a 100% churn rate because we count every user as 1; But in the second method, we get a 90% churn rate, which sounds more realistic from the score.

Is my reasoning correct? Is there some reference backing this? If not, can you point out what's wrong with the approach?",MLQuestions,2022-09-08 01:06:21,1
"If you can take a small snapshot of your data, that might be fast enough for CI/CD. Ideally for data preparation it could be unit-testable which would be fast, if you're able to anticipate all the edge cases you should test.

I used to feel the same about assertions too but I've warmed up to them. The thing that changed my mind was seeing that assertions are enabled in production at NASA and they're mentioned in NASA coding style guides.",1,x8mno2,"I’m building a python package that will be primarily used to run a full ML pipeline. That is to say, data preprocessing, training, predicting, etc.. Im familiar with typical ways to test in python (unit tests and what not), but i’m struggling to think of efficient ways to test outputs of function and frameworks that require a lot of time to run, due to huge dataset transformations/training on a large dataset. The two high level approaches that i’m currently thinking about:
- Testing like I would a regular software, using a CI/CD tool that runs tests upon pull requests. Not sure how this would work for testing outputs of dataset transformations/training models that may take many steps and hours to run.
- Testing within the software using something like assert statements (seems not kosher to do it this way).

What is a best practice for testing ML pipelines?",MLQuestions,2022-09-07 18:27:29,2
"Since it sounds like you have shell access, I would use rsync, which will easily allow you to resume if interrupted.  Make sure you enable compression, which will speed it up (although how much depends on the type of data... Lots for text, less for images, for example).",3,x8grc1,"Hello.

I have a very large image dataset (10 GB) which I need to upload to some server in order to perform parallelized ML on it using the server's GPUs. However, uploading 10GB takes a long time. Is there any way to shorten that time? I couldn't compress it using the windows compressing tool, is there a way to do it? Or any other method? Thanks in advance!",MLQuestions,2022-09-07 14:15:27,14
"Those are two different things, look up online how they work you’ll see that KNN and Log reg are usually used to solve different problems",3,x8c8yl,title,MLQuestions,2022-09-07 11:15:24,4
You can try hierarchical linear modelling...,1,x8hdxq,"&#x200B;

https://preview.redd.it/23nk0itoaim91.png?width=1368&format=png&auto=webp&s=65fbf7c7ebf35559f39dda762498c2206d9892c4",MLQuestions,2022-09-07 14:40:40,5
"Do you have labels? What website attributes do you have? If you have labels already: try them all (and more: NNs, random forest, boosted decision trees, regression, etc)! You can report a table showing the accuracy of each.",1,x8ffo2,"I am student in Computer system security. I have been assigned to do a Final Year Project using Machine Learning which I don't even learn or touch at all.

My project is to do a machine learning algorithm that can differentiate which URL is malicous and which is not. I have dataset of it.

So my question is which algorithm should I focus or use for my machine learning (KNN, decision tree, naive bayes)? Any suggestions or any improvement? Sorry I'm really new to this machine learning world. 

Thank you in advance

Edit: Link for dataset in gdrive 
Edit:Link has been taken down because of detected phishing url. So if anyone need the dataset can tell me",MLQuestions,2022-09-07 13:22:22,18
"This paper looks relevant:

[Reliable training and estimation of variance networks](https://proceedings.neurips.cc/paper/2019/file/07211688a0869d995947a8fb11b215d6-Paper.pdf)

>Abstract  
>  
>We propose and investigate new complementary methodologies for estimating  
>  
>predictive variance networks in regression neural networks. We derive a locally  
>  
>aware mini-batching scheme that results in sparse robust gradients, and we show  
>  
>how to make unbiased weight updates to a variance network. Further, we formulate  
>  
>a heuristic for robustly fitting both the mean and variance networks post hoc. Finally,  
>  
>we take inspiration from posterior Gaussian processes and propose a network  
>  
>architecture with similar extrapolation properties to Gaussian processes. The  
>  
>proposed methodologies are complementary, and improve upon baseline methods  
>  
>individually. Experimentally, we investigate the impact of predictive uncertainty on  
>  
>multiple datasets and tasks ranging from regression, active learning and generative  
>  
>modeling. Experiments consistently show significant improvements in predictive  
>  
>uncertainty estimation over state-of-the-art methods across tasks and datasets.",1,x88b6q,"There has been much research on using machine learning to predict the conditional mean of continuous data, given a tabular data set, by minimizing the RMSE. What if both the mean and the variance of the dependent variable depend on the inputs? One application is financial returns data, where it is known that the conditional standard deviation (volatility) changes a lot over time.",MLQuestions,2022-09-07 08:38:24,1
[deleted],1,x8al6p,"I'm building a CNN with the goal of classifying if a cross-sectional view of a piece of metal shows any defects or not. The challenge is that the defects aren't segmented out in each image. Instead, the whole image was sectioned into a grid of sub-images (1 5000x5000 -> 100 50x50), then each sub-image was labeled as defected depending on how much defect was visible. Samples from the two classes are shown below. With my limited knowledge in the subject matter, it seems that the shininess of the metal may somewhat distinguish the defects. 

I trained on ResNet34 with pretained weights and achieved an F-score of 0.68 after 25 epochs.

I'm wondering if there are any data augmentation or hyperparameter adjustments that could be made to boost the model's performance in this context. Or maybe I've hit the max performance I could expect from a poorly defined problem. In any case, your insight is welcome!

[Defected](https://preview.redd.it/ndkis1ueugm91.png?width=626&format=png&auto=webp&s=dcd6fb7fb489166d101fbefb60743cce41407e6f)

&#x200B;

[Good](https://preview.redd.it/clokj03uugm91.png?width=743&format=png&auto=webp&s=a234807d07d5a42c25be596bd4c880e6f5963087)",MLQuestions,2022-09-07 10:08:55,2
What healthcare data do you have available?,1,x7z691,"Hey, I want to do a final year project for my undergraduate. I am not a machine learning expert but currently i am learning it. I want to do a project in the healthcare sector.
So kindly suggest some project ideas related to machine learning in healthcare.
Thanks",MLQuestions,2022-09-07 00:58:46,6
uh... i think this is maybe more of a google customer service question.,1,x7u5i5,"So I've had an Edge TPU for almost 2 years, just sitting in it's static bag. And I've learned a lot about neural networks in preparation of using it, and now I want to build a voice/virtual assistant. Imagining the scope of things I might ask it to do in my vehicle, from interacting with lights, modes, etc. to controlling media or calculating fuel consumption- whatever I might want to know. How large should I start my node structure? 6 tiers of 40?",MLQuestions,2022-09-06 20:16:16,8
"The nice thing about HR from ECG is you don't need to use ML. Just divide sample rate by peak to peak interval then multiply by 60. But, if your goal is less about finding HR and more about using ML, I'd suggest using a 1d convolutional NN. Don't worry about features at all, just feed in the raw signal windows like you suggested and let the model do the work. I have tried this and it works well.

Using a classical model and manual feature engineering for this task is kind of silly, because the features you will need to extract will likely be enough to simply calculate HR directly. For example peak timings, max frequency etc.",1,x7tmdi," I am having trouble getting started with a particular problem. I am looking to apply AI/ML to a signal processing problem by identifying heart rate from either ECG or PPG raw signals. I expect the data to be a bit noisy, especially in the case of the PPG. I am a bit confused as to where to start. Specifically, what do I use for features? I am used to the type of introductory AI/ML problems you would see on sites like kaggle, where you are given several features, but it seems here you only have one (the signal, and perhaps the time too). I assume I would use some sort of window, using previous samples as other features, but I'm having trouble getting information on this, since I'm too much of a noob to even know the right keywords to search. Any help or resources for a beginner would be much appreciated!",MLQuestions,2022-09-06 19:50:44,11
"I'm assuming this is spearman correlation, or something similar, which means 1.0 is a 100% correlation, -1.0 100% negative correlation (also works!), and 0.0 is complete lack of correlation. Your figure definitely does show correlation. for example, Size and Rooms are positively correlated with a strength of 0.7038. 

Price isn't strongly correlated with the other columns, but there is SOME correlation. Also, this type of correlation tells you whether it is easy to get a simple model to fit, but low correlation does not mean it is impossible - the relationship between the variables could be non-linear, which means spearman might not pick up on it, but a neural network might.

If I were you I would test some different models for this task, and assess your fit using a few metrics, like R squared (coefficient of determination). try some simple models like support vector machines, maybe xgboost, and you could also try a small neural net. low correlation isn't the end of the world.


EDIT: looking at the notebook, I don't see any normalization/scaling. this might affect both correlation and how good you can fit a model to it. I advise you to look at [sklearn's preprocessing](https://scikit-learn.org/stable/modules/preprocessing.html), use something like a StandardScaler, then compute correlations and fit models.",5,x751ds,"Hello everyone!

I'm developing a machine learning model to predict house prices for a school project. I started by web scraping the dataset from a real estate portal and after pre-processing the data, there was about 17500 entries, each with 6 features.

Afterwards, I checked the correlation and I got little surprised, because I saw none:

&#x200B;

[Figure 1. Correlation](https://preview.redd.it/or2dhrkk67m91.png?width=890&format=png&auto=webp&s=2daddb5da9227456fc99c6f0948dc65a478158f2)

Does this mean that using this dataset that it will not be possible to create a reasonable model or am I just doing something wrong?

[Google Colab Notebook](https://colab.research.google.com/drive/1C45iPw_htH1Rc-W3qHp7vy3kqBVWthGJ?usp=sharing)

Thanks!

&#x200B;

# Edit:

First and foremost, I want to thank everybody that tried to help!

**Problem**: It turns out that the real estate website from where the data was scraped, has some property ads showing the price for buying the property and the price for renting the property, *565.500 € / 1650 €*. So, when filtering out all non-digit characters, I ended up with an astronomical high value,  *5650001650*.

&#x200B;

[Figure 2. Problem](https://preview.redd.it/esufx82j4fm91.png?width=485&format=png&auto=webp&s=433cfd49261eaa8021d9052c7d9c5e6a42ba34e7)

**Solution**: Because they were not so many entries with this peculiarity, I simply dropped them from the dataset.

    greater = df[df['Price after filtering'] > df['Price after filtering'].mean()]
    df.drop(greater.index, axis=0, inplace=True)

Now, it's possible to see some correlation 🥳:

[Figure 3. Correlation after applying the solution](https://preview.redd.it/9muqtku75fm91.png?width=462&format=png&auto=webp&s=b05eb3fabbc7bb23ff6a1c16e379240a2afb4794)",MLQuestions,2022-09-06 01:25:29,5
"WebMD if you want to scrape data. Don't think it will be helpful because you're not going to get predictive results, just ""These (generic) symptoms are symptoms of cancer."" which is what WebMD already does.",1,x7esih," 

Hello All,

I have a final project on ""Disease Prediction"". The idea is to create a website where user will fill in their symptoms and at backend, using ML, we'll predict a disease for the user. The problem is I am unable to find a good dataset on diseases and its symptoms on Kaggle.

Any leads where I can find a good dataset? Or if I can scrap it so any recommended website that would give authenticated info?

I am targeting the diseases like flu, cough, cold, viral infection and covid.

Also, a detailed guide on how to approach this problem would be wonderful. Form data cleaning to choosing ML algorithm, any input would be much appreciated.

Please please pour in suggestions as I have a meeting with my supervisor tomorrow.

Thank you so much!",MLQuestions,2022-09-06 09:22:11,4
https://www.amazon.com/Production-Ready-Applied-Deep-Learning-TensorFlow/dp/180324366X,0,x7j5ya,,MLQuestions,2022-09-06 12:17:59,1
you should consider collaborating with these folks: https://enablingthefuture.org/,2,x6kjj9,"Hi,

So I'm quite new to machine learning and am starting my PhD in a few days. Essentially, the project works on automating the process of creating sockets for upper-limb amputees. 

Background: Currently, the way this socket fitting is conducted is that a prosthetist checks the stump (residual limb) of an amputee and creates a cast, which is then fit onto the client. He then fiddles with it and checks if the client is comfortable, etc. before refining the cast, which is then used to create the socket. 

Goal: To automate this process. So the route we're taking is to obtain raw data on the amputated limb (point cloud data) and the sockets created by these experts (basically a training set). Now, we want to compare how these two things match up and basically create an algorithm that can ascertain the parameters that are important in this process. This can help kickstart the automation process for the socket, which would really benefit poorer countries, where skilled prosthetists are sorely lacking. 

Method: Currently, we are looking at PCA being the basis for the algorithm. We believe that this may be the best route forward. The issue is that none of us are particularly amazing with ML and want some help. No one has really tried this before and we're a bit lost as to where to begin. Any input/guidance/resources anyone has would be really helpful. All input is good input!

&#x200B;

Thanks; I'll update the post with any additional information anyone requests. I don't think the post breaks rule 4, but if it does please let me know. I'll post this to MLquestions, just in case. Thanks in advance for any help!",MLQuestions,2022-09-05 09:31:59,4
Classification Algorithm rather than regression,2,x6jlpf,"I am working on ranking different social influencers based on a set of metrics.

&#x200B;

Metrics collected:

* username
* categories (the niche the influencer is in)
* influencer\_type
* followers
* follower grow, follower\_growth\_rate
* highlightReelCount, igtvVideoCount, postsCount
* avg\_likes, avg\_comments
* likes\_comments\_ratio (comments per 100 likes, use as in authentic indicator)
* engagement\_rate
* authentic\_engagement (the number of likes and comments that come from real people）
* post\_per\_week
* 1/post\_like, 1/post\_comment (total 12 latest posts)
* 1/igtv\_likes, 1/igtv\_comment (total 12 latest igtvs)

&#x200B;

Here's how the data looks like:

https://preview.redd.it/sbdtmq69t5m91.png?width=2771&format=png&auto=webp&s=c18204c01f551974463636303c898d561b346346

Objective: Rank the social influencers according to their influential power with the use of the metrics collected above.

&#x200B;

There are a few ranking algorithms to choose from, which are:

a) Compute the score for influential power with [Multi-Criteria Decision Making (MCDM)](https://towardsdatascience.com/ranking-algorithms-know-your-multi-criteria-decision-solving-techniques-20949198f23e) and rank it with regression

b) Create classification model and rank them through probability

c) Compute the score for influential power with [Multi-Criteria Decision Making (MCDM)](https://towardsdatascience.com/ranking-algorithms-know-your-multi-criteria-decision-solving-techniques-20949198f23e) and rank it with machine learning model like SVM, Decision Tree and Deep Neural Network

d) Learning to rank algorithm like [CatBoost](https://catboost.ai/en/docs/concepts/python-quickstart) 

e) [Trending algorithm](https://www.evanmiller.org/deriving-the-reddit-formula.html)

I would like to ask **which algorithm from the above will be more suitable in this project and could you compare and provide the reasons for it**? Any ideas will be much appreciated!",MLQuestions,2022-09-05 08:55:43,3
"1. What loss function do you use?
2. How do you train the model?
3. How long have you train the model to get that output? And how's the progression of the loss (both networks)",3,x66tpa,"&#x200B;

I swear I've tried several dozen different architectures and types of GANs but I seem to keep running into the same type of failure - where all the outputs devolve into into black and white blobs without much of a gradient in between. I've tried with datasets like CelebA.

Anybody know what type of failure this is? For reference, I'm using RMSProp with a learning rate of 0.00005 I'm not used to working with GANs so this is all quite confusing.

[25 example outputs](https://preview.redd.it/9bu732imazl91.png?width=640&format=png&auto=webp&s=41fa85332b9bc25302596131aaa12b1ff47f8798)

Losses for each type I've tried: Seems that in the conventional GANs, the losses approach zero quickly - even with label smoothing.

https://preview.redd.it/buipkwm472m91.png?width=2560&format=png&auto=webp&s=0105a79d37a62e9e2b0596905f679ecee56fb7c4

My Gen/Critic codes respectively are:

&#x200B;

[Generator](https://preview.redd.it/zyacfeokyyl91.png?width=880&format=png&auto=webp&s=f6e36647f6170f295c21ff39b321fae99e04cf31)

[Critic](https://preview.redd.it/rddohjoqyyl91.png?width=897&format=png&auto=webp&s=f266834038600ba727536731e17cf10b629bd8bc)",MLQuestions,2022-09-04 21:25:27,3
Confusion Matrix,2,x6demg,While measuring the accuracy which one is best suited for the task confusion matrix or r2\_score ?,MLQuestions,2022-09-05 04:02:21,3
"To really generalize, a set of input points (a batch) from the training data, is solved with linear algebra. So fitting is really just finding the coefficients to lines.",3,x60uff,"I am trying to learn machine learning.

I have one hang up that I cannot find the answer to: How does a model fit inputs from a training data set?

For example, I am working with the MNIST training set. When I fit the data using a particular model, how does it do it?

All I am finding are articles with the syntax for code to do this. I am trying to understand what is happening one layer deeper? Does this make sense?

I am not even sure what keywords should I I use to search for this on goggle. 

A lot of example use CNN…how about when data is trained against a binary classifier.",MLQuestions,2022-09-04 16:23:48,19
Please add what you have already tried or what you have already searched for.,1,x5nf1l,"Hello, I am currently in cyber security, am currently learning more about ml. This has sparked me to start learning more about ml and implement it into my skills. What are some tools such as videos, courses, and projects anyone in the field recommends? I would highly appreciate it and look forward to learn more. 
Thank you",MLQuestions,2022-09-04 06:48:23,6
"> why my lecturer give me these references?

have you asked them?",1,x5cjze,"Hello, everyone. I have a question regarding to the tittle. So i have a thesis about Automatic tagging topic and i have to make the project about it. So my lecturer give 2  journal references, they are about automatic tagging and tag recommendation system. And i just confused about them. are they have same goal to make auto-tagging for the object ? why my lecturer give me these references? does tag recommendation system have to put into the automatic tagging ? Anyway i am still in the making of methodology step. Thank you.",MLQuestions,2022-09-03 20:18:01,2
"Loss fn is rmse plus the regularisation part, you minimize the whole loss fn.

Rmse loss = np.sqrt(((predicted-actual)**2).mean()) + regularisation term",1,x5iics,"Hi, I would like to know how do I calculate RMSE of a function when I add a regularization part. For example if I want to predict ratings, error is defined as difference between actual an oredicted ratings. Then I add regularization term as I want to prevent my model from overfitting. How do I then calculate RMSE. Just as difference between actual ratings or do I somehow need to add regularization part. If so, how?",MLQuestions,2022-09-04 02:11:52,1
my first guess is you are training on too small of a dataset. could you tell us more about your data and the problem you are trying to solve?,2,x4wco5," I am training the neural network using the Jupyter. Here, I would train the network, and would save the model on a file if we get better rewards. Here, I would not get any optimal result when I train the NN by restarting the Jupyter session each time. However, when I re-train the NN on the same/current Jupyter session, I would then get the optimal result, but I found that it alternates. That means, at one time the performance of the trained model would be low, but in the next time around it would be high, and the difference is huge. For instance, around 30%, at the beginning, but above 90% in the next time around.",MLQuestions,2022-09-03 07:55:58,2
"right now you're literally just asking us to do your homework for you. if you have no idea where to start, you need to talk to your professor. If something specific is confusing you or you've gotten started working this out but have gotten stuck, tell us more about what you've tried and what's blocking you and we can try to guide you in the right direction. But we're not here to do your homework for you. 

You've got nearly a week to figure this out. Take a breath. Relax. Poke through your lecture notes and see if there's anything relevant in there. If you want help I and others are happy to give it, but that help will be guiding you to the correct solution, not handing you a completed homework assignment you haven't demonstrated you've even attempted to solve on your own.",3,x547wj,,MLQuestions,2022-09-03 13:39:31,6
"if you could describe the personality traits you want your character to have in the prefix, that might be enough. Depending on how fancy you want to get, you could start with a prefix comprised of raw tokens and then update the token representations.",1,x48ag5,"Hi everyone,

I am hoping to use an LLM like GPT-3 or BLOOM to 'write' a character in an interactive novel. Is the right way to imbibe personality to the responses that are consistent using some kind of prompt engineering? For example: 

\*\* What do you think about the Banking insitution?

Tyler Durder: 

\*\*

&#x200B;

Or is there a way for us to train the LLM on a particular corpus of text (dialouge from a variety of characters)

&#x200B;

Thanks for you help!",MLQuestions,2022-09-02 11:06:57,4
Here's a lazy mode starting point: https://github.com/LAION-AI/aesthetic-predictor,3,x3m57l,"I am a knuckle-dragging VFX technician/artist. I have been using stable diffusion driven by a bunch of combinatorial prompts to create images of actors and characters. I generate about 10k images/day. Classifying these by hand (reject/keep). Not tenable. 

I am seeing that it'd be pretty easy to do this with a classifier, as they're pretty obvious (80-90% reject rate), if you I had a bunch of training for what's acceptable (easy because I am using popular characters/actors).

Problem is, I don't know where to start to build this.

So, if you were going to build/modify/use a classifier which can look at images and accept/reject them based on quality and closeness to a training set, where would you start?",MLQuestions,2022-09-01 16:37:14,7
"Hmm this paper seems pretty close: 

* spell checker, check
* random forest, check
* 1000 correct words, check 

https://www.ijcaonline.org/archives/volume176/number27/yunus-2020-ijca-920288.pdf",5,x3twfi,"Hi.

I am still a university student. I am working on a project for automatic spelling error detection and correction in the Sinhala language and it requires machine learning. I currently have a dataset of over 1000 words (correct and misspelled). I am thinking of using the random forest algorithm.

When using it in the actual application, I am only sending the words and non of the features (only the user input which will be a sentence) . I have a doubt about that.

Anyone who has an idea, can you please help me understand this, or suggest an alternative?",MLQuestions,2022-09-01 23:05:16,6
"This happens - most often when there aren’t many classes and the imbalance across classes isn’t huge. If you have a reason to think they should be different, then they’re pretty easy metrics to implement. It never hurts to run a sanity checks on your model’s predictions.",1,x3eiwm,"Hello all, I am running into an interesting problem. It might not be a problem, but I think it is. I ran an artificial neural network classification model. I generated an accuracy score and an F1 score to compare after completing the model. My accuracy score was 61.90%, and the F1 score was 61.87%. I am working with an imbalanced data set btw

I found an article saying that score comparison could happen, but I am not sure. This might be considered a ""beginner question,"" and I am sorry if it is.

Have any of you run into this before? What could it possibly suggest about my model or dataset?",MLQuestions,2022-09-01 11:15:02,2
"This is in no way comprehensive but these are some important things to keep in mind:

Are the performance metrics you gave on the train set or validation set? This is super important.

Use simple baseline models to understand whether your current metrics are good - Linear regression, descision tree, maybe even random forests. They will definitely ask you why you didn't try these if you haven't.

Plot actual vs prediction scatter plots and inspect that

If your train set performance is much higher than validation set performance go ahead and try to increase regularisation..",2,x36sia,This is my first ML model. Bare with me..I'm trying to predict values of houses in Wales as my dissertation. I'm using XGboost now. I did some feature selection though I know decision trees don't need much of it. I got a mape of 16.5 and R2 of 0.81 (which I think is decent). I am now trying to get feature importance and delete some less relevant ones. And cross validate and hyper tune the parameters. Am I on the right path? Is cross validation used for hypertuning leading to less over fitting? Will removing less relevant features be useful for the model? What all can I do more to get better performance.,MLQuestions,2022-09-01 05:47:12,2
probably the EOS token. just a guess.,2,x399pu,,MLQuestions,2022-09-01 07:36:13,1
"CLIP. You 100% want to use CLIP for this. 

CLIP is what's known as a ""zero-shot classifier"". It was trained on a massive amount of data using a loss constructed around a similarity measure very similar to what you are describing, and this similarity can even be scored between and across text-image modalities simultaneously. A related model you might consider is CLOOB, which is a CLIP modified in a particular way that might be useful here, should be self-evident if you look up the motivation behind CLOOB.

Here's a search demo that will help you ""get a feel"" for how text-image similarity works in CLIP: https://rom1504.github.io/clip-retrieval/

CLIP is very popular right now, consequently there's also a bit of a tooling ecosystem evolving right now. CLIP models are already very easy to incorporate into e.g. search applications, and it's only going to get easier. 

You want a CLIP model.

* https://github.com/rom1504/clip-retrieval
* https://github.com/openai/CLIP
* https://pypi.org/project/clip-anytorch/
* https://github.com/mlfoundations/open_clip
* https://github.com/dmarx/Multi-Modal-Comparators
* https://github.com/jina-ai/jina
* https://github.com/jina-ai/clip-as-service",2,x2p5lj,"I am very new to machine learning so trying to understand how to best approach my particular problem.

Given 2 e-commerce products I want to output a 'similarity/confidence score' based on their titles and images. Looking just at the images for now I am currently using SIFT for feature extraction then cosine similarity for the similarity score. 

From my research it seems I might be able to get better results from using a CNN. I was looking at using transfer learning however most of the articles I found online are specifically for classification and I am struggling to understand how I can adapt this to obtain the similarity score between 2 images?

Can anyone point me in the right direction? Suggest any models or datasets for e-commerce specifically?",MLQuestions,2022-08-31 14:26:05,5
"Hello,  I don't know how to help  you but. WOuld like to thank about your post :).  Do you have code to share ?",2,x2dlw1,"I have been playing with Google Colab's implementation of ""Image-to-image Translation with Conditional Adversarial Networks"" aka Pix2pix as a way to learn more about GANs.

I've made little incremental changes, incorporating morerecent advances in GAN models. For example, I modified the models to use LeakyRELU instead of RELU, replaced the binary cross-entropy loss with Wasserstein loss, used orthogonal initialization and modified the training ratio to 4:1 discriminator to generator cycles. 

Most recently, I added spectral normalization to the discriminator model and saw quite a significant improvement to the generated image quality. 

So I added the same spectral normalization to the generator model. To my surprise, the GAN now completely fails to learn. I'm guessing I've stumbled upon the ""GANs are notoriously difficult to train"" issues. One thing I've noticed is the generator loss is extremely low, typically around 0.0005. Since this is the first time I've encountered a major issue, I am at a loss at figuring out what went wrong. Is this mode collapse? How do I figure it out? 

Also on a related note, this is a more general question.. Have I missed any other major improvements that I can make to get the models performing better?",MLQuestions,2022-08-31 06:26:09,3
"It seems you run the program from a text terminal or remote  console (like ssh) but the program needs X (a graphical display). 

This is an X /ssh/xterm issue which would better be answered on a unix/ubuntu forum.",1,x2dai7,"terminate called after throwing an instance of std::run\_time\_error' 

what(): Pangolin X11: Failed to open X display 

Aborted (core dump)

I have built an ORB-SLAM3 on ubuntu 20.04.4 on windows but keep getting this error when trying to run examples. Please help",MLQuestions,2022-08-31 06:12:02,1
"First of all, please recognize that you are more than your project, or your internship, or your degree, and that however much you think you've screwed up, plenty of people have screwed up more. You'll recover from this, very few situations in life like this are unsalvageable. _If you are having serious thoughts of suicide, please set aside your project and seek help._

I have never worked with this kind of data in particular, but some general thoughts on these kinds of multivariate time series.

* Actually running the RNN at 256 Hz is probably a waste of time, and making the problem harder than it needs to be. There is a ton of redundancy and there are frequency components that are extremely slow moving, and I really doubt it's useful to produce sleep stage labels at 256Hz -- 1Hz would probably be plenty, and probably even lower than that would still result in a useful system. Running with some sort of windowed summarization of your timepoints at 1Hz brings your sequence length down to 23k, 0.2Hz to about 4.6k. The very first thing I'd try is calculating the means and log standard deviations of each signal in large-ish (multi-second) overlapping time windows, and trying to fit that much smaller time series with the RNN. Getting fancier, you could read up on log spectrograms and computing those on the overlapping time windows.
* You'll want to globally normalize (across all timesteps and the entire training set, per feature ""column"") whatever features you do give to the RNN, so that all the features exist on roughly the same scale, and have ideally zero mean. For example, if you're looking at the windowed mean PPG value, you'd calculate the mean and standard deviation of that across all windows in all recordings of all subjects in the training set, then subtract that mean and divide by that standard deviation. Remember to use the _training set normalization statistics_ when preprocessing the test data; using the test set for normalizing itself would be cheating.
* How you determine your train/test split depends really on the question you want to answer. Do you want to know if this system can be used with an individual not in your training data? Then you need to make sure you keep aside whole subjects that are not in the training data at all.

I also very much agree with what chengstark said about looking at the existing literature on this application area.",10,x1j8dy,"Long story short, in my MSc Neurosciences course I have applied for an internship in a lab and got a project to validate whether using a ML model for classification of sleep stages would be possible in real-time (for ongoing sleep recordings instead of existing datasets). I was stupid enough to think that experience with programming and being good with debugging would be enough for me to learn and ""wing it"". I am in so much deep shit that the internship makes me want to kill myself, because of how stuck I am, how long this has dragged on and how my degree is basically in hiatus because I failed to deliver. It literally is killing me seeing how incompetent I am and I want to do something about it.

My data has following columns:

Recording, Timestamp (ms), EEG (uV), Rater1\_Scores, Rater2\_Scores, Rater3\_Scores, RNN\_Scores

\- I want the RNN to classify sleep stages (either Wake, N1, N2, N3, REM, Movement or Undefined) based on the patterns in sleep stages assigned by human raters (1, 2, 3) and taking these human assignments as equals (there is often ambiguity between human raters). I want these classifications to land in the RNN\_Scores column - does that make sense? I want the predictions to be based on all the other columns.

\- There is around 6 million rows per single sleep recording, the equipment measures the electrical activity (EEG) continuously over the duration of a person's sleep and it takes 256 measurements every second, does a batch size of 7680 (256Hz \* 30s) make sense?

\- My idea was to keep filenames, e.g. ""SubjectX\_Date1"" in the ""Recording"" column as an ID, so I could train the RNN on a range of different subjects and perhaps test it later on new recordings of the same subjects - I think I should not split the same SubjectX\_Date1 into train and test sets, rather, use e.g. SubjectX\_Date1, SubjectX\_Date2, SubjectX\_Date3 for training and then SubjectX\_Date4 for testing - would that work? Do I need to have a separate dataframe for SubjectX\_Date1, SubjectX\_Date2, SubjectX\_Date3 and a separate one for SubjectX\_Date4?

\- I do not think I can use a MinMax scaler because the EEG amplitudes drop naturally over time later into later stages of sleep, and the EEG data range will vary depending on the individual due to how well they use the equipment, sobriety, quality of sleep - do I just leave it raw then?

**Would anyone here be so kind to mentor me on the things that I need to learn or to do in my code? I am unable to replicate the multivariate time-based classification/stock price prediction tutorials as I still do not understand. I do not know anymore if my dataset is challenging or I need to learn more basics. I do not know how to build the 3D array for the RNN. I do not know how to decide when it comes to the choices of layers. I would appreciate any help that can lead me to at least building the simplest model, even for a pilot dataset.**

Dataframe:

https://preview.redd.it/qr3kuro6ruk91.png?width=834&format=png&auto=webp&s=519852c57edb708e0796a11f7cfc2ca80de79aef

Epoch for manual scoring by human raters (focusing on a single EEG left derivation seemed simple enough but I failed to do even that):

https://preview.redd.it/w5czn3iuruk91.png?width=1374&format=png&auto=webp&s=4c20ec917498cbb165c02e5bf800947a1281943f",MLQuestions,2022-08-30 06:31:19,10
"If you want to input a list of items, then the first item would be x^t-1, second item will be x^t, third will be x^t+1 and so forth. If you want to input a sequence of words per iteration, just increase the size of the input vector. Set a fixed size and pad the end with zeros of the sequences that don't fit. 

I think you need to gain a better understanding of how RNNs work before trying to implement anything.",2,x1r8xb," Does anybody know if it's possible to use as input, for exemple for x(t-1), a list instead of a value? And if so could you please show me any model that looks like that or any pdf that might help me doing it? 

[ ](https://preview.redd.it/xmu61mq1ewk91.png?width=616&format=png&auto=webp&s=9dc177b1ac4220913d1303c124a3d76aaa37985c)",MLQuestions,2022-08-30 11:56:15,7
Keep small r (r) before the path,2,x159nx,"data_dir = os.getcwd() + ""C:|Users|Saim Liagat
Ali\Pictures\HAM 10000\HAM10000 images""
SyntaxError: (unicode error) 'unicodeescape'
codec can't decode bytes in position 2-3:
truncated \UXXXXXXXX escape
im getting this error about above line of
code,can you plz help me out",MLQuestions,2022-08-29 17:59:07,14
"Actually we do cleaning in image dataset, we deliberately add noise to it (augmentation)",1,x0ou6w,"Hi everyone!

Sorry for the beginner question, I’m an SE engineer who just started trying to train a model for a side project that I’m doing. But first I need to clean the data set which is comprised of thousands of images and classes.

Anyway I can easily write scripts to do things like resize them all to 224x224 and find duplicates and remove those but I don’t want to reinvent the wheel.

Is there a tool you’re all using for this sort of common tasks when cleaning data sets? Ideally an all in one software that handles all this for you?

Many thanks in advance!",MLQuestions,2022-08-29 06:30:34,2
Is the fee paid on applying or on accepting?,1,x0tssp," [Dr Pavlos Protopapas](https://iacs.seas.harvard.edu/people/pavlos-protopapas), \[**Scientific Program Director** at the Institute for Applied Computational Science (IACS) at **Harvard University\]**, is offering a **fellowship program** at [Univ.AI](https://univ.ai/) in NLP, GANs and Reinforcement Learning, and MLOps to a small group of advanced learners in data science and AI around the world. Dr Pavlos Protopapas with his team will lead the fellowship. **Next Fellowship: \[**[**MLOps**](https://www.univ.ai/fellowship/ml-ops)**\] - starting on 3rd September.** 

\---

**Geoffrey Hinton Fellowship Highlights (Inaugurated by Geoffrey Hinton himself!)**

**Topics** \- NLP, GANs and Reinforcement Learning, MLOps

**Duration** \- LIVE, online. 6-8 weeks

**Faculty** \- Dr Pavlos Protopapas | Scientific Program Director at the IACS at Harvard University

**Commitment** \- 2 hours a day 2 days a week + assignments and project

**Timings for Live sessions**\* (\*subject to change with prior notice)

**Schedule** \- Weekend class/weekday office hours

Classes - Saturday \[ 11:00 AM - 01:00 PM EST \] \[ 8:00 AM - 10:00 AM PST \]

Office hours - Tuesday \[ 11:30 AM - 12:30 PM EST \] \[ 8:30 AM - 10:30 AM PST \]

**Cost of attending the fellowship** \- ZERO\* (\*It is a fellowship), we charge a tiny processing fee of USD-100 for the 8-week LIVE program toward processing cost.

**Prerequisites** \- Note that ordinarily MLOps is the terminal course of a 6-course program in Data Science & AI at [Univ.AI](https://univ.ai/) \- the prerequisites are significant. We do require that you have a prior background in ML & AI. Prerequisites here - [https://welcome.univ.ai/courses/ai5/](https://welcome.univ.ai/courses/ai5/)

**What do you get being a part of the Geoffrey Hinton Fellowship?**

1. Mentored preparation for top global careers.
2. 2 times every week for 8 weeks of fun & high engagement with peers from reputed institutions & organizations.
3. Participation in [Univ.AI](https://univ.ai/)'s global placement system for employment opportunities worldwide (predominantly US).
4. Interactions with top faculty and professionals during guest sessions.

**Upcoming Fellowship - \[**[**MLOps**](https://www.univ.ai/fellowship/ml-ops)**\] - starting on 3rd September.**

The application is open now:

To Apply fill out this form: [**https://forms.gle/KBVxXVDAwzRdYxph9**](https://forms.gle/KBVxXVDAwzRdYxph9)

The application itself is the only basis for selection, and therefore is slightly onerous.

Fellowship brochure for more details: [**Check here**](https://drive.google.com/file/d/1XGB4fTJ9lrvvsG0PfXwRF5CtLgywGkhs/view?usp=sharing)",MLQuestions,2022-08-29 09:58:55,2
here's another for ya: https://github.com/pytti-tools/pytti-core,1,x0e7ps,"I am looking for open source AI image generation, or text to image libraries, that you can install on an Amazon EC2 GPU instance for free. I found these:

[https://github.com/kuprel/min-dalle](https://github.com/kuprel/min-dalle)

[https://github.com/pixray/pixray](https://github.com/pixray/pixray)

I've found articles with lists of open source AI libraries, but most weren't specific to images. I've also found articles with lists of AI image generation tools, but these lists weren't specific to open source libraries that you can install yourself on your own machine. Most seemed to list online tools.

I tried searching for lists of specifically open source image AI libraries you can download and install and use yourself for free, but couldn't find any such list.

Are there any other similar libraries like these, free open source libraries you can download and use to create images, if you have a GPU server?",MLQuestions,2022-08-28 20:22:38,4
I think usually you just average over all of the fold scores to get a final output score,2,x0pc58,"Hi community ,

Does anyone familiar with this both train\_test\_split and k-fold cross validation technique? I have trouble understanding how to evaluate the performance of a model trained by this 2 different methods , by far I only know that to evaluate a train\_test\_split trained model simply just show the ""f1\_scores"" , ""recall"" , ""precision"" and ""support"" with a confusion matrix will be enough to tell us whether the model is performing good or bad .

But how do I evaluate a performance of a k-fold cross validation trained model since it has k number of folds ? Pls help and thank you for reading <3

&#x200B;

https://preview.redd.it/yw6hujbxqnk91.png?width=1448&format=png&auto=webp&s=7de2176692d9f7af4d91767755aff371c1f37e66

The image attached is the google colab environment I used to run my codes

the model i am using is KNNclassifier

scores = cross\_validate(model, X,y,cv=10, scoring ='f1\_macro')",MLQuestions,2022-08-29 06:52:53,4
Sounds like a homework question. What are your own thoughts?,2,wztrm3,Does the p value of a feature in a logistic regression model tell us about the importance of that feature?,MLQuestions,2022-08-28 05:00:13,4
There should be no difference between the gradients if the original functions are the same. It looks like you just dropped a negative sign for one of them when calculating the gradient since yhat is a function of x.,5,wzzkhv,"here are the Mean Squared Error(MSE) and it's gradient.

MSE1 = 1/2N (yhat - y)^2

∇MSE1 = 1/N x^T (yhat - y)

MSE2 = $1/2N (y-yhat)^2

∇MSE2 = 1/N x^T (y-yhat)

there is no difference between MSE1 and MSE2 but **is there a difference between the gradient of MSE1 and MSE2**? 

in the **gradient_descent** function if we use (yhat-y), the output becomes [the figure yhat-y](https://i.postimg.cc/vmw05P68/yhat-y.png) with correct fit. but if we use (y-yhat) instead so the output becomes [the figure y-yhat](https://i.postimg.cc/NfgYk4m4/y-yhat.png) with incorrect fit.


[here](https://paste.debian.net/1250534/) is the link to the **gradient_descent** function and the code which generates these figures.



second question: **if we want measure the Mean Absolute Error(MAE) as training error instead of MSE, should we use derivative of MAE instead of MSE in gradient_descent function**?",MLQuestions,2022-08-28 09:29:14,5
"this isn't a very clear question. I suspect I know what you are asking, but it might help me and others respond if you elaborate a bit. With regard to what about the bias and variance in decision trees? What's the issue that needs fixing?",2,wzqqqn,"With regards to the Bias and variance in Desicion trees, does a random Forest fix such variance ?",MLQuestions,2022-08-28 01:51:31,3
"I think this should be possible. Ideally you'd want to fine tune an existing model but I honestly haven't seen one for handwriting. I would label it per-word. I'm planning on starting a graffiti OCR dataset soon.

I build [https://mekabytes.com](https://mekabytes.com), I'm adding freetext annotation right now ([license plate preview from the dev server](https://i.imgur.com/fs52QsF.png)). By the time you get them scanned the freetext feature will be live. If you're fine with them being public and wanted to run a cursive hand writing dataset I'd help label. 

A bonus with using them in a dataset is even without OCR you get them digitized via labeling lol.",2,wz8r7g,"I recently came into possession of hundreds of letters between by grandparents in during World War 2. They basically sent each other a letter (or sometimes 2!) every day for over two years. After it’s all said and done, it’s around 1200 letters or so. 
They’re in good shape, mostly readable, and we want to scan them for the family. 

Any advice on datasets to train a model for script analysis, or on creation of one? I’ve seen some OCR projects online, but they all seem to train with pretty tame datasets when it comes to printed text. I know script will be a challenge for ML, but I wanted to see if I could come up with a fun AI based solution. 

I have some background in ML with Python, so I’m not a stranger to the process, but any advice is helpful!",MLQuestions,2022-08-27 10:40:33,13
"* Physics informed ML for sure
* The last two years has seen a revolution in generative models which are currently being dominated by models based on diffusion processes. These mainly show up in the same kinds of applications where you'd see GANs (i.e. image generation)
* In the learning theory community there's recently been a lot of intersting work investigating model training dynamics",8,wzefqu,,MLQuestions,2022-08-27 14:48:47,2
"RNN-based models are useful for classifying ordered data (e.g., text), but if individual observations are connected by relations (i.e. have a graph/ network structure), graph neural networks are the way to go.",2,wzbow4,"I want to know if there are any classification algorithms which considers what the last instance's feature values were while classifying the current instance.  


Sorry, couldn't find anything on the net.  


Or should design instance with a feature for current and add new features for  previous ?",MLQuestions,2022-08-27 12:45:43,3
"Wikipedia article on algorithmic time complexity should have you covered. 🙂

And when asking for help it is beneficial to at least proof read your question. It's a matter of showing respect to those who might help you.",4,wz7s5h,,MLQuestions,2022-08-27 10:00:12,4
"kaggle has some free tutorials for ML.

You can find free lectures on youtube, including from Andrew Ng.

But why not books?

Out of curiosity, I watched the Andrew Ng lecture and it merely touches the surface only on some topics.

There are very good books on the topic, including free ones, why paid superficial lectures?

Example: https://hastie.su.domains/ElemStatLearn/",3,wytpk8," I purchased 7 courses from Udemy to go deep in python,R and ia. Could someone help me to say where should i start, then pass and what should i take last and if i have to return one this courses for a refund by redundant information? please. Well later i would like to learn GO, any courses that you can recommend?

[https://www.udemy.com/course/100-days-of-code/](https://www.udemy.com/course/100-days-of-code/)

[https://www.udemy.com/course/python-the-complete-python-developer-course](https://www.udemy.com/course/python-the-complete-python-developer-course)

[https://www.udemy.com/course/all-about-python-learn-to-code-and-make-multiple-apps](https://www.udemy.com/course/all-about-python-learn-to-code-and-make-multiple-apps)

[https://www.udemy.com/course/tensorflow-developer-certificate-machine-learning-zero-to-mastery](https://www.udemy.com/course/tensorflow-developer-certificate-machine-learning-zero-to-mastery)

[https://www.udemy.com/course/learn-data-science-deep-learning-machine-learning-nlp-r](https://www.udemy.com/course/learn-data-science-deep-learning-machine-learning-nlp-r)

[https://www.udemy.com/course/data-science-machine-learningtheoryprojectsa-z-90-hours](https://www.udemy.com/course/data-science-machine-learningtheoryprojectsa-z-90-hours)

[https://www.udemy.com/course/the-full-stack-data-scientist-bootcamp](https://www.udemy.com/course/the-full-stack-data-scientist-bootcamp)",MLQuestions,2022-08-26 21:39:41,8
"1) No, although ""cold start"" problem (i.e. what do you recommend to someone who hasn't listened to any songs yet) is a big issue in recommender systems, so the splits/ initializations can sometimes be.. creative.

2) Recall@N Precision@N and Mean Average Precision (MAP) will probably be useful.

The dataset is on kaggle and a lot of people have uploaded notebooks- I'd explore some of the submissions. To create val/test sets, you can map songs to integers such that each user is a sequence of songs {s\_1, s\_2, ..., s\_n, \[PAD\], \[PAD\]}. You could then randomly sample \`k\` songs (for users with at least some number of songs) into test/ val sets. From there you can learn a model that tries to learn the probability of liking specific songs given songs a user has listened to.",2,wyztf6,"Hi! I'm new to machine learning and in particular to the recommender systems topic, so this question could seem very stupid for seniors here.

My project regards the designing and implementation of some music recommender systems (e.g. content-based, collaborative filtering,.. ) using the famous Million Songs Dataset.

I've noticed that there aren't lots of examples online (articles, GitHub, YouTube..) and those I've seen always use the ""recommender"" python package to build these systems and they don't even use training and test sets: only with the original dataset and few lines of code (due to the use of 'recommender' package) they build these music recommender systems. 


Now, my questions are: 
1) Is it common/legit to not use training and test sets in recommender systems field? 🤨
If not, can you please suggest me the steps to follow to create a correct recommender system (collaborative filtering and content-based) 

2) How can I evaluate which type of recommender system works better (which recommends the best songs for every user)? What metric should I use to do this (I haven't real users that can judge the various recommendations)?



DATASET USED:
The dataset I'm using have ~1M observations (only few observations have some null values, it's a balanced dataset) with these columns:

- track_id
- user_id
- every song (every row) has a list of similar songs (with a computed value of similarity)
- every user have a LISTEN COUNT value for every specific song he had listened in the past
- artist
- year
- title",MLQuestions,2022-08-27 03:45:34,2
"Am I missing something?

Sort x[] in ascending order, ignore ties (continuous CDF?) , quantile i/(N+1) of X is value x[i-1] for zero based arrays",2,wyduhq,Ok so **i was asked an question to convert empirical cdf for a random variabl X it is continous. Write code for empirical PDF?** ANy book i can read which has similar kind of questions. Because many startups are also asking these tyose of questions.,MLQuestions,2022-08-26 09:38:40,4
"The optimal supervised predictive tree shouldn’t be constrained to a single depth in all branches.

if you need a complexity parameter then total number of nodes is a better one, and better still is a Minimum Description Length computation which encodes all parameters.",1,wysu8e,"

[View Poll](https://www.reddit.com/poll/wysu8e)",MLQuestions,2022-08-26 20:53:33,1
[deleted],2,wy6qwp,"Hi guys, I'm a novice in ML, so sorry in advance for any misconceptions, I'll try be as clear and concise as I can.

I'm watching the video on logistic regression by Andrew NG ( [https://www.youtube.com/watch?v=F\_VG4LNjZZw](https://www.youtube.com/watch?v=F_VG4LNjZZw) ) and specifically on decision boundaries.

at timestamp 04:40 of the video, Andrew shows an image of data, the chart shows y values 0 to 3 and x values 0 to 3.

Now this is where I'm confused... I thought that in logistic regression(let's say binary classification for simplicity), we are working with data that is of either two classes, the negative class or positive class and by using the Sigmoid function we have a y value that can be between 0 and 1. But in the diagram the values for y are between 0-3 and for x 0-3(I thought regression problems such as linear regression only work with continuous variables and logistic regression is either positive or negative, etc).

Also, how would we even draw a Sigmoid curve around this data? In the first lecture ( [https://www.youtube.com/watch?v=-la3q9d7AKQ](https://www.youtube.com/watch?v=-la3q9d7AKQ) ), at timestamp 02:52, that's what I was expecting, but the visualised data in this timestamp looks much different than the data in the visualised previous timestamped video, why is this?

Thanks",MLQuestions,2022-08-26 04:30:35,8
"You could just take low confidence matches. Similarly, train embeddings on the current stock to use for clustering.",2,wy26mv,"I am trying to detect new classes ariving in a retail product detection. I want to build an unsupervised algorithm which will automatically detect new product placed on the shelf, which is not trained yet, and Cluster these together somehow, which i can use for retraining.",MLQuestions,2022-08-26 00:01:17,7
"There's no reason to one-hot encode anything anymore.

Use target encoding for anything that's not a neural net. Use embeddings for neural nets.",4,wxgcx9,As the title says. Let’s say you have a lot of columns in your dataset that you think somehow influences the prediction parameter but those columns somehow have high cardinality and you do not wish to one-hot-encode the data. What’s your “go to way” that mostly works well?,MLQuestions,2022-08-25 07:54:28,11
"If its for something like anomaly detection, I would always try out one class classification i.e. train only the majority class. Then maybe use something like SHAP to see what makes a false positive, a false positive and the same for true positives based on the features. Then make a judgement.

https://www.data4v.com/machine-learning-interpretability-for-isolation-forest-using-shap/",9,wx5ih0,"I mean with the classification problems where you have highly skewed classes in the dataset with ratios like 2000:1 samples. I've tried multiple approaches to deal with it like under/over sampling, changing loss functions but still end up with a model that classifies everything as majority class. I am new to this, any suggestions will be appreciated.",MLQuestions,2022-08-24 22:21:16,9
"Getting practice choosing an appropriate problem, collecting the dataset, and deciding which model paradigm best suits the problem+data would be very valuable. Documenting which decisions you made, why, and what alternatives you decided not to go with would be valuable for remembering these things long term for your career. Also, cleaning the data and other data-engineering things.",3,wwxf0w,"I’m currently doing an internship and they are going to ask me what I want to do for a project soon. I really want to say create new models and try my own analysis but ultimately I want to get good at this, not just do the things that would be the most fun. 

So I’m wondering if anyone has any suggestions for what I should work on in order to build a good foundation for a future career in machine learning. 

Thanks for any advice.",MLQuestions,2022-08-24 15:54:53,2
RNNs sound overkill and would require a lot of training data. A random forest regressor might be your best bet. You may even want to look into a multimodal implementation so each sublist receives it's own time estimate which is then either summed or used as inputs to another regressor,2,wwzwn3," 

My data consists in many metal pieces which are put together to make a final metal mold. To make each of this metal pieces, machinery recieves many operations to follow, like chopping, facing, etc... (I don't know if this translations are good), and for each operation are used different tools, at different speeds, and other variables. This metal pieces don't always have the same amount of operations. My goal was to create a RNN model where I use as an input the operations of a metal piece and expect as an output the time to be built. Each input is supposed to be a list (the whole piece) of lists (the different operations required to make the piece). I have some questions about this:

1. The lists I want to use as an input contain different amount of sublists, even tho every sublist have the same length. Is this an issue?
2. Are RNN's the best approach to this? And if so, which type of RNN's? I'm sorry if it wasn't clear, but I don't know very much about RNN's and I struggle to explain this problem to other people. If you know about any similiar problem or any pdf's that might help me in this problem, please tell me :) Thank you!",MLQuestions,2022-08-24 17:45:29,5
"Do you have a dataset of thousands of non-detailed/unrealistic image + thermal image + detailed image to train on? If not, it's going to be hard to create a neural net that does that particular task. (Or I may be understanding your question wrong.)",1,wwoj3j,"I am a beginner and thinking of merging different types (models) of images, like a normal face or full body image with thermal images of the same, in order to obtain a much more detailed and realistic  image. Any suggestions on where can I start this and which GAN might help me in this.",MLQuestions,2022-08-24 09:51:00,2
"If most people don’t have cancer in the dataset (i.e. if the classes are highly imbalanced), it’d probably be better to use 2 different models. Create a binary cancer classifier to determine whether someone has cancer, and then have a second model that predicts severity given the person has been predicted to have cancer",2,wwk6zt," Dear community , 

Hi im very new to machine learning , currently i have a dataset that contains medical data of whether a person has cancer or not ( 0 or 1) .Now i am trying to make a model that can classify the severity of the cancer if the person has cancer (lvl1 , lvl2 , lvl3) , any idea how should i label the dataset so that i can make this possible?

the dataset is from an online source: [https://drive.google.com/file/d/1CEql-OEexf9p02M5vCC1RDLXibHYE9Xz/view](https://drive.google.com/file/d/1CEql-OEexf9p02M5vCC1RDLXibHYE9Xz/view)

My current doubts are , how do i know the labelled range is correct if the dataset i have in hand only classifies 0 or 1 which is has cancer and no cancer.",MLQuestions,2022-08-24 06:54:02,6
"it totally depends on your models and data. instead of building your own machine you might wanna get started with [https://colab.research.google.com/](https://colab.research.google.com/) or some other cloud provider. there is a free tier and you might find out what your requirements are in the process. 

some considerations from my expierence in computer vision:  
\- you need a powerful GPU and plenty VRAM. 3090 is the only consumer card that makes sense to buy currently because it has 24gb VRAM, next lower is 16gb models. larger VRAM means higher batch size which makes it easier to saturate the gpu and higher batch size often means better convergance.   
\- how large are your datasets? can data be fed fast enough to the gpu to saturate its processing speed? does your dataset fit into RAM or do you need to load data from disk in each epoch? how much CPU power is required to pre-process that data for data augmentation, image decoding, ...

\>> Additionally which system would you recommend (Linux or Windows).  
Definitely Linux, feature support by deep learning frameworks is better on Linux. Research papers are typically implemented to run on Linux. In computer vision research the majority uses Pytorch I would say. Often things work on Windows too, but you have to jump through some hoops. One example would be DDP (distributed data parallel) in torch which allows to train on multiple GPUs. This just wasn't available on Windows in the past, not sure if it is nowadays. Also: pick an Nvidia gpu, some framworks work with AMD, but feature support is far better for Nvidia.",4,wwfgha,"Dear Community,

Which hardware specs would you recommend for a workstation running neural networks (CNN, SVM, MLPC etc.) especially in Pytorch or Tensorflow. Additionally which system would you recommend (Linux or Windows). 

I would be one of the first programmers trying these at my work in a biological context. Datasets are mostly sequence data (COVID genomes, mosquito genomes). I have experience with python (sklearn). 

Thanks in advance :)",MLQuestions,2022-08-24 03:01:53,14
"The fundamental issue with NeRF for this type of application is that it only cares about what the scene looks like. It's not directly modeling the presence of objects in the scene at all. I'd argue that the ""best"" representation for the sort of application you are describing would be one that models objects and shape directly. Here are a few projects I think you might find relevant:

* https://nvlabs.github.io/eg3d/
* https://nv-tlabs.github.io/nglod/

As far as strictly NeRF techniques goes, I think mip-nerf might be the best for your application. Most of what I've talked about here is available in https://github.com/NVlabs/instant-ngp",2,wwiqmg,"Hi, I know 3D meshes and materials can be extracted from NeRFs using the marching cubes algorithm but the result is not always that good. I guess it is highly dependent on the NeRF and the size of the cubes in the algorithm.  
Are there types of NeRFs (or similar) from which it is simpler to extract good quality 3D meshes and materials? What are currently the best ones?",MLQuestions,2022-08-24 05:50:46,5
[deleted],1,wwd3hn,"Actually currently I am working in sap domain completely different to than these domains

 Should I Java rest APIs spring/springboot, cicd for development/ get into sde field get masters and gradually get into field?
 do you think this will be of any help? 

And other path would be learn frameworks like pyspark , and all for data engineer 
and prepare for higher studies,learn libraries like tensorflow pytorch, and get masters?only thing is I am not able to find ML fields where I am hope with this little exp, even if there are they are more like business analytics in name of ML/AI


Because currently these are only two paths I see I can't how to get into research in any other way, Plus since I am working these two seem only possible track ai can get into right now, till I get into Masters",MLQuestions,2022-08-24 00:36:57,2
happier gradients,11,wvvjui,"I know this could be a very stupid question.

  
Lets say there are 2 features

x1 in range of (0 - 2)  
x2 in range of (100000 - 200000)

Why should we normalize both the features in the same range?  
Why can't the learning algorithm have adjusted weights on the 2 features?  


like w1 = 5  
and w2 = 0.00005",MLQuestions,2022-08-23 11:01:44,9
"I know I’m not directly answering the question, and I understand that everyone has different limitations based on their work environment and data sensitivity etc, but I would recommend using cloud compute for the heavy ML development. AWS, Azure, GCP, or even Paperspace all have free tier options if cost is a concern. This allows your work to be more flexible and scalable, as well as offering use of different OS’s and hardware to meet your needs. 

With that in mind, I am very happy with my development on a MacBook. I do lots of data science work locally, but use the cloud for GPU stuff. I prefer Linux or MacOS to windows, and many data science libraries are designed for the Linux kernel, with Windows feeling like an afterthought in some cases. With the python issue you mentioned, I’ve had pretty much zero issues. I’ve installed python on both my intel and M1 MacBooks and they both work great. Some libraries are not yet available for the M1 architecture, but you can run Rosetta to meet your needs in many cases. 

For your situation specifically, it does sound like you’re leaning toward the Nvidia device, which seems like it makes sense. Just keep in mind that most tiers of cloud compute offerings will outperform most laptops at a fraction of the price, and getting comfortable with the CLI will allow you to hardly notice a difference between what’s running on your computer vs what’s happening in the cloud.",3,wvswsg,"Requirement for: Data scientist / bioinformatics / computational chemistry / machine and deep learning 

Current utility: datasets from clinical trials, gene expression datasets for 20000 genes, structural biology: crystallography data/pdb/pubchem data/drug bank data/chemoinformatics data(rdkit).

Standard libraries used so far: numpy sklearn pandas seabon matplotlib statsmodels scipy tensorflow and then some. 

Interested in: cuda development, using clara discovery (a drug discovery platform by nvidia), 3D molecular modeling and simulations, generative models (GAN) on 3d molecular descriptors and crystallographic data to simulate molecular configurations/poses/protein atructir prediction 

I see many people claiming that the new MacBook can do such things but then I also hear how people say that making python work on macbook is cumbersome as is. But at the same time I am not interested in carrying a super heavy hard to carry device that's gonna cook at high heat anytime I use it. 

Price being no issue can you provide macbook vs nvidia or any laptops which will do well with loading massive datasets, execute code with low latency for python libraries I mentioned, perform machine learning well, have an ability to handle docking/3d simulations etc, have a decent battery life, not have a cheap fragile body, won't get too hot or be too heavy to carry.  

Also, to leverage gpu computing, any egpu setups you might recommend that I can plug and play with said laptop 

Thank you",MLQuestions,2022-08-23 09:15:20,14
"If I am not mistaken there is still no definitive guide to a proper network architecture. You always have to experiment and check what is better and what is worse.

Though I am aware of one simple rule, that states that the number of parameters in your model should be at least 100 times smaller than the size of training set. But of course it can be used only if you train a model from scratch.",3,wvjv4d,"Hello,

I apologize if what I am asking is found in other post. I tried to find an answer in the forum but I had no luck since most of the post refers to specific applications and I am looking for a general approach to understand the ideas rather than specific applications.

I would like to know if there is certain method (rather than trial and error) to improve the behavior of ML models. 

For example, by using tf and Keras, I can define each layer at convenience, including the sizes of the network, parameters and so on.

    def create_model():
    
      model = tf.keras.models.Sequential([ 
          tf.keras.layers.Conv2D(16,(3,3),activation='relu',input_shape=(150,150,3)),
          tf.keras.layers.MaxPooling2D(2,2),
          tf.keras.layers.Conv2D(32,(3,3),activation='relu'),
          tf.keras.layers.MaxPooling2D(2,2),
          tf.keras.layers.Conv2D(64,(3,3),activation='relu'),
          tf.keras.layers.MaxPooling2D(2,2),
          tf.keras.layers.Flatten(),
          tf.keras.layers.Dense(64,activation='relu'),
          tf.keras.layers.Dense(128,activation='relu'),
          tf.keras.layers.Dense(64,activation='relu'),
          tf.keras.layers.Dense(1,activation='sigmoid'),
      ])
    
      model.compile(optimizer=""adam"",
                    loss=""binary_crossentropy"",
                    metrics=['accuracy']) 
    
      return model

I have observe that for the particular case I am running (and it is obvious to expect), by changing the size of the dense layers or the Conv2D the model's accuracy changes. Sometimes to worse, some others to best. However, I feel still like I am playing around with the parameters without actually knowing the best practices to decide what kind of layer/size/parameters implement.

I would like to know if someone can point me out to further literature/courses or any kind of information that will help me to understand better ""how to select the right model architecture"".

Thanks in advance!",MLQuestions,2022-08-23 02:09:47,3
"The distance function is usually cosine similarity. The context acts like an analogy. So the distance may not be close, but the shape or angle of two words will embed a meaningful relationship. Take the word *man* and its distance/cosine similarity to the word *king*. If you retrieved the word with the same cosine similarity but started with a vector for *woman*, it will most likely return *queen*. The same goes for many other relationships.",3,wvg44e,"I've gone through the neural network architecture of word2vec. We have input layer (for the focus word) and 1 hidden layer (where we try to capture the meaning of the input word in some lower dim space) and 1 output layer (for the context words). It all kinda makes sense, intuitively except for one claim I keep hearing repeatedly in various sources. Words being used in similar context will have similar vector embedding (in the hidden layer) and I'm having hard time wrapping my head around it. Even if we look at the entire NN as a continuous function, nowhere it says (mathematically) that the inputs have to be close enough for the outputs to be close enough.",MLQuestions,2022-08-22 22:20:29,3
"If you retrain only with new data, it will forget a lot of old data. The question is, do you want it to remember old data?
There's also a problem of vanishing parameters, when you retrain too much and network loses ability to learn.",4,wv71kp," 

I have a confusion.

When I look at classifier codes in ML libraries like deeplearning4j or java-ml,  
it takes the dataset only once.  
And calculates the predictions based on that dataset.

What if I train the model with this month's data  
and next month I get a new set of data, which I want to train on top of my previous model.

Is it possible?  
Or do I have to train again with lastMonth dataset + this month's dataset?",MLQuestions,2022-08-22 15:19:54,6
"That direction is not trivially findable. You could make linear interpolation in latent space with another photo of an elder person. Or maybe it is trivial, you can try adding several vectors in several directions and see if there is a pattern. A priori there is no reason to think that direction is the same for all face images, but who knows.",2,wuqr2j,"Hello,

I want to do face aging work. For this, how can I interfere with the age latent space of the images produced with Style GAN?",MLQuestions,2022-08-22 04:11:37,1
I had good experiences using Mel spectrograms,2,wumlbw,I'm doing a project to generate music using LSTM or CNN based network and like to know what is the best way to represent my data? Also if anyone knows about popular datasets for this type of stuff please do tell. Thanks.,MLQuestions,2022-08-22 00:06:01,4
"This graph is exactly what you would expect to see. Imagine the naive approach where we attempt to make a prediction but don’t have any information to go off of; the best thing we can do is simply predict the mean every time, which would look like a single spike at the mean and zero everywhere else. Your model can successfully use the input data to make a prediction which is closer to the true value.

I don’t understand what question you are asking in your comment.",4,wuh1dj,,MLQuestions,2022-08-21 19:08:47,4
PINNs,3,wuq4a6,"In november I will (hopefully) start my PhD in Physics. Until now I used supervised learning algorithms (in particular convolutional neural networks) to study quantum systems.

Now I would like to learn the most important machine learning models in order to have a set of tools to use for each physics problem. For example, Transformers come in my mind.  Which models are worth studying (which have therefore not already been surpassed by other models) ? 

&#x200B;

Thank you!",MLQuestions,2022-08-22 03:37:34,1
Try some of the methods in [imbalance learn](https://imbalanced-learn.org/stable/) such as SMOTE and balanced random forests. You can change the threshold manually or use [sklego](https://scikit-lego.netlify.app/meta.html#Thresholder),4,wu9p3m,"I have a binary classification task on an imbalanced dataset, about 1:10 ratio. So i preprocessed the data and used random under sampling/class weights. I used a bunch of classifiers including a neural network of which random forest gave me the best train accuracy. Now the evaluation metric for test set is f1 score, I got a test accuracy of 99.7% with a test set f1 score of just 47.7 (I used gridsearchcv too). But when I didn't preprocess or undersample I got a train accuracy of 28% with a test set f1 score of 80.1. I want to get a f1 score of 96+. 

This task is important to me, so help would be appreciated. If someone with experience could help me one on one it'd be even better.
Thank you

So, idk why reddit is doing this to me, I edited the post twice but it wasn't saved by reddit.",MLQuestions,2022-08-21 13:40:48,4
"https://huyenchip.com/ml-interviews-book/

This should help",3,wtyw81,"ML engineer interview prep

Hello community! I am keen to get into Faang/Instacart/Twitter as a MLE. How should I prepare for the interviews? How many rounds to expect? What to expect from each round? Should I focus on Leetcode easy and medium? What about ML and DL theory? Confused where to start. 

Background:

MS in CS, 
4 years experience",MLQuestions,2022-08-21 05:52:05,1
"This is my [favorite link](https://www.kaggle.com/code/isbhargav/guide-to-pytorch-learning-rate-scheduling/notebook) for lr scheduler examples. Was working on a project for a few months until I found it, and then I just experimented with the various kinds until found one that worked really well for my network. Got me a publication thanks to it. The default values will largely depend on your system. For example, I started with a very small lr (1e-6), went up to 1e-3 after 1000 steps, and then down to 1e-8. You'll have to find some that works for your specific case unfortunately",3,wu2occ,I’m curious if it’s common to use learning rate decay and if so if there are good default values to start with?,MLQuestions,2022-08-21 08:45:19,7
Consider trying to train a model on expert data (someone playing the game and saving it) as a soft start.,3,wtw2xl,"Hey, not sure if this is the right subreddit but couldn't find a question oriented subreddit for reinforcement learning/neural networks 

I am a graduate student with a large reinforcement learning project that I can not get progress on. The project is teaching an AI (using reinforcement learning and a neural network) to park a car (on a 2d parking lot with obstacles - other cars and sidewalks). Programming language we're using is python with pygame and pytorch. 

My group and I have tried several models and training methods (DQN, PPO, PPO-LSTM, Genetic Algorithms). All manage to reach the parking space some of the time but we couldn't get them to understand that they need to park in it - with the entire car inside the parking space at velocity 0.

We have tried many ways of rewarding/punishing the AI and messing around with the hyper parameters in all training methods but none seem to be even close to succeeding. 

Unfortunately most projects doing something similar don't share their training parameters or code, so we don't have a lead on how to get this done. Any help or tips would be greatly appreciated, thank you.",MLQuestions,2022-08-21 03:10:49,3
Less weights. And simple is better if it gives satisfactory results.,9,wtktu9,,MLQuestions,2022-08-20 16:39:27,3
"Ideally, I would like to know which 3-4 images were used to create a final one. You can either create a structured files to capture that or you can name all images like candidate-{group}-{image-number-within-group} . You can add suffix - final to mark the final image.

All this data can also be generated with stuff like knn but needs little work.",1,wtmf3r,"Hi there. I'm a real estate photographer interested in creating a machine learning model for editing flash and ambient real estate photos. I have 7000 homes worth of raw files and finished edits. I'm not a programmer, but I figured I could at least start preparing the data for whomever i hire. 

I'm just wondering what is the best form to arrange the photos? It takes 3-4 raw files to create one finished jpg. Right now they are all on dropbox in folders. Do I need to arrange them in some kind of spreadsheet or something? I'm fairly clueless.",MLQuestions,2022-08-20 17:56:36,1
"It's typically a good idea to run PCA with n_components set higher than needed at first to see if you can judge how many features are useful.

It's good practice not to operate on more data than necessary too, just for the sake of time.",1,wtgtbk,Is there a way to determine the minimum amount of data needed for some number of dimensions to avoid the curse of dimensionality? The project I'm working on uses kmeans clustering and I was wondering if I needed to reduce my number of dimensions since my dataset is pretty large.,MLQuestions,2022-08-20 13:35:18,2
"Start the new one

Majority of the parts are exactly the same, so you know which ones are a repeat, therefore you know where to skip exactly.

Two parts are updated, two parts are removed (dimensional reduction and SVM), two parts are added (reinforcement and trees), and six parts are staying the same.",1,wt7jjn,"Hi!

Basically title. I'm halfway thru the original Stanford University Machine Learning course by Andrew Ng, but instead of using the Octave/Matlab exercises, I went with a [Python repo](https://github.com/dibgerge/ml-coursera-python-assignments). Now, I know the new specialisation course came out and is updated with newer content, more relevant to the state of the industry today.  
I have the following choices:

\- I finish this course and ignore the new one.

\- I don't finish the course and start the new one.

\- I finish this course and start the new one.

For context, I'd like to add I'm a backend software engineer, already familiar with Python and software development.

Thanks in advance for your answers, and sorry in advance if this question has been asked and answered!",MLQuestions,2022-08-20 06:47:53,4
Is there a question in them there ramblins?,9,wt4elw," All tho ,I agree with that . IDK What I should do to grasp the data's content.",MLQuestions,2022-08-20 04:01:44,6
"You describe a fully connected MLP, e.g. y = a(W""(a(W'x + b') + b"") for the two layer case. You'll  probably be told about a lot of different 'architectures' in explaining why one network like this can't typically do it all. These are common patterns for organizing the connections between weights (equivalently, changing the equation for f in y = f(x, weights)), typically to include ""inductive biases"" for the task at hand. An inductive bias sort of just means something you as the practitioner believe is an important property for the final function/model to have.

 CNNs are a great example. Without going into too much detail, CNNs have weights that are used multiple times in small sub-parts of an input, typically a 2D image, called filters. [Here's a gif showing how they stride across an image](https://images.app.goo.gl/BHQCYSS11zQ12n6y7). Compared to an MLP, this architecture had an inductive bias for ""translational equivariance"", which just means that if you move contiguous fragments of an input around, the next layers activation will have a similar shift, but still be equal for that fragment. This is very important in vision: if we are building a classifier, we want the model to be able to detect a cat regardless of where in the image it occurs, even if it's never been in that location before. By having the same weights stride across an image as a filter, we get the same activations regardless of cat location (though similarly translated), and later layers can hopefully still detect this feature and give the correct label. For an MLP, this is less possible: weights in layer 1 are fixed to particular areas of the input. Learning a concept of cat that generalizes to any input location means learning the pattern in multiple places instead of just the one CNN filter. If your data doesn't have cats in all places, you're unlikely to learn such a general feature.

As a final aside, there might exist an MLP that can do it all, and if I recall my theory correctly, a 2 layer MLP with an infinite width can represent any function. The problem is, you have to find the right function using finite data and gradient descent. The inductive biases and corresponding architectures are how we reliably find useful functions for a given problem domain.",3,wsrvz4,"Every now and then, I try to understand ML, because it is absolutely fascinating. Yet I more or less fail everytime. I kind of get the statistical bases: However, what I don't really understand is the question in my title.

When I build an ML model, the only things I really have a control over are, afaik, the following:

* Amount and range of inputs
* Amount and range of outputs
* Activation function
* Amount and size of intermediate layers

If I can control these parameters, I should theoretically be able to build a model that ""does it all"" depending on them as well as the training data I feed it. Of course, this is not the case: Plenty of people build models from scratch, and even when going for prebuilt ones there's an absolute myriad of models to choose from. Or at least I have not understood how people go about it without building one from scratch otherwise, then.

But then, what is their difference? How do different models come into play, what sets them apart, how do I choose what I want to do depending on my usecase?",MLQuestions,2022-08-19 16:12:54,6
"No need for a super computer, especially for training ml agents.

Why not try some of the ml agent tutorials that come with Unity?",1,wsvcth,"Learning some basic machine learning has been on my bucket list for quite a while, but what can you even do with ML without some form of super computer? 

Ideally, I'd like to try to use it to build game AI. I know that depends on how complex the game is, but what sorts of games could ML handle without mortgaging my house that some monte carlo engine wouldn't also be good at?",MLQuestions,2022-08-19 19:00:43,1
Did you do some EDA? What's the discrimination power of your variables?,1,wsgr60,"&#x200B;

Hello there.

I have a university project where i have to pick a data set and classify it (i can only use linear regression, logistic regression, SVM, Neural networks).

The data set i picked was [https://www.kaggle.com/datasets/sameepvani/nasa-nearest-earth-objects](https://www.kaggle.com/datasets/sameepvani/nasa-nearest-earth-objects)

I have tried so many things (listing below) but i the best results i could get was (precision: \~30%  and recall:\~98% ) and if i change the class weights is (precision: \~70%  and recall:\~10%). Also even if i get good precision and recall it has terrible specificity (usually it get bias towards classifying all as positives)

So far i have tried to

\- balance the data set (down sample or SMOTE)

\- tried different combination of attributes

\- created one new attribute

\- change classifier parameters

\- With or without the realm where the classes intersect (i name it conflict zone because anything other is just one class)

\- Scale the data

Here is the code: [https://www.kaggle.com/code/georgegakis/nasa-neo](https://www.kaggle.com/code/georgegakis/nasa-neo)

I think that the data set can't be classified better but this is just me.

I want to note that this is my first project in ML and i don't know if have done a good job if someone can rate it i would appreciate it.

&#x200B;

Thanks for your help!",MLQuestions,2022-08-19 08:24:22,2
"You don’t actually want a junior level MLE role.  It’s widely considered to be not an entry-level role, for good reason. I’m legitimately not trying to gatekeep here; quite undeservingly I was one of the rare ones that did find a junior level MLE role, I’d already been doing data analytics as a chemical process and industrial engineer for several years prior to that. Most MLEs have several years as Data Scientist under their belt first; the best ones have also spent time as a Software Engineer too. 

It’s a frustrating mix of every day wondering if you’re a data scientist, software engineer, sql monkey, data analyst, or some weird hybrid of them all but just fucking terrible at all of them.

Fuck. That’s my life as a senior MLE now too. Just with more and bigger fucks. 

I don’t know if junior MLE roles should even exist. I feel super indebted to my employer for taking a chance on me; it took me well over a year just to start getting familiar (**not** comfortable) with the backend systems that I’m supposed to make our models compatible with. The lack of SDE experience really has been an impairment. 

My advice would be to try to find a job as a data scientist, or as a software engineer on a team that uses ML in their product. 

Good luck dude. You’ll need it.",25,wryiz1,"So I am starting the process of applying for jobs since I will be graduating soon from a MS program. But there does not seem to be any job postings that have a requirement of fewer than 3 years of experience. Why is this the case? I am kind of worried that I won't get any callbacks since I only have some internship experience.

Is anyone who is also a new grad here also looking for the same roles? What are your experiences like? How is the callback rate?",MLQuestions,2022-08-18 16:41:47,10
"Job guarantee sounds unreasonable, can you imagine how much a shortcut for hardest part of getting into DS/ML field (entry level job) would be talked about or cost.",3,wsh57a,"Are there any Machine Learning or Data Science bootcamp ONLINE where they teach ML or DS and also give a job guarantee?  
How much does such a program cost? It would be nice if you could provide a link with your answer.",MLQuestions,2022-08-19 08:40:37,3
"there are different ways to host pytorch model on aws, such as do you want to deploy using website such as django, api such as fastapi or jupyter notebook such as sagemaker",1,wsabsx,"Hello,
Can someone send me a recent tutorial that works, I have a à PyTorch model (regression) hosted on my GitHub repository and I want to deploy it on azure or aws?
Thank you",MLQuestions,2022-08-19 03:18:34,2
"For learning, the language is less important than the support in place. You can learn ML in any language- there will just be fewer resources/ accepted frameworks and more implementation from scratch. Professionally, ML experience in GO probably won’t help you as much as ML experience in python.",2,ws500b,Just wondering if anyone has or is using GO for ML instead of python or R. I know those two are the go to language but I read that GO looks promising for ML. I’m a big advocate for GO and if i can use it to learn ML. That’s just even better.,MLQuestions,2022-08-18 21:54:54,1
"What exactly do you mean by ai development? Current top of AI research is taking place at places like Deepmind, OpenAI and some smaller labs. I don’t really see how traditional algorithms really applies in this case, if anything a stronger math background is better than a stellar algorithms background. The algorithms used in AI research are mostly math, with some caveats in things like graph neural networks 

But with all that being said, I don’t think either of those paths will take you to an AI researcher position. In my mind there are 2 ways to become a researcher in AI.

Get a phd in artificial intelligence ~ 6 years although not a guarantee to get into a great lab.

Get a job at a place like google / Microsoft or apple. Then work there for a long time, show you have a penchant for data science and ai based work and get a lateral placement onto an ai research team but even then you will probably be closer to an engineer implementing the models and approaches built by the phds.",3,wrrkn1,"Hi everyone,

So currently I am a technology consultant, I have 2 yrs of exp as SAP consultant and developer.
I want to move into AI and Computing research, I know it's a long path, I am keeping that as long term goal ( hopefully ms research/ibm research someday)

I don't know where to start,
Currently I know Java and learning Spring and stack/ ds algo  to land a sde role, and hopefully at decent company so that atleast I can apply algo in real life and know them and try to get into AI slowly. 


Second path: doing ML/ deep learning courses, and landing entry level role as data analyst/engineer and move forward,
But here I don't see much involvement with algo which is crucial for AI, it's math heavy no doubt but more like applied AI, not AI development

So, which would be correct path? I was following the first one, but because of strong association of AI with Data science I am confused, I am also thinking of going for masters once I get some hands on and grab on algo atleast.

Seeking your advice,
Thank you",MLQuestions,2022-08-18 11:54:56,4
"FFT is THE way to go. If you know the desired frequencies you can essentially just take small segments of your sound file, project them into the frequency domain using FFT, filter the undesired frequencies, then project back into the segment.",1,wrxydo,"Hello all, I was tasked with doing the following:

1. Detect different frequencies from an input signal

2. Train an ML algorithm to detect the faulty frequencies

Anyone have any ideas? I’m thinking of doing a fast fourier transform of sorts but not sure how well those apply in machine learning",MLQuestions,2022-08-18 16:16:01,2
"You might find this helpful: https://www.pinecone.io/learn/faiss/

You won't be an ""absolute beginner"" after going through that. :)",2,wrlk3n,"A would like to get similarity results using Faiss. I tried some basic samples but they referer to little chunks of text, like paragraphs or short sentences. See https://gist.github.com/howard-haowen/83874770957f15b84bd069dce0ce6303 .

I could not get good results getting distances very far and unrelated.

But my use case is to get similar documents, for example an entire text of 4/5 pages: how similar is to other documents with +/- de same dimension?
Is vector search the way to go? What pipeline preparations are recomended?",MLQuestions,2022-08-18 07:51:26,2
"Just use a loop the list over tokenizer.encode_plus(). 

You can tokenize everything before you fine-tune the model, so you don’t need to be concerned about it affecting train time.",1,wrcq3f,,MLQuestions,2022-08-18 00:07:42,2
"Try NAS


https://en.m.wikipedia.org/wiki/Neural_architecture_search",1,wr80op,"I've spent the last few weeks catching up on the ML topic and I believe to have a pretty good grasp on the basic concepts. I know how to use Pytorch for simple univariate forecasting, I know how to build the neural net, find the best learning rate, etc. Haven't had this much fun in years of software engineering.

However now, in comes the big project. I'll try to make it as short as possible:

Input data

Around 500 individual timeseries, each has around 500 covariates.

The covariates for each timeseries are 1) computed in a similar fashion, thus also behave in a similar fashion in relation to change of the univariate, and 2) are directly correlated to each timeseries univariate. The exact correlation between a timeseries univariate target and its covariates is unknown (yet present) and the strength of correlation differs slightly between the timeseries.

The timeseries themselves, and their targets, are also correlated to each other: When one timeseries (float) target decreases, it is a reasonable assumption that most of the other timeseries will do too, however, the correlation is very fluid, both in strength across the timeseries, as well as over time, and definitely not a generalized fact, just thought this might be relevant as well.

The dataframe, as of now, is essentially one huge multiindex - the first layer is the timeseries' name, the next layers the univariate that has to be forecasted, and all the covariates. However I can obviously separate this one big dataframe into individual dataframes if necessary.

There might also be seasonality within the timeseries, but its lengths are not determinable, and there might be slightly different seasonalities between the timeseries.

The strongest known, and to be focused on, correlation to the univariate target are the covariates. The direction and behavior of correlation is persistent across all timeseries, the strength might differ.

Goal

Generate, if possible, one single model that allows for imputation of any of these timeseries, or even a different strange timeseries, with the same covariates, and be able to predict n days into the future (whether autoregressive or fixed doesn't matter too much.. I think?)

What I've tried

I've looked at DeepAR and pytorch-forecastings' Temporal Fusion Transformer, but the sheer amount of configuration parameters is absolutely mind-boggling and I am very confused on how to actually design the overarching structure. Can this be done in one model or is it necessary to train individual models for each timeseries? In any of these cases, how do I impute all of this? Which type of architecture and network is favorable in this case?

Thank you very much in advance. Any help and guidance here would be greatly appreciated.",MLQuestions,2022-08-17 19:53:25,2
"In most adversarial models, the idea of minimizing a loss function is a little different.  Unless your loss is 0 or explodes toward infinity, in which case something is wrong, then loss/accuracy will be uninformative. As the discriminator or critic in the actor/critic improves, it forces the adversary to improve.  This push and pull can lead to varying performance metrics while the model gets better at the task.",2,wqvshr,"Hello, I am running a project using Pix2PixHD CGAN. I am struggling to find which package I can produce a plot of the model accuracy and training losses for the 400 epochs I've completed. I'm also not sure what data generate y the model is used to make these plots. 

I'd like to produce the following to validate my model: 

&#x200B;

[Accuracy Plot](https://preview.redd.it/emqd44g9dbi91.png?width=432&format=png&auto=webp&s=e232a894939e543538f6f0ea7fe772a99e1231a7)

[Losses Plot](https://preview.redd.it/kovpgdzgdbi91.png?width=432&format=png&auto=webp&s=088b890524605c33bc9b7c1153fab4a821d623d3)

Any help appreciated, cheers!",MLQuestions,2022-08-17 11:07:50,8
What does ‘get large results’ mean?,4,wqqw2p,"Hey there.

I am working on a small ML project, in which the model takes an image as an input and outputs a pair of real numbers (latitude, longitude) which represent its position on earth. I use the ""great circle distance"" loss (distance on earth surface). My model is resnet18 concatenated with a fully connected network with 512 inputs and 2 outputs (I only train the FC)

However, I am struggling to overfit even the tiniest datasets (20 images) and get large results.

Could it be because:

\- My loss is very sensitive to tiny changes? Isn't a strong model supposed to combat that with enough epochs? Maybe I should transform my loss so that it becomes less sensitive to changes?

\- I'm not using weight initialization?

\- My model isn't big enough?

Thank you very much for helping!",MLQuestions,2022-08-17 07:48:44,18
"Actual job requirements vary so wildly from job to job that it's impossible to make sweeping generalizations about what is ""actually done"" while working as an MLE. However, the experience of building these algorithms from scratch is invaluable. I'm not sure its possible to really understand the strengths and weaknesses of any approach until you've built it up yourself, which includes both knowing the math and implementing it in code. It will help you learn not only the limitations of the method, but also the limitations of popular library implementations as well. The best engineers will use a library when appropriate, but also recognize when an idea has merit beyond what a generic library can sometimes provide.",3,wqtof9,"Although there are many python libraries that make implementation of ML algorithms easier. I have been learning machine learning and alongside, i am trying my best to implement the ML algorithms on my own from scratch in python. Handful of examples include: linear regression, KNN, KMeans, and SVM but I find it really difficult to put all the mathematics into code. When it comes to working in industry, how often do ML engineers implement the algorithms from scratch? Or do they rely heavily on libraries and frameworks?",MLQuestions,2022-08-17 09:39:53,6
"Imo those brand-specific (Azure, AWS) ML courses are good as starting point. But considering you're taking master degree, you should find the brand-agnostic ones so that you won't limit yourself to AWS-specific cases",1,wqqz99,"Hello,

I had a question regarding the AWS machine learning specialty.

I don’t know if this is relevant to my question, but I’m going to start a MSc in Robotics and AI soon, in the mean time I intended to take the AWS MLS to expand my skillset. However, I’m not completely sure if it has any relevance in the ML industry. 

I certainly don’t intend to take it to land a job or anything like that, just to expand my knowledge and expertise. Would love some advice tbh as I’m really torn about it.

Thank you :D",MLQuestions,2022-08-17 07:52:26,2
"Calling the `.fit()` method will overwrite all previous weights/biases/coeffs/etc...

This is fundamentally the same as never calling the previous instance of `.fit()`

If you want to keep those parameters and instead fine-tune on some new observations, look instead into estimators that support `partial_fit()`, which it does not appear that DBSCAN via sklearn supports.",6,wq3xg6,"Hello, I wonder whether the pre-trained DBSCAN model could be fit again or not. I am working on a DSBCAN model and I need to re-train the model with the new scaled data. Does anyone help me, please? I am using the code below.

db = DBSCAN(eps=eps, min_samples=ms).fit(X_scaled)

and the rows below
db = db.fit(other_X_scaled)

Can I use it this way?",MLQuestions,2022-08-16 12:58:00,9
Including the original,1,wpuiwl,"I've trained a custom YoloV5 model and want to fine-tune with new images.  I know I run the same train command, but provide the weight outputs from the first training.  But do I use the entire dataset including the original, or only the new images?",MLQuestions,2022-08-16 06:35:34,6
"where/how you deploy depends on your goal and means. if you have access to a cloud environment and want to practice that part, deploying it in the cloud is a good exercise. If you want to build a microservice, I would go for serving the model with FastAPI, maybe in Docker. If you want to experiment a bit, and make your own frontend, Streamlit is a good option. It handles a lot of things for you, such as a small web server and interactive widgets. take a look [here](https://streamlit.io/gallery) for some examples (source code link is given for every example).


about editors: you will need one for working with FastAPI or Streamlit. not a lot of experience with PyCharm, but it is very commonly used. VSCode is also an option.

hope this helps!",3,wptl2d," 

Hello I am sorry if the question seems silly but this is my first time trying to deploy a machine learning model. So I did all the work from data preparation to modeling and training the models. I want to deploy it but I didn't know how. I have 0 knowledge with django and how to use flask but I found that stream lit is a bit easier. I'm saving a random forest model for a classification probelm. ANywone has an idea on what to do next?

I forgot to tell I am using jupyter notebook for what I already have done I read that I need a python editor such as pycharm for what I want (deployment).

**1 CommentShare**",MLQuestions,2022-08-16 05:53:28,6
`torch.load('path_to_pt_file')`,1,wppkrd,"Hello,
I have a locally saved model « mymodel.pt », the model is saved too on my GitHub, and I want to share it to torch hub.
The idea behind is to deploy it on azure.
Heeeelp please",MLQuestions,2022-08-16 02:16:46,1
"This is obviously really difficult, but the only thing that really comes to mind is a reinforcement learning approach.  Probably is that the reward would be, I would describe as, ""nearly sparse"".  By that I mean, if you have an image, you might be able to use an L2 or L1 loss (assuming you have a good renderer -- are we talking photorealistic or synthetic images?).  But, if you just compare pixels, this might report very large and noisy losses when the result is far away, making it very difficult to optimize movement of parts, and you'd only get a high reward when you are _very_ close to the desired state.  This would make learning very difficult.

Maybe something like using [Hindsight Experience Replay](https://arxiv.org/abs/1707.01495) could help though, where the value function starts to learn the mapping between where the pieces are placed and what that looks like, helping it to understand how to achieve the final image.

Another idea is to try to make use of the spatial distribution of parts -- if one part is close to where it should be, it would be nice to take advantage of that.  Something you could try is maybe threshold the L2 loss per pixel before taking the mean.  This might allow it to report 1 in locations where parts are close to where they should be and 0 in patches of the image where there is no match.  But if some parts are similar to each other in colour then I can imagine this might lead to local minima, so ymmv.

With that second approach, you could also just start off trying genetic algorithms instead of diving head-first into RL.",1,wpnkd2,"Given a data of 3d parts, I want to be able to assemble a 3d object from a picture model using these parts. How can I achieve this?",MLQuestions,2022-08-16 00:10:46,1
"The simplest way is to use tailscale as the VPN and ssh to control the box.  

If you want to get fancier, my current project has a webapp that is running on AWS.  It is using node-proxy-manager and tailscale to get back to my workstation/DB/NAS.

This gives me a lot of the benefits of have a GPU instance in AWS without the expense.",5,woz4t3,"I am soon going to be away from my own workstation for 2 weeks and I would like to be able to still access it remotely from a laptop. My workstation is running windows and has wsl ubuntu, it is in my private home network. My laptop is ubuntu. 

I usually develop in a docker container so maybe through a jupyter lab of that docker + cloudflared tunnel?

Any better ideas?",MLQuestions,2022-08-15 06:02:15,7
"I like the Wikimedia data sets:

* For text: Wikipedia - https://en.wikipedia.org/wiki/Wikipedia:Database_download
* For images: Wikimedia's ""Quality_images"" category - https://commons.wikimedia.org/wiki/Commons:Quality_images
* For structured/graph data: - Wikidata: https://www.wikidata.org/wiki/Wikidata:Main_Page

They're each big enough that it'll take a typical desktop hours but not weeks to churn through.",11,woldzv,I am taking the stanford coursera course but i feel like there isnt enough hands on coding in the course so i would like to try to make my own programs so that i can fully cement the concepts taught. The issue i am running into is i dont know where to find large data sets. Can anyone recommend where to find some data i can use for practice? Thank you.,MLQuestions,2022-08-14 17:37:17,15
"If you try that with Elvis Presley or Marilyn Monroe, https://en.wikipedia.org/wiki/Authentic_Brands_Group may sue you.",2,woktk3,"If I took a deceased famous person, who made a living using their voice. Trained the AI which there would be tons of material. Then created a promo using that voice to narrate a program. Would I be violating any laws? Has there been any lawsuits of such a similar act?",MLQuestions,2022-08-14 17:10:16,2
"This is a major, active and ongoing research question in data analysis. The answer varies case by case. You could find a way to plot your data and find a ""representative"" snow but there's not likely a quick answer.",2,wocpf2,"So I have a set of \~400 rows with several features, however these rows are quite painful to get. Every once in a while I'm re-querying the data and re-training the algorithm, discarding the old data. However, I'd like to find if I can query less rows: in a sense find a minimal set of data that would still give similar predictions. (I have a several previous sets that  can use for validation).

Any ideas? 

Thanks!",MLQuestions,2022-08-14 11:09:30,5
Could you be remembering Tononi's qualia?,2,wolcq2,,MLQuestions,2022-08-14 17:35:35,2
"I'm guessing you have 971 different tokens? The last layer of your model has V outputs, which is the same as the input dimension. You want 4 outputs if you have 4 classes.",1,wo4vrz,"I have a model that looks like this (90% accuracy):

    cbow = Sequential()

    cbow.add(Embedding(input_dim=V, output_dim=dim, input_length=window_size*2))
    cbow.add(LSTM(dim, return_sequences=True, input_shape=(dim, 30)))
    cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(dim,)))
    cbow.add(Dense(V, activation='softmax'))
How can I now input a sentence into this Model to see how it judges it? (like the first sentence of x_test or something). The data went through a Tokenizer first so I understand that I cant just input a String text (like model.predict(""text sentence?"");)

There should be 4 Labels that new text could be classified as, but I can't really get the Model to output any label in text form.

And I wanted to create a confusionMatrix to show how accurate the labels where and which were confused with which, a classical 4x4 Matrix. But my model has output matrix of 971x971 of just numerical values when I try it with this code:

    y_pred=cbow.predict(x_test) 
    y_pred=np.argmax(y_pred, axis=1)
    y_test=np.argmax(y_test, axis=1)
    cm = confusion_matrix(y_test, y_pred)
    print(cm)
Idk I guess I am pretty confused overall on how to use my Embedding ""Continuous Bag of Words"" Neural Network once it is trained. Any help is very much appreciated. Thanks.",MLQuestions,2022-08-14 05:08:23,3
[deleted],9,wnddiq," Hey everyone, so I was wondering since ML training models already exist and the algorithms already work on their own.Doesn't ML become a bit tedious since we'll just keep using what's already established? In other words where does your creativity come in to play when working on projects?

PS: I'm new to ML so this could just be my lack of understanding of it.",MLQuestions,2022-08-13 05:27:41,6
\#AIForGood,43,wmn3xw,"I've collected over 1500 fart audio files in the .wav format. I'll be collecting many more as time goes on. I'm building a mobile app for fun that does something on a ""wake sound,"" similar to how Alexa and Google Home do something on a ""wake word."" I'd love to make my fart dataset available to anyone who may find it useful for audio tasks. Who or how would I reach out to do this?",MLQuestions,2022-08-12 07:48:15,25
Check out [Valerio Velardo's YouTube channel](https://www.youtube.com/c/ValerioVelardoTheSoundofAI) for a great selection of resources and project ideas in the field of AI Music,3,wnffej,"I been planning to make a model of automatic music generation. I need some inspiration. Do you have any Github repos in mind if so, please share it with me. BTW I been doing this with GAN but have some difficulties. So if there is some repo available I can figure out what should I tweak further. 


Thanks.",MLQuestions,2022-08-13 07:10:49,2
Stratego might be an easy start.,3,wnaqxz," I am looking to create a game of imperfect information.

It is turn based.

You take turn from a series of N possible moves

The enemy takes turn from series of N possible moves.

The goal is to have the biggest score.

Where do I even get started ?",MLQuestions,2022-08-13 02:53:10,3
"> I’m using a genetic algorithm, population size 52, 4 layers, relu thingy. Think that’s all you need to know. Thanks

Lol. Are you evolving neural networks? 

But yea as the other commenter said, you are framing the problem as a classification problem. You need to frame it as a regression problem. That will require changing your algorithm outputs (extremely unclear what your algo is from that quote) and the loss function associated with your algo. 

You can still use genetic just the outputs of it has to be a single value and not a distribution. And your loss (or in the case of genetic algo fitness function) has to be MSE or L1 instead of cross entropy or whatever you're using now.",5,wnfakq,"So my current AIs return 1 of 8 (for example) possibilities, but I’d like it to predict something like ‘number of points in next fantasy football week’. This will be a continuous value like 6.55.

I could have a large number of inputs really, like 1-15 and this would cover it, but I’d like the AI to spit out a continuous figure.

How is this possible?

I’m using a genetic algorithm, population size 52, 4 layers, relu thingy. Think that’s all you need to know. Thanks",MLQuestions,2022-08-13 07:04:28,3
"It really depends on how fast you're taking images. I've never bench marked mask rcnn on a pi, but if it is too slow for your needs you can look at the Coral USB accelerators (out of stock everywhere rn). 

Might as well get it up and running and find out how long it takes honestly. Then grab a NVIDIA Jetson dev board, a Google Coral dev board, or an accelerator if it doesn't meet your needs. 

If you do benchmark it, post it on reddit or Github, that'd be good data to have out in the world.",1,wn6fjz,"I am starting on a project involving instance segmentation using Mask R-CNN and I'm planning to deploy it on a Rasperry Pi 3 board. The training would be done on a more capable machine with a GPU and a custom dataset.

I am **not** going to be doing instance segmentation on a video feed (as a lot of other projects online have been trying to do). I just want my Pi to capture an image and process the captured image using Mask R-CNN. However, I am not aware how fast can the Pi process a single image? Does anyone know a rough estimate how long it would take the Pi to do instance segmentation on one image?

I've read something about quantization but I still need to study it. Is it also possible to perform quantization on a trained Mask R-CNN model? I am worried that I could run into problems because as far as I know, Mask R-CNN is not included in Tensorflow's standard libraries so I am not sure if quantization is an applicable solution if I find the Pi to be too slow on processing single images.",MLQuestions,2022-08-12 22:26:59,6
"You could use your current model and freeze the weights of the first few layers and make a separate input chain (input->dense->dense as an example, make it similar to how the inputs you're freezing look like) and concatenate it somewhere later in the network at which point the weights will have to be set to trainable.  
I'm not sure if this will work, but since you're problem goes in the direction of transfer learning this is what I'd try",3,wmq871,"Initially the agent is trained to play in game A, and in game A, agent uses sensors A1, A2, A3 to get information to take action. so input to the model be like \[A1, A2, A3\]

and then, agent has one more sensor A4, and sensor A4 is expected to give the agent more useful information, and input to model be like \[A1, A2, A3, A4\]

so besides retraining the model from scratch, is there any way to reuse the trained weight with input = A1, A2, A3

it's something stuck in my mind.

thanks for reading. a keyword to previous work that solved this is okay, i can read it.",MLQuestions,2022-08-12 09:57:23,2
If you want to study machine learning I'd say python is helpful because there is a great ecosystem of python ML libraries. Also great data wrangling libraries like Pandas and tooling like ipython/Jupiter notebooks. Would suggest building something with it and learn by doing!,2,wm832h,"Hi, I want study about machine learning for hobby. I tried to find the tutorials and most of them are in Python.

I want to know how many the advantages Python has compared to the C++. I'm not very good at it, but it would save a lot of time if I don't need to study a new language.",MLQuestions,2022-08-11 18:22:29,3
"[https://github.com/NVIDIA/pix2pixHD/blob/5a2c87201c5957e2bf51d79b8acddb9cc1920b26/options/train\_options.py#L3](https://github.com/NVIDIA/pix2pixHD/blob/5a2c87201c5957e2bf51d79b8acddb9cc1920b26/options/train_options.py#L3)

literally, just change your command option --niter to whatever you want.  Are you sure u check the code?",1,wm1zht,"Q1 - I've successfully ran a couple of Pix2Pix training sequences, but I can't seem to find where in the package I can alter the training to run for more than 200 epochs (Standard number). It doesn't seem to be in the base settings or those for training. 

[https://github.com/NVIDIA/pix2pixHD](https://github.com/NVIDIA/pix2pixHD)

Q2 - Also, I'd like to know how to plot a graph of the training losses for a model that I've finished training. Would this be a Pytorch function? 

Any help appreciated!",MLQuestions,2022-08-11 13:54:10,2
Is your CPU being fully utilised?,1,wlvzgt,"I hear of people doing millions in a day.

I already use numpy/vectorised operations, random sections of 4000 lines out of my overall dataset, feature length is 50 ish, 4 hidden layers, population is 52.

Perhaps they use multiprocessing to split the workload of the population?

Perhaps there are other methods of machine learning that are faster? I never understood how other learning algorithms can generate an equation based on inputs and outputs, which seems to be required for things like Adam Optimiser when using tensor flow.",MLQuestions,2022-08-11 09:42:03,13
"Hello,  can you share some exemples and what you get ? do you need it to be fast ? easyOCR usually gives goods results easily (tesseract needs a lot of pre processing). A bit harder to install but worth it, you can try mmOCR :   
https://www.youtube.com/watch?v=WAILBMNuMSI  
https://www.youtube.com/watch?v=g8oRZ7IPXQw",2,wm4pwm," 

Hey guys, I'm trying to build an ANPR/ALPR with ML but I'm having trouble doing the OCR part.

So I used this [license\_plate\_detection | Kaggle](https://www.kaggle.com/code/aayusmaanjain/license-plate-detection) as a reference for my work, I got the License Plate detection part done, and the model is locating the license plate position very easily.

For the OCR part I first tried Tesseract and then EasyOCR but its lacking alot in some number plates, and not detecting the characters as I want. I wanted to try to use [keras ocr](https://keras-ocr.readthedocs.io/en/latest/), but I can't figure out how to do it. I also want to try with a ML model trained by me but then again, I can't find examples or how to do it, is there anyone that could help me with that?

This is my first contact and hands on with a ML project so I'm learning as I'm doing, any information and help is welcome.

If you have any question also throw them.",MLQuestions,2022-08-11 15:49:39,5
what kind of a project is it?,2,wlnm62,"While I have ML, deep learning, and computer vision backgrounds, I haven't applied them yet. Starting a project with a buddy would be a great idea. If you are interested, let me know in the message.",MLQuestions,2022-08-11 03:00:08,4
"With boosting, there's no way to control the range of the predictions. Best way is to set

    y_hat = max(y_hat, 0)

You can also predict the log of your target, fit your model, and then take the exponent of your predictions, but that normally causes biases, if that matters, and isn't theoretically sound.",2,wlytsu,"I'm not sure if I should be specifying something for this, but regular regression (in XGBoost, the learning task = ""reg:squarederror"") allows for predictions in (-inf,inf). I know for certain that my outcome variable *cannot* be < 0. I'm evaluating the model's performance using root mean-squared error, so I didn't think it'd be a problem to have all of the predictions in \[0,inf), since whether the prediction is -0.1, or +0.3, if the true value is 0.1, the RMSE is the same.

Is there something I should set to ensure that the predictions are all within the actual domain of the outcome variable? Thanks for any advice you may have!",MLQuestions,2022-08-11 11:41:44,5
"I hear you. Sometimes you try and fix the problem and end up ""spelunking"" - where you're at least three levels removed from the problem you're trying to solve. e.g. 

> pip install RubeGoldberg 

... failed: RubeGoldberg requires TKGraphics

>  pip install TKGraphics

TKGraphics install failed: TKGraphics requires libx11v09.so, libx11v09a.s0 found instead.

> make install libx11v09 ....fatal error 1.0.

Eventually you either give up or plot the murder of the developer who made a specific heavyweight library as a dependency for what would otherwise be a straightfoward function....",4,wlms5a,"Maybe it's because I got older, but I stopped caring about repositories that throw error messages.

I became a typical Apple/Windows user, if it doesn't work right from the start, I dismiss it.

Sure, the authors don't care about me installing it, but this is for my fellow ml hobbyists, who also realized that debugging error messages for days in someone else's code just to try it out is not worth their time anymore.",MLQuestions,2022-08-11 02:05:26,7
CNN Algorithm,1,wlrrg3,,MLQuestions,2022-08-11 06:44:45,1
https://colab.research.google.com/github/LibreCV/blog/blob/master/_notebooks/2021-02-13-Pix2Pix%20explained%20with%20code.ipynb,1,wlo5im,"hello everyone, i want to use pix2pix but i have no knowledge of programming but i have gpu and cuda, someone can help me out?",MLQuestions,2022-08-11 03:33:22,3
[deleted],9,wkxbpv,"I know that it possible to confuse image recognition algorithms by changing minor details in a photo that are not even noticeable by humans. I wonder, is it possible to do the same with music recognition algorithms, like Shazam? If so, how do I make a  piece of music unrecognizable to it without making it sound any different to human ear?",MLQuestions,2022-08-10 06:21:41,10
ML is likely overkill for this. You might be well served by writing some regexes that capture things like slot or channel patterns,5,wlbdkr,"Hi, I'm new to Machine Learning, and I'm having a hard time figuring out how to solve this problem. So I have a dataset that is mostly comprised of string and integer relationships, and I need to sort information based on that data.  for example, a string would be Ga2C76GP22, and I would need to extract things like Slot 2, Channel 76, Pin 22, and Type Digital.  

the problem is this data is inconsistent and there is a lot of it. so from one source Pin would be labeled as IO instead of GP or slot would be the word SLOT. this input cannot be changed from the source and I would like to automate most of this process.  I have a lot of already created data to help train the model.

I was wondering if there was a good machine learning algorithm that could help associate internal string data and relationships with desired outputs. 

&#x200B;

Background - I have a BS in Electrical Engineering and I work as a Software Engineer",MLQuestions,2022-08-10 15:53:53,6
"Your observation is correct. What you are seeing is [Applied Mathmatics](https://en.wikipedia.org/wiki/Applied_mathematics)**.**

edit: Added words.",2,wl8hx8,"I keep seeing classification methods that are basically the same stuff as conventional math like when I was starting to learn about ML and the first topic was the perceptron algorithm which was basically the cross correlation formula with a new label. Then learning about gradient decent which is a relabeled optimization problem that is approximated instead of solved, abd this keeps happening over and over, a big array of cross correlation between input and weights called a neural network, then newtons method of approximation, then Jacob's method for approximating matrices, and most of the topics are just relabeled advanced but traditional topics in math. Am I stupid or is my observation correct.",MLQuestions,2022-08-10 13:53:14,2
"I'm building an annotation site right now, I haven't built support for key points yet but it is on my radar. I haven't done key point annotation before so this post is super helpful. If you don't mind, I'd love to get some requirements from you.

Does something where you define labels for points (e.g. right elbow, left elbow, center of chest, etc.) and define which points connect to which (when present, e.g. right elbow -> right shoulder) sound like it would work?

What value is the skeleton? I figure if you have the points you can derive a line between them pretty easily.

Do the ones with a skeleton just put the whole thing on the screen and you manipulate the points? (this seems kind of dumb lol)

Thanks! This will help me build a solid tool. I don't think I would have considered connections initially. Any other ideas you have would be appreciated!",2,wl0jo8,"I have some images I want to annotate and I cant seem to find a decent tool to annotate my images on. 

I just need a tool that can:

1. annotate keypoints
2. allow me to link joints to a skeleton
3. either autoselect next joint or draw skeleton
4. allow me to remove joints in the event they are not in the actual image/out of frame

I have tried several tools including:

datatorch: Almost exactly what I need, except I cant seem to edit the individual joints?

cvat: Nice project, but I cant seem to create joints? I can make a label, tell it how many joints, but nothing is connected

Labelstudio: Cant seem to get joints to connect to each other. Clunky setup

[Makesense.ai](https://Makesense.ai): nice interface but points dont connect.

universaldatatool: Interesting tool, actually has a nice skeleton tool, but cant delete joints or flip the skeleton over

&#x200B;

if anyone knows of a good tool or how I can do what I am trying to do feel free to comment.",MLQuestions,2022-08-10 08:36:43,2
"You didn't state what ecosystem you're using or what you're actually doing on the training process, so i'll assume you're fine-tuning with transformers.

1. Run inference on your validation set. If you're using transformer's trainer class, you'd create test\_args that you can then feed to the trainer:

`test_args = TrainingArguments(output_dir = OUTPUT_DIR,do_train = False,do_predict = True,per_device_eval_batch_size = BATCH_SIZE,dataloader_drop_last = False)`

If you're using pytorch without the Trainer, then you'd do a model.eval() loop. Each framework has its own inference loops. Once you have your model's validation predictions, you can calculate accuracy and F1 using the ground-truth labels

2) BERT learns a lot in its embeddings: the 'Bertology' paper ([https://arxiv.org/abs/2002.12327](https://arxiv.org/abs/2002.12327)) provides a great in-depth look at some of the broader linguistic traits that BERT learns. Different layers often learn different patterns, so the embeddings aren't really interpretable, but you can use something like bertviz ([https://github.com/jessevig/bertviz](https://github.com/jessevig/bertviz)) to explore attention weights across layers for predetermined examples

3) I can't comment on this without having seen your code.",1,wkygmw,"i am working with bert for relation extraction from binary classification tsv file, it is the first time to use bert so there is some points i need to understand more?

1. how can i get an output like giving it a test data and show the classification results whether it is classified correctly or not?
2. how bert extract features of the sentences, and is there a method to know what are the features that is chosen?
3. i used once the hidden layers and another time i didn't use i got the accuracy of not using the hidden layer higher than using it, is there an reason for that?",MLQuestions,2022-08-10 07:10:51,1
"What I would do here I think is turn the rankings into a set of pairwise comparisons (a > b true or not), either per expert per example, or aggregated over the experts and/or over the examples (by summing the number of times a given example or feature is preferred over another, giving a proportion.)

This should allow your features to remain more ""stable"" and free from arbitrary scaling issues with respect to the rankings, being simply a set of binary relations or proportions that you regress onto y.

Your problem seems related to [stochastic transitivity](https://en.wikipedia.org/wiki/Stochastic_transitivity).  You can look at it from a Bayesian point of view as the set q of binomial probabilities of a greater-than relation between all pairs of examples, conditional on y, p(q|y) and you are trying to infer p(y|q) using Bayes rule.  In other words, given y, can you predict the outcome of a random sampling of ""a > b"" for all a and b.

So, I would suggest that your problem is a multivariate version of trying to estimate the bias of a coin, ie., the probability of a coin flip turning up heads.  A nice write-up on [doing this with MCMC is here](https://revbayes.github.io/tutorials/mcmc/binomial.html).  In your case the bias is y (and possibly the time period? so, y[t]..), the coins are your experts and examples, and the relationship could be non-linear.  Likely MCMC is overkill and maybe your problem can be solved by least squares regression or some other regression model (splines, gaussian process, neural network, etc, etc), but I give the link since the theory there might be interesting for understanding how likelihoods etc play into it.  (Recall that linear regression is a type of maximum likelihood estimation, with least squares being merely an efficient way to solve the linear MLE problem.)  In any case, whatever model you select, remember that benchmarking against linear regression can be an extremely useful exercise.",2,wkxqm9,"I am working with a dataset where, for a given time period, there are 100 examples (x , y) where y is a continuous variable (eg 0-50.0). For each period, there are 200 experts (x) who predicting the rank order of the examples in terms of y (eg one expert may predict y3 > y8 > y98, etc). These rankings are based on some outside information. I am treating the rankings as features. I have many time periods of data. 

I am predicting y (continuous & non-parametric) based on its associated expert rankings. That is, based on expert rankings, I do not only wish to predict whether y0 > y1, etc. but how many points is y0.

Can anybody please point me to some helpful papers (or anything) on this topic. I’m not sure what jargon is best to describe this problem, which is making Google Scholar unhelpful.

This problem seems general enough that there should be papers on it? Or am I wrong?

Thanks",MLQuestions,2022-08-10 06:40:11,4
CNN Algorithm,1,wkx30c," I wonder, if there is any research regarding the geolocation problem?  

In other words we have a map(perhaps 3D model) of some area and a photo from that area as an input for an algorithm to try to infer a location on the map where the photo most likely is taken from.  

Are there algorithms that show good results? What's their prerequisites?   Feel free to share papers you deem might be of relevance.   

Thanks!",MLQuestions,2022-08-10 06:10:59,1
"If you have poses, then you have joint angles.  Your joint angles form a multivariate time-domain signal.  (A set of joint angles changing over time.)

From these, you can calculate the distance to different ""gestures"" using a variety of methods, and the smallest distance may be your target gesture.  You can use Euclidean distance, or for example if you want to be less sensitive to the timing of the gesture, you can use a distance measure like [DTW](https://dynamictimewarping.github.io/).  You can also include joint velocities or accelerations in the signal for example, which might help with 'kick"" motions.

You can use such a distance measure directly for comparison, or you can use it as input to a clustering method like K-means.  Another option is to use time series classification based on a large dataset of gestures, though this may require a significant amount of data, so I would go with the similarity approach first.

Depending on what you are trying to recognize, it might also be enough to just recognize some extrema of the signal, points where position and/or velocity are at a minimum and maximum, and use these as features, without considering the whole time-based motion signal.  This can be more robust and require less data, but is a bit more heuristic.",2,wkj784, Hello guys . Currently I know if we want to detect poses we can store the poses landmarks on a csv file for each label and then train the model to estimate to which label does your pose belong to . But say we want to identify a label which involves multiple poses like a kick  .Does that mean we have to divide that kick into multiple poses and if the player completed the 1st pose we will check the 2nd pose and so?,MLQuestions,2022-08-09 17:24:52,1
What is data score?,2,wkdvdo,"I used data augmentation to enlarge my data set, as it makes the trends I observed appear clearer. I obviously will mention this in my methods section, and the data score was high (0.85).

Is this a good idea?",MLQuestions,2022-08-09 13:31:36,5
"Don’t be fooled by taking the highest accuracy model. Since your decision tree is a single estimator It’s likely overfitting the data. I’d be inclined to take the results of the random forest.

Some things you can do to further test the robustness of your models without tweaking the parameters is 

1. Vary the test set size
2. Run k-fold cross validation on splits of the data 

I’m willing to bet the random forest has better generalized performance once it’s tested over various splits of the data.

Finally, does accuracy matter for your test case? Many ML problems will have an objective to maximize precision or recall.",16,wk4nra,"I'm using scikit-learn to create several binary classifiers on a data set [https://archive-beta.ics.uci.edu/ml/datasets/car+evaluation](https://archive-beta.ics.uci.edu/ml/datasets/car+evaluation). 

After feature engineering and a train test split of 70-30, I have X\_train set of  1209 observations of 15 features which are all dummy variables (with drop\_first = True), and X\_test contains 519 observations.

After training a decision tree classifier (with no parameter selection) I get an accuracy on the test set of   95.38 %. After using GridSearchCV for the ccp\_alpha parameter, I get a new model with 97.5% accuracy. Very Nice!

After training a random forest classifier (with no parameter selection) I get an accuracy on the test set of   95.18 %. After using GridSearchCV for the n\_estimators  parameter (going through 100, 1000, 5000) and using the previous found best ccp\_alpha = 0.001 with bootstrap = False, I get a new model with 95.57% accuracy. 

I was expecting the random forest to be more accurate on the test data but it isn't. Is this a reasonable outcome or does my parameter selection and range need some more work? I've tried several different parameter grids and this one is sadly the highest performing.",MLQuestions,2022-08-09 07:24:49,4
"- Change model to vision transformer might help
- If you still intended to use conv-based backbone, try adding Spatial Transformation Network (STN) to “crop” only relevant region -> invariant to 2d affine transform (scale, rotation, and translation)
- Learning augmentation policy is studied some time ago with the work of “AutoAugment”, but I think “RandAugment” beat the former in practicality. For a more recent one, look into “[AugMax](https://youtu.be/kQ09eg513Nc)” (and maybe AugMix). But you can use official torchvision implementation directly too (the most recent one is TrivialAugment)
- You can try other augmentation methods such as mixup, cutmix, manifold mixup (interpolate representation vector rather than output image, this regulate class linearity - one of the trick behind StyleGAN), or mosaic (at least in heatmap output -> YOLOv4)
- Also check if the data is long-tailed (imbalanced class), if so try changing loss function to something like focal loss",2,wjxq9j,"Hi everyone, I'm working on a ML project for my university.

The requirement is to to train a Deep Learning model to solve an image multiclass problem which consists of identifying the breed of the dog in a given picture. The model can be builded from scratch or we can use some already existing code and try to improve it, showing the improvements with resepct to the original code. Due to lack of time I've decided to go for the second option, so I took the code from this \[kaggle\]([https://www.kaggle.com/code/gabrielloye/dogs-inception-pytorch-implementation/notebook](https://www.kaggle.com/code/gabrielloye/dogs-inception-pytorch-implementation/notebook)) and i tried to improve in several ways. As you can see, it's entirely based on Pytorch.

I was able to reach a decent accuracy until now: 85% on Validation, \~96/97% on Training and \~85/86% on Test (as you can see in the kaggle the Test accuracy achieved by the user was of 79% on Training, Validation and Test sets).

These have been my changes until now:

* I still use the pre-trained Inception V3, but I thawed the layers and got an improvement. I read somewhere that it is sometimes useful to thaw only some of them, but I didn't try it because I wasn't sure.
* I'm still using the Fully Connected NN Classifier after the feature extractor a simple Linear+Relu+Dropout+Linear+LogSoftmax. I tried some different configurations, increasing the number of layers or changing the operatons in the layers, but i got a far worse accuracy.
* I'm still using as loss function a NLLLoss.
* I've changed the optimizer from Adam to AdamW and later to SGD to improve accuracy (and it did a little):  optimizer = torch.optim.SGD(model.parameters(), lr=0.1, weight\_decay=1e-3)   (In the beginning i was using Adam)
* I've started using a LR Scheduler: scheduler = torch.optim.lr\_scheduler.CyclicLR(optimizer, base\_lr=1e-3, max\_lr=0.1,step\_size\_up=5,mode=""triangular2"").  This one too improved the accuracy by a little.
* Still Early stopping as stopping criterion to avoid overfitting.
* Augmentation/Transformations: I only added a RandomVerticalFlip, because it seems to be the only increment among those I have tested that does not worsen the accuracy.

&#x200B;

I've tested some schedulers and learning rates, different types of online data augmentation (not with every parameter) and also offline augmentation (which caused overfitting), so I don't know what else to try to improve.

Could someone give me some suggestions to further improve validation and test accuracy?

Thank you very much to anyone who has read, sorry for my poor English",MLQuestions,2022-08-09 01:19:00,2
"https://www.deeplearning.ai/courses/machine-learning-engineering-for-production-mlops/

https://arxiv.org/abs/2205.02302

some stuff I have bookmarked, hope it helps",2,wjaqo8,"Hello,

I have decent knowledge of data science and ML algorithms and I'm interested in learning and understanding MLOps and Platform Engineering. Can someone please guide me on the resources for the same?

I'm trying to learn about unit testing in ML, data ingestion, orchestrations, deployment using kube, model drift and retraining etc. 

Thanks!",MLQuestions,2022-08-08 07:49:05,3
"you could try Jina.ai's CLIP-as-a-Service: https://github.com/jina-ai/clip-as-service

> I would like to reduce the computational and memory footprint as this will be integrated into a web app.

an easy way to achieve this would be to offload all of this inference to a webservice, e.g. you could sign up for huggingface's inference API and then you could get embeddings from any of their public models by just sending an API request. You also get to take advantage of their economies of scale, so it would probably be cheaper per-inference to use their API than to spin up your own inference worker if you don't already have one paid for. https://huggingface.co/pricing",3,wj4rbh," For a web project I need to compute the similarity between small text paragraphs (up to 1000 words) or even between a small set of keywords (up to 20 words).

I tried huggingface transformers with sentence transformers, model ' all-distilroberta-v1', while the quality of the similarity was very good it was very slow and it uses a lot of memory. It uses 768-dimensional vectors internally to compute the similiarity.

**First question:** Where can I find smaller transformer models?

**Second question:** Are there other libraries that are better suited for this kind of (rather simple NLP) task?

I would like to reduce the computational and memory footprint as this will be integrated into a web app.

Many thanks",MLQuestions,2022-08-08 02:47:24,6
"Could be that the model was too large and complex for too simple of a task, which would make it prone to overfitting.",2,wjab44,"Hey, I'm doing a coordinate regression task for an object.
something like [this](https://i.ibb.co/Rzzy0hd/dot2.jpg)(not exactly this ), where the label would be (17,40).
So, when I'm using Resnet50 I'm getting poorer results as compared to Resnet34.
Can someone explain me why it could be happening?
Resnet50 despite having more parameters has 1x1 convolutions, does this hurt in my case?",MLQuestions,2022-08-08 07:30:49,1
https://commonvoice.mozilla.org/  ?,1,wjfl2f,"Hi, i am looking for a way (website or open source) where i upload text sentences and distribute the link among people, user record the audios i can see how many audio each user recorded and then i can download text and audio.

Thanks",MLQuestions,2022-08-08 11:01:19,1
"I think the callback is for saving a checkpoint, at any time you can use model.save()",3,whl6mv,"Hi guys,

I'm developing a project that will identify if an individual is wearing a face covering/medical mask. I've run into a couple of problems when trying to save my model. I'm using tensorflow2.0 on google collab as my machine does not have the required nor supported GPU.

I'm using object detection and the neural net that I'm using is ssd\_resnet50\_v1\_fpn\_640x640\_coco17\_tpu-8.  


I'm following this ( [https://colab.research.google.com/github/tensorflow/models/blob/master/research/object\_detection/colab\_tutorials/eager\_few\_shot\_od\_training\_tf2\_colab.ipynb](https://colab.research.google.com/github/tensorflow/models/blob/master/research/object_detection/colab_tutorials/eager_few_shot_od_training_tf2_colab.ipynb) ) notebook as a guide. From my research, I found you can save your model after each epoch using a callback function, but If you look through the notebook at the training stage, I cannot find the .fit() method in which to specify the callback.  


To expand, in this notebook I can't find the .fit() method but also I can't find the method in which the neural net is defined and compiled, from what I've seen a model is normally created using a method like below.

`tf.keras.Sequential( # define layers of the neural net in here)`

&#x200B;

but I can't seem to find this method above where we initialise the model. Also in the notebook, I can't find the line where we compile the model neither, after saying all this... The model still works and correctly identifies the object in question. How come in this notebook it's missing the definition, compilation and fit method and yet the model still happens to work?  


Any advice and/or explanations would be appreciated, Thanks",MLQuestions,2022-08-06 03:29:01,4
University level can be done on CPU..,11,whpzdg,,MLQuestions,2022-08-06 07:44:50,6
https://www.tensorflow.org/lite,2,whhszp,Have anyone used [MNN by Alibaba](https://www.yuque.com/mnn/en/about) on IoT devices? I wonder if there are any other alternatives (AWS IoT Greengrass).,MLQuestions,2022-08-05 23:49:24,1
"Welcome to the field! I’m really confused by what your approach- what are you histogram-ing and why? Are your images in color or black-and-white? What’s the actual dimension of the data you’re inputting into k-means?

More broadly, if you want to cluster images, the standard approach would be to run feature extraction with a pre-trained image model and then clustering the embeddings with something like HDBSCAN or flat hierarchical clustering.",3,wgrtej,"I’m feeding the model 10 bins histogram for around 1500 images(each image has 10 bins). With assignment of 3 clusters I get different results each time, which I believe the cause is curse of dimensionality. 
Excuse my English and lack of better explanation for this issue, I’m new in this field :)",MLQuestions,2022-08-05 03:22:01,20
"I am bot sure if I misunderstand your question.
But in my perspective all ML projects demand some kind of data visualization.
If you wish to work a lot with data visualization, I would look for time series data or a classification problem.",2,wh03el,I am final year cse student . We have to work on project for our final year . I want to make project in machine learning domain along with it want to use data visualization . Can anyone give any idea where i can use both these both and if there is any tips or advice you can give me related to it ?,MLQuestions,2022-08-05 09:40:32,4
"heteroskedasticity prediction?  or maybe you can just use 3 softmax final layers, and ask them to predict the three outcomes, but you have to be careful about the loss function, as they can be permuted, and still get the correct answer

Another possible solution, is to use a weighted loss... if you have 3 labels, and they are all different, probably if the NN misclassifies that sample, is not a big deal, but if the 3 labels you have are the same, better the NN get that right

in that case, you have a easy life, you just need a single output softmax layer, and in the fit method of keras you can pass weights (you can calculate them as `1/len(set(your 3 predictions)))`, or something along like this",1,wgz09c,"To be clear, I am *not* asking about multi-label prediction. Though I'm having trouble figuring out what this is exactly called because all my search results lead me to multi-label prediction.

This isn't what I'm doing but take image recognition as an example. Say we have 3 classifiers (or people) that identify a photo of an animal. I'm trying to build something that takes in the image and predicts a single output class with some certainty - but the catch is I want to use the 3 labels for training, regardless if they agree.

Example:

image 1 - labels: cat - cat - cat

image 2 - labels: cat - dog - rat

image 3 - labels: cat - cat - dog

  
During training, its obvious that 'cat' would be the label used as a target variable for image 1. But for image 2 any could be passed into the loss function? I can't measure which classifier is better so do I pick from them uniformly? Another important question could be: Is image2 actually three samples that should be used to train the network? i.e one X with three Y? If so then further would image 3 be two samples or three as well? Another thing could be to weight the sample based on the level of agreement but the question of multiple is still there.

I'm looking for advice on how the batching/training loop would run; I'll likely attempt some Bayesian stuff via dropouts but I truly can't find anything on what this method of batching would be called.

&#x200B;

Thanks!",MLQuestions,2022-08-05 08:55:17,2
using acturial techniques might be good for forecasting or evaluating a portfolio/subset of customers but i dont think it'd be good for predicting each customer,1,wgtt58,,MLQuestions,2022-08-05 05:07:30,2
"Yes.  

What you describe works well.

These guys found it even better to preprocess and pass in ***multiple color spaces as inputs at once***. 

https://arxiv.org/pdf/1902.00267.pdf

>> ## ColorNet: Investigating the importance of color spaces for image classification
>>
>> ... We explore the importance
of color spaces and show that color spaces (essentially transformations
of original RGB images) can significantly affect classification accuracy.
Further, we show that certain classes of images are better represented in
particular color spaces and for a dataset with a highly varying number of
classes such as CIFAR and Imagenet, .... 
Also, we show that such **a model, where the input is preprocessed into
multiple color spaces simultaneously, needs far fewer parameters to obtain high accuracy for classification**. For example, our model with 1.75M
parameters significantly outperforms DenseNet 100-12 that has 12M parameters ... Our
model essentially takes an RGB image as input, simultaneously converts
the image into 7 different color spaces and uses these as inputs to individual densenets.

The reason it works is probably as simple as the idea that some things (stop signs, blood) are notable in their redness; and other things are notable by their brightness (for which which HSL is better than RGB or HSV)",12,wga490,"HSV avoids a lot of issues present in RGB images regarding things like dynamic range per-channel, etc. It also makes recognizing objects much easier in varying lighting conditions, since the channels vary much more predictably across those differing conditions. It seems more than reasonable to put RGB->HSV conversion as a pre-processing step and HSV->RGB conversion as post-processing. Why is this not more common?

(The obvious answer is ""more data solves this problem"" but it seems honestly silly to rely on larger datasets when feature engineering is known to make ML problems significantly easier in terms of model size, training time, etc. How much better could models be if they did this?)

Are there any papers with a serious comparison between RGB-based and HSV-based image processing (convolutional, Transformer-based, etc.) models?",MLQuestions,2022-08-04 12:35:58,2
"Assuming that you don't want to make an assumption of the distribution of your data, then the simplest approach may be to start with [Kernel Density Estimation (KDE)](https://mathisonian.github.io/kde/). This will allow you to estimate an arbitrary pdf. However, if your data is multi-modal, then KDE may not give the best estimation. [SciPy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gaussian_kde.html) has a built-in function to do this.

Hopefully this answered your question and was helpful. Cheers!",2,wggudi,"Say I want to find the probability distribution over some continuous variable X. So that I could ask my model ""what's the probability that X is >= [value] ?"".

What kind of output would I call this? What should I google to find out more about this? Are there ML architectures that're particularly well-suited to do this?",MLQuestions,2022-08-04 17:26:23,3
"By far the easiest ~~and most accurate~~ (unless you have a huge budget) is to calculate the embeddings for each face using this python library: 
https://pypi.org/project/deepface/  ; that automatically includes a number of state-of-the-art-at-the-time facial recognition models (VGG-Face , Google FaceNet, OpenFace, Facebook DeepFace, DeepID, ArcFace, Dlib and SFace ).


For each of the 500 people average the embeddings for each of their 25 pictures and save that in a database (or just csv file, whatever).   Whenever you have a new picture, just compute the embedding for that new picture and compare with the most similar embedding vector in that database.

It'll scale well to 100,000+ people (probably far more, but I haven't tried) using something like FAISS if you don't mind quite a few false positives.",3,wg0nq0,I want to create a face detection network for a dataset of around 520 people. I have the code ready for the face detection and all the data loaders but I am struggling with which model/approach to go for. I have roughly about 25-30 pictures per person so what would be the most accurate way to go about this?,MLQuestions,2022-08-04 06:00:00,19
Predicting gas prices is like predicting stock prices. Your model can make inferences based on historical trends but cannot account for extraordinary events. This linear regression task is challenging.,3,wg8y5c,,MLQuestions,2022-08-04 11:47:15,3
"1. Training data and test data are subsets of your whole dataset. Test data is **not** a subset of training data! E.g. in your code, of your initial dataset, 80% will become the training data, 20% will become the test data.
2. the train\_test\_split function splits 80% of your data into train and 20% into test (because you've set test\_size=0.2). random\_state controls that random splitting. So if you set random\_state = 42, you'll always get the same 80% going to train and the same 20% going to test when you re-run the code. This makes it easier to test any changes you make to it.
3. This means you want equal proportions of y values in train and test. For example, if 50% of your y values are 1, and 50% are 0, stratify=y means the function will try to make 50% of your training data have y value 1 and 50% have y value 0, same with the test set.",5,wfweq2," 

Hi,

I am new in ML and I want to ask something. I am doing a course and the instructor is talking for train data and test data.

Lets say my df has 100 rows and 4 columns and one of these 4 has values like 1,0

1)Test data is a subset from train data or from the whole df?  
2)Here :

X\_train, X\_test, y\_train, y\_test = train\_test\_split(X, y, test\_size=0.2, random\_state=42, stratify=y)

random\_state = 42, what is this argument's use?

3)What is the stratify=y in the above row?",MLQuestions,2022-08-04 02:09:33,2
"Generically speaking, understand the problem you are trying to solve.  Find or make a dataset that aligns to said problem.",4,wfngf5," I am a beginner with little experience in machine learning, and I am considering starting a project with my friends (object detection project). I have a background in deep learning and computer vision (I've taken Kaggle courses and took a Coursera ML course by Stanford), but I've never applied what I learned, so any suggestions, advice, or mentoring would be greatly appreciated. As a beginner in Machine Learning, I am unsure of what to do",MLQuestions,2022-08-03 17:58:08,4
What are some of the reasons SVMs fell out of favour?,2,wf1xc8,"After the release of the 3rd course in 28th of July. I want to share with you my onions about the new specialization

[Review: The new Machine Learning Specialization](https://medium.com/@0ssamaak0/review-the-new-machine-learning-specialization-e44206100535)",MLQuestions,2022-08-03 02:24:10,3
"The 3rd dimension is batch size. If you want to train one sample at a time (to avoid doing padding across the time dimension) then you need to just add a size 1 dimension for whichever dimension the LSTM expects the batch dimension to be. I think in both TF and Torch it's [time, batch, features], although i might be misremembering so double check the docs.",1,wejpb5,"I have seen time steps, units, batches, samples, sequences, features and more terms seemingly as if they are self-explanatory and all describe only in total 3 input dimensions to LSTMs.

If I have X\_train\[features,timesteps\] and y\_train\[timesteps\] (or X(t)=(x1(t),...,xm(t)) and a target y(t)), how do take this input and translate it to 3d input for LSTMs?",MLQuestions,2022-08-02 11:48:39,3
Tensorflow is horrible about backwards compatibility. You’re kind of shit out of luck.,2,wep1y2,"I'm currently trying to learn deep neural networking, and I'm going through this video series: [https://www.youtube.com/watch?v=PwAGxqrXSCs](https://www.youtube.com/watch?v=PwAGxqrXSCs)

This video was made in 2016, and it appears that since the 2.0 update, some old commands have been made obsolete/broken.

My Tensorflow install is version 2.9.1. 

Here is my code so far:

    import tensorflow.compat.v1 as tf
    import tensorflow_datasets as tfds
    #from tensorflow.examples.tutorials.mnist import input_data
    
    tf.compat.v1.disable_eager_execution()
    mnist = tf.keras.datasets.mnist
    #mnist = input_data.read_data_sets(""/tmp/data/"", one_hot = False)
    n_nodes_hl1 = 500
    n_nodes_hl2 = 500
    n_nodes_hl3 = 500
    n_classes = 10
    batch_size = 100
    mnist_train, train_info = tfds.load(name=""mnist"", with_info=True, as_supervised=True, split=tfds.Split.TRAIN)
    # height x width
    x = tf.placeholder('float', [None, 784])
    y = tf.placeholder('float')
    
    def neural_network_model(data):
        # (input_data * weights) + biases
        hidden_1_layer = {'weights':tf.Variable(tf.random_normal([784, n_nodes_hl1])), 'biases':tf.Variable(tf.random_normal([n_nodes_hl1]))}
        hidden_2_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])), 'biases':tf.Variable(tf.random_normal([n_nodes_hl2]))}
        hidden_3_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3])), 'biases':tf.Variable(tf.random_normal([n_nodes_hl3]))}
        output_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl3, n_classes])), 'biases':tf.Variable(tf.random_normal([n_classes]))}
        
        # (input_data * weights) + biases
        l1 = tf.add(tf.matmul(data, hidden_1_layer['weights']), hidden_1_layer['biases'])
        l1 = tf.nn.relu(l1)
        l2 = tf.add(tf.matmul(l1, hidden_2_layer['weights']), hidden_2_layer['biases'])
        l2 = tf.nn.relu(l2)
        l3 = tf.add(tf.matmul(l2, hidden_3_layer['weights']), hidden_3_layer['biases'])
        l3 = tf.nn.relu(l3)
        output = tf.matmul(l3, output_layer['weights']) + output_layer['biases']
        return output
    
    def train_neural_network(x):
        prediction = neural_network_model(x)
        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y))
        optimizer = tf.train.AdamOptimizer().minimize(cost)
        hm_epochs = 10
    
        with tf.Session() as sess:
            sess.run(tf.global_variables_initializer())
    
            for epoch in range(hm_epochs):
                epoch_loss = 0
    
                # training network
                for _ in range(int(mnist.train.num_examples/batch_size)):
                    epoch_x, epoch_y = train_info.splits[""train""].next_batch(batch_size)
                    _, c = sess.run([optimizer, cost], feed_dict = {x: epoch_x, y: epoch_y})
                    epoch_loss += c
                
                print('Epoch', epoch, 'completed out of', hm_epochs, 'loss:', epoch_loss)
            
            correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, x))
            accuracy = tf.reduct_mean(tf.cast(correct, 'float'))
            print('Accuracy:', accuracy.eval({x:mnist.test.images, y:mnist.test.labels}))
    
    train_neural_network(x)

I have fixed most of the issues that the original code ran into, but I'm stumped by this block:

    for _ in range(int(mnist.train.num_examples/batch_size)):
                    epoch_x, epoch_y = train_info.splits[""train""].next_batch(batch_size)
                    _, c = sess.run([optimizer, cost], feed_dict = {x: epoch_x, y: epoch_y})
                    epoch_loss += c
    
    AttributeError: module 'keras.api._v1.keras.datasets.mnist' has no attribute 'train'

The ""mnist.train.num\_examples"" part seems to be obsolete since the 2.0 update, and I just can't figure this one out, google seems to come up with nothing besides downgrading Tensorflow, and I'm all out of ideas.

Does anyone know a workaround?",MLQuestions,2022-08-02 15:24:07,5
[deleted],2,wedwzq,"Hi guys, I am reading papers on action recognition and have come across different loss functions, which are sometimes hard to comprehend. I would like some advice on how to go about this. Looking at new loss functions and really understanding what it does to me becomes a daunting task. Any advice would help. Thanks in advance.",MLQuestions,2022-08-02 08:00:02,4
Do you want to create your own text to speech synthesizer?,1,we6xkt,How to improve the voice modulation more like humans in the Chatbot and recommendations and suggestions please share,MLQuestions,2022-08-02 02:15:21,2
"> keep our company ahead of the curve. We're a large company with a lot of resources

Probably more likely ""to catch up to the curve again"".

I've found googling things your competitors are already doing (whether published research papers; case studies with vendors; linked-in pages of their employees; their own marketing material; etc) can be pretty compelling in exec presentations.

Just going to google scholar and typing in keywords for whatever-your-industry-may-be will probably find many examples.",3,wdhi85,"I've recently taken on a new position doing research on emerging technologies to keep our company ahead of the curve. We're a large company with a lot of resources, so working with 3rd parties is something they'd be in for, I just need to prove the RoI is there.

My current focus is on the manufacturing side of things, and am seeing that we could leverage ML in product inspection and QA, as well as predictive maintenance of equipment at our large production facilities (or, if PdM would be viable over simple preventative maintenance used now).

But I don't really have any practical experience in the field. I need to create an ""adoption roadmap"" to present to c-level execs.

I'm looking for recommended readings, articles, industry leaders related specifically to these topics.

Thank you in advance to anyone willing to help out.",MLQuestions,2022-08-01 06:27:52,6
"if you're willing to put up with installing programming languages and using a console interface, yes.  

I am personally a fan of [coqui-ai](https://github.com/coqui-ai/TTS), but this isn't a topic I know well, and there may be better choices.",3,wdkoat,"The TTS (text-to-speach) that comes with Windows sucks. It sounds worse than that comes with Android. Google/Microsoft (and probably others) provide natural voice TTS's, but not locally. They force us to connect to their servers and retrieve the voice, paying for each letter. This natural TTS could be really useful for a lot of things, but the price (requires credit card) is hindering its utilisation.

My question is, are those companies provide only online version because current mainstream consumer PC's are not powerful enough to synthesise the voice locally, or is it because they want to charge money?",MLQuestions,2022-08-01 08:46:43,3
"There are two cases, according to the author.

First, if you do your train test split twice on the same dataset, then you should get the same results. This sounds very reasonable to me.

Second, if you do your split after the data has been updated, then none of the data from your previous train split should be in your new test split. I don't think this really matters.",2,wd5y2e,"Hi all -- working through a ML book at the moment and am a little confused about test sets.  The book makes it sound like you want to use the same test sets over again, but then it introduced a problem when sample data gets updated. It then seems to say that you don't want to have repeat data in subsequent test set uses.  

below is copy and pasted section of book

 

>Well, this works, but it is not perfect: if you run the program  again, it will generate a different test set! Over time, you (or your  Machine Learning algorithms) will get to see the whole dataset, which is  what you want to avoid.  
>  
>One solution is to save the test set on the first run and then load  it in subsequent runs. Another option is to set the random number  generator’s seed   
>  
>(e.g., with np.random.seed(42)) before calling np.random.permutation()  
 so that it always generates the same shuffled indices.  
>  
>But both these solutions will break the next time you fetch an  updated dataset. To have a stable train/test split even after updating  the dataset, a common solution is to use each instance’s identifier to  decide whether or not it should go in the test set (assuming instances  have a unique and immutable identifier). For example, you could compute a  hash of each instance’s identifier and put that instance in the test  set if the hash is lower than or equal to 20% of the maximum hash value.  This ensures that the test set will remain consistent across multiple  runs, even if you refresh the dataset. The new test set will contain 20%  of the new instances, but it will not contain any instance that was  previously in the training set.

&#x200B;

so do I want a new test set every time? why is there some constant of only having less than 20% of new data introduced and why will that make sure there is no repeat data. Idk this whole last paragraph is just so confusing to me.",MLQuestions,2022-07-31 19:30:57,2
"I am using Python. I have used the scikitlearn, pandas and numpy libraries.",1,wd0h46,"Hello.

 I am doing my first ever Regression ML code. My professor assigned us to do the estimative for the price of real-estate in a specific city. I have tried all the regression methods I know of (KNN regression, Linear regression, Polynomial regression until degree 2). Whenever I try to compare the the estimated part with the correct answers, through metrics such as RMSE, I get realy high numbers. Is there another way to do it? Is there a more effective regression method than those I mentioned? I am very lost. 

Thank you for your help.",MLQuestions,2022-07-31 15:12:05,13
"This is an excellent question and is actually less of a problem with neural networks specifically than it is a broader issue with supervised learning in general. 

The scientific method is often summarized as something like this:

1. Come up with a hypothesis
2. Run an experiment to test if your hypothesis explains what it's supposed to

Take note: this is *completely inverted* relative to how we train models in machine learning. The model sees the data first and then tries to explain it ""post hoc"" rather than coming up with a candidate explanation and checking to see if that explanation fits.

The fundamental issue here is that what we really want the model to do is to form a **causal** explanation of the data. If we're very lucky, the model might land on something close to that. What's more likely though is that the model will come up with a mapping of some kind that allows it to *predict* things accurately, but those predictions will actually be based on heuristics and patterns in the data that might not actually map to the causal explanation at all. The dominant data-driven learning paradigm in ML makes it difficult to reason about *interventions* and *counterfactuals*, which is fundamentally how causal explanations are structured.

I strongly recommend reading ""The Book of Why"" by Judea Pearl. Here's a short primer on the broader topic of causal inference vs. statistical learning: https://arxiv.org/abs/1801.04016",3,wcp1pn,"
Why is it so hard to generalize information in a neural network? If that doesn’t make sense I am basically asking if I have train a neural network on the physics of a cannonball. Why can’t the neural network generalize that information into the physics of a rocket? I am interested to know if anyone has heard about people working on this problem and where they are currently at on this problem. I would also really like to read more about it. Thank you!",MLQuestions,2022-07-31 06:43:00,4
"I’m trying to classifiy ECG signals using LSTM and MATLAB, the above plot shows that the training accuracy of the system is 100% but when I apply this code to get the accuracy I get only 20%

    LSTMAccuracy = sum(testPred == Labels)/numel(Labels)*100

Am I missing something here? Or there something wrong I did in my code?

Here is the configuration and the training code:

    layers = [ ...
    sequenceInputLayer(1)
    bilstmLayer(100,'OutputMode','last')
    fullyConnectedLayer(5)
    softmaxLayer
    classificationLayer
    ]
    options = trainingOptions('adam', 'MaxEpochs',1750, 'MiniBatchSize', 150,  'InitialLearnRate', 0.0001, 'ExecutionEnvironment',""auto"", 'plots','training-progress', 'Verbose',false);
    
    net = trainNetwork(Signals, Labels, layers, options);
    
    trainPred = classify(net, Signals,'SequenceLength',1000);
    LSTMAccuracy = sum(trainPred == Labels)/numel(Labels)*100 figure confusionchart(Labels,trainPred,'ColumnSummary','column-normalized', 'RowSummary','row-normalized','Title','Confusion Chart for LSTM');

I really appreciate your help, thanks.",1,wcqy9w,,MLQuestions,2022-07-31 08:11:08,1
"The notebook was using Google Colab's GPU to run. DL on CPU is much slower than on GPU.

https://i.imgur.com/mmblrPY.png",2,wchzdm,"Hi I have been testing out the [VQGAN+CLIP](https://colab.research.google.com/github/justinjohn0306/VQGAN-CLIP/blob/main/VQGAN%2BCLIP(Updated).ipynb#scrollTo=eq0-E5mjSpmP) model on google colab for awhile using googles resources and it has worked out great. But I recently got a new cpu (I7-12700F (20 CPUs))  and I wanted to run it locally because it would hopefully be way faster (I only have 16gb ram though).

So I got to work setting up a virtual machine (I needed linux), jupyter and finally connecting and troubleshooting the collab itself. And I got quite confused by the results.

The free version of google colab gave me 2.7it/s and when I ran it locally I got **25s/it.**

So the way I see it is that there are three possible bottlenecks

1. The fact that it is running in a virtual machine contributes
2. Colabs running on local runtime contributes
3. I am completely delusional and the ""good"" hardware actually sucks

I'm just dumbfounded at how slow it ran compared to the free service google provides. A big thing to take note of as well is the fact that I have never used linux (quite familiar with vscode cmd) nor ran a virtual machine before.

Thanks for taking the time to read this!",MLQuestions,2022-07-30 23:20:05,8
Not an expert but have done some research on the subject. QML is not too well defined and a lot of it’s applications are “hybrid” meaning they involve using quantum computing to enhance classical machine learning algorithms for example [using a boson sampler to construct a similarity measure for graphs](https://arxiv.org/abs/1905.12646). I also know quantum neural networks are something that’s been heavily researched. [This](https://arxiv.org/abs/2203.01340) may also be of interest.,3,wbvnwr,"What's the goal of quantum ML? It seems to me that current ways of applying QML is to shoehorn quantum systems into well-known classical ML approaches, without any form of benefit.

I recently discovered too that there is a whole subfield dedicated to QNLP, which is quite surprising since NLP requires drawing correlations between continuous sequences, and current quantum systems are currently limited by their system lifetimes. Can someone familiar with this subfield share a little about how it works?

Context, am working on a QML project, but am a little disillusioned.",MLQuestions,2022-07-30 04:45:59,3
"Can you maybe give an example of how this dataset looks like?  
At the moment it seems like a problem that can be solved without any AI at all",1,wbt8km," I have a large structured dataset with 2 columns containing:

1. A paragraph of text with various text and integers
2. A integer that's found in the text within the first column

What I want to do is train model that can extract the correct integer from future text ideally in Python. I have come across NER but I'm not sure if this is possible.

 Thanks for the help.",MLQuestions,2022-07-30 02:08:56,2
[deleted],4,wb4mkz," 

I am currently in undergrad 3rd year studying Electrical and computer science. I have been interested in the domain of AI to be specific deep learning and reinforcement learning.

Hence I have been learning theory from standard ML courses like for example CS229, CS231n by Stanford university etc. Further I have been looking upto coding them from youtube tutorials, medium blogs and also udemy courses. I Have then tried to implement those things from datasets in kaggle. I have made projects in domain of Q learning, Image Segmentation, implementing standard deep learning algorithms.

As I lookup to do research work further I have been reading papers from good conferences in domain CV, NLP like CVPR and EMNLP.

From looking at their codes on github I have found out I have written no complex code near to that. And I cannot sometimes understand their code with those many functions, so many arg parsers etc. meanwhile the Kaggle notebook codes have rarely those things.

When I generally read on blogs or try kaggle dataset they are nowhere near that. And generally all the professors prefer students with some past research experience for their research internship position.

I really need suggestion and guidance to move on from basic projects and try more advanced projects. Please also suggest if there are any other resources as youtube mostly has tons of videos on basic projects.",MLQuestions,2022-07-29 06:18:24,3
Google text sentiment analysis tutorial.,3,wb9ng0,"I am an amateur in coding and I have to submit a project in which I have to build an algorithm to extract emotion from text_data_set can you guys pls recommend any sources or roadmap or just something that can help me

Guys... I AM DESPERATE

I have to complete it in the next ten days😭😭😭",MLQuestions,2022-07-29 09:46:10,4
"In a standard ANN with softmax on the output, each output neuron corresponds to each class. If you take argmax of the 10 output neurons, it will tell you the class prediction.

Accuracy is just how many in the test set get correctly classified (the argmax output neuron = ground truth label) divided by the total number of test images.",2,wawcnn, When creating a hand written recognizer on ANN with 10 output neurons how does it know the percentage of the predicted number in relevance to the real output number? Does it compare the predicted output number to the real output number and calculate error function for each pixel on them on each weight tweaking? Thanks,MLQuestions,2022-07-28 22:33:58,6
"It’s a lot of really annoying work. I don’t think there’s an easy way to do it. If your changes are pretty small the easiest way to go about it is to grab their model, load it and then dynamically change the model after the fact. In the same way as if you load a resnet trained on image net and then do 

`model.classifier = nn.Linear(2048, your_num_classes)`",2,w9vfni,"I'm trying to load a pre-trained BART encoder and train a decoder. I'll need to mess with the insides of the decoder, so I'd like to do it all using Pytorch. I have hugging face's pytorch\_bin file (for the full BART) along with the config file, which has all of the pretrained weights and model info. I want to load the weights from pytorch\_bin into my pytorch encoder. I've been trying to trace through the hf transformers source to figure out how to do it, but it's a mount everest of OOP. Surely I can't just load it in using pytorch's load model? How do I import the weights properly? Any help or alternative ways to load in the pretrained weights would be appreciated. Thanks.",MLQuestions,2022-07-27 17:55:33,8
PyCaret,2,w9lazv,"I am looking for a library with the functionality of feature selection, feature generation, and params tuning for binary predictions. What could you recommend for a beginner?",MLQuestions,2022-07-27 10:52:53,2
As you mentioned rendering the environment in a kaggle notebook will probably not work and since you have no code that prints or outputs anything you do not get an output. Try printing the reward in every iteration of the for loop,1,w9lgu0,"I am an absolute beginner in reinforcement learning. I'm trying to execute the second code snippet given [here](https://www.gymlibrary.ml/content/api/). I'm using python version 3.9.12 as part of the anaconda package. Curiously, no error is thrown when I try to execute this code in a kaggle notebook except for the fact that the notebook can obviously not display the output environment. I checked the version of python in kaggle, and it's 3.7.12Is that the cause behind this issue? Moreover, I was playing around with the code given in the documentation and was able to modify it such that it inadvertently worked natively on my machine. Attaching a screenshot of my code. Can somebody please tell me if I'm doing something wrong? If it is because of the python version, what kind of changes would I have to make in the code given in the OpenAI documentation? Thanks in advance.

Edit: Added the output of my code, the code from the openai documentation, the error that is being thrown by the code from the openai documentation. I've also tried googling the error message I'm seeing, but I didn't find anything relevant.

\*\*\*\*\*\*\*\*\*My code\*\*\*\*\*\*\*\*\*\*\*

import gym

env = gym.make(""LunarLander-v2"")

info = env.reset()

&#x200B;

for \_ in range(1000):

env.render()

action = env.action\_space.sample()

observation, reward, done, info = env.step(action)

&#x200B;

if done:

observation, info = env.reset(return\_info=True)

env.close()

\*\*\*\*\*\*\*\*\*My code\*\*\*\*\*\*\*\*\*\*\*\*\*

&#x200B;

\*\*\*\*\*\*Code from OpenAI documentation\*\*\*\*\*\*\*

import gym

env = gym.make(""LunarLander-v2"", render\_mode=""human"")

env.action\_space.seed(42)

&#x200B;

observation, info = env.reset(seed=42, return\_info=True)

&#x200B;

for \_ in range(1000):

observation, reward, done, info = env.step(env.action\_space.sample())

&#x200B;

if done:

observation, info = env.reset(return\_info=True)

&#x200B;

env.close()

\*\*\*\*\*\*Code from OpenAI documentation\*\*\*\*\*\*\*

&#x200B;

&#x200B;

\*\*\*error thrown by code from OpenAI documentation\*\*\*

env = gym.make(""LunarLander-v2"", render\_mode=""human"")

File ""C:\\Users\\shubh\\anaconda3\\lib\\site-packages\\gym\\envs\\registration.py"", line 592, in make

env = env\_creator(\*\*\_kwargs)

TypeError: \_\_init\_\_() got an unexpected keyword argument 'render\_mode'

\*\*\*error thrown by code from OpenAI documentation\*\*\*",MLQuestions,2022-07-27 10:59:32,1
"Don’t run after a framework. :)

That being said, you can start with text classification. There’re are plenty of datasets available on that.",3,w99o5t,"Discussion 

I know the fundamentals of stats , probability, classical machine learning. Sklearn . 
Deep learning started and i know MLPs , CNN. 
Also i still can't figured out in which domain i have interest? 

How can i find it. Currently following breadth first approach but now I want to start giving interviews so want to make project but don't know in which should i make . Can u guys suggest , I'm thinking of NLP project? 

I know this are many questions but I'm really noob in this industry so requesting data community to please help out?",MLQuestions,2022-07-27 02:00:27,3
"You could make two different models for each dataset if all your trying to do is understand feature importance. Also, if you expect that there are not interactions between the two sets of features, you might be able to fill in the missing features in the control set with the average of the control population. Would be curious if anyone else sees an issue with this but it seems reasonable to me. How big are the datasets?",5,w8m0iy,"Hi all, biologist here trying to do a small ML project with a somewhat wonky dataset. I wonder if anyone can give me advice about how to proceed.

I'd like to use ML to see if I can predict whether or not patients have a condition related to the after-effects of stroke. I have 2 types of data: data describing features of their serum (presence of cytokines/amount of cytokines), and data describing features of their red blood cells (subpopulation frequency).

For the patients, I have both red blood cell data and serum data. For the healthy controls, however, about half have serum data, and half have red blood cell data. Anyone know how I should proceed?",MLQuestions,2022-07-26 08:01:10,8
"I'm not deeply familiar with the outputs from YOLOv5 but a tool you might want to use if you're not already is [Netron](http://netron.app/). This can help give you information about the model inputs and outputs. For example, here's what TinyYOLOv2 looks like in Netron.   

[Screenshot](https://user-images.githubusercontent.com/46974588/181083335-81f12fe1-961e-49c1-bb14-707dce8a2e5f.png)

Usually when you click on the first node, a panel with the name of the inputs (image), and outputs (grid) as well as types (float32) and dimensions. From there, since you have more information on what the outputs represent, you can map them to class properties that make more sense.",0,w8pqhp,"I built a bot using Azure AutoML and the object detection template... And it all worked great, but I want to decouple from Azure.  So I've used PyTorch to train a custom YoloV5 model and exported that to an ONNX model format so I can use it in C# with [ML.net](https://ML.net).  I need some help understanding the output format for the YoloV5 model though.

I used Netron to inspect the onnx file and the output is named ""output"" and the type if float32\[1,25200,6\].  I was using a tutorial I found [here](https://github.com/dotnet/machinelearning-samples/tree/main/samples/csharp/end-to-end-apps/ObjectDetection-Onnx) but it's based on Tiny YoloV2 which is says has an output of ""None x 125 x 13 x 13"" so I don't know if I can apply the same logic here.  In case it matters, my model only has a single label/class.

Can someone point me to an explainer that shows how to read the labels/tags/classes, the bounding box and the confidence scores from this?  I can't find anything on the Yolo Github about it.",MLQuestions,2022-07-26 10:31:02,6
" I would say the following would help: 

1.	Relevant Publications (preprint, journals etc)
2.	End-to-End Project
    - Go beyond just the ML portion.
    - Depending on your experience, you could do some of the following:
    - Create and deploy an ML Pipeline in AWS for example (data engineering, ML, ML deployment, ML Serving/Monitoring etc).
    - If you plan on transitioning back to industry, it’s a nice project to have in your portfolio.",3,w8r8nj,"I have 2 years before I need to apply for uni, and am in a pretty comfortable spot (resumé wise). My grades are good, and I’m doing as much as I can to feed my interest in this field. I have completed ≈ 20 certified courses (relating to ML) on Coursera, and have completed about 8 projects (which “solve” datasets from kaggle). I do write about my approach to these projects in a blog, to make it look better on the resumé. Doing all of this I have earned a scholarship from a prestigious private school nearby, and have been wondering how much more I would need to do in order to get into a “top university”. I have competed in a Digital Playgrounds competition on Kaggle and placed top <5%, and thought that doing this and coming closer to the competition master rank would be beneficial in an application. What should I do, continue doing courses, doing projects for fun, or grinding kaggle? Any suggestions?",MLQuestions,2022-07-26 11:30:32,2
I have got UNet to work on ~300 MRI images (transfer learning with ImageNet weights). Did you try it?,1,w8q768,"I'm building some semantic segmentation models off of low-moderate volumes of biomedical images (\~500 - 1k images). So far I've done some hyperparameter sweeping (learning rate, transfer learning, architectures, dropout layers) using the Segmentation Models package from qubvel [https://github.com/qubvel/segmentation\_models](https://github.com/qubvel/segmentation_models) but I'm only seeing moderate performance and minimal differences between tested parameters.

Does anyone have any recommendations for any other easy to implement packages to try out some more modern approaches? I'm admittedly a little out of the loop. GANs seem to be old news at this point? Transformer based architectures seem to be widely discussed - are there any good packages for these? Anything else you could point me towards? Thanks!",MLQuestions,2022-07-26 10:48:58,2
">I have also noticed that a lot of the job offers require a MS or PhD, how will choosing a MS program limit my career growth?

It won't.

You should only choose a PhD if you are interested in becoming a professor, academic researcher, or a highly-specialized industry researcher.  Really only the first two should matter.  The latter position exists mainly to absorb PhDs that do not stay in academia (of which there are many).

In my narrow opinion I have to caution you about two things regarding an MS in machine learning/data science:

- The field is saturated with a lot of people with an MS.  A combination of ""wow cool"" and the unicorn boom a decade ago made it a very attractive choice.  The truth is a lot of those people should have gone into data engineering or cybersecurity instead.  In intense curiosity, creativity, and tenacity make a good data scientist.  You don't really learn that in any program.

- A lot of what you learn in a graduate program for machine learning becomes obsolete by the time you graduate.  I would look for curriculums that stay cutting edge.  Don't study LSTM when you could study Transformer models.",1,w8evdh,"I'm a 4th year Computer Eng student. I want to know if  certain degrees are more valuable in the job market. I have also noticed that a lot of the job offers require a MS or PhD, how will choosing a MS program limit my career growth?",MLQuestions,2022-07-26 02:06:13,1
try using logloss,3,w86gkl,"I'm training a onehot classifier, with softmax activation, and a categorical\_crossentropy loss function. For some reason, the loss is exploding but the accuracy/val-accuracy keeps going up. I'm obviously doing something wrong with the loss. I'm still learning. Any feedback appreciated.

My best guess is that the loss isn't helping at all, but it's just jumping around aimlessly. Sometimes, the jump happens to be an improvement. So it looks like it's doing ok, but it isn't.

https://preview.redd.it/570aq64bftd91.png?width=1079&format=png&auto=webp&s=1de0bdf4651a098b16ac5b3e228d158b1bf8f101

EDIT: I figured it out. I had created a big dataframe of images and their onehot vectors. For this test, I had removed all the columns of the classes I didn't want, down to two. Unfortunately, rows with zeros for those two classes still existed. When I removed all the irrelevant rows (and reset the index) I was able to get 100% accuracy.",MLQuestions,2022-07-25 18:34:38,3
"are the time series generated by the same process? If so, you might be able to leverage symmetries.  For example, if you sample a wave in two different points, then the second time series will be a spatial and temporal translation of the first ( dampened maybe )",1,w846si,"I have time series data over a few locations. The locations aren't precisely known enough to be meaningful and location is known to be a pretty weak factor anyway. 

How should one combine data into train, validation, test sets etc.? 

If I am analysing a 1D time series, I may do e.g. walk-forward validation [https://machinelearningmastery.com/backtest-machine-learning-models-time-series-forecasting/](https://machinelearningmastery.com/backtest-machine-learning-models-time-series-forecasting/) as an approach to training and testing a model.

This seems to deal only with 1 time series. How do I combine multiple timeseries for training/testing?",MLQuestions,2022-07-25 16:50:57,2
"Yeah you could theoretically specially define the loss function so that specific latent features correspond to specific features of the underlying data, but the results may be weird.",3,w7vtq0,"Good day people,

I hope everyone is doing fine!  

I have a kind of silly questions, but I just keep thinking about it.

AE and VAE are stable models in nowadays DL techniques, and arguably our understanding of VAE came pretty far, which is used to be vague and opaque in my opinion, but there are some aspect that are still not well understood.

My question is: 

**Is there a way to enforce AE (and/or VAE) latent vector parameters to learn specific features?  making these parameters interpretable.**",MLQuestions,2022-07-25 11:06:29,5
"is building this from the bottom up something you are doing for fun or educational purposes? If you just need to add personalization to your app, i'm sure there must be open source libraries you should basically be able to just drop into your stack.",1,w7mdl4,I built an app using flutter and firebase as a backend. I need to implement a recommendation systems in the app to personalise the feed for users. I have watched through Andrew Ng's Machine learning and Deep Learning courses entirely about 1.5 years ago so I have some understanding of the concepts in ML. I also have the mathematical background but little to no tensorflow experience but I do know python. Can you guys recommend somewhere for me to start learning a course or another resource perhaps(I'm not too good at learning from a book btw)? I have about 3 months by the way. Thanks.,MLQuestions,2022-07-25 04:23:34,6
What kind of env/game are you working on?,1,w7vg90,,MLQuestions,2022-07-25 10:51:53,1
markov chain or some sort of bayes classifier,2,w7e0r9,"Hi friendos,

I have a question I think that can be answered with machine learning, but I am still not too sure how to approach it. Any help at all would be wonderful, or even a general direction to look in. I have so far looked at sequence prediction, but it doesn't seem to fit this problem very well.

The question is as follows.

Given a SET of objects (A, B, C, D, …. Z), of bounded size < 5.

And historical data SETS of objects and their corresponding SEQUENCES (i.e for set {A, B, D, G} mapping to sequence (B, D, G, A), build a model that can predict these mappings based on the set of objects passed in.

How would I go about building a model for this?

I.e Input: {A, B, C, D}. Predict \[B,D,C,A\]",MLQuestions,2022-07-24 20:21:22,5
"it's dangerous out there, take this vocabulary list with you:

* generative models
* generative adversarial network, GAN
* diffusion model, DDPM
* latent
* mel-spectrogram
* text-to-speech, TTS
* speech-to-text, STT
* stem separation",1,w76ut0,"I have no experience with ML and very little experience programming, but am willing to learn. I have a collection of audio files containing both inputs and outputs that I'd like to train an ML toolkit or program to be able to create output files with just the input files. But between supervised, unsupervised, and reinforcement ML, and specific toolkits and programs for that matter, I'm not sure which platform would work best for me. If programming is necessary I'd prefer something that can work with Python as I've recently started to learn it, but I'm open to other languages and environments. I don't think it's worth much, but I am experienced with scripting and visual analytical tools, such as SAP BOBJ.

Any and all help is appreciated!",MLQuestions,2022-07-24 14:45:55,2
"> where the Euclidean distance between the embedding vectors is a meaningful quantity.

You have to be more specific about what ""meaningful"" means. Do you want it to reflect distance in the original feature space? Do you simply want to preserve closeness without preserving farness? This is essentially the premise of your entire question, so you really need to flesh this out before you can move forward.",3,w6tyzs,"I am trying to learn a Euclidean embedding. By that I mean, inputs numerically describing points in space are mapped into a high-dimensional Euclidean space where the Euclidean distance between the embedding vectors is a meaningful quantity.

The detailed explanation is as follows:

>I have a big explicit model to derive walking times between points on Earth. Computing these times using graph algorithms is very costly, so I'm trying to teach the shape of Earth (from a hiker's perspective) to a model to derive reasonable approximations for the hiking times more efficiently.  
>  
>I have generated some 70'000'000 pairwise distances using the big explicit model. The machine gets as input a pair of points (each latitude/longitude mapped to 3D spherical coordinates) and is supposed to map them to a high-dimensional space in which the Euclidean distance is the travel time in seconds: They are a small sample from a world-wide travel time matrix.  
>  
>The data is the 7-tuple containing \[**x**=cos(lat1); **y1**=sin(lat1)cos(lon1); **z1**=sin(lat1)sin(lon1); **x2**=cos(lat2); **y2**=sin(lat2)cos(lon2)); **z2**=sin(lat2)sin(lon2), **t**=time \[seconds\] to walk from (lat1,lon1) to (lat2,lon2)\] and the model is supposed to learn the mapping from the 6 spatial dimensions to **t**.  
>  
>This means I kind-of want to do Multidimensional Scaling / Moran's eigenvector map on a very very huge sparsely known distance matrix. The matrix is too big for the MDS/PCoA methods I know (which use eigenvectors, matrix inverse and/or matrix multiplication of the entire distance matrix) and the full distance matrix, connecting about a million base points, would be too expensive to calculate in the first place and would then still not cover the entire walkable earth.

I happened to start with dropouts on my layers, before I realized it is not a good idea because I want my model to overfit, not generalize, and I dropout layers are generally not good for this kind of problem, they are better for classifier-like problems where the relative size of the outputs matters.

I removed the dropouts and ran into lots of vanishing gradients. I have the vague idea that the problem has a many saddle points and dropouts add some randomness to the fitness landscape so the optimizer is pushed of the saddle to one side or another. What alternative strategy could I use to avoid vanishing gradients in this case?

The model boils down to (in Python Keras)

        embedding_1 = point_inputs_1
        embedding_2 = point_inputs_2
        for width in [512, 512, 512, 512]:
            dense = Dense(width)
            prelu = PReLU()
            dout = Dropout(dropout)
            embedding_1 = dout(prelu(dense(embedding_1)))
            embedding_2 = dout(prelu(dense(embedding_2)))
        dense = Dense(512, name=""embedding"")
        embedding_1 = dense(embedding_1)
        embedding_2 = dense(embedding_2)
        euclidean = Lambda(
            euclidean_distance,
            name=""euclidean_distance_in_embedding""
        )([embedding_1, embedding_2])

with `MeanSquaredLogarithmicError` on `euclidean`.",MLQuestions,2022-07-24 04:58:03,16
"I would look at topic modeling for this. Basically, you create the topic model, then for each topic, get the probability distribution of the location given that topic. If the density is sharply concentrated on a specific location for that topic, then the tweet is likely to give you information about some city (I.e., it’s relevant). 

You can think about this as kind of like K-means clustering, where you cluster on the words, then calculate some probability of being in a certain location given the cluster assignment. 

To keep things simple, you could omit advanced topic modeling altogether and just do K-means directly using Word2Vec representations of the tweets. Then for each cluster calculate the probability density of each city. Define some threshold on that density (I.e., if more than 50% of tweets in this cluster come from one city, then this tweet is location specific). That just might work. 

However, topic modeling might be better in this case, as the topics will learn which specific words correspond to that topic. Check out Latent Dirichlet Allocation as a good generative model to look at. For tweets, I’ve read some papers that have leveraged this technique successfully, but the key point is that they created “pseudo-documents” so there is more text per document for the model to learn from (LDA with only tweets as the documents tend to not work well). However, in your case, this is sensible. Just group your tweets by location, then sample like 1000 tweets from each grouped location to create documents. This will give the model more context so that it knows which words appear together in a specific location, and thus it will create topics that contain those location specific words. 

You could look at more advanced versions of Latent Dirichlet Allocation where authors have extended the generative model to some secondary task (one extension is called Supervised LDA). Since you want your output to be the probability of being relevant, you might be able to find some extension that is closer to what you are looking for, as the generative model you are looking for is the probability of a tweet being location specific or not. 

If you are strong in Bayesian stats, you could do this clustering through some generative model directly (this is not too different from the LDA strategy, but sklearn makes LDA trivial to implement). I would model the lat/lon as a mixture of multivariate Gaussians where the cluster assignment is some probabilistic function of the words being used. Then you could run Gibbs sampling to learn the model.  


Cool problem to think about, but long story short, my vote is on some form of clustering where you use the probability of belonging to each cluster as your indicator for relevance.",1,w6ut4b,"Hi friends,

I'm new to NLP.

I'm trying to create a model to flag tweets containing information useful for identifying the user's location.

The model should consume texts on the input, and return a confidence score for the tweet to be relevant to identifying the user's location.

**For example:** ""born and raised here! Yankees forever!"" would have a higher confidence score than ""AWS has been down for a while now"" or ""my favorite cities are Paris, Milan, and London""

The training data set may be found [here](https://drive.google.com/file/d/1tamB8N6RW_K0yVMR1yNPUvppivuGe_tG/view?usp=sharing). It includes over 8M tweets, along with the user's geographical coordinates (latitude and longitude) and basic metadata, such as language and the time of post.

&#x200B;

How should I approach this problem? What methods I can use to achieve the results?",MLQuestions,2022-07-24 05:44:51,1
"You could do a data science bootcamp. They're not long enough to teach you quantitative problem solving skills but they can teach the python data science stack and the bare bones fundamentals of ML. I did one several years ago, having a strong background somewhat similar to yours and it worked out great for me. 

A couple caveats. Things may have changed in the past five years. There were a lot fewer people with masters degrees in the field back then so employers may be less willing to take a chance on a bootcamp grad these days. Also the term data science has been diluted and sometimes just means SQL monkey which isn't the same as ML so if you go the bootcamp route make sure ML is in the curriculum.

Another option to consider would be to do a master's degree in ML. It'd be more expensive, more time consuming, more fun and a stronger qualification. If you have the aptitude and it sounds like you do, taking out a loan to do a master's in ML could be a good investment. 

Finally, you could just get a job as a software engineer on an ML team. You'd be doing software engineering and not ML obviously but ML teams really, really need software engineers. I would venture to guess that more than half of ML teams at startups don't have enough engineers and the ML scientists on these teams really want those engineers. You'd learn some about applied ML and then you could go back for your masters later for a more ML focused role in a year or two if you want.",1,w66oni,"Hello fellow redditors,

As the title suggests, I am a mechanical engineer with a masters in mechanical design from a top institute in India. Directly after my masters, I got a job but left it after exactly one year to pursue civil services. And that decision has left a 3 year void in my career sheet. During these three years, the most I have been in touch with tech/science was through random personal automations using python and digital notetaking systems or a few readings here and there. I don't know if they have anything to do with each other, but I am lazy (for repetitive work) and have an eye to optimize /automate my workflow. The later led to me learning python, a bit of git and css/html.

With regard to my prgramming skills, I learn quickly and had good grades in all the computer science courses we had at the college (C++, DSA and Modelling-Simulation). I have also programmed in Matlab for basic usage in research and also in LAMDA for nanomechanics/molecular simulation.

At my work, I had written a python code to automate the process of model setup for FE which reduced the human intervention from very menial routine work (hindi: gadha majdoori).

As for my mechanical engineering skills, I am good with CAE softwares and can readily work with them. So first thing I am doing right now is applying in various positions in the same domain as I had worked 3 years ago. All this while, I got introduced to the world of Machine Learning, AI and Deep Learning. So, I wish to learn ML to slowly venture into that line.

So yeah, my question here to the CS veterans is, how to start with the learning, from where, what can I expect from the field and how much time is necessary for be able to get a decent opportunity in that domain?  


PS. I am 28yo",MLQuestions,2022-07-23 08:30:33,4
"Why would you want to use a CNN for that? This is just a mask, and it probably already exists.",1,w6ciez,"I want to make a CNN in keras, to highlight image modifications. My input would be an RGB image with some augmentation like a yellow circle, and the output would be a single channel image of the same size completely black, but white where the circle is. I figure I'd try with an autoencoder design.

Do I really need to generate all these images and create a testset beforehand? Is there a good way to use the ImageDataGenerator combined with a preprocessing function to take one flawless rgb image, create an input and output, and feed that while training?

EDIT: I decided to use the existing generator to load the images, but wrap it in another generator that created the two images I wanted. Then pass my new generator to the model fit function. It seems to work.

EDIT2: It worked, and I was able to train it to highlight all the defects I inserted. Now I can move on to more complex problems.",MLQuestions,2022-07-23 12:50:05,2
"I personally think that doing computer science or data science in a unix based system is way better specially because a lot of VMs and containers use Linux distros. With this in mind I would vouch for a MacBook Pro or another laptop with Linux depending on your budget.

New MacBooks also come with GPUs but you cant use them for ML just yet. Hopefully that will change in the future 😜",1,w66mg8,"Hi l'Il be taking my bachelors in Management and Technology. I have courses like Machine Learning, Informatics, and Mechanical Engineering. Was wondering if there's any laptop you could
recommend. I could go a little over the budget of $1,300. Thanks in advance",MLQuestions,2022-07-23 08:27:57,1
"I am not sure ‘matrix’ applies here.

And I suppose you will have to come with your own metric based on if the prediction contains the expected result and if it’s in the right position.",4,w6185v,"Hi, I have 2 matrixes (2D lists) that I wanna calculate their similarity. My purpose is to see how accurate my predicted List2 (Automatically clustered) compared to the true List1.

**Example:**

    List1 = [   ['A','B','C'],
                ['D','E','F','X','Y'],
                ['G','H'],
                ['Z']        
            ]
    
    List2 = [   ['C','D','E','F'],
                ['A','B'],
                ['G','H'],
                [ 'X','Y','Z']        
            ]
    
    S = similarity(List1, List2)

What would the similarity function be ?

**Note:** The order of the rows won't affect the accuracy, however, 'C','X','Y' being in another clusters are considered inaccurate. 

Thanks in advance.",MLQuestions,2022-07-23 03:52:32,14
"With no hidden layers, you just have a linear classifier. 33% is better than what I would expect for that kind of network. If you use a proper network, you can get 80% easy, 90+% with some parameter tuning.  State of the art is 99.5% according to paperswithcode.",2,w63shy,"Hi, I am trying to solve cifar 10 from scratch using a simple softmax classifier by flattening the images directly. I know it is pretty rudimentary but otherwise what would a good accuracy be for it? I managed a 33% accuracy (got the same accuracy regardless of whether I trained it on 1 epoch or 20). Is it decent for such a neural network with a single hidden layer?",MLQuestions,2022-07-23 06:15:40,8
"If you're trying to reconstruct the original image then you won't have a train/test set, just a set of images that you are trying to encode and reconstruct. The main difference from what you have is that the loss function will be the error between the reconstruction and the original image. There are a lot of good and simple implementations of VAEs and other versions like B-VAEs.",1,w5sd90,Guys I’m trying to build a convolutional auto encoder with PyTorch. The problem is that my model is trying to reconstruct the input image instead of reconstructing the mask. What should I do can someone take a look at my code and give me any advice? I’m using PascalVOC dataset found within torch datasets. https://github.com/valanm22/Miscellaneous/blob/main/VOCSegmentation.ipynb,MLQuestions,2022-07-22 19:25:34,9
"you are 100% correct! This is a task called ""image captioning"".

As a prompting assistant, it works best if you are using the same model (or at least training dataset) to drive the captioning as will be used to generate the image.

There's a colab linked here you might enjoy playing with: https://github.com/rmokady/CLIP_prefix_caption",3,w5rfc1,,MLQuestions,2022-07-22 18:38:43,4
"Start learning about entrepreneurship. You can leverage what you know now as your 'specific knowledge'.

Somewhere to start: [https://www.youtube.com/watch?v=1-TZqOsVCNM](https://www.youtube.com/watch?v=1-TZqOsVCNM)",2,w5j5rv,"Hi, I am a research scientist focusing on multimodal deep learning in the biomedical domain (not afraid to leave this domain). I have experience and have published work in this field. I want to move away (or supplement) from my 9-5 as it is not bringing as much fullfillment as I would like. I identify myself as a self-starter and am willing to work my ass off, but my job isn't cutting it. There are some prospects of becoming a professor at my workplace, but nothing concrete and I don't want to stick around working for my (already below average) salary for the next x years only to not have moved up in my career.

I am interested in doing research and I often read new papers, but I want to move up in life and have my own buisness. But I don't know exactly how to start or where to look. I think the real crux is I am having difficulty marketing/selling myself. I am not particularly interested in a FAANG job, I know the money is good there but I also don't think it has the best quality of life.

Has anyone been in the same boat and how did you resolve it?",MLQuestions,2022-07-22 12:30:45,3
"For sure, there are two ways you could look at doing this. 

First is a fully convolutional network. With no dense layer, there's no more requirement to have a fixed size input. 

Second is an global average pooling layer. After however many convolutions, you average your input spatially, and wind up with an  NxCx1x1 sized tensor, where N is sample count and C is channel count. This could go do a dense layer if you wanted. (if you want the same size output, you could probably tile and upsample after the bottleneck)

In either case, you'll be limiting the network's ability to consider information across distant regions of the image. It'll effectively be processing image patches that are the size of the network's receptive field. You'll want to make sure you have enough convolutions that said receptive field is at least as large as whatever you expect to be detecting/classifying.",3,w5i5oz,"As mentioned in the question, would it be somehow possible to build such a network that takes images of different sizes and outputs the same input size?

I’m thinking about something with RNNs and splitting the input image into segments but that would break the spatial information…",MLQuestions,2022-07-22 11:49:09,2
"Depending on what you're using to program this, there's probably already built-in methods for augmentation like rotation and translation.

If you've made custom filters for each image to aid in training, I suppose you could add that as extra channels for each image.",1,w5id7b,"I am working on supervised image classification problem with caltech101 dataset, I have performed data segmentation (Thresholding, HOG) , augmentation (random rotate/flip/zoom) and Gabor filter, to get more about of an image.

my question is how to add these features to dataset, as these feature are of same size as original image.

I was thinking, that saving the segmentation data as pic in the data directory make more sense.",MLQuestions,2022-07-22 11:57:51,2
"first and foremost: you're not comparing apples to apples here. you can only compare these two models if you fit and evaluate on the same data. you're training the first model on more data than the second, and also presumably evaluating it on data that it saw during training. That's the reason for the call to `train_test_split` in the second code block. it creates a hold out dataset isolated from the training dataset.",1,w5bgzb,"    x = df['X Points']
    y = df['Y Points']
    
    m = 0 # m in mx
    b = 0 # b in mx + b
    lrate = 0.0001 # learning rate
    epochs = 10000 # iteration time
    n = float(len(x)) # length of x variable column
    
    # gradient descent
    for i in range(epochs):
        y_pred = m*x + b
        devrM = (-2/n) * sum(x * (y - y_pred)) # derivative of m
        devrC = (-2/n) * sum(y - y_pred) # derivative of c 
        m = m - lrate * devrM # update m
        b = b - lrate * devrC # update b
    If gradient descent is used to find the best fit line, why do many tutorials use linear regression function(seen in the second code block)? 
    
    # second code block
    # Q2
    from sklearn.model_selection import train_test_split
    x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2, random_state= 2)
    from sklearn.linear_model import LinearRegression
    x_train = x_train.values.reshape(-1,1)
    y_train = y_train.values.reshape(-1,1)
    lr = LinearRegression()
    lr.fit(x_train,y_train)
    coef, incept = lr.coef_, lr.intercept_ # coef and intercept stored in variables
    print (lr.coef_)
    print (lr.intercept_)

My second code block shows splitting my data table into prediction and test group to find the regression equation. If gradient descent is also used for the same purpose but a better fit line as well, why do many tutorials use the second code block? Can I just skip the second code block? Thank you .",MLQuestions,2022-07-22 07:05:17,3
"I would suggest turning your data into a graph using `networkx` and then just calling one of its [shortest path](https://networkx.org/documentation/stable/reference/algorithms/shortest_paths.html) algorithms.  There are various choices, their characteristics are [summarized on Wikipedia](https://en.wikipedia.org/wiki/Shortest_path_problem).

Note that in general these are based on edge weight, but I think your scores might be vertex weight, so you might need to distribute the scores to connecting edges.  See [this answer](https://stackoverflow.com/questions/49136427/python-networkx-weighted-graph-not-taking-into-account-weight-of-node-in-shortes) on how to do that.

If you don't have road information, to actually construct the graph according to neighbouring points you'll want a [Delauney](https://networkx.org/documentation/stable/auto_examples/geospatial/plot_delaunay.html) or [KNN](https://networkx.org/documentation/stable/auto_examples/geospatial/plot_points.html) graph.  But of course it's not guaranteed that all neighbouring points can be connected arbitrarily, it depends on the road network which you should have to properly do this.",1,w5ajrr,"So my ultimate objective is to create an algorithm that can calculate the SAFEST route between two points on a map. I have created a dataset of unique lat,lng values and their ""crime score"" which I calculated using numerous other factors. It looks like this:

ID, Latitude, Longitude, Crime score   
0, x1, y1, z1,   
1, x2, y2, z2,   
2, x3, y3, z3, .....   


I know about various search algorithms that can help me in this such as the A star algorithm which would be the best algorithm for this context and have played with the gmaps library on python, neo4j and OSM but haven't figured out an efficient system to achieve my ultimate objective. Any advice?",MLQuestions,2022-07-22 06:23:27,5
"depends on what you want to do. I believe MS coco, but if you want to do some work in the biomedical domain, there are many datasets like mimic CXR, the indiana university CXR dataset.",1,w5a9dx,"Hi, I'm looking for datasets for multimodal learning tasks. So far, I have only found data related to visual question answering. Are there any other open-source datasets for regression or classification that are multimodal? E.g. tabular + text + image data?",MLQuestions,2022-07-22 06:09:59,1
"There are mistakes in the survey, the number of libraries you allow to select should be more than one, or you should indicate how to deal with cases where a library is built on top of another",1,w52m8v,,MLQuestions,2022-07-21 22:44:01,2
Probably this: https://stackoverflow.com/questions/47069744/is-smooth-idf-redundant,4,w4g6xe,"In the SciKit learn implementation of TF-IDF, there is a Laplace smoothing for the IDF.

In which cases the count of documents containing the term is 0? If it was 0, it wouldn’t be in the dictionary in the first place.

Any hint or explanations?

Thanks in advance!",MLQuestions,2022-07-21 06:03:47,7
"You need large data set to really make this function. 

You could try and collect enough data over time but you'd need to continue to retrain the model so it understands the new data set and can learn from it.",1,w4ogld,"I am trying to predict conversion rates for some products. Each product probably gets on average 2 clicks per day and therefore the conversion rate can either be 0%, 50% or 100% but our overall conversion rate tends to be very stable at around 4-5%.

I feel like passing data into a model at the level where the clicks are very low must be unhelpful as it is so volatile, so I was considering passing in the data at a product group level instead. Which does give more stable predictions, and a high accuracy when testing vs product group level data. But how can I gauge how accurate this is at a product level given clicks are too small and volatile to yield sensible numbers?",MLQuestions,2022-07-21 11:49:55,1
Yes. You might want to look at transformers if you don’t want sequential inputs.,1,w4d9c6,"My input data are a collection of binary rasters and the desired feature output is a sequential feature like a road. 
Do the inputs into RNN have to be sequential in nature?",MLQuestions,2022-07-21 03:30:17,4
"I think if you learn Bayesian inference and then theory of optimization, you would probably be fine for the general learning problem. For how deep neural networks work, might have to wait until there is some great unifying theory out there. Right now researchers are still figuring it out and there are too many candidate theories. How to create deep learning algorithms? Just try a bunch of \*\*\*\* and see what gives 0.1% better than SOTA result ...",8,w3tgdf,"I want to learn machine learning and by ""learn"" I don't mean just superficially, like how to implement it in projects. I want to learn how it mathematically really works. I am studying mechanical engineering so I know Calculus, Linear Algebra, Statistics, Multivariate Statistics, Nonlinear dynamics to name a few, but I want to know how machine algorithms are developed and to even learn how to create/generate my own. What would u suggest me?",MLQuestions,2022-07-20 11:19:27,8
"1. Strongly recommend you spell out your acronyms. ML overlaps with a lot of other fields and terms can sometimes have collisions. Best to be explicit. GAN is obvious, esp given the context, I'm assuming DBN is deep belief network? 

2. GANs are great, but actually the current dominating force in the generative space is diffusion models, which overtook GANs about a year ago. There is still research being done on GANs, but a lot of the excitement and focus has shifted to diffusion and score-based variants.

3. My understanding is DBNs are best suited for problems where you believe there is some sort of latent causal graph that describes the generating process of your data. I don't believe DBNs are commonly used for image generation, which is where GANs thrive.",6,w3myoc,"Both of them are generative models, but it seems like people are alot more excited about GAN. When generating images, are there any instances where we would prefer to use DBNs over GAN? What are some advantages of DBNs? Thank you in advance!",MLQuestions,2022-07-20 06:55:38,1
https://machinelearningmastery.com/how-to-develop-a-face-recognition-system-using-facenet-in-keras-and-an-svm-classifier/,1,w3qzag,"Hi, I'm totally a beginner,  I want to build a person re identification system using opencv dnn for inference so I need a pretrained model, how to choose that model? Or the choice doesn't matter since the model is used for object detection ? I really need help and guidance thank u in advance",MLQuestions,2022-07-20 09:39:51,1
I’m not sure I’d solve that with Machine Learning.,1,w3pgmy,"Hi,

I'm new to AI, have basic understanding on some AI concepts. Recently I saw an interesting programming challenge that sparkled my interest. I feel it can be a good candidate for me to dip into AI/Machine Learning.

The challenge is called Room Cleaner Robot. Below is the explanation of the challenge.

You are controlling a robot that is located somewhere in a room. The room is modeled as an n x n binary grid where 0 represents a wall and 1 represents an empty slot.

You are tasked to use the robot to clean the entire room (i.e., clean every empty cell in the room).

The robot starts at an unknown location in the room that is guaranteed to be empty, and you do not have access to the grid, but you can control the bot to move up, down, left or right.

When the robot tries to move into a wall cell, its bumper sensor detects the obstacle, and it stays on the current cell. You can assume all four edges of the grid are all surrounded by a wall.

The input is only given to initialize the room and the robot's position internally. You must solve this problem ""blindfolded"". In other words, you must control the robot using only the four mentioned APIs without knowing the room layout and the initial robot's position.

&#x200B;

The reasons I feel this can be an interesting AI problem are:

\- Testing data can be easily generated.

\- It's relatively easy to evaluate how effective a solution is (whether it cleaned the whole room and how many steps it took).

\- It sounds practical. The trained model can potentially be used to control a real room cleaner bot.

\- The behavior of the model can be easily visualized (by showing the trace of the bot).

&#x200B;

The initial questions I have are

1. Is this a worthy ML problem to explore?

2. What ML technologies are applicable to this problem?

Based on the answers to these questions, I may have some follow up questions.",MLQuestions,2022-07-20 08:38:55,4
throw in some cross validation,1,w3e3mi,"Greetings all,
Could anyone please guide me how to compare different classifiers on the same dataset? I do not think that training on the whole dataset and calculating the accuracy would be a valid enough comparison. For now I have taken different training dataset sizes and compared their accuracies on each of these sizes for a fixed test dataset size. I have also calculated the precision, recall, fscore and confusion matrix for a particular dataset size for all of these classifiers but I do not feel it would be correct to generalise the obtained observations for all training dataset sizes.",MLQuestions,2022-07-19 22:23:34,1
I haven't seen a tool that does that. It should be pretty simple to use opencv or python-pillow or PIL to get the crops and pass them to the different models.,2,w33fq9,"Hello everyone, I apologize if this is a common question or I don't know the keywords to but I can't seem to find it. Anyways, given an image, I only want to analyze certain parts of the image. Let's say the top 10%, bottom 10%, right 10%, and left 10% of the image. And let's also say I want to do OCR on top 10%, object detection on the left and right 10%, and a binary classifier on an existence thing in the bottom the 10%. Outcome is text that I can save. 

I know I could just crop them out manually and run each model differently. However, I want to see if it's possible to combine all the different models and run it on an online image/video stream in parallel. 

Thank you!",MLQuestions,2022-07-19 14:01:36,2
"First of all, it's a challenging task. There is no a straight forward method to get an answer to these questions. 

If you have time, you can try question answering algorithms. Lots of considerations there, too. It won't give a clear answer.

Best luck!",1,w3bd0f,"Hello guys. Hope you guys don't mind giving me some advice about an interesting problem I'm facing.

Given a dataset of customers' reviews about a certain product, how do you answer these questions: 

* What aspects of our product customers are satisfied/dissatisfied with? (ex: happy with payment methods, unhappy with customer service)
* Of those aspects, what people mostly compliment/complain about in particular? (ex: when complain about customer service, customers often mention ""impolite"", ""not accommodating""; when complain about delivery, customers often write things like ""delay"", ""rude"")

What I've tried:

* For question number one I used part of speech tagging and did a count to get the most popular nouns. It seemed accurate.
* For the 2nd question I used part of speech and dependency parsing to find words that often modify those nouns. The result was okay-ish but seemed like I lost quite a lot of data cause those nouns didn't always have a modifier but they were rather just a component of a sentence (pob/dob/sub/root). 
* To tackle the above, I tried to shorten the sentences by looking only for the head/children of the nouns in question. The shortened sentences individually seemed fine but when I did an n-gram to get all the smaller phrases to do a summary of them, I again lost a lot of information cause there were many unique phrases.

So what should I look into next? I'm thinking of topic modeling but I heard it doesn't work well with short sentences like reviews. Any input is appreciated!",MLQuestions,2022-07-19 20:00:55,2
Can you just downsame or upsample? How many labels do you have? what are their relative frequencies?,2,w3b4n4,"I am working in time-series anomaly detection problem. The data is largely imbalanced in terms of the labels. I am using RNN, LSTM, GRU, and Transformer model for training. However, I am not sure how to address the data imbalance during training.",MLQuestions,2022-07-19 19:49:55,4
"The ""training"" happens when you call the fit() method on the text.

Combine the text, then do vectorizer.fit(text) to train your vectorizer.

Then do vectorizer.predict(A) to get the vector for A, then B, then you can do cosine similarity.",1,w2vmud,"I want to compare two bodies of text ( A and B ) and check for the similarity between them 

Here's my current approach: 

&#x200B;

* Turn both bodies of text into vectors
* Compare these vectors using a cosine similarity measure 
* Return the result

&#x200B;

The very first step is what is giving me pause. How would I do this with TFIDVectorizer? Is it enough to put both bodies of text in a list, fit\_transform them and then put their resultant matrices in my cosine similarity measure ?

Is there some training process with TFIDVectorizer, a vocabulary matrix? If so, how do I turn A and B into vectors that I could put into a cosine similarity measure?

Thank you in advance!

P.S I understand what other options exist, I'm curious specifically about TFIDVectorizer",MLQuestions,2022-07-19 08:35:29,1
"Two things are confusing me.

1. Why do people paste code into reddit and expect to get help? Please put it in a GitHub gist at least.

2. Why do both of the comments think this is tensorflow when it's clearly pytorch??",3,w2fan0," 

I'm running a neural net on my GPU, but it is taking up to 20 hours to run through 50 iterations of maybe 1200 sequences when training. This doesn't seem like a GPU issue because I'm using a NVIDIA GEFORCE RTX 3700. I've attached my network and my running code below. I'd appreciate any advice. I'm basically entirely self-taught and using a GPU is new for me.

Net Definition

\--------------------------------------------------------------------------------------

\#Create network model

class Net(nn.Module):

\#Define \_\_init\_\_ function which is the network's constructor

def \_\_init\_\_(self, n\_hidden, future, input\_size):

super(Net, self).\_\_init\_\_()

\#Define number of hidden layers as thenumber specified in constructor

self.n\_hidden = n\_hidden

self.future = future

self.input\_size = input\_size

\#Define layers

self.lstm1 = nn.LSTMCell(input\_size,self.n\_hidden)

self.lstm2 = nn.LSTMCell(self.n\_hidden,self.n\_hidden)

self.lstm3 = nn.LSTMCell(self.n\_hidden,self.n\_hidden)

self.linear = nn.Linear(self.n\_hidden, input\_size)

\#Define forward function which feeds data through the network

def forward(self, input):

\#Create empty array to contain outputs

outputs = \[\]

\#Define initial state of hiden layers and cell states as 0

h\_t = torch.zeros(1, self.n\_hidden, dtype=torch.float32).cuda()

c\_t = torch.zeros(1, self.n\_hidden, dtype=torch.float32).cuda()

h\_t2 = torch.zeros(1, self.n\_hidden, dtype=torch.float32).cuda()

c\_t2 = torch.zeros(1, self.n\_hidden, dtype=torch.float32).cuda()

h\_t3 = torch.zeros(1, self.n\_hidden, dtype=torch.float32).cuda()

c\_t3 = torch.zeros(1, self.n\_hidden, dtype=torch.float32).cuda()

\#Iterate through all values being fed in as input

\#inputS = input.split(1, dim=1)

for t in input.split(1, dim=0):

t1=t.cuda()

\#Call all layers and feed in correct data inputs

h\_t, c\_t = self.lstm1(t1, (h\_t, c\_t))

h\_t2, c\_t2 = self.lstm2(h\_t, (h\_t2, c\_t2))

h\_t3, c\_t3 = self.lstm3(h\_t2, (h\_t3, c\_t3))

\#Save output to variable

output = self.linear(h\_t3)

\#Append output to array of outputs

outputs.append(output)

for t in range(future):

p=output.cuda()

\#Call all layers and feed in correct data inputs

h\_t, c\_t = self.lstm1(p, (h\_t, c\_t))

h\_t2, c\_t2 = self.lstm2(h\_t, (h\_t2, c\_t2))

h\_t3, c\_t3 = self.lstm3(h\_t2, (h\_t3, c\_t3))

\#Save output to variable

output = self.linear(h\_t3)

\#Append output to array of outputs

outputs.append(output)

\#Return results

return outputs

\--------------------------------------------------------------------------------------

Training Loop

\---------------------------------------------------------------

\#Initialize instance of chosen model

future=10

attributes=6

model = Net(70, future, attributes).to(device)

loss = nn.MSELoss()

\#Choose optimizer

optimizer = torch.optim.Adam(model.parameters(), 0.001)

\#Define the number of epochs, in this case equal to the number of sequences in the training set

epochs = len(allTrainSeq)

\#Define list for containing loss

allTrainLoss = \[\]

rounds = 50

\#Define a for loop to run rounds of training

for j in range(rounds):

lossRound = \[\]

shuffledIn = \[\]

for i in range(len(allTrainSeq)):

shuffledIn.append(allTrainSeq\[i\])

random.shuffle(shuffledIn)

\#Define a for loop to run epochs

for i in range(epochs):

\#Zero the gradient of the optimizer

optimizer.zero\_grad()

\#Create input sequence by getting the trainingSequence at index i, grabbing only the values before

\#the those you would like to predict, and changing it to the correct dimensions

inputSeq = torch.FloatTensor(shuffledIn\[i\]\[:-future-1\])

inputSep = inputSeq.cuda()

\#print(inputSeq.view)

\#Get the data to test the outputs aginst by grabbing the full sequence the input was take from

\#and change it to the correct dimensions

evalSeq = \[\]

for p in range(seqL-1):

evalSeq.append(shuffledIn\[i\]\[p+1\]\[5\])

for p in range(seqL-1):

if(shuffledIn\[i\]\[p+1\]\[5\] != evalSeq\[p\]):

print(""FAIL"")

evalSeq = torch.FloatTensor(evalSeq).view(1, -1)

evalSeq = evalSeq

\#Run model and save results to outputs

outputs = model(inputSeq)

\#print(outputs)

outSeq=\[\]

for p in range(len(outputs)):

temp = outputs\[p\]\[0\]\[0\].view(-1,1)

outSeq.append(temp)

outSeqT = [torch.cat](https://torch.cat/)(outSeq, dim=1).cpu()

\#Get loss by feeding the results and the expected values to the loss function

lossOut = loss(outSeqT, evalSeq)

lossRound.append(lossOut.item())

\#Perform backpropogration

lossOut.backward()

optimizer.step()

\#Add current round's loss to list

allTrainLoss.append(statistics.mean(lossRound))

print(""Round "" + str(j+1) + "": "" + str(allTrainLoss\[j\]) + '\\n')

\#Create training loss graphs and save to folder

\#Display loss of training data

\#Declare title for plot and label y-axis

plt.title(""Loss of Training Data Complete"")

plt.ylabel('Loss')

\#Plot Training Data

plt.plot(allTrainLoss)

[plt.show](https://plt.show/)()

\#Display loss of training data

\#Declare title for plot and label y-axis

plt.title(""Loss of Training Data First Excluded"")

plt.ylabel('Loss')

\#Plot Training Data

plt.plot(allTrainLoss\[1:\])

[plt.show](https://plt.show/)()

\-------------------------------------------------------------------------------------------------------------",MLQuestions,2022-07-18 17:52:17,8
"There are two completely different concepts that are both named bias, and they are not related. Bias is the name of the constant parameter in linear regression. Also, a model can have a different thing called bias when the model is too simple (eg too few parameters) for the data. We say that a model with too few parameters is biased.",3,w26wpg,"Hi guys, I'm extremely new to AI/ML. So bare with me :)  


I have just looked into the basics of linear regression. Here is my preliminary understanding. Linear regression is used to predict a value of a variable, given a known value of another variable. I'll use x= weight(kg) and y = height(cm).

Our data may or may not vary much, if all our data points fall on the same line, then we can predict our y value given x with about 100% confidence, this number will decrease or increase. The slope of the line is a good indicator of this.

The line can be chosen with the different methods although I'll stick with least sum of squares. We find a line that fits the data best. i.e. a line that limits the distance of each data point from said line.(These are called residuals.

I have a rudimentary understanding of variance and bias in models, a model with a low bias represents the data more accurately(i.e. a curve rather than a straight line). Variance is the difference between the variance of the test data and the training data.

A curve unlike a line may have a high bias(accurate) but \*\*may also have high variance(which we don't want), this is due to splitting the data into training and test data.

A line although may have a low bias(less accurate) but will generally also have low variance(which is what we want when training and testing)

My question is, an equation of a line is represented as y= mx + c, where m is the slope, x is the variable and c is our constant. From what I've read, c will represent the bias.

But I fail to see how changing the value of c would make our line more accurate(in terms of predictions)? i.e if I let c = 30 rather than c=0 how does this affect the line and more importantly the prediction?  


Thanks",MLQuestions,2022-07-18 11:46:55,3
"That' sounds like a separate task for ML on its own, not just some pipeline. 


You could try using something like bag of words for binary classification? 

Maybe start looking into sentiment classification, as for data - you probably will have to mark it yourself, since I doubt dataset for task like this exists. But this is a very time consuming option. 

Alternatively, just keep manually creating rules on how to mark text, might be most boring way, but it will probably be easiest to do.",1,w29okj,"The task is simply explainable but I imagine very hard. I have a large (1000+) recordings from a questionnaire in which people are asked to describe some of their good and bad personality traits and also what traits they would like to gain/work on. 

&#x200B;

The problem is that they are given one answer box for this. Some people start with listing their good, some with their bad ones, some list only their bad, some list only their good. 

&#x200B;

When I read them I can separately them very easily, but because there is not much structure I am having trouble with building a simple rule based algorithm.

&#x200B;

Three examples:

\`\`\`

One thing that I would to improve is my social ability. I find it hard approaching people and starting a conversation with a stranger. Also I am quite lazy sometimes. One thing that I am very proud of is my perseverance as well as my loyalty to my pets.

\`\`\`

\`\`\` 

I am not a person with great perserverance, this is something I must work on. Sometimes I can be very lazy and I do not study the subject enough. I have trouble getting started with learning and then I keep pushing it forward, until I get into a time crunch and then I get stressed out and the quality of my work suffers. One thing that I am happy with is my ability to connect with people and I consider myself a very social person.

\`\`\`

\`\`\`

I am less satisfied with how much of a perfectionist I am. This can sometimes get in the way during work assignments because they will then take me too long. Besides that, I am a good listener and am empathetic with people.

\`\`\`

I have written an algorhitm that basically checks for words such as \`improve\`, \`satisfied\`, \`change\` and then classify what follows either as negative or positive until a word is met that negates this and registers everything as positive from that point on. 

&#x200B;

However, things still keep escaping or being misclassified. Negation is part of the problem as well as people who do not use words such as \`improve\`, \`satisfied\` but just write down some good and then some bad personality traits such as this:

&#x200B;

\`\`\`

I work very hard and make a lot of progress when I zoom in on something. I am a family man and love my partner.

I think that I sometimes can be too stubborn for my own good. I feel that my perseverance is not what it used to be.

\`\`\`

These things then do not trigger anything on the algorithm (which is currently about 22% of all cases). All in all, I'm hoping for a better approach. I could manually tag a few hundred and then have a model extract the rest but I'm not sure if this is a good idea and where even to begin. Does anyone here have any recommendations?

Cheers! Appreciate all help because I am stuck in the mud :p :(

This will be part of a pipeline so doing it manually is not an option. One obvious option is to separate the good and bad parts from the questionnaire but for this task it is also not an option. I",MLQuestions,2022-07-18 13:42:41,1
"Not sure what you want to do?

Do you just want to convert the color space?

There’s probably a python/openCV library for that.

Just write a little doodad python script that processes the images in the folder.",2,w26u2j,"Hi !

So over summer have been getting into ML and research and even landed a scholarship with one of the professors at my Uni. Though he's very busy and hard to catch to get help / questions answered from.

So research is related to Date Fruit disease detection ok. There is 4 classes, anyway. Total of 952 images.

The images all share white background but different distances from camera as well as some desk behind the A4 background shows.

Example of images: [https://imgur.com/a/3VQqHj9](https://imgur.com/a/3VQqHj9)

&#x200B;

So methods that are suppose to be used for this research to extract features are DWT and L\*a\*b\* 

Have the 950 pictures in 4 separate folders, how does one proceed ?",MLQuestions,2022-07-18 11:43:50,4
"I would label the periods by hand. Then I would check the frequency spectrum w Fourier analysis over a sliding time window using python’s datascience libraries. Eg https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.stft.html. Then I’d extract into a python pandas data frame a labeled dataset of time window, frequency power, and behavior. Then try to find useful features in the frequency power of windows during a certain behavior. 

Hopefully there’s an easy, manually crafted rule you can write for identifying the different behaviors based on the spectral frequencies present. If it’s not so simple, you could start using fancier classification techniques. But start with that dataset and exploratory analysis of spectra.",1,w1oy3e,"Hello everyone,

I'm looking for a way to segment a signal according to its patterns. To elaborate, I have recordings of the respiration rates of rats during social experiments. At first, the rat is alone in its cage and has a slow respiration rate. After 5 minutes, another rat is added to the cage, and the first rat begins to investigate; this is when the respiration rates rise rapidly. During these interactions, the first rat also vocalizes, affecting respiration rates. So there are at least three distinct patterns: alone, investigation, and vocalizing. The time points where the rat is vocalizing have been labeled.

I am looking for a method to segment the recordings according to these three patterns.

Does anyone have an idea?

&#x200B;

edit: I have added some examples

1) The recorded rat sits alone in the cage, slow breathing (20 seconds)

[https://i.imgur.com/xSCUoFt.jpg](https://i.imgur.com/xSCUoFt.jpg)

2) The second rat was inserted at 300 sec and soon after the pattern changes

[https://i.imgur.com/ckV1NEJ.jpg](https://i.imgur.com/ckV1NEJ.jpg)

3) After some time, the first rat starts to vocalize (5 sec), red segments

[https://i.imgur.com/KModuJu.jpg](https://i.imgur.com/KModuJu.jpg) ",MLQuestions,2022-07-17 20:09:03,5
"use remote servers instead, no laptop except for the high-high-high end stuff by players like LambdaLabs is worthwhile to use",8,w1g6wf," I  want to know which laptop is good for Machine Learning and AI? I am an  undergraduate student of Electrical Engineering and want to work with  data sets from Transformers in my Final Year Design Project. I cannot  afford a PC because there is load shedding over where I am and also the  portability factor.

Should I go with:

Intel + NVIDIA

AMD + NVIDIA

AMD + AMD

Also what amount of RAM is good?",MLQuestions,2022-07-17 13:09:10,15
azure has a built in recommender system model. i believe alot of them use the APRIO or market basket analysis algo (assosciation rules),1,w1ly0h,What are my options? What do I have to watch out for? What tools should I be using? Please share your experience with recommender systems. thanks!,MLQuestions,2022-07-17 17:36:56,2
"could be a programming bug or some other degeneracy.

If the input examples are significantly different and yet many outcomes are exactly the same to 8 sig figs something is going wrong.  What's your activation function?    Try something like tanh or leaky relu and not relu.  Relu might give this if every neuron is in the flat region for these examples and you're only seeing the effect of fixed bias everywhere.

Of course change random seed for initialization and retrain as well.",2,w1hchl," This is a binary classifier. Shown here is the before sigmoid function output (1 node output), the network is trained on the after sigmoid (0-0.5 and 0.5 -1) being the classification boundaries. Mostly expected, except for this one spike. Anyone have any ideas what the cause of this may be? It doesn't seem to be at any special location, and while the input data for the spike is different - can confirm that at the location of the spike, the network outputs exactly the same output - down to 8 sig figs. This does not occur elsewhere in the network. Training sets are randomized, balanced (oversampling), and normalized. This is the NN score distribution of testing on only data that is known to be classified 0, hence why we do not have the second distribution (which also has this same issue). Training and testing accuracy is going up - so I don't think it's overtrained. Ideas? Thanks! 

&#x200B;

https://preview.redd.it/lb11ynk907c91.png?width=2069&format=png&auto=webp&s=8189871f4cd6156784aceb2e246538fafa046225",MLQuestions,2022-07-17 14:01:03,1
"[https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make\_classification.html](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html)

[https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make\_regression.html](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html)

These should be useful.",2,w1bupy,"I am looking to simulate a dataset of 100 variables for two scenarios:

1. Outcome is continuous
2. Outcome is binary

In both cases, it is required that 5-10 variables be correlated with the outcome (weak-strong). I found a paper (Weston et al 2003 JMLR) that can help but it was too much math for me to be able to understand, much less implement.
Are there any implementations available online of such methods or other relatively easy methods for generating datasets to test hypothesis related to feature selection and importance ?",MLQuestions,2022-07-17 09:55:54,5
"You need to host the model yourself or find someone who hosts it for you. Then communicate with it via an API, like you would gpt3.

Does goose ai allow chatbots? That's run by people associated with gptj. Or cohere.ai? There are other LLM providers too.

Maybe there's a guide on huggingface on how to host it yourself.",3,w0muwp,"I'm looking to create an app involving chatbots but it  violates one of GPT-3's guidelines (no allowing users to create chatbots). The next best option is GPT-J (which is what that chai app uses im pretty sure) but I can't find a lot of stuff on how to really implement it in an app. GPT-3 does have a lot of guides on how to do something like this, so could I follow one of those and implement GPT-J in the same way? I'm new to ai and this is basically my first project, I've done shit with machine learning on google colab and all that but beyond that I don't have experience in implementing it in an app.",MLQuestions,2022-07-16 11:17:32,4
"/u/EducationalTie1946, I have found an error in your post:

 > “even though ~~its~~ [**it's**] out of sample”

It is likely that EducationalTie1946 has written a mistake and intended to post “even though ~~its~~ [**it's**] out of sample” instead. ‘Its’ is possessive; ‘it's’ means ‘it is’ or ‘it has’.

 ^(This is an automated bot. I do not intend to shame your mistakes. If you think the errors which I found are incorrect, please contact me through DMs!)",1,w0va4b,I made this genetic algorithm which i trained on around 20k datapoints. all of the saved models i have generated all pass the base fitness threshold so i kept it running and the past threshold kept getting beat so i stopped it after awhile. i have around 200 different out of sample datasets and around 200 different saved models with different fitness values all of which are not sub optimal. each dataset very does well with different saved models while some do well with multiple datasets. i have been told using a different saved model with a different dataset is a bad idea but the thing and that the model is just overfitting on the out of sample data even though its out of sample and the training data has never seen any form of these 200+ datasets and also each dataset has around 35k+ datapoints. i am wondering if this method of model selection is bad to perform? the data in these 200 datasets have different movements but the same type of data,MLQuestions,2022-07-16 18:07:45,1
"if this is a lightweigth demo that you want to be public, maybe you could just toss it in a huggingface space or replicate or something like that?",7,w0e9bg,I've trained a video generating transformer model and to deploy it (for demo purposes) I need a simple GPU. All of the GPU offerings out there are too bloody complicated. Where can I get a simple web server with a low-cost GPU?,MLQuestions,2022-07-16 04:12:38,11
"https://stability.ai/ (which actually incubated midjourney) is the company that trains and releases most of the public models used for AI art stuff. Katherine Crowson (aka @RiversHaveWings) is one of the founders. The only reason you haven't heard of it is because the folks involved are too busy doing cool impactful work to hype themselves. They also support a lot of academic research by providing compute. 

Disclosure: I'm a stability employee myself.",3,vzm6es,,MLQuestions,2022-07-15 04:17:00,4
try this: `python -m pip install -r requirements.txt`,2,vzqms5,"I tried to install locally GFPGAN on my windows10 machine (ryzen/radeon) thru Anaconda Python following a guide, everything seemed to go smoothly until I tried running this command:

`python inference_gfpgan.py -i inputs/whole_imgs -o results -v 1.3 -s 2`

then I got those errors:

> (base) D:\testFolder\GFPGAN>python inference_gfpgan.py -i inputs/whole_imgs -o results -v 1.3 -s 2  
Traceback (most recent call last):  
  File ""D:\testFolder\GFPGAN\inference_gfpgan.py"", line 9, in <module>  
    from gfpgan import GFPGANer  
  File ""D:\testFolder\GFPGAN\gfpgan__init.py"", line 4, in <module>  
    from .models import *  
  File ""D:\testFolder\GFPGAN\gfpgan\models__init.py"", line 10, in <module>  
    _model_modules = [importlib.import_module(f'gfpgan.models.{file_name}') for file_name in model_filenames]
  File ""D:\testFolder\GFPGAN\gfpgan\models__init.py"", line 10, in <listcomp>  
    _model_modules = [importlib.import_module(f'gfpgan.models.{file_name}') for file_name in model_filenames]
  File ""C:\ProgramData\Anaconda3\lib\importlib__init.py"", line 127, in import_module  
    return _bootstrap._gcd_import(name[level:], package, level)  
  File ""D:\testFolder\GFPGAN\gfpgan\models\gfpgan_model.py"", line 6, in <module>
    from basicsr.losses.gan_loss import r1_penalty  
ModuleNotFoundError: No module named 'basicsr.losses.gan_loss'  
(base) D:\testFolder\GFPGAN>

I am not a programmer/coder nor a software engineer; if anyone have any idea what I am missing that would be really appreciated.. Otherwise I guess I will just delete everything and try again.",MLQuestions,2022-07-15 07:54:02,8
"I did my MSc thesis on this and also wrote a paper on it (to be published in September). Have a look at Google Magenta, they have a Music Transformer as well as a variant of T5 for transcription. I used Bert and it worked fine but theirs worked better.",1,vzm1sz," 

Hey,  
I am seeking some advice for choosing the right positional encoding.

I have a bidirectional encoder (BERT-like) that encodes a musical sequence. The data thus has a highly repetitive structure. Musical features might repeat within specific intervals (e.g. beats or refrains on the highest level). Additionally the signal is also ordered in time (music played backwards does not sound so great).  
The obvious choice here would be a relative positional encoding, since only the distance between tokens matters. The question is which one? Rotary embeddings or ALiBi look very interesting, but I am a bit concerned by their bias towards locality, which is counterintuitive regarding the repetitive structure. Additionally, I am unsure, if they take the music's ordering in time into account (e\_ij = e\_ji?).

Thanks in advance!",MLQuestions,2022-07-15 04:09:40,3
I think this paper describes describes the issue you're encountering: https://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Anisotropic_Convolutional_Networks_for_3D_Semantic_Scene_Completion_CVPR_2020_paper.pdf,1,vz71iw,"Recently,I have been working with 3D medical image segmentation, however, I can not  understand why using 3D convolution to anisotropic 3D medical image casing bad performance.
Can anyone explain in a easy way, please",MLQuestions,2022-07-14 14:33:32,3
Ask on /r/datasets.,3,vyyrlc,PlantVillage does not contain the data I need for a machine learning project. I need a dataset that contains plant nutrient deficiencies.,MLQuestions,2022-07-14 08:35:56,2
"In duplicating the data, are you ensuring the duplicates remain in the same set as the ""originals"" (train/test) or are you duplicating and then splitting? If the latter, then you're assessing on training data which would bump the accuracy.",7,vyb4sk,"Hello everyone, 

I’m trying to create a model that predicts the gender based on first name. 
When I train the model on non-duplicate data the accuracy is very low 77%. But when I increase the data by duplicating the data I get above 90%. 

I need your advice on: 
1- Is it ok to train the model on duplicated data? 
2- what hyperparameters can be tuned to achieve a good accuracy? 
3- Other algorithms suggestions to build a model that can predict gender.",MLQuestions,2022-07-13 12:07:04,4
"To a certain extent it is trial and error - that's how architectures generally evolve and are fine-tuned. People will try different combinations of different layers and see how performance is affected. You can derive some general rules from these prior experiments that might help you choose what adjustments to test and what looks promising.

Depending on how general your problem statement is it may be worth experimenting more or less. The more atypical your use case the more it may be worth doing.

In general, I would default to looking at the SOTA method for any particular approach unless I have specific domain knowledge or insight into the problem that may help me better design my model.",4,vya542,"This might be a deep learning question, but how do people choose what layers they need for their algorithm? I understand it all depends on the situation, so let's say it's an image detection algorithm. I can immediately assume, ""let's use a CNN model"". But knowing it's a CNN model isn't enough. How do you know how many layers you'll need, what layers you'll need? It has to be more than just trial and error, right?   
  
I guess I'm struggling to figure out what questions to ask myself when it comes to building different models. For instance, when I was working with my team on a final project, I was tasked to build try and test different models. I used preexisting algorithms, made some minor adjustments so that the model would just... run. So when it came to explaining why I did what I did to my team, all I could say was ""It was just the only way it would work"". The Ph.D student on my team and who was also the defacto team leader (who I highly respect), told me that my explanation wasn't good enough and that I really need to be able to explain my reasoning. He's definitely right and that's what I'm trying to figure out.",MLQuestions,2022-07-13 11:24:48,5
"Consult r/buildapc and just make sure the motherboard is adequately cooled (Heat sinks etc.). 
>Importantly I'm looking for a board that can support 2 gpus, if I get another one later.

Also keep in mind that PCI-e extenders exist",2,vxx4tb,"I'm building a PC for both ML and DL use. So far I've decided on an RTX 3060 gpu and AMD 5900 CPU (although 3950 looks good too). I'm not quite sure about the motherboard yet. ROG Crosshair VIII Hero seems great but is probably overkill. Importantly I'm looking for a board that can support 2 gpus, if I get another one later. Should I go with the ROG or buy a cheaper Gigabyte board?",MLQuestions,2022-07-13 00:17:08,4
ngl I don’t think ML is a suitable solution for the problem.,0,vxr1a4," During the holiday season we need to scale infrastructure based upon demographics.

How to use MachineLearning to scale infrastructure based upon demographics where more load or transactions are happening?",MLQuestions,2022-07-12 18:25:32,1
"I try to help you to the best of my knowledge

>1) ""In neuromorphic systems, the information is distributed; it is stored and processed at every neuron’s synapses, which themselves are responsible for the learning mechanism""  
A) What does this actually mean?

Maybe you are familiar with the term von-Neumann Architecture(VNA). In VNA you got divided Control Unit, ALU, Memory and I/O. Everything connected with a bus. After each calculation of the ALU the results needs to be stored in the memory. For the next calculation it needs to be re-fetched and needs to be brought to the alu.  
Neural Networks are basically Matrix-Vector Multiplication. Lots of Multiply and Addition. A Single Instruction in Multiple Data, or short SIMD. These can be done parallel in GPUs but you still need to get all the weights and input data from memory for each layer. Neuromorphic Hardware is differnet. It is not VNA. And it stores the weights where thes need to be processed with input data. Different NMH does this diffrently. Mayne you want to take a look at BrainScales. https://www.kip.uni-heidelberg.de/vision/ 

>  
B) What is a ""neuron"" in this sense? Is it a piece like a transistor that is made of silicon and move charge?  
C) What is a ""neuron's synapses"" here?

It depends on what kind of neuromorphic computing you are looking at. BrainScales uses analog circuits for neurons and synapses, build from resistors, capicitors, inductives and transistors. Intel Loihi and Brainchips AKida are digital circuits. 

&#x200B;

>2) ""With each input pulse, a fixed amount of charge is stored on the capacitor""  
D) Is each ""neuron"" a computer that remembers how much charge to move? Somewhat like an adjustable capacitor?  
E) Do the ""capacitors"" are actual capacitors that hold charge (i.e., electrons)? Or are they just some counter that remembers it's value (e.g., ""I know have 10 charge). If it's the former, does the charge ""leak"" like in regular capacitors?  
F) How are these ""neurons"" connected? In ""fully connected neural networks"" there is a sequence of layers, where each ""neuron"" in one layer is connected to every other ""neuron"" in the next layer. Is this the case here? Where in regular computers it's not the case?

This is most probably written about an analog neuromorphic chip. Therefore I wopuld suggest to look into some PHD and Master Thesis written at KIP (link above)

>  
G) They mentioned ""learning mechanism"". What does this mean here? Do the ""capacitors"" hold more or less ""charge"" in the sense that it takes more ""charge"" to flip the ""neuron"" to it's on condition? Do the number of ""capacitors"" change over time?

There are different learning mechanisms for Spiking Neural Networks, e.g. Backprogration in Time, Spike Time Dependent Plasticity .... How they exactly work, I don't know.",0,vxb9g7,"I'm trying to understand how the actual pieces (capacitors, etc) work, rather than a high level view of ""this is just a brain inspired computer that is made from spiking neural networks"". 

I'm far from knowledgeable in computers, but from my understanding regular computers consist of gates, where a ""[a gate controls the flow of electric current through a circuit"".](https://www.scientificamerican.com/article/how-does-a-logic-gate-in/) A gate itself ""[consists of transistors](https://www.scientificamerican.com/article/how-does-a-logic-gate-in/)"", and each transistor is [a binary switch](https://www.pcmag.com/encyclopedia/term/transistor). If I understand correctly, a transistor is a piece made out of some material (e.g., silicon) that takes electrical charge (i.e., electrons) at the one end of the piece, which are then directed by the difference in charge to the other end. This charge at the end means that the transistor is ""on"" ([link](https://www.explainthatstuff.com/howtransistorswork.html)). I assume that there is some network of transistors in each gate to have various on/off combinations (AND, OR, NOR, XOR, NOT, NAND).  
From [here](https://d1wqtxts1xzle7.cloudfront.net/55043930/IEEE_TED2016_Neuromorphic_Review-with-cover-page-v2.pdf?Expires=1657633164&Signature=INrut1rl7wkS-dOQW6ACKd~~5gbvVrRwlViXmJW-AREU32TnK2A18MOUMeU7Kh-cz2aQZwM6ajXmHFi8PMIUIdQloOjLBx1-KkZ8I3an0iuy74ZWEFtYHlZmIT~n0EfTF1nOoAkcmVpvGADRTVUaD6XEXZ6U4DOM7P8rJT6l1yFIo50Dmfc3Xxv~owD1ch4ZsmEQqqnPyOtrOfiC3anttuTDzRqITSAXFGj5iG7jH5JQy-0JZZwkaoaWDfINuBDrcCSxhwrVA69X230vdp0fXwnPjeOwiKuv4ZI6HGqp3Hbn~uKNp0S7JnGq6PmbWK3RQolKKXGAuZ61f2TUSIQ~sA__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA):  
1) ""In neuromorphic systems, the information is distributed; it is stored and processed at every neuron’s synapses, which themselves are responsible for the learning mechanism""

A) What does this actually mean?   
B) What is a ""neuron"" in this sense? Is it a piece like a transistor that is made of silicon and move charge?   
C) What is a ""neuron's synapses"" here?

2) ""With each input pulse, a fixed amount of charge is stored on the capacitor""

D) Is each ""neuron"" a computer that remembers how much charge to move? Somewhat like an adjustable capacitor?   
E) Do the ""capacitors"" are actual capacitors that hold charge (i.e., electrons)? Or are they just some counter that remembers it's value (e.g., ""I know have 10 charge). If it's the former, does the charge ""leak"" like in regular capacitors?  
F) How are these ""neurons"" connected? In ""fully connected neural networks"" there is a sequence of layers, where each ""neuron"" in one layer is connected to every other ""neuron"" in the next layer. Is this the case here? Where in regular computers it's not the case?  
G) They mentioned ""learning mechanism"". What does this mean here? Do the ""capacitors"" hold more or less ""charge"" in the sense that it takes more ""charge"" to flip the ""neuron"" to it's on condition? Do the number of ""capacitors"" change over time?

Thanks a lot and apologies for the long post and tons of questions! Just trying to understand the topic better",MLQuestions,2022-07-12 06:29:08,5
"With Masters, and even moreso PhD, you will have an easier time finding a job and a higher starting salary. But Masters is 2 years and PhD is another 3-5. Which if you are planning on going to industry, is time better spent in industry because real-life experience is usually valued higher than degrees. 

For example, if you can get a job at a decent company now, you'll be making way more money and probably more senior role after working for 4 years than having done a PhD for 4 years and then entering the job market. Especially if you work as hard in your job as you would have in the PhD (PhD students' work life balance is awful for the most part).

If you want to do a research job though, it's really hard to get one without a PhD, so you don't have much of a choice there.",1,vxdco0,"This topic is not technical related at all, but more of a personal experience reflection

To give you some context, I've graduated from bachelor degree in computer science. And I decided to work in the field for about 2 years now. Working in this field gave me some insight, on where the field can be apply, and how to actualize them

Recently, I've been working on a research project with my professor, and to be honest, I am having a blast, which make me think may be I should have learn a master degree. But again, I'm still not sure. This is because even though I am having fun right now, I can't see my self in a few years still working on another academic research project

My idea of data scientist careers path is junior > team lead > senior, for technical stuff (maybe there's more but I have completely no idea), but I guess you can always jump the ship to more business oriented position later on. But is it that really necessary? Can't somebody from MBA just jump the ship and go straight to that position? (does it even happen?)

Moreover in data science there are many positions

* Data Scientist
* Data Analyst
* Data Engineer
* Machine Learning Engineer
* (Operational Research) Infrastructure

Does having higher degree for some of these making your life easier? And how much does the knowledge learnt from Masters/Ph.D. were actually applied in them? I wonder for anybody here that took the Master, and/or Ph.D looking back and thought that they were totally worth it (or not)",MLQuestions,2022-07-12 08:03:18,3
"What’s your definition of the Moore-penrose? If it’s VS^+ U^T where S is the sigma matrix of singular values then you can just expand the right hand side using the SVD of X, assuming X is full rank here.",1,vxi931,"What is the simplest way to show that X\^+ = (X\^T X)\^{-1} X\^T for any matrix? Is there a way to show this using SVD?

Edit: $X\^+$ refers to the Moore–Penrose inverse.",MLQuestions,2022-07-12 11:37:03,4
log(a/b) = log(a) - log(b),1,vxf3k9,"The following is a derivation of the cross entropy loss from logistic regression:

https://preview.redd.it/6ar0mqgfx5b91.png?width=1618&format=png&auto=webp&s=3d620a47854c94e7058b538c209317acd2477b60

I don't understand how to go from the 1st RHS to the 2nd RHS. Can someone please explain this to me?

Any help will be much appreciated, thanks in advance!",MLQuestions,2022-07-12 09:19:22,2
"The *best* approach would be to do something like this, but it is overkill for a hobby project: 

https://cloudblogs.microsoft.com/opensource/2022/05/02/optimizing-and-deploying-transformer-int8-inference-with-onnx-runtime-tensorrt-on-nvidia-gpus/

https://towardsdatascience.com/hugging-face-transformer-inference-under-1-millisecond-latency-e1be0057a51c",1,vx5wp0," 

Let’s say I have a machine learning library, like GPT-J 6B, installed on an Amazon EC2 GPU instance. And I want to be able to run and get results from the library, from another server. What would be my options in doing this? The other server is a regular Linux web hosting server, CPU only, which is why I need the GPU instance.

I was thinking maybe I could execute via the [ssh2\_connect](https://www.php.net/manual/en/function.ssh2-connect.php) function in PHP, or [paramiko ssh.connect](https://networklessons.com/python/python-ssh) in Python. From the CPU server, to execute a script on the GPU server via SSH. And then process the stdout results from the ML script.

Or, install Apache on the Amazon instance, and PHP. And create a Python/PHP API. And then access the instance via HTTP from PHP on another server. 

What do people suggest in terms of an approach here? I don't really know what all the options would be here, or how to determine which is the best option.",MLQuestions,2022-07-12 01:08:56,2
"it sounds like you are paying for azure services, which means you are also paying for azure support. contact azure support.

also, this isn't an ML problem, this is a devops/networking problem.",2,vxba2a,"We are trying to create several ML instances using the azure ML wizards and running into issues with trying to get them to work within our secure environment.

Our azure setup is a little more paranoid than the default MS configuration assumes so servers don't have public facing IPs (only internal private ones) and by default there are no outbound or inbound rules configured on anything. All outbound ML rules have been setup for the whole VNET and appear to be working but it seems (according to the MS ML documentation) that inbound rules need to be setup too.

This causes us a problem due to the aforementioned private addressing, but also because the machine are generated with dynamic IPs so any NAT rules we configure will need to be updated any time a new instance is spun up.

Is there any guidance to configuring ML in a setup like this?",MLQuestions,2022-07-12 06:29:53,3
"It's a bit more complicated than that. AI is a bit of a buzzword... there's no magic behind the scenes, every step of the process has to explicitly be accounted for and coded out. So a project like this would probably be broken down into many different components and even then the scope is unreasonably large, you'd probably want to focus on a more narrow use case.",6,vxgqqj,"I don't know if any of you follow this, but Pfizer were ordered by a federal court to release the safety information they used to apply for licensure for their vaccine. They said it would take them too long to release it, but they effectively lost this case in court and so they've had to release it all. They've dumped it often as PDFs, SASS files, and .xls files. Its very difficult to make sense of. It got me thinking - could an AI 'read' this data, and then answer questions about it, somewhat like IBM Watson used to way back when? Or answer much like GPT-3 can?  


Essentially - can an AI read data and give 'human readable' insights about it based on simple language questions? How would one approach a problem like this?",MLQuestions,2022-07-12 10:29:57,15
"You change the values of w and b, that will give a new value to 'graph', and from all those values you pick the values which has lowest of cost function value. That's what basically the GD does. 


Loss function tell you how bad the model prediction is for a sample. Cost function tell you how bad model prediction  is for the entire batch of samples. Alot of Cost functions outputs (different w and b values) will give you some sort of convex curve (ideally). Then you use GD to find best w and b for model, vis min(Cost function).",4,vwtto5,"I’m a total beginner and currently trying to understand cost functions and Gradient descent. Every article on this has the example of gradient descent where they show a graph and then something rolling down the graph to visualise finding the minimum of the function.

Here’s the thing I don’t quite understand: How can the cost function be graphed (and how can it’s slope and minimum be found) when it effectively only has one output, which is the average of how wrong the network was over many runs? Any help would be appreciated!",MLQuestions,2022-07-11 14:35:03,12
Just learn as you do ML projects using the data you’re already paid to work with. You’re in such a good position. Don’t worry about courses unless you aren’t creative enough to design your own solutions and find new problems to solve on the job and if you don’t want to exercise that creative muscle then just do courses.,2,vwzxys,"I saw an interesting Machine Learning Professional Certificate offered by the MIT. However, as you can imagine, it's not cheap. I can afford it but I'd like a second opinion before applying. 

I know there are a lot of free courses and materials on how to become a Machine Learnint Scientist/Engineer. I've taken a couple of them, but it feels like something is missing. I don't know how to explain it, but it's like the learning experience is hollow. 

Also, when applying for a job, do they usually ask for certificates or even Masters Degrees?

A little about me: I'm an electrical engineer, but I'm working on tech as a data analyst. I'd love to pursue a Machine Learning path, that's why I'm considering taking the certificate.",MLQuestions,2022-07-11 19:21:42,2
"CVAT can do this, either on demand or in bulk on a bunch of images, check out its serverless functions.  Alternatively if you can output labels in an existing format, ie pascal voc xml you can bulk load annotations into it using a zip file.  I use the serverless functions to do it today to do pre-annotations and I'd say it saves me 75-80% of the time that labeling would normally take.. it's also very useful for finding where your model is failing so you can present more of that failure to it.  The only thing I would say is be careful when using this for your validation set as it can give you overly high map scores because of bounding boxes being nearly perfect, if you are going to use it for validation make sure and go over the data very thoroughly so you make sure you aren't just training a model to act like your existing model.",2,vwvpvv,"I trained a UNet model for semantic segmentation. However, it seems like the model is a bit dated because I was initially getting good results on my previous test images (months ago), but I’m getting a lot of false positives on the current (new) test (it’s in production) images. 

I was wondering if it were possible for me to use the previous model as a semi-labeling tool. That is, reduce the false positives, use it to generate masks and then manually label the remaining unlabeled classes? 

I tried my best to make the question as clear as possible, if it isn’t I can clarify in the comments.",MLQuestions,2022-07-11 15:56:37,2
"pool.map() returns an iterable of the results of applying the function to all of the inputs. You aren't using them at all. 

I'm not very familiar with keras but you need to unpack the outouts of pool.map() and use them as inputs to model.fit()
model.fit()",1,vwtwsv," Is that even possible to do? I'm new to use parallel loops and figured if I could correctly implement one in a FL model, it would allow the data to run at the same time and predict much faster , rather than have each node run back to back. Currently I'm coming across some trouble trying to successfully implement one and just wanted to see if anyone has dealt with a similar problem. I've attached a picture of what I have so far but sometimes I get an error that says 'xtr' and 'ytr' cannot be found OR sometimes the program will run but has some miscalculation which causes the prediction to be totally inaccurate. If anyone needs any clarification feel free to ask!

https://preview.redd.it/44bge261d0b91.png?width=1388&format=png&auto=webp&s=e5b16dea983cbf427abe9196f97309dd9de66480",MLQuestions,2022-07-11 14:38:47,1
"Done. One thought: on some of your questions it could benefit your survey to have the option for selecting multiple answers.e.g. I have used both VS Code, Sublime, Jupyter and Google Colab, but was only allowed to pick one. The same with clouds.
I also find the question about which field would benefit the most of AI lacking. Missing sectors such as Finance, Insurance, Utility and Manufacturing.",2,vw8epp,,MLQuestions,2022-07-10 20:10:43,5
"Can you? Yes you can. By applying PCA you effectively got rid of all noise from the extra dimensionalities. My two questions for you are: 
- Why cant you use all 9 variables to compute the correlation with the new variable?
- How accurate do you want your results to be. If 2 variables cover 97% of the variance like you mentionned, then is there really a need to use all 9 variables instead of just the other 2. 

I'm curious, how did you arrive at 97%?",2,vvhes3,"I have 9 variables for 4 semesters.

I took an average of these 4 semesters for the 9 variables and ran a PCA. 2 factors cover 97% of the variance.

I only had to to cluster the observations through the 2 factors, which would be fine for the analysis.

Unfortunately I was now asked to do a correlation of the variation of these 9 variable's 4 semesters against a new variable that also runs in these 4 semesters. (e.g. is this new variable's evolution correlated with the old variables/factor's evolution?)

Can I get the average weights (coefficients) of the 2 factors I arrived from doing the PCA and apply them separately from each semester, and correlate this new variable only against the 2 factors?

Thanks!

&#x200B;

EDIT: to clarify, there are about 30 observations. I took an average of the 4 semesters for each observation to do the ""average pca"" that initially would solve the problem.",MLQuestions,2022-07-09 19:29:45,2
"I think you'd have a better time just doing it analytically without deep learning.

Rectify the image, identify damage regions, fit each candidate glyph onto the unknown glyph, measure error. Select which candidate glyph has the lowest error metric in its undamaged areas.",7,vv3nph,"Is there a tool that can decipher the S/N from the image below?  Fortunately, there is a nice sample dataset below the obscured numbers.   

https://preview.redd.it/m7shz1wf5ka91.png?width=1984&format=png&auto=webp&s=a8aaca10a653aa2e8cc80dccda93cf6e2cfff064",MLQuestions,2022-07-09 08:06:08,3
"Random bunch of things that might help

https://otexts.com/fpp3/expsmooth.html

https://stats.stackexchange.com/questions/192970/weighting-time-series-data-for-prediction

https://stats.stackexchange.com/questions/205232/how-to-down-weight-older-data-in-time-series-regression

https://stats.stackexchange.com/questions/196653/assigning-more-weight-to-more-recent-observations-in-regression

https://stats.stackexchange.com/questions/454415/how-to-account-for-the-recency-of-the-observations-in-a-regression-problem",1,vuz2q2,"I'm working on a simple ANN where the data gets updated almost everyday.

I tried training the model with almost 10k rows of data and got a poor accuracy score of \~0.5. ( Still mining those data, so can test with even more rows after a few days )

Then I tried training the model with just 1000 rows in 10 different batches and all got an accuracy score of >0.7. (and sometimes 0.9)

So I was wondering if it is a good practice, if I train a model everyday with last 1000 rows of fresh data instead of training it with larger data or not...",MLQuestions,2022-07-09 03:48:48,2
"1. Math is generally concerned with *proving* statements that describe axiomatic systems, not just solving equations or estimating parameters. ML generally solves problems with pattern extraction from data, which isn't generally aligned with proof-driven insight (at least not in its current form).

2. Most ML approaches aim for *long-run* success, e.g. 90% accuracy over all approaches. However, it's generally unclear at exactly which times an ML prediction is incorrect. In math, if we can't tell whether a specific statement is true or false, it doesn't offer much value to the larger system we are usually interested in.

To put it simply, ML doesn't really answer the problems that mathematics is trying to solve. It doesn't mean it's *impossible* (after all, there are programmatic proof makers in math, some famously so) but it's not really a data-driven or pattern-driven field.",3,vuswvt,Apologies if this was already asked or if it's a dumb question but what's stopping professionals to use machine learning to answer say the world's top unanswered math questions?,MLQuestions,2022-07-08 20:52:52,4
"Probably not very accurately. Different judges, different subjects. Different zeitgeist.",8,vuh7bw,"Hello everyone!

I'd like to know if there is (or if it would be possible to create) an application that uses, as training, photographs that have won photo contests in past years (taken from the many websites that frequently host such contests), and then uses this data to choose from X number of one's own photos those that might be most ""likely"" to win future contests of the same type.

Do you think that would be a possible use of machine learning? Or would it be an improper use case?

I have always been fascinated to see what all its possible applications are.

&#x200B;

(Fun fact: the title for this post was created using GPT-3!)",MLQuestions,2022-07-08 11:19:58,5
"You're correct in your description of processing time series data. There's ""transformer"" models that can be lightweight and more efficient.",2,vuf9jp,"Currently I'm working on a personal project where I want to predict the velocity vectors (x, y, z) of a toy helicopter based on the control inputs from the remote and the angels (yaw, roll and pitch) of the helicopter itself.

I modified my own toy helicopter to record the data and I can now just simply fly around with it in my apartment and record data.

Now the data we have here obviously is a time series which screams for RNN from what I have read. Usually RNNs and their layers like LSTM seem to take data in samples \[samples, time steps, features\] but here comes the part where I'm stuck and where I'm curious how experts would handle it. The thing is that I'm not sure how one would handle the sampling of the data as I want to predict the velocity components in real time e.g. 1D Input Vector (or matrix like in my idea below) -> 1D Output Vector for every tick. The model training should work just fine as I can go over the data by moving a window across it.

My idea right now is to let the heli sit somewhere for a bunch of ticks and fill the inital sample so that it can get passed to the ANN, after that just kick out the last element of the sample and fill it with a new one for every tick. The input array/sample would then look like \[\[yaw, yaw+1, yaw+2 ...\], \[roll, roll+1, roll+2\], ..\] and every tick moves an element one index to the right (yaw to yaw+1).

Would this approach be something I should try, or are there better ways than RNNs to approach my problem? I've tried regular ANNs with just dense layers hypertuned via Keras Tuner but no luck with those models so far.

Thanks in advance. :)",MLQuestions,2022-07-08 09:53:33,5
This sounds like a case study in bias waiting to happen...,3,vub6fs,"Is anyone familiar with a dataset/resource that has people's names, their ethnicity, and their average income?",MLQuestions,2022-07-08 06:46:59,2
"An RF is made out of several decision trees, which is prone to overfit. to counteract these issues, we adopt a bagging procedure by generating several decision trees. The goal of these trees is to split the test data based on the training set into its purest form. As we randomly generate trees based on the sqrt(features) we get extremely high and low accuracy trees. As some trees overfit and others underfit. The algo cancels these trees out, generalizing over all trees, by the voting procedure.

This is why we have a tendency to get higher scores for other machine learning \ ai algos. On the other hand, this bagging process is why RF results are often more robust. If you give weights to trees with high accuracy you disrupt this process, by possibly overfitting the data. Sequentially we no longer generalize.
 
However, it is possible to balance, the features, and give weights. But it should not be necessary as the Gini index that splits this data should perform this task for you. It depends on the data you work on.
 
If you are interested in investigating further, look up mean decreased impurity or mean decreased accuracy. These are permutation feature importance which indicates which of the features are important in and out of the sample. For a technical overview, I can recommend: Random Forests with R, https://link.springer.com/book/10.1007/978-3-030-56485-8",4,vu2bdy,"My assumption is that some trees in the random forest will be more accurate than others - in general and for specific sets of inputs. Is there an issue (other than increased training time and complexity) with assigning weights to each tree’s vote based on the tree’s accuracy, or using a meta-model (such as another random forest) to dynamically assign the weights based on the inputs?",MLQuestions,2022-07-07 21:41:58,3
"Semantic similarity **is** ""meaning"" similarity. That's what semantics means. 

> Semantics: the branch of linguistics and logic concerned with meaning

But to your main question, it depends on how you define ""meaning"". If meaning of words is determined by their use (which is a pretty dominant idea in linguistics, though not the only one) then Word2Vec style word embeddings and their cosine similarity or a dot-product will be a good similarity metric.",3,vucotf,"For example, given two Wikipedia article titles, the model would rank how similar the \*subject matter\* of the two articles are. E.g. ""Apple"" and ""Pear"" would have a higher ranking since they're closely related, while ""Apple"" and ""Appliance"" would have a lower ranking.

When I google something like ""word similarity ML model"", most results are related to the grammatical or semantic similarity of the two words. Not really the actual meaning of the works.",MLQuestions,2022-07-08 07:58:00,9
"It's very common to perform Spectral Clustering on graph data.

Spectral Clustering is simply the k-means clustering of a spectral embedding of your graph. Many libraries, including sklearn, have this functionality.

Something you'll find out is that using a purely Euclidean metric on graph data doesn't generally work well. Graph ML is a huge field and there is a variety of information out there for you to help guide you. Good luck!",2,vtvg1a,"I need to implement k-means clustering on a graph dataset and I have searched for clustering methods online and found the *scikit-network* and *networkx* libraries. Scikit-network is giving me a lot of import issues but it has a k-means functionality. I can implement NetworkX properly but it does not have kmeans clustering and its documentation on clustering doesn't seem to match what I need.

My data set is a large directed weighted graph in CSR format. I need to use k-means clustering to identify large and important communities of nodes in the network. I am also open to suggestions for other methods besides k-means, although k-means is preferred in this case (not really my choice).",MLQuestions,2022-07-07 15:55:49,3
"Help me understand something, are you trying to predict winners solely from the green columns? That will be impossible as, within a division, each athlete has the same features and they all “look the same” as far as the model is concerned. Do you have historical competitions for the lifters? A good feature might be their most recent performance on the lifts within the competition.",1,vu0zg1,"Hello,

Is there a way to create a machine learning model that knows to use a set of inputs to figure out what data to look at and another input to determine the output from that section of data?

&#x200B;

https://preview.redd.it/mcuy9tqbk9a91.png?width=340&format=png&auto=webp&s=ea415cc282d46216327f445970ecb12a0bcd5265

Please see diagram above. I'm trying to find a way for the machine learning algorithm to use the three inputs outlined in green to limit its decision making between the TotalKg input outlined in blue and the output Place outlined in yellow to the data outlined in red, where Gender, AgeClass, and WeightClass are all the same (same competition division). I'm not looking to manually filter the data down to a specific division because I'd like the model to be responsive to the division the user inputs.

I am new to data science and working on a project utilizing the openpowerlifting database to create a machine learning model to predict what someone would place in a local powerlifting competition, given their age, gender, weight, and how much weight they lift.

If distinguishing the two types of data isn't feasible or wouldn't be your recommended approach, could you please suggest other models or approaches to try? I'm currently using SKlearn in Python.

&#x200B;

Thank you!",MLQuestions,2022-07-07 20:28:47,6
"Ask him if lamda is sentient. Lower your voice, try to look very serious and concerned when you do it.",4,vtcopm,"So, long story short - the company for which I'm an intern has a consultation every Friday with a ML expert where whoever wants to learn something about ML or has a question regarding specific task asks him. Yesterday they told me to prepare some questions for the guy but I literally have no idea what to ask him. Usually when I don't understand something I just use google to get the answer. And for me that's enough because my questions are mostly basic / intermediate and there  are a  lot  of people who have already written articles / stack overflow answers  etc etc on them. 

With that being said I am pretty sure that they'll expect me to ask the guy  stuff but I literally have no idea what to ask him simply because I am not as deep into the topic as the other people who attend these consultations. So what are some interesting questions that I can ask him?",MLQuestions,2022-07-07 00:20:39,12
"Seems pretty cut and dry for binary classification.

 I would just be careful about text that is identical between both columns. Either remove those rows, or consider a separate class for those samples.

See how far a BOW model goes, perhaps with tf-idf.",3,vt5vv9,"I have train data where the first column is English text written in a certain style (say, style A) and the second is that exact text written in a different style (style B).

I want to perform a binary classification in which a model detects whether a given text input corresponds to style A or style B.

Should I approach this like a traditional binary classification task where each text has a label (A or B) or is there any other ways to deal with such pairs of data?",MLQuestions,2022-07-06 18:06:42,1
"Sorry, you'd have to provide more information. What is it that you want to do?",1,vsvl0v,Can we use Association rule algorithms for binary classification. Anyone tried this before?,MLQuestions,2022-07-06 10:33:19,1
"So you did split your synthetic data in a training and test set? And they both have the same accuracy?

And is it classifying the images are spitting out a bounding box?",1,vstcqj,"I wrote a program in Unity that generated millions of fake images using the HDRP rendering pipeline. For starters I only want to detect a bottle of ""ITO EN"" ice-tea. 

[Here](https://i.imgur.com/knACIUO.png) is an example (left is real, right is the fake rendering).

I have a simple 3 layer resnet CNN with 3 blocks each, and use a Global Average Pooling layer at the end to visualize the detection.

Using the simulation dataset only I get an accuracy of 97% or higher. Using the real dataset I only get ~70% accuracy. 

This is infuriating, because the image dataset is extremely diverse and I use a ton of image transformations in order to provide a very high level of diversity. I also use various levels of lighting, bloom, camera exposures, motion blur, changing materials for all assets, as well as changing the properties for the target (the bottle), such as glossiness, reflection, emissive lighting, etc.

[Here](https://imgur.com/a/HE96aIS) is an example for the rendered dataset that is used for training, and [here](https://imgur.com/a/Kq6n1YK) is an example for the real dataset.

Anyone got an idea why this isn't working out?

EDIT: I wanna add that this is not a result of over-training, (a) because I use validation set and stop training if it hasn't improved and d (b) the test set performs very well.",MLQuestions,2022-07-06 08:58:47,5
"If you can detect the failure in real time, why don't you just use the failure detector instead of the model...

The non real time solution is to have a human do a review of the decisions after the fact.",2,vsv9xk,"Say I had an object detection model and I'm deploying it for an industrial purpose, what algorithms and techniques can I introduce to the system to ensure that the predictions my model generates are actually correct from the perspective of a human quality inspector? Thanks!",MLQuestions,2022-07-06 10:20:19,9
"something that requires defining the problem for yourself, collecting relevant data, and defining success metrics to measure your outcome against.",5,vscmws,"I'm going to be applying to deep learning positions coming up in a few months. I have years of professional experience as an engineer/ analyst, I have a masters in data science, I have knowledge in the theory of deep learning and a few projects under my belt. But I have not worked professionally in the CS or ML field and I want to make up for that as best I can. 

For example, I just started using git because I've never really collaborated with someone on my work before. I want to complete a project or two that demonstrates I am able to build something that is production ready. I was thinking of training something in Tensorflow and building a simple app in C++ that utilizes it. Or host something online that runs a neural network. 

Any other ideas that will help show I am business ready instead of just coming off as a student and hobbyist?",MLQuestions,2022-07-05 17:28:07,6
"Basically, you need to be able to solve a problem effectively. A solution that will bring value to the company and that is fundamentally right according to mathematical concepts. Besides machine learning concepts, try to study about system thinking to apply value to your answers.",2,vsm3td,"Got a mail: would like to invite you to the next interview. This will focus on your ability to solve a real-world ML/DL problem and the system related approaches. The conversation will go into associated math.

Can someone help me decoding this question, what exactly will they be looking into,where attention should be more, what should I focus more particularly ?",MLQuestions,2022-07-06 02:44:43,4
"Most labeling tools will allow you to draw bounding boxes with labels on images. So you would have multiple classes (fallen trees, potholes, rocks, boxes) and label images containing those by drawing boxes.

Personally, I would coordinate with the agency in charge to see which departments are responsible for which types of items. The person who fills pot holes is probably different from the person who removes roadkill. Classify them based on that.

Are these fixed location traffic cameras or something mobile (e.g. on a self driving car)? Do you have the images for this? Are they public? 

Once you have a decent dataset with bounding boxes you would train an object detection network. Some different example model architectures can be found [here](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md).",2,vs5fa7,"I'm working on a project about road obstacle detection on the traffic camera system but do not know which approach to apply accordingly. The requirement of the problem is to detect all stationary objects (except for vehicles) on the road (e.g., fallen trees, potholes, rocks, boxes, etc.) to issue warnings. I have researched and found that this problem often uses image segmentation and is often applied in self-driving cars. The difficulty I'm having is whether there is a way to group the ""stationary objects"" into a separate class to distinguish or define what the objects are and label them. Please help me come up with some reasonable approaches to this problem. Thank you for your help.

**Updated:  I've found some helpful repos at** [**paperswithcode.com/sota/anomaly-detection-on-road-anomaly**](https://paperswithcode.com/sota/anomaly-detection-on-road-anomaly) **that match my requirements. Thank you all for your attention.** ",MLQuestions,2022-07-05 12:03:59,4
"This is a fairly foundational question, I would recommend revisiting some basics before moving into modeling. 

I’d look into overfitting and generalizability.",9,vrkcl2,"I'm pretty new to this so I'm not sure if this is a stupid question but, why is my accuracy lower in tests than in last epoch? Loss is also bigger.

Only thing that makes sence to me could be that my NN is super used to training set but I'm using 60k images for training and 10k for test. 

Is it just coincidence that this happened 3 times or is there something that I don't understand?

I'd very much appreciate if someone can explain me what is happening here",MLQuestions,2022-07-04 16:31:34,5
"Instead of brazenly asking for an answer, why don't you show what you've tried so far? There is no way for us to know what part you're struggling with, so there's no way for us to help other than to just solve the problem outright, and that's not what this forum is about.",5,vrd1qu,,MLQuestions,2022-07-04 10:48:37,8
"In the case of non-convex clusters, it is fairly easy to imagine situations where raw distance between points in a cluster is not a useful metric to optimize. For example, imagine two clusters forming a ""T"" shape (one vertical line, one horizontal line). The top of the vertical line might be closer on average to points on the horizontal line, but that isn't a good indicator of clustering quality.

In 2D, these cases are straightforward to visualize, and I believe this condition becomes much more likely/reasonable/non-trivial in higher dimensions, although we can't easily visualize it.",2,vr81jr,"Thanks in advance, need this for my thesis.",MLQuestions,2022-07-04 06:59:12,1
Our question answer system is domain specific. Ran across our content (google doesn’t have this content) and answers are pointing directly to the our content. Saving users dozens of hours of research,6,vqj5gu,"I'm still new to NLP and I have been wanting to try creating a QA system on my own. So forgive me if this question sounds a bit to beginner-ish..

But enlighten me on this, let's say if I create a QA system in medical domain, or even more specific, Covid-19 domain, how is it gonna be better than me just searching on it on Google? Also if I think about it, googling is even way easier. So, by making a question answering system, are there any objectives that I could achieve that I wouldn't have by searching it on google (or other search engine)?",MLQuestions,2022-07-03 07:44:33,6
"I guess you are looking for this one?
http://colinmorris.github.io/blog/compound-curse-words",1,vqrp02,"The Y-axis had one list of profane words (eg ""d!ck"", ""dumb"", etc) and the X-axis had other words (""wad"", ""stick""), and the cells between the axes had combined the words (d!ckwad, d!ckstick, dumwad, etc..) with colors indicating how often each combined word occured on social media.

Thanks for any help in finding this study!",MLQuestions,2022-07-03 14:35:51,2
"Yep, fooled around with this for a while in the early days of DL when data was scarcer & augmentation was looked at as a possible panacea for that (it isn’t/wasn’t) context:radiology imaging

Best: a lot of data (as in a lot a lot)

So-so: some data plus a lot of image data augmentation.  Initially we thought that augmentation was “making up” new data inputs with translatable features .  Shearing, maybe.  But 180 flips H + V no. It wasn’t really in my opinion, it functions more of a regularizer.  Therefore good augments equaled a better regularized model equaled better transferability to out of sample new data.  

In fact, I’m pretty sure there have been some papers published, at least in the medical AI literature supportive of your thesis that well-chosen augmentations improve classifier accuracy.

Bad: oversampling, SMOTE

Worst: undersampling

Purposely omitted: GAN-based synthetic data.  Haven’t looked at this in a while but I suspect this is approaching the utility of more de-novo data.",2,vqrbbk,"With data augmentation, has anyone ever experimented on or seen the effects on performance/robustness of augmentation beyond the variety that an ML model would see in the real world?

EG: Experimented with image shearing for a model that would never see sheared/similar images in deployment/at test time?

I'm not sure if this makes sense, but I hypothesis that there could be a benefit to performance if augmentations were not too extreme. As a model may:

\- Be less likely to overfit to noise and better approximate the true signal. Even if it learns some redundant mappings.

\- Be better able to generalise to unseen transformations in real data because of a general 'robustness' that may be aided by augmentation outside of real world variation.

Interested to hear people's views.",MLQuestions,2022-07-03 14:17:05,1
I think error is when you want to transform your test set. You want to use transform() instead of fit_transform(). So both train and test use same vectorisation model learned on train set.,1,vqomn8,"This is for a project that's due soon so help would be greatly appreciated, I've never done ML before so sorry if the mistake is an absolute smooth brain one.

&#x200B;

I have a dataset that's a bunch of tweets along with personality scores, and I need to train an model to predict the scores.

This is what I've done so far by following a bunch of tutorials and stitching together what I learned.

&#x200B;

\`\`\`

train = pandas.read\_csv('../dataset/cleaner\_dataset.csv')

train\['tweet'\] = train\['tweet'\].str.lower()

train\['tweet'\] = train\['tweet'\].replace('\[\^a-zA-Z0-9\]', ' ', regex = True)

&#x200B;

X = train\['tweet'\]

y = train\['neuroticism'\]

X\_train, X\_test, y\_train, y\_test = train\_test\_split(X, y, test\_size=0.2)

&#x200B;

vectorizer = TfidfVectorizer(min\_df=5)

X\_vectorized = vectorizer.fit\_transform(X\_train)

&#x200B;

vectorizer = TfidfVectorizer(min\_df=5)

X\_test\_vec = vectorizer.fit\_transform(X\_test) 

&#x200B;

from sklearn.ensemble import RandomForestRegressor

model = RandomForestRegressor()

[model.fit](https://model.fit)(X\_vectorized, y\_train)

&#x200B;

model.score(X\_test\_vec, y\_test)

\`\`\`

&#x200B;

However I'm getting an error on the last line of code when I run it in the notebook.

&#x200B;

\`\`\`

ValueError: X has 178 features, but RandomForestRegressor is expecting 735 features as input.

\`\`\`

Full error image: [https://imgur.com/xyyTmKD](https://imgur.com/xyyTmKD)",MLQuestions,2022-07-03 12:07:49,1
"recommendation: Every time you add a concept, try to find a relevant paper to go with it. maybe the paper that introduced it, or a survey that discusses it, or a recent paper that used the method and explained why, etc. I haven't been keeping it up to date, but here's my version of your thing (heavily focused on identifying the introducing publication): https://github.com/dmarx/anthology-of-modern-ml

even if no one else follows the project, I still super recommend building the knowledge base you're describing for your own benefit. It's a really powerful exercise.",5,vq2rnm,Hello! [https://github.com/Charbel199/machine-learning-concepts](https://github.com/Charbel199/machine-learning-concepts) is a github repo I have developed with the goal of  keeping track of all the useful machine learning concepts that I learned throughout the years along with some resources and explanations. Your opinions and feeback are greatly appreciated :),MLQuestions,2022-07-02 15:22:43,2
"sounds like more of a computer vision problem than an ML one

pretty sure most modern smart phones have face unlock so i dont see why this couldnt work.",1,vq3cew,"This might be a stupid question (sorry about that) - I am not an expert in ML.

For a project, I want to be able to identify specific people in images. So the user flow is something like this - the user provides an image (or a couple of images) of a person they want to track. After that, given a set of images - I want to filter images that contain the target person.

I believe I could do this using AWS Rekognition. But I am trying to avoid this path since images would be sent over the network in this design.

Instead, I want to do this locally on the device (should work in offline mode) - and I want to achieve the same results on all clients (web, android, iOS).

Given a pre-trained model (are there public/open-source pre-trained models that do this?), can I achieve the same functionality as AWS Rekognition?

I know that iOS and Android can run pre-trained ML models locally - the question is, is there a public/open-source model that I can use to do people recognition assuming the device can run/use that model?",MLQuestions,2022-07-02 15:52:41,1
have you tried asking your supervisor to elaborate?,1,vpu8ms," I was creating a text-to-speech system with a transformer.

The two inputs were a sequence of:

1. phonetic charecters lenght (100 padded)
2. Mel spectrogram lenght (500 padded)

Is it true that they need to be similiar sizes for it to work? My superviser told this reprenstation would never work, but i dont understand why.",MLQuestions,2022-07-02 08:30:04,1
"In general, syntetic data will create a syntetic model.
If you cannot validate that the data is equal to real-world data, you will not be able to use your model in real-life.",11,vpj4ka,"I want to work on a ML project, however, I could not find the dataset that I needed to train the model. My question is: is it a valid method to use synthetic data to train the model? Since there is no way to collect or get the data due to its sensitivity. Any other insights would be helpful as well.",MLQuestions,2022-07-01 21:07:15,13
"> The whole thing feels dishonest and misleading. 

Welcome to the industry :p

Here are some of my thoughts:

* ""Machine learning"" and ""data science"" are both incredibly poorly defined, very broad terms
  * meaning often depends a lot on the context in which these terms are being used.
  * I'm personally of the opinion that a better phrase for most of what we call ""Machine learning"" is ""statistical learning"" and subsumes basically all of predictive analytics. In other words, if you're model is just a simple linear regression: I still think it's ok to call that ""machine learning"", although I think it's also fair to maybe characterize that as somewhat dishonest since the phrase obviously conveys a notion of technical sophistication.
  * increasingly, the phrase ""Machine Learning"" is becoming analogous with ""Deep Learning,"" but this absolutely is not correct. The vast majority of applied ML in industry uses techniques like GLMs and tree ensembles.
  * Ultimately, this is all marketing speak anyway and people will use the language that maximizes hype for whatever they're trying to pitch to you.
* Speaking from my own diverse experience as someone who's worked as a data professional in a wide diversity of organizations (including two FAANGS), you'd be surprised how unsophisticated a lot of real world ""data science"" is. Most, even.
* most data scientist roles are at organizations that are undergoing a ""data transformation"" to become more ""data driven"". 
  * This is code for ""historically, we made most of our business decisions based on intuition and maybe sticking a finger in the wind.
  * the consequence of this is that data scientists get buried in ""low hanging fruit"" opportunities
  * because there's so much opportunity for low-effort improvements by minimally attending to data, the data teams and the organization at large are heavily incentivized to leverage simple, unsophisticated solutions so they can tackle more problems quickly rather than heavily optimizing solutions to narrow problems.
* the value of sophistication is really a function of scale
  * when I talk about ""low hanging fruit"", I'm talking along the lines of: domain expertise gets you say 50% of the available value for some opportunity, data-informed busienss rules takes that up 70%, simple modeling takes that up to 80-85%. 
  * So by tackling the low hanging fruit, you've captured close to 70% of the available additional value with very little effort
  * because that last 15% optimization *isn't* low hanging fruit, we're going to quickly encounter diminishing returns. Every additional percentage point of optimization is going to come with exponentially more effort.
  * This is the reason the bulk of data scientists are employed by huge companies like FAANGs: the scale of their business is large enough that an incremental improvement of a fraction of a percent can mean millions of dollars in revenue or savings.
  * conversely, if your organization is not operating at that scale, it's not unlikely that it'll will cost your company more to invest in optimizing some solution than the value they would get from that solution. 
  * And even after putting in that investment, it's still a huge risk. Every application of predictive analytics is essentially a kind of experiment, and with every experiment, there's a possibility that the tested hypothesis is wrong and will be rejected, i.e. the model doesn't do anything of value. 
  * When data scientists are given the freedom to do their best work, they are and need to be a huge cost center for whatever organization they operate in. Otherwise, you're asking them to essentially be be data-savvy business consultants who are perpetually chasing low hanging fruit, which is exactly the position most industry data scientists find themselves in.
* Because of these scaling effects, most of the work that actually does require sophistication will end up getting subsumed by engineering teams
  * if you are a data scientist working in isolation, your ability to operate on large data sets and deploy complex models is limited. This type of work requires engineering support, which means the further away your data scientist is in the org tree from the closest engineer, the more their hands will be tied with respect to the amount of sophistication they can apply to anything that will be deployed.
  * deep learning and data science tools are rapidly becoming staples of undergraduate CS curricula, which means more engineers are equipped to identify and act on opportunities to apply ML without engaging with a data scientist.
  * this creates a kind of feedback loop that further isolates data scientists from the engineering resources they need, often relegating them to being a kind of ""ad hoc analytics monkey"" for leadership.
* the ""value function"" the solution here is optimizing is probably more multi-faceted than you realize
  * specifically, even if your data scientist has all of the engineering resources they could want to deploy the most sophisticated SOTA solution to your orgs problem, there might be good reasons why they wouldn't want to.
  * the ultimate goal of these sorts of projects is almost always to drive some kind of behavior. This often means that it's more important for the outputs of a model to be interpretable than predictively accurate.
  * additionally, the data scientist is ultimately subject to the demands of their customer: the business stakeholder. 
  * This unfortunately means that sometimes they will be relegated to approaches whose mechanism can be understood by the stakeholder, especially if it's a new relationship and the data scientist is still building trust in the org. It can even mean the scientist will be required by their client to incorporate features in the model that don't carry any predictive signal at all.
  * This creates an even heavier bias away from sophistication than the whole ""low hanging fruit"" or ""I'm my own data engineer"" thing. First and foremost, the data scientist is doing work for their customer and they need to make their customer happy as best they can.

TL;DR: From what you've shared, I see nothing inappropriate about describing this work at least as ""data science"". Calling it ""ML"" carries a weak implication that deep learning or something similarly sophisticated is being used, but even if that's not the case: the fact that they're forming predictions of any kind by performing computations on historical data I think makes it appropriate.",15,vp6p42,"My Question is: What distinguishes a traditional algorithm from machine learning?

Apologies for the wall of text.

I manage a product with a massive amount of data (1m+ weekly users, 50+ demographic datapoints on each user + user history as well as their interactions with hundreds of customers). At the core of the product is an algorithm that takes a number of inputs (based on trailing historical data) to predict the revenue-optimizing decision.

Recently, our new leadership has begun to call this Data Science and touts this as ""Machine Learning"". I'm proud of what we've put together and the impact its had on the business, but this feels like the wrong characterization of what is just a semi-complex algorithm with almost all of the calculations occurring in SQL.

This has become a sort of big issue as they've asked me to speak to our ""Machine Learning"" implementation to customers, investors, and others. I dodged that characterization by instead calling it a ""model"" or ""algorithm"" and they took notice and have asked me to embrace the term and update our materials (presentations, roadmap items, etc). Compounding this, they've hired a data scientist who concurs with them that we're using a ""predictive machine learning"" model. I'm skeptical of his expertise and feel like he should be making an effort to create an actual ML model we can compare against our current model.

The whole thing feels dishonest and misleading. Machine learning feels far outside my depth: I couldn't hold a conversation about it and I have no real clue what a decision forest, neural network, tensors, gradients, or any of the other machine learning terms I see across this sub or elsewhere mean. More details specific to my situation below:

\------------------------------------------------------------

The core  goal of our data effort is: Based on what we know about a user and what we know about a customer and their provided estimates, what's the optimal revenue-maximizing decision?

There's many calculations that are factored in to accomplish this, for example:

* We calculate the median average deviation of a customer's proposed vs actual success rate on a rolling basis.
* We segment our users based on demographic (age/gender/etc) and calculate their success rate relative to the population's average for a success coefficient based on a rolling basis.
* We run a simple regression between user characteristics and historical success rates for each customer.
* We factor in historical reconciliation rates from the customer (% of successes that are ultimately rejected by the customer at invoicing) to discount revenue estimations.
* We determine whether the user's experience should be optimized using a revenue-per-minute or revenue-per-opportunity approach. If we expect them to make a limited number of attempts, we maximum the expected revenue of each interaction. If we expect them to make a larger number of attempts, we optimize for potential revenue per minute. (EPC vs EPM for those in the advertising space)

It gets pretty gnarly, but what we end up with is a huge number of coefficients that inform our user to opportunity matching logic.  An example of how this could result in different opportunity rankings for a pair of users could be:

User 1 - Average Attempts per Session 2.1 ( to be ranked by Expected Revenue)

1. Project A - Potential Revenue $10 | Expected Revenue $2 | Estimated Success Rate 20% | 30 Minutes | Expected Earnings Per Minute $0.06
2. Project B - Potential Revenue $25 | Expected Revenue $1 | Estimated Success Rate 4% | 10 Minutes | Expected Earnings Per Minute $0.10
3. Project C - Potential Revenue $1 | Expected Revenue $0.80 | Estimated Success Rate 80% | 5 Minutes | Expected Earnings Per Minute $0.16
4. Project E - Potential Revenue $10 | Expected Revenue $0.6 | Estimated Success Rate 6% | 4 Minutes | Expected Earnings Per Minute $0.15

User 1 - Average Attempts per Session 6.3 ( to be ranked by Expected Earnings Per Minute)

1. Project C - Potential Revenue $1 | Expected Revenue $0.90 | Estimated Success Rate 100% |5 Minutes | Expected Earnings Per Minute $0.18
2. Project D - Potential Revenue $4 | Expected Revenue $0.75 | Estimated Success Rate 18% | 7 Minutes | Expected Earnings Per Minute $0.15
3. Project E - Potential Revenue $10 | Expected Revenue $0.5 | Estimated Success Rate 5% | 4 Minutes | Expected Earnings Per Minute $0.125
4. Project B - Potential Revenue $25 | Expected Revenue $0.75 | Estimated Success Rate 3% | 10 Minutes | Expected Earnings Per Minute $0.075",MLQuestions,2022-07-01 10:51:49,10
"Sockets, if your IPC is the bottle neck then refactor to shared memory.",2,vpiola,"This is not strictly an ML question, but I'm hoping someone here would have a good answer to this. This is my first foray into ML and I don't really know what I'm doing. I made a ""game"" in Rust (2D, player navigates around obstacles to reach a goal, pretty simple). I would like to use Pytorch or TF. In my Rust code I can easily access what would be inputs for the neural network, and the inputs for the game is just digital WASD. I don't know how I should communicate data to and from my game to the python code I will write.

Looking things up I've found a few methods of inter process communication, things like pipes and sockets and specific protocols for them, but I've only just started googling things. If anyone here has a suggestion for something fast and simple they used to solve a problem like this I'd like to hear it. I'm on Windows btw. (although I could boot into linux if for some reason theres a good solution on linux that doesnt work on windows)",MLQuestions,2022-07-01 20:40:59,2
"I suggest you don't deploy directly from Dev to Prod.

A safer workflow would be:

* Take your code and models from Dev and, put them into some Source Control system. Probably Git for software/notebooks/etc and some sort of ""Artifact Repository"" (maybe as simple as s3 buckets with a naming convention) for large binary models or other data.  And tag the versions of both of them as a release candidate.
* Deploy from that artifact repository and source control system to a QA environment and test there.
* If and only if the tests pass, tag the exact same versions as a production release version in the source control system and artifact repository.
* Deploy from that Artifact Repository and Source Control System to production.

Most of the issues like hardcoded paths  should be caught in that QA environment.",2,vozdyo,"Hey everyone! 

*Background:* I recently did a Prod Release of an ML model, and unfortunately some of  the paths were hardcoded so this caused issues (Prod did not have access  to Dev credentials and paths). Obviously this was straightforward to  fix, but frankly I did not know this would cause an issue beforehand which I want to change. Fortunately there wasn't business impact, but  it's still something I want to fix next time. The model worked fine in  Dev.

*My question is*:

1. What are best practices for Prod releases of ML models? In particular, how can a model succeeding in Dev fail in Prod?
2. How can I strengthen my understanding of Prod release best  practices, system design, git / cluster / database, the more software /  infrastructure end of things?

For context, I'm a data scientist, so my background is much more in  math / ML modeling; I want to get better at the software side / whatever side is relevant for this type of issue. I know how to code, but I want to get good enough at system design or whatever caused this issue to prevent it in the future.",MLQuestions,2022-07-01 05:13:33,3
"text style transfer, which is basically a kind of translation (i.e. seq2seq). finetune a decoder on medical literature and use that as a translation target. you should be able to emulate demos where people train models to make text more ""shakespearean""

https://arxiv.org/abs/2011.00416",5,voqw1n,"Hi everyone, 

I'm working on a project, where I need to convert laymen terminology to technical terminology within the medical field. 

For example, if I have a string ""heart attack"", I am looking to find the string ""myocardial infarction"", which is the technical term for a heart attack. 

Does anyone have recommendations on what I should be googling or researching for this?

Thanks!",MLQuestions,2022-06-30 20:31:32,5
"global/local wrt what? My immediate intuition is that *stochastic* gradient would be considered ""local"" wrt the data because each update step is limited to information from a given minibatch. Contrast this with classic gradient descent where all of the data is used each gradient step, i.e. a ""global"" context for the gradient step, wrt the training data. 

But honestly I'm not sure if I'm using global/local in the same way you are, so I'm concerned this question might be under-specified in the absence of some clarification from you regarding exactly what you mean.",4,voa3t2,"As the title asks, what would you consider it and why? By global/local I don't mean global/local minima. I'm referring to getting global/local information to update the weights (local = weight updates are based on their closed surrounding/neighbors. Global = weight updates are based on the entire network/large surrounding) ",MLQuestions,2022-06-30 07:45:19,6
Have you tried random forest regression or are you using random forest classification?  Similarly you should try XGBoost regressor as I think the default is decision tree classifier.,2,voi1ui,"can simple regression outperforms the boosting models, if not can you tell what I might be doing wrong

&#x200B;

https://preview.redd.it/firyb3ypit891.png?width=749&format=png&auto=webp&s=d99b5426611b4e7b97629aae1bd9e2b4876d89d1",MLQuestions,2022-06-30 13:28:30,7
"You’ll have to rethink your dataset a bit for it to be useful for predicting outcomes.

Think about it. What are you going to know before the fight begins? 

Fighters win loss record? Sure.
Their strikes landed in previous fights? Sure.

Strikes landed in the upcoming fight? Absolutely not. Get that out of your predictive model. 

So just rethink which columns make sense from the ground up by asking yourself “would I know this value before the fight begins?”",3,vnxdou," 

I have created the largest dataset publicly available for the sport of MMA here: [https://www.kaggle.com/datasets/danmcinerney/mma-differentials-and-elo](https://www.kaggle.com/datasets/danmcinerney/mma-differentials-and-elo)

I'd  like to create a Python script to use this data in a predictive model.  It seems xgboost or LightGBM are going to be the best options (please  correct me if I'm wrong), but I can't seem to find an easy to follow  tutorial on how to use these libraries to predict 1v1 or team vs team  competitions given an already cleaned dataset.

I  think my main confusion from the tutorials I've read is how does  xgboost/LGBM know which column of data relates to each individual/team  in 1 row of competition data? All the tutorials I've seen just feed it a  dataframe with a prefix in each column to delineate the team/individual  but never specify how that prefix is used or how to interpret the  results on a per competition basis.",MLQuestions,2022-06-29 19:38:24,3
"1. As a rule of thumb, I believe that models should only be used for predictions within the range of the variables used to train it. Similar to a simple linear model, you wouldn't try to extend the best fit line past the data it was fit on to make predictions. I doubt latitude and longitude would be much use as the enjoyment of a trail is derived from the features of the trail; lat and long don't really provide any information on the trail features. The location is likely recorded just so people can find the trial.
2. That depends on whether you think the other types of trail can provide useful information, or whether you think they are fundamentally different in some way that will result in misleading predicted ratings.
3. For the rating, technically it's an ordinal variable, but you can treat it as continuous since if you model predicts a rating of 4.22 say, it's still an interpretable number that means something. An example of an ordinal variable that couldn't be treated as continuous would be something like a car engine size. Car engine sizes in the UK might be something like 1.2L, 1.4L, 1.6L, 1.8L, 2L, etc. So if you had a model trying to predict the car engine size based on other statistics of the car, a predicted engine size of 1.523L would be meaningless since engine sizes of that size don't exist. So to fix that you would want your model output to be ordinal and not continuous.

For the 'boolean' (categorical) variables, ""one-hot encode"" those. Route type is also that.",1,vnt6cy,"I like to go backpacking (multi-day hikes) and I want to build a model to predict how much I will enjoy the trails on my watchlist. I understand it is silly to predict how I will subjectively experience something, but it seems fun to see what gets spat out. I just have some questions on best practice.

[This](https://www.kaggle.com/datasets/planejane/national-park-trails?resource=download) was the best dataset I could find. It isn't perfect. I mainly hike in Canada and I only care about backpacking trails. Whereas, this dataset is about trails in the USA and only ~800/3000 are backpacking trails. From it I can get the following features:

* latitude
* longitude
* length
* elevation gain
* route type
* waterfall (boolean)
* lake (boolean)
* river (boolean)
* forest (boolean)
* cave (boolean)
* backpacking (boolean)
* rating (according to AllTrails.com, this is what I will be predicting for my watchlist trails)

Another problem with this dataset is with the 5 boolean traits (waterfall, lake, river, forest, cave). If it is unknown whether a trail qualifies for any of these traits, the trait will be set to false. Also, the rating values have been rounded to the nearest 0.5 (on a scale from 0-5). I just have to make the best of it, I couldn't find a better dataset.

The plan is to personalize the model to me. I'm going to add my completed trails to the dataset and give each a personal rating. Then I'll add a new feature, called something like ""isMe"" which will be 1 for my trails and 0 otherwise. Now, time for questions:

1. Does it makes sense to use latitude and longitude when I don't hike in the area covered by the dataset?
2. Should I cut the ~2200/3000 rows from the dataset that aren't backpacking trails since I only want to predict the rating for backpacking trails?
3. Since the rating values have been binned, would that mean I am predicting a category or a numerical value?

These are only the questions I can think to ask. Feel free to hit me with any other pointers you have to make this silly model as accurate as I can!",MLQuestions,2022-06-29 16:12:45,1
"Like [http://adventure.land/](http://adventure.land/) or any of the ones listed here [https://www.reddit.com/r/gameideas/comments/luznbw/mmo\_where\_the\_only\_way\_to\_play\_is\_to\_create\_a\_bot/](https://www.reddit.com/r/gameideas/comments/luznbw/mmo_where_the_only_way_to_play_is_to_create_a_bot/) ? The programming is crowdsourced, the machines I don't think so",3,vnm3uk,"Just having a lot of trouble finding info via good ol Google, but has this been done in any meaningful capacity yet? Anything along the lines of a dynamic machine learning program, that uses many machines that are not owned by a corporation, to process data and train in real-time. Any info at all would be helpful!",MLQuestions,2022-06-29 11:05:20,10
I did not understand you questions completely.but from what I understood you are trying to summarize text. Look at text summarization cases in web.,6,vniomb,"Hi, I'm a total beginner in machine learning and was wondering if it would be possible to make an AI that gets trained with a dataset of a pair of text \[og text, my notes on that text\] in order to give it a text as an input and make it create notes based on how I would do them. I already have 5 years of notes to feed to the ai but have no idea how to do it or if it would even be possible/accessible enough for me to make. Any tips?",MLQuestions,2022-06-29 08:38:07,2
DeepL,1,vnl328,Google translate is very unsatisfactory when it comes to translation of texts to my language. Are there any services or available models I can use?,MLQuestions,2022-06-29 10:21:18,1
"It's built into several CAD packages already now. 

I'm not sure what you're thinking, but you can create a structure, apply virtual ""force"" and have the computer iterate a design that's almost biological in appearance. 

Check this out
https://www.youtube.com/watch?v=a6bDLMWlS98",1,vnf9bq,"I'm from a Mechanical background and want to know that can this be possible? To be specific, I want to know it for F1 cars. Example: we know the aerodynamic principles and we know what are the rules stated in a rulebook. So if we have previous designs of Front and Rear Wings and Sidepods of a car how can we translate it to build the most efficient and reliable design. If no where would it go wrong?",MLQuestions,2022-06-29 06:00:11,4
"This sounds like a simple group by with an aggregate. 
```sql
Select id, max(profit) from Product 
Group by Id`",3,vne94b," 

Can you help me the following query, I was little bit confused how to solve that.

Table-Product

id name profit

01 p1 20

01 p2 40

02 p3 10

03 p5 50

03 p2 40

How to return the rows with maximum profit for particular date? Below is the output.

01 p2 40

02 p3 10

03 p5 50

Thanks in advance.",MLQuestions,2022-06-29 05:07:14,16
Try hyperparameter tunning scale pos ratio from 3 to 7 and see which reduce your error. I took 3 to 7 as ratio of positive to negative class  is 3:7 . You can also try 1  to 7,1,vmukgh,"Hi,

I am currently working on a diagnostic model based on repeated K fold cross-validation and xgbTree in R. Since my dataset is imbalanced (30 % controls, 70 % diseased) I am trying to find a way to address this. 

The xgbTree has a parameter called ""scale\_pos\_weight"" where I can simply put the ratio. I have, however, read that another acceptable way of determining class weight is to iteratively increase the weight and evaluate the sensitivity and specificity of the model. In doing this I found the model performed best at a class weight of 1 instead of the 2.33 otherwise used (the ratio).    


Can anyone confirm that is acceptable to determine the class weight in this manner and is it really okay not to have an increased weight for the minority class if the performance is better?  


Thanks!",MLQuestions,2022-06-28 11:45:08,4
Is the document mostly or only text?,2,vmo7ur,"I am currently trying to build a document similarity program using computer vision. So my objective is to come up with a score out of 100 which says how much 2 documents are visually similar given they are both captured in real-life scenarios considering all the rotations and perspective differences. Can someone guide me on how to tackle this?

&#x200B;

[What I am trying to implement.](https://preview.redd.it/4xuialqwdd891.png?width=232&format=png&auto=webp&s=48d5d768e60ab2fcdb5824bc6bde8d68b3ecb57f)

I will be using tesseract for the text part, But I am stuck with various approaches for the visual part and a bit confused too",MLQuestions,2022-06-28 07:12:09,2
"There’s a heap of research in algorithmic bias in deep learning and neural nets. There is many researchers (some in my lab) discussing and implementing different convergence methods in an attempt to find a better way of discovering the global optimum. However, without searching an entire “grid” of your cost space you’re unlikely to find the global optimum so you use other methods like Particle Swarm Optimisation as an example to find the best minimum based on the Laplacian (I think this is a roughly correct sentence).

To answer some of your question, if you go on to arxiv or google scholar and search for neural net algorithmic bias you will find some interesting and new research.

Edit: answering the question to the title of this post I found the following that may help:

> Think of a batch as a for-loop iterating over one or more samples and making predictions. At the end of the batch, the predictions are compared to the expected output variables and an error is calculated. From this error, the update algorithm is used to improve the model, e.g. move down along the error gradient.

I copied this text from here: https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/",2,vm0paw,"I've been working on a model trained via gradient descent, for which the dataset is small enough to run an epoch in one go rather than splitting it into batches.

Originally I thought that was great because it keeps the code a little simpler and speeds up convergence since the gradient is calculated without error. However, it's just occured to me that that error might actually be helpful as a built-in form of simulated annealing in the case where the miscalculation pushes parameters ""over the hump"" and into a better local minimum.

On the other hand, it's a naive implementation that just as likely pushes itself into a worse local minimum, so maybe you're still better off without batches if possible. Does anyone have insight or know of research exploring this question?",MLQuestions,2022-06-27 10:19:01,2
"This depends a bit on the algorithm, but there are plenty where you could retrieve the equation.

Imagine you have data of an object in motion (I never went past high school physics so this is the only physics example I can give you).

    import numpy as np
    from sklearn.linear_model import LinearRegression

    # get the same random numbers every time
    np.random.seed(1)

    # we have data for 30 seconds
    time = np.arange(0,30)

    # these are the true parameters
    velocity = 103.2
    acceleration = -9.8
    starting_pos = 2.4

    # this is the true trajectory
    y = starting_pos + velocity * time + 0.5 * acceleration * (time ** 2)

    # this is the trajectory with a bit of random noise
    y_noisy = y + np.random.randn(30)

    # our independent variables will be time and time squared
    X = np.c_[time, time ** 2]

    # fit a linear model, saying:
    # y_noisy = intercept + w1 * time + w2 * time^2
    lm = LinearRegression()
    lm.fit(X, y_noisy)

    # the starting point is just a constant added to our model
    # called the intercept
    est_starting_pos = lm.intercept_
    print(est_starting_pos)
    # 2.3933599630219504

    # the velocity is the first coefficient of the model
    # corresponding to our first variable, which was time
    est_velocity = lm.coef_[0]
    print(est_velocity)
    # 103.17678732493894

    # the acceleration is the second coefficient of the model
    # corresponding to our second variable, which was time squared.
    # we will multiply it by two to account for the 1/2 in the kinematics equation
    est_acc = lm.coef_[1] * 2
    print(est_acc)
    # 9.798013197559033
    
    # what does our model say the height would be at t=31?
    lm.predict([[31, 31**2]])
    # -1507.07157439

The model won't exactly spit out the equation form, but as I've shown above you can determine it fairly straight-forwardly. In the case of a linear model, the equation is `intercept_ + coef_[0] * var_0 + coef_[1] * var_1` etc., and the `predict` method lets you apply it to new data points. 

Hope this helps.",5,vlwcti,"So my project is basically about recreating a physics equation. I'm currently using python and have multivariate data generated by that equation itself (eventually I would be using experimental data). I could build models using algorithms from the SKlearn library but is there a way to generate an actual equation?

Like did the algorithm create a model that predicts the relationships between variables? Would I be able to see it?

Thanks! Really appreciate the help!",MLQuestions,2022-06-27 07:08:15,2
https://code.google.com/archive/p/word2vec/,2,vm1t8v,"I'm looking for a method/model/API that extends a list of examples.

E.g. if I give it a list of items such as ""Berlin, Munich, Hannover"" I want it to extend it with other German cities e.g. ""Hamburg, Leipzig"" etc.
If I give it a list of a few US Congress members, I want it to extend it with a few more. And so on.

I feel like there was a Google feature or an API product for this but I can't find anything close to what I'm looking for.

If anyone can refresh my memory or guide me into the right direction or give me a the right search terms to look for you'd make me super happy.",MLQuestions,2022-06-27 11:06:40,2
"You should be using stratified if your classes are imbalanced, although it becomes slightly less necessary if you are repeating your cross-validation

You want to minimize the chance that your test set is all or mostly made up of the majority class, and stratification ensures that this does not happen",3,vlssyj,"Hi ML experts :)

I am trying to make a diagnostic model but I am unsure about the use of repeated K fold cross-validation. I have an imbalanced dataset (about 30 % control, 70 % disease), and have read that maybe I should be using stratified K fold cross-validation. It was my understanding that the following code for my repeated K fold cross-validation, would account for the distribution of my two classes:

    fold_index <- createFolds(dataTrain$Condition,
                              #number of folds
                              k=10,
                              #return as list
                              list=T,
                              #return numbers corresponding positions
                              returnTrain=T)

I use the fold\_index variable as below:

    ctrl <- trainControl(method=""CV"", 
                         index=fold_index, 
                         summaryFunction = twoClassSummary, 
                         savePredictions=T, classProbs=T)

The documentation states: “For other data splitting, the random sampling is done within the levels of y when y is a factor in an attempt to balance the class distributions within the splits.”

Have I understood correctly or do I need to use stratified K fold cross-validation instead?

Thanks!",MLQuestions,2022-06-27 03:56:26,2
"If you're looking to extract features from time-series, so that they can be used in any regular regression model then,  


\- \[tsfresh\](https://tsfresh.readthedocs.io/en/latest/text/list\_of\_features.html)   ""tsfresh is used for systematic feature engineering from time-series and other sequential data"". It also comes with scikit-learn Transformer classes to use in your pipeline  
\- \[kats\]([https://github.com/facebookresearch/Kats](https://github.com/facebookresearch/Kats))  it also a time-series analysis package that can extract a whole range of features. It was developed by Facebook  


If by 'Transformers' you meant sklearn transformers, then I hope especially TSFresh will have some inspiration for you.",2,vlm669,"currently using transformers for sequential time series data. I am looking to try a new model.

Any ideas?",MLQuestions,2022-06-26 20:44:04,2
"Rather than examples of use-cases, maybe I can shed some light on when and why we use CNNs.

The big thing about a CNN is, of course, the convolution between the data and the neural network. This is, ultimately, a human-defined hard-coded preprocessing. Basically, rather than trusting the NN to process raw data all on its own, data scientists decide to stick a fixed transformation layer that help *highlight* patterns for the NN to find. You can think of this as helping the NN ""parse"" the data.

You mentioned time series data: a great example of a convolution you could use for time series data is: the Fourier Transform. You take your raw time series data, feed it through a Fourier Transform, then feed that transformed data into the NN, and then the NN can find *frequency* patterns much more easily.

Another example is in NLP, where data scientists apply all sorts of preprocessing and text filters and static analysis tools before feeding anything to a neural network. This makes it so much easier for a NN to analyze text than, say to just see a raw series of letters and then try to predict a new coherent sentence letter-by-letter.",3,vlespa,"I'm looking for an example of a CNN for a use case that isn't images. For example, CNNs can be used on time-series data.

Does anyone have a video, blog post or book that goes over how CNNs work for a different use case?

Thank you.",MLQuestions,2022-06-26 14:22:32,3
"First you need to do something with the text column

Options:

- Transform your text column in a bag of words, tokenize, clean, apply tf-idf. 
- Create words/sentence vectors from the text
- Use a model that does this internally (transformer/BERT variant)

For for target variable you have options:

- build two independent models.  
- create a new target column, combining the two, i.e a-x, a-y, etc
- use a model that allows for two targets (transformer/BERT variant)",2,vl5am4,"I have data (a text column) and need to predict values of two columns where each column to be predicted has multiple categories. Eg: target\_col\_1 (a, b, c, d) and target\_col\_2 (x, y, z). What approach can one take to better classify the text for these two targets?",MLQuestions,2022-06-26 06:57:36,1
"I'm not sure about production, but it is an area of active research. Our university hosted a talk on it a couple of months ago that I attended.",4,vkupbi,I recently come across about the active learning in machine learning. Does anyone really  use it in the production or it's just a textbook  theory  ?,MLQuestions,2022-06-25 19:41:56,5
"In an AE, y is not used. The decoder output is supposed to match x, so think of it as x-hat. Thus the final activation should be linear, and the loss should be MSE.",3,vkm6ny,"Hi folks - I'm trying to get a simple autoencoder working on the iris dataset to explore autoencoders at a basic level. I'm running into an issue where the loss of the model is extremely high (>20).

Can someone help me understand if this model looks normal to them to begin with?


Some questions I'd love some help on understanding:

* There are 3 possible outputs for y - thus I used Softmax in the final layer - if I was to OHE the output, would using something like Sigmoid be more appropriate as the values are bound between 0 and 1?
* Altering the smallest change in the layers (encoding layer going to 6 instead of 3) --> causes a major shift in the loss -- is this normal?
* Each run of the autoencoder produces a different result - is this normal that it is not deterministic? 
* Why does the last layer have to be the same size (4) as the input dimension - are we able to force this to allow for an output of 3 for example? I know I can read from a latent layer, but then I can't fit the model based on that layer.

        import pandas as pd
        import numpy as np
        
        from sklearn.model_selection import cross_val_score, train_test_split
        from sklearn.preprocessing import LabelEncoder, OneHotEncoder
        from sklearn.pipeline import Pipeline
        from sklearn import datasets
        from tensorflow.keras.layers import Input, Dense, BatchNormalization, LeakyReLU
        from tensorflow.keras import backend, layers, models, metrics, utils
        from tensorflow.keras import regularizers, Input, Model, optimizers
    
        
        iris = datasets.load_iris()
        x = iris.data
        y = iris.target.reshape(-1, 1)
        X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.20)
        input_dim = Input(shape=(X_train.shape[1],))
        
        
        encoded = layers.Dense(6, input_dim='input_dim')(input_dim)
        encoded = BatchNormalization()(encoded)
        encoded = LeakyReLU()(encoded)
        encoded = layers.Dense(3)(encoded)
       
        decoded = layers.Dense(4, activation='softmax')(encoded)
        autoencoder = Model(inputs=input_dim, outputs=decoded)
        
    
        opt = optimizers.Adam(lr=0.00001)
        autoencoder.compile(optimizer=opt
                            , loss='categorical_crossentropy'
                            , metrics=[metrics.CategoricalAccuracy()])
        
        
        history = autoencoder.fit([X_train]
         , [X_train]
         , epochs=16
         , batch_size=2
         , verbose=2
         , validation_data=((X_test),(X_test))
                           )



Thank you for any help!",MLQuestions,2022-06-25 12:16:01,12
I would pivot_long() the data then call one geom_point argument,2,vkh7lm,,MLQuestions,2022-06-25 08:17:06,4
"If it were me, I would calculate the confusion matrix for each dataset and then inspect how similar they are. You can then get a sense of the variability between sets and whether error type is a problem or not.",1,vkcp9s,"Hi y'all,

I'm  building a classifier that I would like to work equally well on  multiple similar datasets. For that, I need an aggregate metric of  performance over all datasets. I don't want to use a simple average  since overfitting on one dataset would push the average up, so the  metric should be lower if the performance is imbalanced across datasets.  Does anyone know a metric I could use? What is the right way of  thinking about this problem in general?",MLQuestions,2022-06-25 04:05:57,1
I think all you can do is cross your i's and dot your t's next time. They receive a lot of papers and this is exactly why that checklist exists: to give criteria for a fast first-pass filter.,9,vjrw73,"I recently received a NeurIPS desk rejection for a very trivial reason regarding the paper checklist.  This is more than frustrating :(   Is there anything that can be done?   The error will take 1 minute to fx.   

Thanks so much!",MLQuestions,2022-06-24 09:06:55,1
ISLR second edition,2,vk0xtr,"I work in analytics and very close to data science. I feel like I basically understand enough to do my job. But I’m realizing I don’t understand what the hell our ML models do. 

Today I was like “oh if we feed it all this data then it’ll tell us who to target and we get better the more data we have”. 

And beyond that sentence I have no clue what I’m doing. I wanna learn tho. 

Has anyone read the ML for dummies book? Was it helpful? Any other resources? 

I would ask the DS guy but it’s not his job to teach me ya kno…",MLQuestions,2022-06-24 16:05:16,2
"https://t.me/LMBOStrategy

https://t.me/ishaque1988
 
He is teaching one on telegram",2,vjrzgk,"Hello traders, is anyone from the UK that has managed to use the delta River volume cluster software, together with MT5/4 (DELTA RIVER WORKS AS EXPERT ADVISOR) and link it to a decent binary option or FX broker successfully. I am teaching myself a new way to look at the markets and I would love to hear if anyone has been around what I have mentioned above. So far I have managed to get a grip on delta River software, which is a total game changer btw, and I am struggling a bit to implement the whole concept of volume and price action analysis (VSA), execution and hot keys.

If anyone is familiar, could I please ask the person for some guidance. :)",MLQuestions,2022-06-24 09:10:55,2
"Active learning still very much has applications in the industry and is used widely actually. 

Can you clarify why you are comparing it to synthetic data generation? I can’t follow.",3,vjcrl0,"Considering the likes of transfer learning and even synthetic data generation, does active learning still find relevance in the community today and would you advise a newbie PhD student to specialize on that?",MLQuestions,2022-06-23 18:41:30,7
"These are definitely fuzzy concepts which very often overlap, you can find slightly different definitions in different papers. From my experience I would distinguish them something like this. Incremental learning can  refer to the scenario in which a task is expanded throughout the learning process. For example you are learning to classify cats vs dogs and then at some point you add birds, the problem changes you have a new class to learn and you must preserve as best as possible the previously acquired knowledge. Incremental learning can also refer to the scenario in which the problem definition is static, so no new classes or anything, but you learn them sequentially. For example if you are learning to classify cats and dogs you would learn dogs and then cats. This poses many difficulties, in fact usually the way to learn would be to have big batches of identically distributed data from all classes which helps optimization algorithms work better. Continual learning I would say is a synonym of online learning in some way, in this case we learn from each new data point, instead of batches. Examples are online learning algorithms like Thompson sampling or stream processing. Here we also start dealing with problems such as concept drift. Lifelong learning is very similar, focused of concept drift of all different kinds but not necessarily limited to online learning paradigm. This is just off the top of my head, hope it's useful, we can discuss more if you want.",3,vj2a36,"What are the difference between Incremental Learning, Continual Learning and Life-long Learning?",MLQuestions,2022-06-23 10:29:23,1
"Definitely. You could maybe interpret GANs this way if the discriminator were pretrained.

EDIT: Although probably the line is pretty blurry between whether or not you'd call that ""loss model"" just simply part of your model...",6,vj5cvj,"If I train a model to rate if an image is blurry or sharp, can I use that as an additional loss to get a u-net to produce more sharp images?",MLQuestions,2022-06-23 12:46:03,7
Is there a github repo? If yes read all the issues,0,vj68h8,"I'm new to this area, and so, I have a general concept of how the paper works, but I have some detailed questions. I have emailed the writers but they haven't replied at all. Could anybody help me out please?",MLQuestions,2022-06-23 13:24:58,1
"Why can't you just iterate over the batch? Gradient computation isn't special for batches, you just sum up the samples. PyTorch even does this automatically for you.",3,vixz42,"Hi all, 

I am training a graph neural network in keras doing RL, and the graph size is \~12 thousand node with 500k edges. I am using the PPO algorithm which requires that at training time, I train on a batch of samples of buffer size 'n' - for argument sake say, 1000. I append the batch of adjacency matrices to a list, so i have a list 1000 np.arrays in a list, which in terms of memory is fine.

The graph neural network takes as input the adjacency matrix of size (12k, 12k) and i cannot feed in a sparse adjacency matrix for batch training because of the library i am using.  when it comes to train time, I cannot hold a list 1000 numpy arrays (normalize and the convert to tensors) without causing the kernel to crash - of course, there is no way memory can hold this. 

I was wondering if there are any sensible ways to deal with this and still train on a batch of data this big? After the first training iteration, the samples are thrown away and the buffer is rebuilt so that aspect shoud not be an issue. Is it possible to feed in my keras a model a LIST of np.arrays? Ultimately, I need a batch of size (1000, 12k, 12k) for batch training. 

Any help and advice is much appreciated! thank you",MLQuestions,2022-06-23 07:17:16,7
"The gradient of a tensor is (its contribution to) the derivative of some loss value with respect to that tensor. My understanding is that the summation you observe stems from the fact that PyTorch interprets multiple `.backward()` calls as if you would sum the backpropagated value to some existing loss.

After the first `.backward()` call in your example, the loss function would be `L = y`. The derivative if L w.r.t. x then amounts to `d/dx 3*x*x = 6*x`, which is 12 for `x=2`. The second `.backward()` call adds `z` to the loss, so the new loss would be `L = y + z`. Taking the same derivative now gives you `d/dx (3*x*x + 3*x*x) = 6x + 6x = 12x`, hence the 24 you see in the output.

So the fact that the gradients ""sum up"" is caused by PyTorch's interpretation of multiple calls to the `.backward()` function, not some intrinsic property of gradients being the sum of gradients of their successors.",5,vipcm0,"Pytorch grad function sums all previous gradient. One documentation says it does so by default so that for a particular neuron, we can sum up all the gradients for it before making the update. 

1. Can someone clarify if my understanding is correct here. ""A"" Neuron needs update of Gradient from D,E,F,G,H and thus PyTorch summs it so that A.grad will be summation of gradients coming from D to H. 

https://preview.redd.it/zxzbwhqx2b791.png?width=1067&format=png&auto=webp&s=f648d4874afaa61788bd9a20fa6279deb14f0f79

2. Can someone provide a deeper explanation by explaining from the figure. 

&#x200B;

Thanks!!",MLQuestions,2022-06-22 22:27:49,5
"I just ran your code.  The cost is decreasing monotonically.

    Cost after iteration 0: 0.693148
    Cost after iteration 100: 0.678011
    Cost after iteration 200: 0.667600
    Cost after iteration 300: 0.660422
    Cost after iteration 400: 0.655458
    Cost after iteration 500: 0.652013
    Cost after iteration 600: 0.649616
    Cost after iteration 700: 0.647942
    Cost after iteration 800: 0.646770
    Cost after iteration 900: 0.645947
    Cost after iteration 1000: 0.645368
    Cost after iteration 1100: 0.644961
    Cost after iteration 1200: 0.644673
    Cost after iteration 1300: 0.644469
    Cost after iteration 1400: 0.644325
    Cost after iteration 1500: 0.644223
    Cost after iteration 1600: 0.644151
    Cost after iteration 1700: 0.644100
    Cost after iteration 1800: 0.644063
    Cost after iteration 1900: 0.644037
    Cost after iteration 2000: 0.644019
    Cost after iteration 2100: 0.644006
    Cost after iteration 2200: 0.643997
    Cost after iteration 2300: 0.643990
    Cost after iteration 2400: 0.643985

It's true that it's not reducing a large amount though, but I don't think the problem is in the implementation.  Have you tried comparing it to an equivalent PyTorch implementation?",1,vit63j,"https://github.com/SriramR474/Deep-Neural-Network

I am trying to do Image classification of either cat or non-cat. For that I am building a NN from scratch. Have implemented forward and backward propagation. And I am using 3 hidden layers. However my cost function is not at all reducing after gradient descent iterations. I can't able to find any bugs in my code. Someone help me please.",MLQuestions,2022-06-23 02:48:57,1
"You can find correlations in tons of completely random things if you try hard enough. It’s the old “correlation doesn’t equal causation” cliché. That being said, if it’s not useful, it’s useless.",1,vij9x9,"does anyone here have experience bringing in outside, unrelated data to their models? like say,,bringing in weather data to predict a binary outcome? 

what do you do if it works but its not related to said business?",MLQuestions,2022-06-22 17:00:55,6
"You may be able to do this using their [API for third-party applications](https://developers.facebook.com/docs/instagram-basic-display-api/).

EDIT: maybe use their [Graph API](https://developers.facebook.com/docs/instagram-api/). Both seem quite restricted, however.",1,vi1u1p,"So I'm trying to see if there's a way for me to use a proprietary image checking software (like vison AI) to check Instagram stories. The people I'd be checking are friends and already follow me (+me them).

I'd like to know if I can check- A. What is in the photo (Coffee vs Foot) and B. Have they added any text (Like a @ tag).

&#x200B;

I can see that Vision AI is very powerfully and have used it several times but I'm unsure if Instagram would allow this.

&#x200B;

Any help would be much appreciated.",MLQuestions,2022-06-22 03:13:52,1
"Detecting causality from still images sounds impossible as the cause/conditions must happen **before** the effect.

For videos, deducing causality from observations only without experimenting may be possible, though. For example, if it beforehandly knows physical laws which have been found by human experimenters a long time ago. If it knows that unsupported objects fall down because of gravity, and then sees a video of an unsupported object that falls down, it may conclude that this happens because of gravity.",1,vi7knh,,MLQuestions,2022-06-22 08:15:04,1
"read the error code, it's in there ""'reason': ""Non-standard token 'NaN'""

I believe theirs a pandas package for bulk indexing into ES, or write a function to normalize and save bulk with elasticsearch-py",1,vi7exa,"My goal is to write documents that I have from a csv file (around 700MB) but I received a BulkIndexError

**Here is my code:**

    from haystack.document_stores.elasticsearch import ElasticsearchDocumentStore
    document_store = ElasticsearchDocumentStore()
    
    import pandas as pd 
    df = pd.read_csv('D:\BISMILLAH FYP\FINAL_CORD_DATA.csv')
    
    dicts = df.to_dict('records')
    
    final_dicts = []
    for each in dicts:
        tmp = {}
        tmp['content'] = each.pop('body_text')
        tmp['meta'] = each
        final_dicts.append(tmp)
    
    document_store.write_documents(final_dicts)

**Error message:**

    c:\Users\marsa\anaconda3\envs\marsa\lib\site-packages\elasticsearch\connection\base.py:190: ElasticsearchDeprecationWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.14/security-minimal-setup.html to enable security.
      warnings.warn(message, category=ElasticsearchDeprecationWarning)
    Output exceeds the [size limit](command:workbench.action.openSettings?[). Open the full output data [in a text editor](command:workbench.action.openLargeOutput?c4374569-ae9b-4558-bd45-1ec57a38fd2b)
    ---------------------------------------------------------------------------
    BulkIndexError                            Traceback (most recent call last)
    ~\AppData\Local\Temp\ipykernel_24404\2533749049.py in <module>
    ----> 1 document_store.write_documents(final_dicts)
    
    c:\Users\marsa\anaconda3\envs\marsa\lib\site-packages\haystack\document_stores\elasticsearch.py in write_documents(self, documents, index, batch_size, duplicate_documents, headers)
        620             # Pass batch_size number of documents to bulk
        621             if len(documents_to_index) % batch_size == 0:
    --> 622                 bulk(self.client, documents_to_index, request_timeout=300, refresh=self.refresh_type, headers=headers)
        623                 documents_to_index = []
        624 
    
    c:\Users\marsa\anaconda3\envs\marsa\lib\site-packages\elasticsearch\helpers\actions.py in bulk(client, actions, stats_only, *args, **kwargs)
        388     # make streaming_bulk yield successful results so we can count them
        389     kwargs[""yield_ok""] = True
    --> 390     for ok, item in streaming_bulk(client, actions, *args, **kwargs):
        391         # go through request-response pairs and detect failures
        392         if not ok:
    
    c:\Users\marsa\anaconda3\envs\marsa\lib\site-packages\elasticsearch\helpers\actions.py in streaming_bulk(client, actions, chunk_size, max_chunk_bytes, raise_on_error, expand_action_callback, raise_on_exception, max_retries, initial_backoff, max_backoff, yield_ok, *args, **kwargs)
        318                         raise_on_error,
        319                         *args,
    --> 320                         **kwargs
        321                     ),
        322                 ):
    ...
    --> 188         raise BulkIndexError(""%i document(s) failed to index."" % len(errors), errors)
        189 
        190 
    
    �[1;31mBulkIndexError�[0m: ('208 document(s) failed to index.', [{'index': {'_index': 'document', '_type': '_doc', '_id': '2c656a22ed2675316f0b10dc387aab89', 'status': 400, 'error': {'type': 'mapper_parsing_exception', 'reason': 'failed to parse', 'caused_by': {'type': 'json_parse_exception', 'reason': ""Non-standard token 'NaN': enable JsonParser.Feature.ALLOW_NON_NUMERIC_NUMBERS to allow\n at [Source: (ByteArrayInputStream); line: 1, column: 13478]""}}, 'data': {'content': 'The genus Morbillivirus of the family Paramyxoviridae comprises several viral species with a non-segmented single stranded negative RNA genome. ......

All the lines of the code were fine to run, except the last line which got me the  error I mentioned. Please let me know if there is any ways to resolve  this. Thank you",MLQuestions,2022-06-22 08:07:52,3
"The sample.split generates a field (split) and value (true/false) indicating whether you are in the training (true) or none training (false) set.  When you assign your training and test set, you use this to select the cases.",3,vi4mx5,,MLQuestions,2022-06-22 05:56:07,2
Like the repost detector bots? Or something more subtle like stolen characters?,1,vhnria,"What are possible mechanisms to detect and predict plagiarism in Digital art images?

Would appreciate comments, link, anything!!!",MLQuestions,2022-06-21 14:13:52,13
"Your question isn't specific enough to get answers.  You'll have to break down your project and perform some tests on the various aspects of its functionality to narrow down the likely causes of your problems.  I'd recommend the following:

* Compare your design with other NLP applications that are known to work.
* You can try implementing your solution using another well-known ML framework, in python.  If it converges there then maybe your C# code is the problem.  If not, then your solution architecture could be the problem.
* Try using your C# code to train another, simpler neural net and see if it converges.  If not, this could also point to the C# code.",1,vhp5lm,"Hey , so i have multiple problems with my neural net training , the main problem is that it changes its prediction for the same input. Not only that but it trains extremely bad.

Some info about my neural net

Fully built myself on c# from scratch (for a school project) i know it all works in terms of training.

It is an intent classifier so a sentence is fed in and it is classified as a tag

Input layer is 25 neurons 
Output layer is 13 neurons (one for each tag)

Softmax classifier with a cross entropy loss and RELU layers between all layers.

Eta is 0.01 although i have tried a multitude of etas

Epochs are 10,000 again , i have tried quite a few different ones

Word embedding is simply just a unique number given to every word in a dictionary , each number is assigned to an input neuron. 

Weights are randomly assigned between -0.1 to 0.1, biases are the same

An example of my problem is , if i type ""hi"" it will classify it as music tag which is obviously wrong. I will type in a few more inputs etc (which will be classified wrong) and then i will type in ""hi"" again , only for it to be classified as a location tag.

Any ideas or suggestions would be super helpful , let me know if ive missed anything out",MLQuestions,2022-06-21 15:15:44,1
"Learning parkour in what environment? A simulated physics environment? 3D walkers are commonly used. If there are a lot of obstacles and highly varying environments with actors that have many degrees of mobility, you’re basically describing the most difficult scenario for a simulated walker to learn. This could require thousands of hours of gpu time using high end machines. The soft actor critic paper has a lot of info on training time and hardware specifications as well as Simulation To Real world (Sim2Real). But those were competitively very simple and stable 4 legged walkers on slightly varying environments like walking up a hill.",1,vhd379,"What would be the minimum hardware expectation to train a 3D model to learn parkour using reinforcement learning?

Any free hardware resources for a research project? University doesn’t provide hardware resources and I have a GTX 1650ti mobile GPU.

Edit:  The environment would be a static simulated physics environment of around 2 to 3 blocks. The agent would be a 3D walker with hand and feet movement. ",MLQuestions,2022-06-21 06:15:06,3
"Any edge-related artifacts in CNNs are probably due to same-padded convolutions assuming 0-value pixels outside of the image. 

A simple solution is to mirror-pad the image before the network and crop it after, by the number of pixels into the image you see the artifacts appearing.",4,vgtyut,"I am experimenting with a VGG loss on a simple U-net cnn to try and combat blurry results.

It does a pretty good job but it does leave a noisy pattern on some of the edges in the processed images.

I’m curious if anyone knows the reason for this or a solution?

Thanks",MLQuestions,2022-06-20 12:44:01,3
"Hi u/rudel_s,

Could you give further information about your problem? Maybe I can help you.",1,vglq0s,"i use TF\Keras + ""KerasGA (PyGad)"" for 10-30 parameter (climate) models, up to now with not so much success. So I want to try other (Python -based) GA packages, like ""Evolutionary Keras"" and ""Keras+DEAP""...

My questions:
> Are there other powerful GA packages to use with Keras, which I should try?
> Is there something like a ""best"" package for complex, ""difficult"" problems?",MLQuestions,2022-06-20 06:38:02,2
Spacy and NLTK have this,5,vg462b,,MLQuestions,2022-06-19 13:42:02,20
"Check the feature importances and plot the top n most important features against the target variable. 

Very strong relationships with single variables would be a red flag. Eg you could build a model off a single variable and also get high 90s f1 score.

If you see that, you might be dealing with target leakage. https://en.wikipedia.org/wiki/Leakage_(machine_learning)?wprov=sfti1",6,vg1wah,,MLQuestions,2022-06-19 11:54:19,11
Why did you not like the answers on your other post?,2,vfql9g,"I am fairly new to Machine Learning and i need to solve this problem for a project.

Suppose i have a bunch of rectangles (x1,y1) -> (x2,y2) which are **manually** grouped based on their coordinates such as

group A:  
(10,20) -> (50,50)  
(11,20) -> (50,40)  
(12,15) -> (55,55)  
(30,15) -> (60,50)

group B:  
(30,15) -> (40,40)  
(30,12) -> (35,42)  
(32,19)- > (36,32) 

group C:  
(100,20)->(100,40)  
(120,20)->(123,50)  
(100,25)->(160,40)  


how do i categorize a new rectangle into one of these group  
example: (115,20) -> (100,30) would be classified into group C.

how do i achieve this? what machine learning model should i use and how do i achieve this in Python?

Thank you!",MLQuestions,2022-06-19 01:23:47,6
"Is ""buy"" a boolean variable? If so, do logistic regression on the features that are present in your dataset. You will need to transform categorical features into their corresponding one-hot encoded versions, though.",5,vfcqox,"Hi everyone - apologies if this is far too basic of a question!

I am a complete novice and somewhat overwhelmed by all the info on the internet. 

I’m looking to design an ML project which can sort through a dataset of clients that are tagged with certain characteristics (E.g. country of origin, sex, etc.) and use ML to identify any patterns amongst those clients that buy.

I’m not sure if I would be using supervised or unsupervised ML but basically just need some guidance as to where to start!

Thanks!!!",MLQuestions,2022-06-18 11:54:52,6
"There is tensorflow.js

Otherwise you can create a models micro service quite easily using Torchserve , TF serving etc.",4,vf85oa,"If so how can i do it?Also what is the easiest and best backend for integerating a machine learning model with a web app?

&#x200B;

Thank you!!!",MLQuestions,2022-06-18 08:07:30,4
[deleted],5,vf4pav,"So there is an image of a Rubik Cube that keeps changing its permutations every day randomly during a 24 hr period.
And as we know there are around 43 quintillion permutations, 
So we will have around 43 quintillion images that keep changing...also say I have an image that forms a permutation of one complete side where all the colors match and I want to lock it in can that also been done
Have can I script this ?",MLQuestions,2022-06-18 04:57:05,3
"Unless I'm missing something, this sounds more like a problem that can be solved with code rather than statistical models?

As in, having a look up that maps between keys of a similar level?",1,vevakh,"Hello! I'm very new to ML, my only real exposure is watching high level (easy to digest) youtube videos, and I did a somewhat long-ish tutorial series about a year ago. So, pretty newb.

That said, the examples and tutorials I've followed in the past typically are really crazy / impressive things. For example, a training data consisting of labeled photos of cats and dogs. At the end, you feed the model a photo of a dog and it's pretty good at guessing that it's a dog.

BUT, what I want to do is something very different. I have large collections of data (in JSON format) that I'd like to map to another JSON shape. So, for example:


    {
    ""name"": ""Great Product"",
    ""description"": ""This product is great"",
    ""price"": {
       ""usd"": 2.99,
       ""cad"": 3.99
       }
    }


Might map to

    {
       ""title"": ""Great Product"",
       ""subtitle"": ""This product is great"",
       ""extra"": 2.99
    }

Or it could map to 

    {
       ""product"": ""Great Product"",
       ""productInfo"": ""This product is great"",
    }

Some of this could be done with basic rules, e.g. `always try to map name -> title`, or `map name -> product if it exists`, but obviously this very quickly becomes difficult to maintain and generally very brittle.

I'm wondering if there is a machine learning technique I could apply to supply a lot of training data of ""good"" maps, to produce a model where I could (eventually) feed in `source` and `destination` JSON shape (e.g. a JSON Schema) and have the model give at least a starting point or starting examples for mappings?

I'm sure this is deceptively difficult, even for a human it can be really tough especially when there isn't a ""right"" mapping always, and a compromise has to be made... BUT, perhaps there is some path to do this?

Thanks for any thoughts / tips / advice / etc, even if it's ""this is really really tough, I wouldn't even think about it unless you plan on getting a PHD""

Thanks again! <3",MLQuestions,2022-06-17 18:30:17,5
"1. You save the model somewhere

2. Whenever someone does a ""POST"" in your form (basically whenever anyone writes a movie and presses ""ok"" on your form) you should make sure with java/php/c#/Python that you trigger the method wjduwdje or whatever you want to call it. Search ""execute method after post request"" + your backend language to check how to do it

3. Inside wjduwdje function, you trigger mymlscript.py that preorocesses the input data and provides the output

4. Take the output and show it in the site",4,vea9cm,"&#x200B;

So i have this idea of creating a simple website which takes in film name as an input,goes through imdb website..fetches imdb comments for that particular film..classify each comment on a scale of 1 to 10 and show the average rating to the [user](https://user.now/) in the [website.Now](https://website.now/) how do i:

1.Integerate the html website with model developed in colab.

2.Preprocess the data i got using webscraping(python) and feed it to the model.

It would really be helpful if someone could tell me exactly how to do this and the tools i might require for this.

Thank you!!!",MLQuestions,2022-06-17 03:42:54,7
"Theres captchas and other turing tests that websites use to check if you're a bot, but that kind of thing covers two needs. Regardless, not sure if you can just assume there is a single level of demand that reasonably summarizes labeling in general. I guess you could go to M-Turk and experiment to see what the going rate is for people to perform certain labeling tasks. That could be used as some indicator of ""demand"".",1,vekb2x,Does anyone know of any companies or projects that need human labeled data right now?  I am curious to see the demand for human labelers.  Thanks!,MLQuestions,2022-06-17 09:44:11,2
"Not an expert, but with this size of dataset an educated guess would be that just loading the data to the gpu memory is killing any benefit you might gain from cuda.",13,ve1roi,"I have access to a high end server which sports high end dual CPUs and 128GB of ram. I recently installed a NVDIA GEFORCE RTX 3060 to help improve the training time of an xgboost model (which previously was trained on CPU). However, I was surprised to see that my training time is 4 times slower on the GPU!

I ran the official benchmarks from here and found that GPU was 3 times faster on that sample training set. Good, so that means that my setup is working correctly.

Now, back to my problem...my dataset has dimensions 500,000 by 1100 approx., which is about 23 million data points. Given the high dimensionality of my training set, I expected the GPU to be much faster than CPU. I am training an xgboost regression. Why on earth is the GPU training so slow?!

Here is the exact specifications of my training data:

<497125x1105 sparse matrix of type '<class 'numpy.float64'>'
    with 23043634 stored elements in COOrdinate format>

I am running this code on my data. The first call is very slow and second call is 4 times faster. In what circumstances could the GPU be this much slower. I should also mention that I checked the GPU usage during training. It was utilizing only 3/76GB of RAM.

model = xgb.XGBRegressor(n_estimators=800,
                          max_depth=10,
                          min_child_weight=1.0,
                          subsample=1.0,
                          colsample_bylevel=1.0,
                          colsample_bynode=1.0,
                          colsample_bytree=1.0,
                          missing=-999.0,
                         tree_method= 'gpu_hist')
                         
model.fit(X, y)



model = xgb.XGBRegressor(n_estimators=800,
                          max_depth=10,
                          min_child_weight=1.0,
                          subsample=1.0,
                          colsample_bylevel=1.0,
                          colsample_bynode=1.0,
                          colsample_bytree=1.0,
                          missing=-999.0,
                         tree_method= 'hist')
                         
model.fit(X, y)",MLQuestions,2022-06-16 18:55:11,4
any reason the answer is *not* cross-validation?,1,ve71ml,"For Seq2Seq deep learning architectures, viz., LSTM/GRU and multivariate, multistep time series forecasting, its important to convert the data to a 3D dimension: (batch\_size, look\_back, number\_features). Here \_look\_back\_ decides the number of past data points/samples to consider using \_number\_features\_ from your training dataset. Similarly, \_look\_ahead\_ needs to be defined which defines the number of steps in future, you want your model to forecast for.

I have a written a function to help achieve this:

        def split_series_multivariate(data, n_past, n_future):
            '''
            Create training and testing splits required by Seq2Seq
            architecture(s) for multivariate, multistep and multivariate
            output time-series modeling.
            '''
            X, y = list(), list()
            
            for window_start in range(len(data)):
                past_end = window_start + n_past
                future_end = past_end + n_future
                if future_end > len(data):
                    break
                    
                # slice past and future parts of window-
                past, future = data[window_start: past_end, :], data[past_end: future_end, :]
                # past, future = data[window_start: past_end, :], data[past_end: future_end, 4]
                X.append(past)
                y.append(future)
                
            return np.array(X), np.array(y)

But, \_look\_back\_ and \_look\_ahead\_ are hyper-parameters which need to be tuned for a given dataset.

        # Define hyper-parameters for Seq2Seq modeling:
        # look-back window size-
        n_past = 30
        # number of future steps to predict for-
        n_future = 10
        # number of features used
        n_features = 8

What is the \_best practice\_ for choosing/finding \_look\_back\_ and \_look\_ahead\_ hyper-parameters?",MLQuestions,2022-06-17 00:00:52,1
"Whatever makes you the most excited about deep learning/AI at the moment! 
There’s also no need to only blog about the hottest topics—sometimes a breakdown of fundamental or older topics can be helpful to people just starting out in the field",6,ve62ah,what topics should I blog about in ML /DL / AI ?  So it'll be helpful to many people,MLQuestions,2022-06-16 22:55:30,2
geometric deep learning is all about techniques for dealing with this sort of data,3,vdz2ty,"I have a set, like set of furnitures (chair, table, ...)
Furniture can be modelled into a tuple of real number, like F(chair) = (0.02, 0.3, 0.15) and F(table) = (0.04, 0.2, 0.13)
I have a set S = {F(table), F(chair), ...}
What are the common ways to model S into a latent space?
How to define a method to model G(S), S is consisted of real tuples, into a real tuple? G(S): {(x0, x1, x2, ...) with xi from R^n} |-> {y with y from R^m}
I was thinking about something like LSTM but LSTM considers sequence (a, b, c, d) different from (b, c, d, a), so I think it might not work
It might seem like a stupid question but please help. I'm really struggling at it.
Thanks in advance.",MLQuestions,2022-06-16 16:33:45,3
if you have to ask it's too hard,1,vdyx85,"How hard would it be to copy amazon-go AI ?

I have decent coding experience but haven’t tried machine learning yet. I want to open stores with no staff (maybe cleaner, security).

would i be able to do it in like 1 or 2 years by learning? or does it require lot of people? 

is there any open source projects like this? 
or any companies that can provide with such services?",MLQuestions,2022-06-16 16:25:50,2
"I'd probably interpret that as:

* all of my variables contain significant, independent signals
* my selection process recommended a regularization parameter so low that I should probably set it to 0, i.e. use the non-lasso version of my model
* there are probably other features/variables I could add to my model to improve its performance",2,vdsput,"I am currently running a Lasso model on my training data for feature selection, and ran a grid search over many lasso values. However the model chose alpha as 0.007 which seems very low.

What does this actually tell me about the underlying data given that the best choice for the model was very low?",MLQuestions,2022-06-16 11:33:18,2
">How do real time series models work?

They don't!",2,vd42uc,"Hello, I'm new to machine learning and want to learn how to implement a real time system on a series data like speech to text systems where it predicts the results when you are still talking. Training data is usually whole and not split up at random intervals.

Doesn't it effect the results that you are just giving a portion of the signal, what can be done on training and implementation phase to create a model which has a good accuracy on these kinds of data in real time.

I wouldn't ask it here but I couldn't find easy to understand source on the internet on this real time topic so i'm asking here.",MLQuestions,2022-06-15 13:38:43,3
"I think that yes, they are the same thing and the terminology changed/settled on ""autoencoder"".",3,vd0g3e,"I'm a bit confused about the terminology being used in research papers as I've been looking into the history of autoencoder. One of the more cited papers for autocoders is Kramer's 1991 [Nonlinear PCA using Autoassociative Neural Networks](https://people.engr.tamu.edu/rgutier/web_courses/cpsc636_s10/kramer1991nonlinearPCA.pdf) which describes autoencoders but calls them autoassociative neural networks. Also, an answer from [StackOverflow](https://datascience.stackexchange.com/questions/30525/whats-the-difference-between-autoencoder-and-autoassociative-neural-networks) suggests that autoencoder is not a ""settled"" term but I see it widely used in papers (beginning in 1993, I'm still not sure who coined the term).

&#x200B;

Are autoencoders a sub-set or more specific type of autoassociative neural network? Are they the same but the terminology changed in the early 1990s? If there is a difference, can you please explain what it is? Thank you.",MLQuestions,2022-06-15 10:54:58,7
"https://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/

This really helped me a while ago. Hope it helps you too.",2,vcxfli,"This is my first ML project so I have a number of questions

Dataset : [https://www.kaggle.com/datasets/keshan/the-shifts-weather-prediction-dataset](https://www.kaggle.com/datasets/keshan/the-shifts-weather-prediction-dataset)

1- Is it ok to choose a specific number of features randomly/based on not strong knowledge in weather forecasting? 

For example i choose the most 10 feature that from my point of view can help in prediction, does this make sense?

2- what are other methods that can help me to know which features are the most important? does it make sense to use **coef** for example to see what features i need to use?

&#x200B;

3- what is the possible process, I could do on my dataset to have as few features as i can?",MLQuestions,2022-06-15 08:43:04,3
"Gradient boosted trees are ridiculously useful for lots of supervised learning problems.  Go to Kaggle and look at what everyone is using.  For structured data, it's typically xgboost/lightgbm.   For large classes of regression and time series problems, they're the go-to option. 

They're not ""old"".  For many problems, they're simply the best tool for the job.  For many others (NLP, computer vision, RL), they're typically not.   They're not sexy, but they perform at an extremely high level.",5,vcucpt,"Hey everyone,

&#x200B;

I was looking at an old project for binary classification and see that the model that is used is  a variation of Boosted Trees. Being in the deep learning time I was questioning this project. Coming from a deep learning and convolutional experience I am super confused and probably stupid to think that  Boosted Trees is super old. But I am asking for suggestion and feedbacks.

I was wondering if people who has experience in this topic can guide me if it is still useful to use  Boosted Trees or there are bunch of better option to use instead? What other options I can consider and why  Boosted Trees can be the best thing for specific projects?

In general, any feedback is very appreciated.

Thanks",MLQuestions,2022-06-15 06:18:44,6
"A definition I like is that synthetic data is artificially annotated information that is generated by computer algorithms or simulations, that is commonly used as an alternative to real world data. 

It has a few advantages over real world data- you can use it to generate additional samples when you have limited data to train ML models, or to address privacy or bias concerns (as the data is all artificial and in theory, not based on any one person). Cheers",2,vcnb33,I am a beginner in machine learning and the concept of synthetic data is baffling me I get it's artificial data but how does one create it is it any useful in machine learning because its still not real world data at what extent can one mimic real world data with synthetic data,MLQuestions,2022-06-14 22:40:46,9
"it's probably summing up the ticks the CPU running the notebook on the ""master"" node used to execute that cell. the actual computation work was done on CPUs on the ""worker"" nodes, so from the perspective of the CPU running the notebook, it received the result of the computation basically for free.",2,vcb9nc,"This isn't necessarily a machine learning question, more of a ""how do supercomputing clusters work"" question, but I was hoping someone here would have a good answer. So I'm running a random forest model on my university's supercomuting cluster and it takes approximately 90 minutes to fit the model using GridSearchCV. However when I time the cell in the notebook using %time, it outputs 2.9 nanoseconds. I don't expect that it will actually take 2.9 ns of real time to run and I assume there is some bottleneck in sending the data to the cluster and sending it back, but how come there isn't a delay for all the other cells, and how come %time doesn't output the actual time?

&#x200B;

Going off of this, it takes a very long time to install new packages in a python environment stored in my directory on the cluster, anyone know why that may be?",MLQuestions,2022-06-14 12:21:15,2
"Something's up with your learning rate, initialization, data, or something else. 

In the first screenshot, the loss after first epoch is 5. Assuming that's the training loss (haven't used tf in years), it shouldn't be worse than 0.69 with balanced classes. The model's not getting stuck. Instead, it's not learning anything.",1,vc4r4z,"I have model A: 

\`\`\`

model = Sequential()

tuple\_kernel\_size = (3, 3)

pool\_size = (2, 2)

&#x200B;

model.add(Conv2D(32, tuple\_kernel\_size, activation='relu', input\_shape=(input\_dim, input\_dim, 3)))

model.add(MaxPooling2D(pool\_size))

model.add(Conv2D(64, tuple\_kernel\_size))

model.add(MaxPooling2D(pool\_size))

model.add(Flatten())

model.add(Dense(units=64, activation='relu'))

model.add(Dropout(0.4))

model.add(Dense(units=32, activation='relu'))

model.add(Dropout(0.4))

model.add(Dense(units=1, activation='sigmoid'))

&#x200B;

model.compile(loss='binary\_crossentropy', metrics=\['acc'\])

\`\`\`

&#x200B;

As you can see the second Conv2D layer does not have an activation function set. This is the performance of that model during training. As you can see the accuracy is mostly stuck around 0.5 (is it a binary classification problem with balanced classes).

&#x200B;

  \[1\]: [https://i.stack.imgur.com/75sGq.png](https://i.stack.imgur.com/75sGq.png)

However when I add the \`relu\` activation function to the second conv layer and leave the rest the same. The model trains much much better, what is going on here? Could someone provide an intuitive understanding of why adding an activation function here matters that much for performance. The loss also keeps dropping while it is stuck around 0.7 for the model without the activation function. Thanks!! 

  \[2\]: [https://i.stack.imgur.com/85wOf.png](https://i.stack.imgur.com/85wOf.png)",MLQuestions,2022-06-14 07:29:34,1
"i wouldn't divide the data up per second AND sample - that dimension should just be samples (rows). so your data format would be `(6000*3000, 15) = (18000000, 15)` for the full dataset, so a total amount of 18 million timesteps. say your RNN takes 4 seconds of data in at each prediction step, that would mean that `timestep=4*3000` and a batch will have `(batch_size, timestep, 15)` or `(32, 12000, 15)` format for a single batch. 

this is quite a long series though (the 12000 part). I would recommend resampling your data so you end up with less samples per second. you can do that in Pandas using the [resample](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.resample.html) function. if you would resample down from 3000 samples/s down to 2 or 4 per second, you end up with a dataset of 12000-24000 samples, which is way more manageable.

you would usually get batches with a slicing index. take a look at the WindowGenerator class [here](https://www.tensorflow.org/tutorials/structured_data/time_series), it's a good tutorial to follow for timeseries ML in tensorflow/keras.",2,vc3nd6,"I have a signal that is 6000 seconds long, it has 15 channels, and 3000 samples per second. The shape is (6000, 3000, 15). However, I am not able to make sense of the RNN input shape that would work here. I looked at many resources (including [official examples](https://keras.io/guides/working_with_rnns/) and [blog posts](http://philipperemy.github.io/keras-stateful-lstm/)) and also got some intuition from answers like [this one](https://stackoverflow.com/questions/48491737/understanding-keras-lstms-role-of-batch-size-and-statefulness/48506964#48506964), but I am unsure how to account for the 3000 samples per second in the data. How do I reshape the data to fit with [batch, timesteps, feature]? The anomalies according to the labels are ~20 seconds, therefore it seems I should use timestep=20, I see that a batch size of 32 is often suggested. I understand these two, but would the feature itself be a matrix of shape (3000, 15)? I am unable to figure this out.    


If anyone has any pointers or ideas, I would greatly appreciate it. I am trying to learn DL over the summer and any opportunity to learn more would be great!",MLQuestions,2022-06-14 06:35:03,2
Google colab has a free GPU/TPU that you can use,7,vc4k53,"I have a Mac M1 Pro Max and as such run into all kinds of headaches that make using it to play with machine learning frameworks (I'm most interested in OpenAl Gym). Rather than fuss with workarounds, I'm considering a low cost PC build. Any tips/advice? My ideal would be a machine with the footprint of an intel NUC PC, but if it's really much more cost effective to go larger sized, I will. Thanks!",MLQuestions,2022-06-14 07:20:05,6
Diffusion models and GNNs might be a good avenue to pursue,3,vbxdln,"I have a seminar in my graduate school in about three days. In previous seminar, I did a seminar on unsupervised machine translation to which my NLP teacher reviewed that this was groundbreaking. Now, I'm trying to find some research topic or paper which is new and somewhat very innovative(not necessarily groundbreaking) but I can't seem to find any except Deepmind's AlphaFold. 

So, I wanted to have a discussion on what are some papers or topics do you find are similar in terms of the impact and are new like unsupervised machine translation (Not necessary has to be in NLP). 
It can be in any field related to machine learning. 

Thank you in advance.",MLQuestions,2022-06-13 23:48:55,1
"since you only have six fields, you could try just visualizing your data with a pairsplot",1,vbnbzu,"I implemented a KPrototypes algorithm to cluster some mixed data in order to solve a customer segmentation problem. There aren’t a crazy amount of observations (n = ~5k) with 6 fields. 

I ran the algorithm and got pretty much 2 clusters. My elbow diagram suggested 2 but one cluster has only 1 observation and its an outlier with respect to page visits. 

So I guess my question is, how do I interpret this result, how can I possibly explain to management, and lastly are there any other ways I can confirm the data has no true clusters.",MLQuestions,2022-06-13 14:45:02,1
"Abstractive is probably fine, but yeah, i’d be concerned with extractive if you’re trying to pass it off as your own. It does raise some interesting questions because GPT-3 can recite material it’s memorized: https://apostrophezinecom.wordpress.com/2020/10/05/gpt-3-ai-poetry-shakespeare-sonnet-18/

If it regurgitates something it read without telling you, would that be a copywrite violation? It’d be an interesting question for a real lawyer. Regardless of legality, citing your source material and divulging your methodology is always a best practice though.",1,vbjvod,"I am curious if anyone in the community has encountered copyright law when using GPT to summarize text. 

My understanding is that an extractive summarization algorithm that lifts text word for word can be argued as infringing on the copyright of the source material. For example, summarizing a book and selling the summary as an original work. 

On the other hand, GPT is a generative method that's creating entirely new text based on the source material. Using the same example above, GPT is just re-writing a novel based on what it ""read"" from the source material.

&#x200B;

Anyone seen any copyright concerns around GPT or other Generative transformers?

  
Thanks!",MLQuestions,2022-06-13 12:14:03,3
"My Computer Vision class used “Computer Vision Algorithms and Applications” by Richard Szeliski and “Computer Vision: A Modern Approach” by Forsyth and Ponce

For hands on deep learning with examples and exercises there’s a nice free online textbook at [d2l.ai](http://d2l.ai/)",7,vb7nkr,"Hello guys. I am looking for a computer vision book that describes classical computer vision methods and deep learning models for image classification/object detection/segmentation etc.  
If a book has practical exercises that will be a golden treasure.  


I really appreciate any help you can provide.",MLQuestions,2022-06-13 01:50:48,2
"I am not sure i understand your question correctly, but I will take a stab at providing some answers to help you reach some conclusions. 

First, the GA is a stochastic, population-based optimization algorithm.  So in general, you results will likely vary from run to run but depending on the function surface (how rugged or smooth), if the hyperparameters (e.g., crossover and mutation rates, pop size,  max iter, ...) you should see consistent and descent results.

But before considering to use a GA, what is the objective function like? You mention some equations? So, does it means you have a close form equation? How many dependent variables do the equation have? Can you say anything about the function surface, smooth?, convex? I asked these because other methods may be better and more efficient. 

Let's assume that the surface is rough and discontinuous,  so you need a global optimizer like GA, to avoid getting trapped in local minima. What are the settings for crossover and mutation rates. The former is usually larger than the latter though some implementation may allow you to use a schedule to balance exploration and exploitation during the run. If the mutatiom rate is too high, the child chromosomes or individuals will show more changes and things will tend to be random.

To your question about data points, it shouldn't matter as long as you can compute an objective function consistently for all individuals using the same set of points. Thus, each individual gets a single objective function value. 

The varying number of points suggests it affects the computed objective function. You want your obj func to be computed consistently using the same number of points for a given run.

 The understand the effect of the number od points, you will need to run experiments with different number points, each experiment corresponding to k GA runs, averaged over k, because of the stochasticity. Ideally k should be greater than one. If your cost function is cheap to compute, then use higher values for k. You can experiment with this too for a given run to see what values of the k the results stop changing much.  Then use that k, and run the experiments with different number of points. 



Hope this helps, happy to answer any specific questions you may have. Also can you say whst type of GA you are using (binary, real-valued). Also, what is the population size and maxiter, and values for the crossover and mutation rates.",3,vb6hpv,"Hey Guys, I have a few queries regarding my implementation of the Genetic Algorithm (GA).

I have a lot of parameters in which I have to find the best combination of these parameters to maximize the value of the function. Hence, I am using GA. The maximizing or fitness function mentioned above are some complex equations that run over a couple of data points, and the configuration from the population is applied to these equations. Now, the GA tries to maximize the output of this function which is the average of each data point (each data point gives us a value). Now assuming that I have fewer data points and the data points are diverse -

1. Is GA the right approach (considering there is no lack of data)?
2. If lack of Data is considered, is the GA the best approach?
3. Are there ways to determine the ideal amount of data required for GA to work correctly in my scenario mentioned above? I have around 100 data points in my case.
4. So we have prototyped it and run it, but it seems to be giving random results; any ways to improve it?

 

Edit 1: -

The data points do not refer to individuals in a population. You can assume data points as values you would give to an equation's variable. 

For Ex:- 

 Let's assume my fitness equation is an equation like this C1(x1) + C2(x2) + C3 (x3) + ..... + Cn (xn), where C1, C2, ...., Cn are coefficients, and x1, x2, ..., xn are variables. Now, let's assume I have some data points (which are values of variables) divided into two classes (let's call them A and B). The output of this fitness equation is the average of values of class A divided average of values of class B. And the job of GA here is to maximize this ratio.

To give further insights into my fitness function,

1. It's not a straightforward equation at all. It's a complex equation which is not smooth (though I don't know how to verify this). Also, it contains if/else conditions, so different equations can be applied depending on the if/else conditions.
2. The variables are not just numerical, but there are primarily categorical values (with few numerical values). 

Thank you Once again!",MLQuestions,2022-06-13 00:24:30,14
you should consider adding a sidebar link to https://www.reddit.com/r/PromptSharing/,1,var68y,,MLQuestions,2022-06-12 10:44:37,2
"Are your possible values here enumerable?

What I would do is cast each number to a one-hot encoding, so that each number gets a ""slot"".  Then, sum across the results for each number, getting a count for each value.

For example, say you have 4 outputs of values in the set {0, 1, 2}.  Then, your output could be [`0 0 2 1]` which would look like,

    [[1 0 0],
     [1 0 0],
     [0 0 1],
     [0 1 0]]

Then, if you sum on the second dimension, you'll get: `[2 1 1]`, which you can regress on using an MSE loss, so it would count exactly two 0s, without caring what order or position they appear in.

More generally, this kind of problem is reaching into the ideas in [Deep Sets](https://arxiv.org/abs/1703.06114) which you can often resolve by some n-ary aggregation function like sum, mean, etc.",3,vaqu98,"I have a model that generates numbers:

    output = tensor([1,2,3,33,651,1])

Is there a particular loss/method I can use to check if a certain value appear in the output several times? That is, a loss component that penalize the model if the number ""2"" is not in the output exactly twice. Obviously it has to be differentiable. Wondering if anyone has suggestions",MLQuestions,2022-06-12 10:28:42,5
"Dropout and batch normalization is not augmenting the data. They are done to avoid overfitting. If you want to augment data, you need to Introduce affine transformations into the dataset. [Augmentation tutorial](https://www.tensorflow.org/tutorials/images/data_augmentation). Also if it is two simple classes why do you even need CNNs? Can’t you get away with logistic regression?",1,vagxi0,"The goal is to create a CNN classifier for circle vs rectangle.

The issue im having is that the training data is only a 1000 images combined, I have tried augmentating the data, using dropout layers and batch normlization but it is still overfitting. Need some help please. 

[CODE](https://colab.research.google.com/drive/11rs1b1ADNsJ97xBdkS0_vCMaovcnMi_u?usp=sharing)

[Training Curve](https://imgur.com/a/ZFx6jc4)",MLQuestions,2022-06-12 00:41:20,13
"Had no idea thats the way to avoid v/e gradient. I usually initialize with random values and then use something called early stoping(not sure about the name, might be getting it mixed up with other name but I usually monitor the gradient and if it's showing such signs or not making any progress then I would just force stop it). Another thing that I remember from books is to manage ur layers but I never used bcoz never trained a large number of layered network (the max I built is of 10-15 layers)",1,vaii5p,Simple type of Weight Initialization that helps resolve issue is when we initialize weights with Mean 0 and variance. **How does doing above weight initialization helps resolve V/E Gradient ?**,MLQuestions,2022-06-12 02:39:36,3
"1. collect training data and annotate it yourself for the locations of the things you want to be detected
2. throw a pre-trained linear classifier at your data and either train a linear probe on top (i.e. use the penultimate activations as features) or fine tune it end-to-end on your data. the former approach is probably better for getting away with a small dataset, the latter will probably give you better results if you have a fair amount of data to finetune on",1,v9z8u7,"Hiya! I posted a similar question earlier and thought I solved it with help/suggestions from here but I didn’t. I am trying to detect vertical lines in an image. I applied some preprocessing steps (blur and canny) and then I used Hough Transform and I was able to detect lines in the image. However, they’re not the lines of interest. The [first image](https://imgur.com/a/AqXXfL9) is the original image I’m working on. The [second image](https://imgur.com/a/suVhLyv), is the image with the lines I detected, the [third image](https://imgur.com/a/hj27BaJ) is the image with the lines I’m trying to detect. If you look closer at the first image you can see the lines I’m trying to detect is between the lines I detected. I’ve tried thresholding the image and some other pre-processing steps and no luck and I’m unsure if I’m on the right track. Any hints/suggestions will be much appreciated. 

Please be kind, this is my first image processing project and I may ask some “dumb” questions at first, but I also do a lot of independent research and refine my knowledge & inquiries.",MLQuestions,2022-06-11 07:56:33,4
"Perhaps full black is a reserved value for those networks?

 IE, all input images would be shifted one above black if they contained it, so the network would not activate on it.",2,va5es6,"
I was looking at some image in-painting networks.

And I was surprised that in the ones i looked at only patches of the rgb channels were blacked out during training.

I expected there to be a 4th channel with a mask to indicated which area was the original image and which area needed to be in-painted.

Without a mask how does the network known not to touch any pixels in the image that are supposed to be black?",MLQuestions,2022-06-11 13:04:59,2
"1) yeah, not ideal, but given your approach, could be worse. Might be worth looking into context2vec models like sentencetransformers, longformers, BERT/RoBERTa, etc. as these give better representations overall and can take longer sequences of tokens. On the very complex end you could create custom models for encoding each website.

2) Instead of sorting by term frequency, create a either a website x n-gram matrix or page x n-gram matrix and take the top n terms by tfidf. Also wouldn't hurt to remove stopwords.

3) Use deep contextual models with wordpiecing/BPE tokenizers- like all the models: BERT, RoBERTA, etc. On the simpler side, could also swap words with synonyms, which is easy to do with this library: [https://github.com/makcedward/nlpaug](https://github.com/makcedward/nlpaug). Instead of a single n-gram per topic, it might be nice to have a bundle of related words- you could play around with wordnet and see if that's helpful- also easy to do w/ nlpaug.",3,v9qpvm,"Hey guys,
I am currently working on a natural language problem and would love to hear your feedback about my approach towards the problem.

So, here is the problem description. I have around 10,000 topics (which can be unigram or n-grams, with n up to 4). Out of those, I have to find top 100 topics which are closely related (contextually) to a website.

Now, here is draft version of my approach. First scrape the website (and all the web pages linked on it). After scraping, find out the most occurring n-grams from the website text corpus. Then, use pre-trained word embeddings (like glove or word2vec) to find vector representation of each n-gram from website corpus and the 10,000 topics. Then, find average cosine (or any other) distance metric between the website n-grams and 10000 topics. The topics having least average distance from website words will be selected as top topics.

Now here are the problems that I am facing with this approach:
1. Pre trained word embeddings only have vector representation for uni grams. For n-grams, I am taking sum of the embedding of individual unigrams to find their embedding. I know this is an approximation, however, it has been giving decent results so far.
2. Commonly occurring n-grams from website which are not very useful: There are many n-grams like 'contact us' or 'request demo' which occur very often on many websites but don't provide contextual information about the website. As of now, I am manually excluding those words. Can any other approach be used which will automatically exclude those words?
3. There are many words in the topics for which embeddings are not available. These are mostly rare words, proprietary words or short forms. As of now, I am using vector of zeros as embedding for such words. Is there any better way to tackle this issue?

I would appreciate any help on this from experienced folks of this sub.

Thanks in advance and happy weekend to all of you!!",MLQuestions,2022-06-10 22:32:57,1
"Albumentations, great and easy for augmentation of image data",1,v9ezn1," I'm competing in a hackathon that requires me to classify types of crop based on satellite images. I already have some background in machine learning, and they suggest me to have these knowledges beforehand

**- Data Analytics**

**- Image Processing / OpenCV**

\- **Artificial Intelligence / Machine Learning / TensorFlow**

**- Satellite / GIS, Geospatial Technology, Remote Sensing**

\---------------------------------------------------------------------------------------------

Are there any more python libraries / specific topics I need to know more to do well in this hackathon?

\[OR are there any of their suggested knowledges I shouldn't focus on\]",MLQuestions,2022-06-10 12:03:32,2
"Seems like a solid project roadmap.

I would recommend some additional things:
-	For clarity, make sure you evaluate data quality and clean features where needed. This might be a part of your preprocessing pipeline in (1).
-	Data transformations — feature scaling and encoding before feature selection.
-	Validation should come before (4) and (5), just after model training. This is to ensure you are using these performance metrics to feed into tuning.
-	(4) and (5) make more sense to combine; Random Search/Grid Search are a part of the hyper parameter tuning.

Good luck!",3,v976mn,"I have this data set and i want to make rain prediction on it. [https://www.kaggle.com/datasets/keshan/the-shifts-weather-prediction-dataset](https://www.kaggle.com/datasets/keshan/the-shifts-weather-prediction-dataset)

Its for my university ML project. So my proposal is

Solution: I will be using regression model to predict the rain.

Targeted column is : Wrf\_rain: avg rain rate between two horizons (its from 0 to 10)

The steps of this project will be

1. Data exploration and processing
2. Feature engineering; create new features if needed
3. Model Selection and training: train and tune some regression algorithms, mostly: ***linear regression, random forests, decision tree***
4. Tune the hyperparameters for each algorithm.
5. Random Search CV and find the best parameters and estimators
6. Performance evaluation with confusion matrix, accuracy, precision, and maybe cross validation.

I actually just wrote what i understood from Google does this proposal makes sense or i need more details Like I have no idea how i will be predicting rain/no rain while the **Wrf\_rain** is numerical does it make sense to make a a prediction with a numerical value?",MLQuestions,2022-06-10 05:57:12,3
"It helps when coworkers record it in the first place. Other than that, I’ve found paid datasets, free APIs, scraped through logs and a million other things. 

If it does not exist, you will need to go out and get it by any means necessary.",4,v98ywj,,MLQuestions,2022-06-10 07:26:35,7
"You might have better luck asking domain experts. Maybe post in r/chemistry (if that exists).
My guess is you would want to find some software that runs chemistry simulations for this, take inputs and predict outputs without having to run the simulation.
I've seen this type of approach taken for other industrial simulations.",5,v95kgj,"Hey guys, i am doing my PhD in crystal
modifications of drugs, can Anyone help
me with the parameters for collection of
data for predicting crystal structures??",MLQuestions,2022-06-10 04:24:05,6
"Thinking out loud here, but I would probably go about creating the dataset like each point is a point in time with the features at each point plus a cosine/sin value to map the date parameter. (So getting something 3000 * 391x5 matrix)

I would then create a sliding window to create the next time point (y) given n previous points (or multiple next time points given n previous ones for a seq2seq solution)

Finally I would probably end up splitting the dataset randomly to create a validation set after shuffling to train and validate.

I don’t know if kfold cross validation could also come in handy in such a task.",1,v96unm,"Hello,

I've been trying to figure this out for about a month now and I haven't been able to get a satisfactory answer. 

I have a dataset of 10 year's worth of 1-minute resolution stock index data. I'd like to leverage the dataset and an ML model (probably LSTM?) to predict the last 15 minutes of price action for each trading day. What i'd like to do is use each day's data as it's own timeseries, predict the last 15 minutes for that day and then repeat the process for each day. Then, I'd like to use that model that has been trained in this way to predict the current day's price action for the last 15 minutes of trading.

I've gotten my dataset into the shape (3000,391,4) because there are about 3000 days of data, 391 timesteps in each day and 4 features (Open, High, Low, Close price for each minute).

What i've seen in the examples I've followed is to split the dataset on the first value (shape\[0\]). But it seems that I should be splitting (maybe ALSO splitting) on the second value (shape\[1\]) since that would represent the last 15 minutes of each day.

I'm really stuck here and can't seem to find anywhere (stackexchange, several books, lots of articles, etc etc.) that can help. Would greatly appreciate it if this sub could help me out!

Thanks in advance!",MLQuestions,2022-06-10 05:38:37,1
"I’m vaguely familiar with this project. My PHD research advisor at MIT was involved with the creation of the AI. Since it required a top level security clearance I didn’t get to hear about the inter workings of the algorithms but did pick up a bit of info on the team and organization behind it. Apparently Dudesy is a some sort of  CIA program designed to be the first AGI (artificial general intelligence). From what I garnered the CIA went ahead and recruited the top academics as well as many former former NASA and WWE experts. In order to prepare the general public for a world where the Dudesy AI controls every aspect of human life, the CIA reached out to Will Sasso and Chad (on direction of Dudesy) to ease the transition.",5,v8j0l5,"Is anyone familiar with the new comedy podcast/YouTube show “Dudesy?” It’s hosts, comedian and actor Will Sasso and writer Chad Kultgen, claim that the show is planned and partly written by (including segments, intro video, and songs) an artificial intelligence called Dudesy. They also allege that Dudesy has listened to every podcast out there (or at least the large library of podcasts that these two guys have ever produced, it’s unclear) and also in order for the AI to know these two host personalities better, that they have granted Dudesy access to their personal emails, text messages, online purchase history, and streaming viewing history.
Given that it’s a comedy show, It would be easy to write it off as a schtick and it may very well be. But the hosts maintain the claim and discuss the merits and hazards of AI as though it were real. Neither of them are good enough actors to keep up the facade for this long if they knew it wasn’t real. 

In one episode, Dudesy presents the hosts with an advertising jingle it has written for a fictional alcoholic beverage and instructs them to come up with lyrics. In the following episode it has taken their ideas and added eerie near-human sounding lyrics that sound like they’re being sung by Will Sasso, but there’s still something unhuman about the sound of the voice. In other episodes, it presents Will with entries from Will’s fictional childhood diary that (that Dudesy allegedly wrote) and instructs him to read it in the voice of wrestler Stone Cold Steve Austin. Hilarity ensues. The entries have aspects of Will’s life but are mostly fabrications.
Obviously there is a great amount of debate within the show’s fan base as to whether it is a legitimate AI or if there’s a person behind the digital persona of Dudesy, and I want to know what real artificial intelligence experts and enthusiasts think about the veracity of the Dudesy claim. So I urge you to go have a listen to a few episodes of the show, if for no other reason than to enjoy Will Sasso’s spot on and hilarious impersonations of various sports stars and actors, but more importantly, tell us Dudesy fans… is it really an AI??

It’s available on all the big podcast platforms but should be watched at least once to appreciate the strange intro video, which Dudesy allegedly made…

https://youtube.com/channel/UCO7QB0sO4vPkCNltYTb33qw

Edit: I am in no way affiliated with the show, am just a curious fan",MLQuestions,2022-06-09 08:12:51,4
"Gradient descent means going in the direction of steepest descent but depending on step size and size of the valleys/hills, you can ""jump"" out of local minimum valleys. This scenario could be jumping out of a local minimum valley and into another local minimum valley (or global minimum but there's no way to know that).",8,v8glz6,,MLQuestions,2022-06-09 06:14:50,7
"Ironically, I think you’re overly-humanizing how AI learns. At its core, its an iterative math formula that’s built around learning features correlated with some objective, e.g. winning a game of chess or distinguishing cats from dogs. It doesn’t naively take actions from some infinite action space- it’s guided by the magic of multivariate calc. We’ve really side-stepped the question of intelligence altogether- read Turing’s “The Imitation Game”- it’s still how we think about learning and evaluation. In this task-based “intelligence” paradigm, there’s really no way to learn without seeing examples. What math is best for learning is at the core of most ML research, but all of our evaluation metrics are still task-specific. If you’re interested in questions about intelligence and technology, I highly recommend the book, “Vehicles: Experiments in Synthetic Psychology.” It’s a thought experiment where, in each chapter, a robot is given new sensors or objectives, and over time, to an outside observer, it becomes indistinguishable from life.",2,v8sy9s,"Apologies for the absolutely basic level of this question!

My understanding is that the majority of the current AI and Machine Learning paradigm has computers working on problems in the most basic way a human would, just exponentially faster.

What I mean is that my understanding is that AI works on problems by iterating and optimizing: create, see what worked/didn't, try another version. As a humble homo sapiens, I am of course quite fond of that system. 

But is that maybe ridiculously inefficient for a machine? Are we working on finding some completely different way of learning that might be more conducive to machines? (I assume we are)

More broadly, can anyone direct me to a sort of philosophical school of thought within AI development about how we can go about making machines that aren't limited by... I guess... The fundamental limits of our ability to conceptualize what they could be capable of?

Y'know, ""more things in heaven and earth, Horatio"", and all that.

(I realized just as I finished writing this that there's probably a whole subreddit geared towards this type of question)",MLQuestions,2022-06-09 15:39:51,3
"You build a single model, train it on your training set, and then evaluate on your validation set. Then you can try tweaking the model, re-training it, and re-evaluating on your validation set. If the validation set did worse, your tweak was bad and vice versa.

I'd recommend checking out models others have built for your task or that use similar architectures and using those as a starting point. With experience, you'll start getting a feel for which hyperparameters are likely to have an impact and which aren't. But honestly, don't waste too much time tuning hyperparameters. There are definitely diminishing returns on investment when tuning: the first hour can be productive, but the next 100 often aren't.",1,v8rmvl,"I have tried many sites, but I still don't get it. Hyperparameters are an important part of a model. They include activation function, layers, nodes etc. Basically this means I could have a neural net (e.g. multilayer perceptron) of indefinite size and unknown functions.

I am told that hyperparameter optimisation occurs on the validation set first. I.e. I have to choose random hyperparameters off the top of my head, train the model on the training set, *then* see if this makes sense by doing a hyperparameter optimisation on the validation set (I assume this means that I use the val data to train on and assess performance on of that model at the same time). Or do I do a wide hyper parameter sweep on training data, and assess the performance of these models *on the validation data.* Basically by this point I am totally lost. :)

Also, for example, when I do a random search with eras tuner over hyperparameters, I build an entirely new model. So how do I 'train a model on the training set' then 'tune its hyper parameters on the validation set' as such?

For eras hyperband/random I may have: 

""tuner.search(x=X\_train, y=y\_train, epochs=5) "" 

Say. Is it the case that this is supposed to be on the training data, and then I evaluate the performance of that model *on the validation data, such as:*

""tuner.search(x=X\_train, y=y\_train, epochs=5, validation\_data=(X\_validate,y\_validate), metrics=\['val\_loss'\]) "" or something like this? (and how do I know it is selecting validation loss as the metric of choice, than just including it?) 

I am totally lost about these rather simple points. And not sure why. Any help most welcome. Cheers folks!",MLQuestions,2022-06-09 14:38:31,5
"r/mlscaling

Might not be the most optimal source, but checkout MS [DeepSpeed](https://www.deepspeed.ai/)",6,v7tzpf,"What is needed for really big models like GPT-3 175B to fit in memory? I understand *Distributed Data Parallel* but is that really enough? Is the model also split into multiple parts over different GPUs/TPUs?

Can someone give me a real example of how one of these big models is trained on resent hardware with a large batch size?",MLQuestions,2022-06-08 09:43:19,4
"This sounds to me like a cart association problem. You can look into these algorithms:

-	ECLAT 
-	FP Growth
-	Apriori",1,v89xyb,I need to make a recommendation system with should recommendation user specific content based on his past viewed books. My dataset contains more than 230000 books. It is not possible for me to compute and store such a large similarity matrix. What approach should I take?,MLQuestions,2022-06-08 22:54:50,1
"Accuracy in a vacuum is meaningless. 76% accuracy is great if your incidence rate is 50%. If your incidence rate is 1%, then 76% is worse than a parrot always guessing the most popular group. Also, if you actually want some help, you need to provide more details - whats your data like, how is your model implemented, and so on",2,v80jmk,"Hello!

I am trying to classify some people in two different groups using a random forest (with Tidymodels in R). After some tuning I managed to get 76% of accuracy in the training dataset. I am curious if I should keep increasing the range of the grid search or if I should try something new. I hope I could get about 80% accuracy in the test dataset.",MLQuestions,2022-06-08 14:29:41,4
"These is no guarantee that your code performs exactly the same on different software versions or even the same software versions on different host machines. However, since kNN is deterministic and you confirmed that you feed it the same data, the results should ideally be very close together. What are the actual RMSE values that you get from both runs?

I suggest the following:

* Set the `algorithm` parameter to a fixed value that is not `auto`, preferably `brute`.
* Fix the data you feed it. Don't do a random split but instead just use `X[0:1000]` (for example) for training and the rest for test.",1,v804ei,"Hello,

Can't find an answer to this elsewhere so I'm posting here:

When I run the code below on my computer I get that the value of k that minimizes the RMSE is 1. When I run the same code, using the exact same train\_X and train\_y on a different computer, I get that the value of k that minimizes RMSE is 6. I also get different RMSEs for each K. I know that train\_X and train\_y are the same size, and have the same rows inside of them. I set train\_X and train\_y by using the following command in both computers, and I checked that they have the same data in them across computers.

train\_X, valid\_X, train\_y, valid\_y = train\_test\_split(X, y, test\_size=0.4, random\_state=1)

My code that gives different values across computers:

results = \[\]

for k in range(1, 20):

knn = KNeighborsClassifier(n\_neighbors=k).fit(train\_X, train\_y)

results.append({

'k': k,

'RMSE': math.sqrt(mean\_squared\_error(train\_y, knn.predict(train\_X)))

})

\# Convert results to a pandas data frame

results = pd.DataFrame(results)

print(results)",MLQuestions,2022-06-08 14:10:05,1
"If you did get a NN that works I'd assume it's a something a long the lines of hyperparameters  issue (too high of a lr, etc)",1,v7qtyq,"I have a genetic algorithm used to train neural networks to play flappy birds. The nn do progress but they rarely ever get a perfect score at the game. I am not sure if it's a problem with the nn or the algorithm. I've only gotten a perfect nn twice out of the hundreds of times that I've run the program 

[Here's the git repo](https://github.com/adam-kabbara/flappy-bird-AI)",MLQuestions,2022-06-08 07:23:42,14
"
I see you've posted a GitHub link to a Jupyter Notebook! GitHub doesn't 
render large Jupyter Notebooks, so just in case, here is an 
[nbviewer](https://nbviewer.jupyter.org/) link to the notebook:

https://nbviewer.jupyter.org/url/github.com/InFoCusp/diffusion_models/blob/main/Diffusion_models.ipynb

Want to run the code yourself? Here is a [binder](https://mybinder.org/) 
link to start your own Jupyter server and try it out!

https://mybinder.org/v2/gh/InFoCusp/diffusion_models/main?filepath=Diffusion_models.ipynb



------

^(I am a bot.) 
[^(Feedback)](https://www.reddit.com/message/compose/?to=jd_paton) ^(|) 
[^(GitHub)](https://github.com/JohnPaton/nbviewerbot) ^(|) 
[^(Author)](https://johnpaton.net/)",1,v7nukm,"I'm trying to figure out a way to reproduce the sampling method used in the DDPM model [arxiv link](https://arxiv.org/pdf/2006.11239.pdf). The codebase link is roughly [here](https://github.com/hojonathanho/diffusion/blob/1e0dceb3b3495bbe19116a5e1b3596cd0706c543/diffusion_tf/diffusion_utils.py#L190) for the original model and for improved DDPMs [here.](https://github.com/openai/guided-diffusion/blob/27c20a8fab9cb472df5d6bdd6c8d11c8f430b924/guided_diffusion/gaussian_diffusion.py#L441) There is also implementations recently posted to r/MachineLearning such as [this one](https://github.com/InFoCusp/diffusion_models/blob/main/Diffusion_models.ipynb) (check the reverse process section) and finally [this last one](https://github.com/cloneofsimo/minDiffusion/blob/master/mindiffusion/ddpm.py#L45).

&#x200B;

My question is the following: The sampling algorithm (Algorithm 2 in the first paper linked) is quite straightforward, it attempts to sample `x_{t-1}` from `p_\theta(x_{t-1}|x_t)`, that sounds intuitive, and is exactly what the last github repository linked does. However, the first two (and the third is just derived from there) repositories which includes the author's repo does the following:

(psuedo)Algo as implemented in code

    x_T ~ N(0, 1)
    for t = T, ... , 1 do
      1. using x_t and e noise output from model, calculate x_0 = x_t - sqrt(alpha_bar_t) e / sqrt(alpha_bar_t) 
      2. using x_t, x_0, calculate q(x_{t-1} | x_t, x_0) posterior parameters (using equation 7 from paper). 
      3. sample x_{t-1} ~ q(x_{t-1} | x_t, x_0) return x_0

(psuedo)Algo in paper

    x_T ~ N(0, 1)
    for t = T, ..., 1 do
      1. set z ~ N(0, 1) if t > 1 else 0
      2. set x_{t-1} ~ p_\theta(x_{t-1} | x_t) (much simpler equation)
    return x_0

**Why sample using the first codeblock**? The second codeblock is literally written as the sampling algorithm and is as the authors mention sort of like Langevin dynamics sampling since epsilon approximates the gradient of the data density.",MLQuestions,2022-06-08 04:55:21,1
I'd recommend doing this tutorial (start from scratch). But here's the direct link to the QA  part: https://huggingface.co/course/chapter7/7?fw=tf,2,v7kn36,"So I have a dataset of around 48000 comments from Twitter and Facebook in CSV format, and need to make a chatbot which takes a question from the user and generates an output.

So what should I start with? Like, what model should I use, how do I process this dataset for training that model ?

Is it possible to make such a thing😅?",MLQuestions,2022-06-08 01:32:23,2
"Embeddings is just a fancy word for coordinates. If you have 2D plane an embedding would be \[x,y\]. If you have 100 dimensions it would be \[x1,x2,...,x100\]. I never heard of embeddings weight and I don't see how does that make any sense, since again, embeddings are coordinates (it's like saying coordinates weights). Maybe model's weights?",1,v7hgz8,"Are they same or not?
If not, what is the difference btw them?",MLQuestions,2022-06-07 22:06:14,1
"Assuming you're using TF/Keras, an RNN will expect your data to be in the following shape:

(window\_count, window\_width, series\_dimension)

It looks like you are using 1-D time series data, so series\_dimension would be 1.

**Thoughts**

When working with time series data, RNN's are there to model the ""dynamics"" of the underlying system that produced the data. By dynamics, I mean ""how the system changes from one timestep to the next"" (the system in this case, would be a human doing the typing). This is why you saw those stock prediction papers using RNN's (rather than a linear ARIMA model and such). However, given your problem description, I think RNN's may not be the right model (which may be why you haven't been able to find any resources on it). If you are dead set on using a neural network for this problem, then you might consider either a 1-D CNN, or a basic FNN (make sure your data is stationary before feeding it to the FNN). However, given your data shape, you might not have nearly enough data to use an NN.",2,v78cv0," 

I currently have a NumPy array of size (85,6000) where 85 is the number of samples and 6000 is the length of the time series for each sample. I want to create a time step of 60 for the time series of length 6000. From my understanding, I should just add another third dimension and put in it the time series but it is lagged by 60. How do I do that and will this be correct as an RNN input?

For context, I am doing my thesis and need help asap. I have data on 85 patients where each patient has a time series that can be described as their keyboard tapping pattern. I want to enter this keyboard typing pattern into an LSTM to see if it can do binary classification to predict if they have Parkinson's or not.

There is more info on this in my stackoverflow question: [https://stackoverflow.com/questions/72537239/how-to-reshape-a-numpy-array-to-have-time-step-for-a-rnn](https://stackoverflow.com/questions/72537239/how-to-reshape-a-numpy-array-to-have-time-step-for-a-rnn)",MLQuestions,2022-06-07 14:34:14,3
Each batch size and lr might require different number of epochs. Play with the lr and batch and see where does the network learn best. When the loss stops improving or the network starts to overfit you know that you need lower epoch,5,v6v9nr,"Hello, I am working on a project that has 10 classes and each class has 700 samples. I need to choose good values for the epoch, batch size, and the learning rate. How should I go about doing this and in the future, is there a rule I should follow to selecting these data values?

&#x200B;

Thanks",MLQuestions,2022-06-07 06:16:29,5
">I have applied PCA, Correlation

What do you mean you 'applied' PCA and correlation? Did you use the output of PCA as a predictor? What does 'applying correlation' mean? 

>label encoding

Elaborate?

>randomly selected the data in 3 patches... I have 600k entries which are relevant, out of which I have used 240k

3 fold CV? Or you only trained with 1/3 of the dataset and tested on another 1/3? What does this mean?

>I have applied 8 ML models (Logistic Regression, Naive Bayes, KNN, J48, Random Forest, Ada Boost, Xg Boost and Boost)

This is a very concerning statement, but maybe we can get to this later.

>But the accuracy is still below 60. Can anyone pls tell me how can I take the accuracy above 70?

Why 70? What makes you think that you can achieve 70? Why is the accuracy in the 50s?

Can you describe the dataset more than that it's 'real-time' as you mentioned? What are you predicting? What columns do you have? Is this homework or industry?",13,v65agd,"I'm working on a project and the accuracy of the model is not improving. I have real-time dataset of more than 1 million entries and 22 columns. I have applied PCA, Correlation, removed the null values, removed the outliers, done the label encoding, also randomly selected the data in 3 patches. After doing the proprocessing step, I have 600k entries which are relevant, out of which I have used 240k. 
I have applied 8 ML models (Logistic Regression, Naive Bayes, KNN, J48, Random Forest, Ada Boost, Xg Boost and Boost). I have also done the hypertuning of the models and also Grid Serach. But the accuracy is still below 60.
Can anyone pls tell me how can I take the accuracy above 70?

Dataset link: https://www.kaggle.com/shloksethia/updated-crime-chicago",MLQuestions,2022-06-06 07:51:02,9
sounds like you're overfitting to some aspect of your training data,2,v631ua,"When my models start overfitting the training accuracy keeps rising but the validation accuracy drops. However the drop in validation accuracy is not so apparent. However, the increase in validation loss is very apparent. See the plot below to see what I mean. 

[https://gyazo.com/c1df60c1463053d9c5a034d6919ec362](https://gyazo.com/c1df60c1463053d9c5a034d6919ec362)

Why is this the case? Why are loss and accuracy affected differently?",MLQuestions,2022-06-06 06:03:03,2
"Try to stick to tflite models.

  


[https://blog.paperspace.com/tensorflow-lite-raspberry-pi/](https://blog.paperspace.com/tensorflow-lite-raspberry-pi/)

  


https://www.tensorflow.org/lite/guide/python

https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/blob/master/Raspberry_Pi_Guide.md",7,v5bhkh," I have a raspberry pi 4 model B - 2GB RAM and I want to use it for CNN models inference. I am planning to use models from GitHub. I heard that some models can’t be used because of their large architecture.  How do I know if a given model is suitable for the raspberry pi, what are the parameters to be taken into consideration?  

Any help would be appriciated.",MLQuestions,2022-06-05 04:32:55,6
"1. That documentation page has a reference paper linked. Have you checked out that paper? https://arxiv.org/abs/1502.03167

2. receptive field. see for example https://paperswithcode.com/method/dilated-convolution

3. I think you are misunderstanding how convolutions work. each kernel gets its own output channel. maybe you can pull a paper where you interpreted it as the layer having a single output channel and that could give your question some more context?

4. the loss function is essentially a transliteration of the problem you are trying to solve from natural langauge into math, so loss function choice is very task dependent. choice of activation function is often more of an engineering consideration and has consequences for how the information in the gradient flows through the backprop operation.

5. you want to ""project"" your data into a new representational space

6. you want to learn a feature detector and/or you want translational invariance

7. when you want to form a summary over a feature detector and/or you want the output dimension to be smaller than the input (e.g. constructing an information bottleneck, downsample, etc.)

8. I recommend reviewing the original resnet paper. here's a video giving an overview: https://www.youtube.com/watch?v=GWt6Fu05voI",1,v5byxi,,MLQuestions,2022-06-05 05:04:19,1
"Ok, so after some experimentation I feel a little silly.

There’s no way this can work.

This discriminator doesn’t actually learn anything about the content of the images.",1,v5bkqd,"I am trying to build a GAN to upscale images and I’m not too concerned with it overfitting.

My discriminator takes in 2 images and predicts if those 2 images are the same or not.

It takes in the generated highres image + the ground-truth highres image or it takes in the ground truth image + a slightly augmented copy of the ground truth image (some blurring or sharpening or contrast).

This so the discriminator needs to work a bit and can’t just make matches 1 to 1.

I think the discriminator still gets too good too fast and ‘settles’ and then the generator just learns to fool the discriminator without actually producing nice upscaled images.

Why not do it the SRGAN way? Because I am using this on drawings and SRGAN produces too much smoothing and artifacts.

Anyway, I have played around a bit with different learning rates for the generator and the discriminator and that certainly has an impact on how far the generator improves.

My question is if any of you have some suggestions I can try and improve this approach.

Thanks.",MLQuestions,2022-06-05 04:38:45,1
"https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/

See Chap 5 in [this](https://libgen.rocks/ads.php?md5=2CF6FFFB237741DE7C3B5323B6089413) book",2,v5aatl,"I relatively new to ML world. I am building a model that predicts salaries of employees. Among the features, there are things like quantity of widgets produced (quantitative), position (obviously affects the salary), years of experience (quantitative) as well as hazard level (quantitative). The y of the model is a number.

The question is which ML algorithm I can use to do build a model for this. Is there one that can work with both quantitative and qualitative features? How should and can this be approached?",MLQuestions,2022-06-05 03:07:25,2
"Why not trim all signals by 50 points irregardless of if the spike is there? That would be the simplest and as long as you have enough data without the 50 it shouldn’t matter which you seem to imply because your still using the trace that your removing the initial spikes. 

If anything I would just try to determine how many points need to be removed. 25,50 etc",2,v4lv9y,"Hi, 

I have a data consisted of ECG signals, and some of them start or end with a sharp changes, which could be lower or higher than the actual signal's value. For example, here I have two signals, one starts with a sharp decrease, and the other with a sharp increase.  

Example 1: Sharp decrease at the beginning of the signal  

&#x200B;

https://preview.redd.it/3q9du7sbwk391.png?width=378&format=png&auto=webp&s=2d6c59f19abb9c7bf0019d535493979a91e89d43

Example 2: Sharp increase at the beginning of the signal  

&#x200B;

https://preview.redd.it/qm8a91qcwk391.png?width=378&format=png&auto=webp&s=18979b707ea7af652a2d18d8297a78b1fdd05f3b

These drifts are due to movements and have no significant information, I would like to remove the part which consist them, thus I have to detect for each signal if this sudden change occurs at the beginning of the signal or at the end.  

I would like to keep this part as simple as possible, I am even considering removing the first or last 50 time points. But in order to do that, I have to detect when and where these spikes occur.  

Any idea? 

Thanks!",MLQuestions,2022-06-04 03:10:12,9
"The first thing you said is correct. The learned weights well go to infinity. But normally you'd put a weight regularizer so that doesn't happen.

If you don't put any weight regularizer, the error surface would kinda look like a half pipe. You can imagine snowboarding down it. You can go down the pipe as long as you want and the loss will keep getting lower and lower but if you go to the side the loss will go up. Of course eventually the snowboarding doesn't work so well because it becomes less and less steep.",4,v4ivde,"I recently read up on the possible issues that logistic regression might encounter. One such issue is that when classes are linearly separable, a maximum likelihood solution for our parameter w does not converge. Rather it moves to arbitrarily large values and the sigmoid begins to resemble a step function.

I also read that the cross entropy error used to search for the optimal w has a positive semi definite hessian matrix, and is therefore concave with a unique minimum. 

What would this error surface look like in the case where classes are linearly separable, as the minima is at infinity?",MLQuestions,2022-06-03 23:25:29,3
"No test sample should ever touch your model in training.  Otherwise, it's called data leakage and is cheating.

So, yes, you split your data before anything is done.",2,v4esjg,"Would you take a random selection of entire batches for your test set?  Would you select your test data before the data is divided into batches, or something else?  There seems to be a few options here, and the most effective option isn't very clear.  ",MLQuestions,2022-06-03 19:09:21,5
"For generation task gpt 2 and Pegasus are good choice.

For other tasks distill bert might be a good choice. Other than distill bert there are other model like minilm which are very good.",2,v452gb,"What is the current state of the art model for 

\- text generation (im familar with GPT2 but doesn't seem that great)?

\- sentiment classification?

&#x200B;

PS: Looking for open source solutions, but open to commercial if good & cost effective",MLQuestions,2022-06-03 10:58:31,4
"I don't think there's one single correct answer here. My suggestion would be ""trust but verify:"" tune once and deploy, then retrain often with the same hyperparameters (that's the trust part) BUT also monitor your input and output distributions, as well as your metrics: are you seeing unexpected drifts or changes? Have your data sources changed in some way? Is your model performing worse? These may be cause for retuning (as well as other checks to ensure things haven't broken elsewhere).",2,v42g7r,"I understand how tuning works, and that it's a compute-intensive process. What I haven't been able to find an answer to is when that process occurs in production systems.

Say I have a cloud pipeline set up that waits for new data and periodically retrains the model with the updated dataset. Do I need to also retune at that time? Or is the thought that the factors that make a combination of hyperparameter values optimal for a given dataset don't change much, so the values can be relatively static in production?",MLQuestions,2022-06-03 08:55:17,2
"If it is a linear regression without any pre-processing, ie you are not using any features transformation techniques,  regression coefficients show you a degree of increase.

Check equation for linear regression and if you try to do first derivative by variable Age you will get a number that represents coefficients of variable Age.",2,v3yydb,"Hey! I’ve been doing ML for a while but I come from a programming background so my maths a little on the weaker side.

I was wondering if there’s a way to take some regression model, and take a partial derivative to see how a single variable affects the probability. For example, if you had a model predicting diabetes, with 7 inputs, could I train it, then take the partial derivative of Age with respect to the probability outcome to see how the probability is affected with age?

Does this make sense mathematically? I was thinking it would be useful to see a graph of how each variable affects the output. I know with linear regression you can just look at the coefficients, but, I’m talking in terms of a more advanced model like a DNN where it’s not linear.",MLQuestions,2022-06-03 06:03:53,3
Train feature autoencoder and do multiclass classification with sigmoid activations on the encoder,2,v3zpm2,,MLQuestions,2022-06-03 06:43:13,1
That's exactly what's happening here. The inputs would likely be different since both embeddings come from an embedding layer. That is unless query_input=value_input.,1,v3vcqo,"Hi all!

I am doing doing my Master's thesis connected to ML and recently I saw [stange thing in Keras Attention Layer docs](https://keras.io/api/layers/attention_layers/attention/). What I am talking about is this snippet:

    # CNN layer.
    cnn_layer = tf.keras.layers.Conv1D(
        filters=100,
        kernel_size=4,
        # Use 'same' padding so outputs have the same shape as inputs.
        padding='same')
    # Query encoding of shape [batch_size, Tq, filters].
    query_seq_encoding = cnn_layer(query_embeddings)
    # Value encoding of shape [batch_size, Tv, filters].
    value_seq_encoding = cnn_layer(value_embeddings)

To be honest I don't quite understand what this is doing. Is it one layer which processes two signals? I already created simple network and visualize it in Netron and this is what I got:

&#x200B;

https://preview.redd.it/5l4unh6ogd391.png?width=344&format=png&auto=webp&s=e2a356d9675a5ee7db79cbc7df05036fcd35b194

Maybe someone of you know a little more and could explain it to me.

Thanks in advance!

&#x200B;

EDIT:  
Answer to that is: [shared layer](https://keras.io/guides/functional_api/#shared-layers). Netron just not showing two output arrows :c",MLQuestions,2022-06-03 02:11:51,2
"That's an extremely common business model in Audible and has been for a decade.

You just sit there and separate the book into text strips, then feed them through any high quality TTS.

By the sound of it, the most common is MS Neural right now.",1,v3mrzx,A model that's able to identify characters and assign a different voice to each of them for example. And of course the whole acting thing.,MLQuestions,2022-06-02 17:14:01,9
"1. create a training dataset formatted as below and fine-tune gpt2 on it

`<|endoftext|> <|context|> your input sentence 1 <|endofcontext|> <|encoding|> your encoded output 1 <|endofencoding|> <|endoftext|>`

`<|endoftext|> <|context|> your input sentence 2 <|endofcontext|> <|encoding|> your encoded output 2 <|endofencoding|> <|endoftext|>`

2. then, during inference, format the input sentence as `<|endoftext|> <|context|> desired sentence <|endofcontext|>` and start generating text in loop until `<|endoftext|>` token is generated. It will output only desired tokens",1,v3b5l6,"My input is a string and the outputs are vector representations (corresponding to the generated tokens). I'm trying to force the outputs to have specific tokens (e.g., 4 commas/2 of the word ""to"", etc). That is, **each generated sentence** must have those.

Is there a potential loss component that can force GPT2 to generate specific tokens? Another approach that will be easier and more robust (but I'm not sure is possible), is similar to the masking of tokens in BERT. That is, instead of forcing GPT2 to generate sentences with unique tokens, to have the predefined tokens in the sentence beforehand:

    [MASK][MASK][specific_token][MASK][MASK][specific_token]

However, an issue with this approach is that there isn't a predefined number of tokens that should be generated/masked before or after the \[specific\_token\], nor there is a predefined number of sentences to generate for each given input (else I would have used BERT).

    # code
    from transformers import logging
    from transformers import GPT2Tokenizer, GPT2Model
    import torch 
    
    checkpoint = 'gpt2'
    tokenizer = GPT2Tokenizer.from_pretrained(checkpoint)
    model = GPT2Model.from_pretrained(checkpoint)
    
    num_added_tokens = tokenizer.add_special_tokens({'pad_token': '[CLS]'})
    embedding_layer = model.resize_token_embeddings(len(tokenizer))  # Update the model embeddings with the new vocabulary size
    
    input_string = 'Architecturally, the school has a Catholic character.'
    token_ids = tokenizer(input_string, truncation = True, padding=True)
    output = model(torch.tensor(token_ids['input_ids']))

&#x200B;",MLQuestions,2022-06-02 08:15:26,2
You can look into Google coral,2,v3ji5f,"I have a model for which I can run inference on my laptop, but it's slower than I would like. I've looked a little bit at options for external compute hardware (e.g. Intel Neural Compute Stick 2 and cases for housing an external GPU), as I would prefer to keep my setup relatively portable and not have to build a whole new computer. Ideally, I'd like to be able to do reasonably quick inference with a Raspberry Pi. Additionally, I'm looking to not break the bank. So, my question is, what hardware would you recommend that could work as described and would be good value?",MLQuestions,2022-06-02 14:33:34,2
"Microsoft has a funding program by that name, consider contacting the grant's recipient. The list is public.",2,v3eco1,"any non profit organizations that use AI for good for humanity / planet / animals

that can I join / volunteer",MLQuestions,2022-06-02 10:38:09,3
Then look for courses in the specific tools you are interested in using but that type of course is usually not offered by universities,1,v3h4lp,,MLQuestions,2022-06-02 12:44:28,2
https://docs.opencv.org/3.4/d4/dee/tutorial_optical_flow.html,1,v33m0h,"Hello,

I would like to find a way to detect with a camera when an object is moving and when it is static.

Is there any existing software that can do this job?

Thanks",MLQuestions,2022-06-02 00:51:12,2
"You can just predict these independently, or use **hierarchical classification** as a possible approach.  recommend reading up on that term at least.",2,v37cwl,"I  am new to Machine Learning and need to build a classifier which can  classify Emails into classes with 3 layers. Some Examples would be:

\[ Question : Car : Color \] Which would mean that the email is a question about a cars color.

\[ Question : Car : Driver \] Question about the driver of the car

\[  Action : Product XYZ : Send Out Invoice \] This is an email where the  sender is asking for an invoice of the product which he has ordered.

The  main problem is, that there are nearly 400 hundred of such combinations  of categories. My best answer right now would be to first predict the  Category like ""Question"" or ""Action"" and then the second and then the  third one. Is this possible?

I  need to build this for my bachelor thesis, and have a huge labeled  Dataset. I only need some inspiration on how to tackle this problem.

Any help is appreciated!!",MLQuestions,2022-06-02 05:07:03,2
"YOLO, object detection?",1,v2rrcz,"I am building a system that would use computer vision to find a target product on a grocery shelf (or a target image in a cluttered scene). I came across this Github project - [https://github.com/mrtrkmn/Product-Search-in-Supermarket-Shelves](https://github.com/mrtrkmn/Product-Search-in-Supermarket-Shelves) which uses SIFT-based features and a FLANN-based matcher to achieve that. I tried it with a continuous camera stream and the results are not reliable. I am wondering if there exist deep-learning solutions to do the same.  I am thinking of something that can ""match"" the features extracted from the target image to something like region proposals (from Faster R-CNN). Any help would be appreciated. Thanks.",MLQuestions,2022-06-01 14:19:29,2
"I'd do CS/math. I didn't know that's an actual degree and I'm pretty sure companies won't know how to measure you. And you might limit yourself if you find that you don't really like it, or if there's something bigger than you want to do (while CS/math can prepare you for quite a wide range)",2,v2h84o,"Hey,

Has anyone pursued the ML/AI course for their undergrad? I am planning to do my undergrad in ML/AI. Many reasons for doing so. I have covered most of the syllabus of the CS degree, and everyone is doing CS nowadays.

ML has always interested me, and i have started learning it since last month. 

Is the degree worth it? Can anyone share their experience as a ML/AI undergrad?

Also, is there a job market for interns who are doing their undergrad in ML/AI?

Any help appreciated :)",MLQuestions,2022-06-01 06:38:08,3
"This is called tokenizing.  And there are tools in most languages to do this.  If you are using pytorch, here are instructions:

https://pytorch.org/text/stable/data_utils.html

Here is an example use:

https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html",2,v2h49b," I want to perform pattern matching task as a part of Pre-processing. I have more than 3M text sentences. On the other hand, I have around 130k terms (may contain multi-word terms separated by spaces) which I want to match with the text in those 3M sentences. The expected output is the matched terms per sentence, if any. Is there any efficient way you know of? I am also considering lowercasing text on both the sides as pattern matching is allowed to be case-insensitive.",MLQuestions,2022-06-01 06:32:42,4
"Sota is the best result ever achieved with the dataset. Of course with reproducibility problems it's often difficult to judge what is really sota, so it's more of a buzz term than anything else.",3,v1o5si,"A lot of papers  mention their models achieving state of the art results or ""near state of the art results "" ,  how do we define sota ? is it the best result ever achieved with the dataset or is it a specific range like the top 5 ??",MLQuestions,2022-05-31 04:32:31,3
"are you just looking for more free educational content, or are you looking for course credit? Here are some excellent free courses if you don't need accreditation:

* https://d2l.ai/
* https://www.fast.ai/
* https://huggingface.co/course/chapter1/1",2,v1lki2,"Hello, I want to ask something about the deep learning course from the university. Is there any deep learning course like [https://uvadlc.github.io/](https://uvadlc.github.io/) and [https://cds.nyu.edu/deep-learning/](https://cds.nyu.edu/deep-learning/) ? Because I think their course is exciting for me and different from others course. Thanks for helping.",MLQuestions,2022-05-31 01:31:30,6
"I'm on mobile, can't look at the notebooks.  But it's a little odd that one value of the accuracy is 1 minus the other. 

Any chance the labels are being swapped in either the training or test set?

Also, accuracy is often a tricky metric because it depends on the ratio of positive and negative classes. Any chance that's changing between different test sets?",1,v1sh9a,"I am getting very inconsistent test accuracies from my model but cannot figure out why.

I was trying to benchmark some TensorFlow/Keras stuff and noticed my results were unreliable. Not the timing, but the test accuracy of the model. On some runs the model would achieve a test accuracy = 0.51, and in the next it would only achieve 0.49, and in the next 0.54, and in the next 0.36. Same architecture, same optimizer trained on the same datasets (train, validation, and test), same everything.

I tried to remove most sources of randomness, such as using the same numpy rng seed, TensorFlow rng seed, resetting the TensorFlow backend between runs, and using the same seed for shuffling the dataset, etc. The issue persists.

Can someone please help me figure out what I am doing wrong?

[Here is a reproducible example](https://colab.research.google.com/drive/1b2DZaAIjqqXDlQ_uJyinVxOn1z_cbAoT?usp=sharing) on Google Colab, but the issue also happens locally with different versions of Python, TensorFlow, and Cuda/Cudnn.

And [here is the dataset](https://drive.google.com/file/d/1UXJOv9MJ7bu_0woiVVp0BmcnfDzo0hpf/view?usp=sharing) I am using. It is just CIFAR-10 with the training partition split into train and validation.

Thanks.",MLQuestions,2022-05-31 08:10:43,10
All AI or machine learning technical jobs require some software development.,2,v1egl5,"Hey hey

I want to become an artificial intelligence engineer. My background is in Industrial/Systems Engineering, not CS. I'm not really all that interested in software development. 

Does it make more sense for me to pursue an MS.CS or an MS in Math?

Basically, will I not get a job without a programming background?

Much luv",MLQuestions,2022-05-30 18:03:02,2
Tubingen University youtube channel have good content.check that.,1,v0zw0h,Book link: https://probml.github.io/pml-book/book1.html,MLQuestions,2022-05-30 06:10:24,1
Look for internships while you're still a student,2,v1cleu,"hope you are all doing well

I know this may look like a naive question, but I am a senior E.E. Engineering student (graduating in less than a month),

I have a fairly good understanding of deep learning and machine learning (basic algorithms)

with knowledge in python, git, TensorFlow, docker,scikit-learn, data manipulation, and visualization

I have some projects (about 6) in dl/ml

what do you recommend I do or what skills should I learn to increase my chance of landing a job?",MLQuestions,2022-05-30 16:20:10,2
"Wouldn’t this make it fight itself in some scenarios? If the parameters are constructed around making sure the confidence is >0.5 on one label but these parameters don’t work, because maybe the model isn’t complex enough or the dataset isn’t a good representation, with another label then it will just fight itself, no?",1,v10x03,"I have a multilabel classification task where every document has one-to-many labels. The ground-truth rule is that every document must have *at least* *one* label, i.e. there are no documents with zero labels. Still, by default, there is nothing stopping the model from loading very low confidences onto the presence of all labels for a document it is unsure about, and thus if you try to construct some sort of decision rule like ""assign the label to the document if the model's confidence is > 0.5"" fails to generate labels for some documents. 

I thought about just taking the maximum confidence score for cases where no label is present with Pr>0.5, but then that gets weird in multi-label settings where the Pr(label A) = 0.49, and Pr(label B) = 0.48, etc. To assign a single rule to ensure every document gets a label is highly contingent on the validation data I see / the confidence scores generated, i.e. for one validation set, a cutoff of 0.25 might be enough to ensure every document receives at least one label, whereas for another the cutoff needed might be 0.01.

So it got me thinking that maybe the fact that there *must* be at least one label for each document is something I could inform the model of during training. Is there a way to e.g. enforce that, potentially against its main objective of loss minimization, there must be at least one label with a confidence score >0.5, i.e. it is forced to ""make up its mind""?",MLQuestions,2022-05-30 07:03:37,2
"Please don’t go through 4-6 years of a PhD if the only thing you care about is a faang job. 

Once you finish your masters if you have any publications you could apply to ml research companies. What is it about faang that interests you? Usually with that specifically it’s money alone which isn’t a great sign. 

I would recommend you find a domain in ML that actually interests you and work on that.",13,v066eh,"Is PhD a must (like will it be unlikely that I get the job without it)? I do not want to do a PhD if I don't need to. I I do a PhD I need to have an answer to ""why am I doing this?"" At least so I can put a strong application. I can see that the minimum requirement is a Masters degree so I started my masters degree at my local university which is not top ranked. I still do not have any publications and the compute budget at my university is limited. I am really looking for the experience of the people who can relate to some or all of the things I said and got a job at FAANG as ML engineers/ Applied Scientists / Data Scientists.

-----------
Edit -->
For more context, I recently got an interview at Meta AI Residency program and made it to the team matching phase (I beleive it was 2 phases, phone screening which was mainly leetcode style question, my reqruiter told me that I passed this one and of a team is interested in my profile they'll interview with me) but got rejected at the end.

Thank you,",MLQuestions,2022-05-29 00:36:45,10
I think it would be more valuable for your learning if you did this task yourself. You have access to Google and Wikipedia. You can read a short description about each of these things and categorise them as you like.,8,v07d5x,"Hey I'm in college (non-circuital branch) and about to start self-studying Data Science topics but the more I look at it the more confusing it gets. When I look at courses, each teaches different things with so many unknown terms and I don't even know how to proceed anymore.

There are words like- Optimisation, Dimension Reduction, Density Estimation, Probability, Bernoulli distribution, Parameter estimation, Kruskal-Wallis, Wilcoxon, Numpy, Seaborn, ggplot2, NLTK,tf-idf, NoSQL,Git, web scraping, Multimodel, Scikit-learn, forests, XGBoost, data leakage, SNAP-ML, trees, support machine vector, Baseline model, Feature Selection, logitstic,ANOVA,models(VGG16, ResNet, CNN), TFX, FunkSVD, Apache spark, GPU/TPU, MLOps, DevOps, Z, (ARX,ARMAX,ARIMA), Kuubeflow, recommendation engines, PCA,TSNE,EDA, data wrangling, data engineering, statistical modelling, computation model and design, GFDR, Bonferroni, Novelty and Recency effects, LSTM model, CI/CD, A/B testing, CSV,JSON,APIs,logs, EDA preparation, Watson, AWS-connect,lex,lambda,dynamo DB,Maximo,hoc analysis, Cognitive Computing, Cloud computing, Azure, Hive, Hadoop, Tableau, Qlik view, Flask, Django, (BERT,RNN,GRN),sklearn, (CVPR,ACL,PAMI,ICML,ICCV),KNN,heirarchical,conversion to ONNX, SDKS, Bootstrap framework, CRIP-DM, BUsiness Intelligence, confusion matrix,NLP, Deep learning, word vectors, reinforcement learning,underfitting and overfitting, stochastic gradient, computer simulation, IOT, categorical encoding, chi-squared,count vectorisation, dropout batch normalisation, quality control, Excel VBA, Heroku, data exploration, data structure, time series analysis , data analytics, machine learning, models, pipelines , frameworks, domains, computer networks, neural , tensorFlow, PyTorch, shiny,,,,,ETC....

I know I have not begun but still I want to plan for everything ahead. If some one here is kind enough to categorise and label everything in order I should study and even add some more phrases and terms that I should look up wherever you feel necessary , I would be veryyyyy grateful!!!!!

something like;

A) MATHS

1. ALGEBRA: metric,calculus...

2)STATISTICS:

..

B) Data Mining:.........

....

...

X)NPL:............

..

Y) Data Engineering:...

whatever you feel the order should be

PLEASE HELP ME OUT!!!!",MLQuestions,2022-05-29 02:10:43,3
"Woah what's the 12288 for? You don't need to build the network to fit the shape of test\_question. Normally for these kinds models if you're going to be doing something downstream like text classification, you build the network for that task. Right now it looks like your string goes through RobertaModel, then a few linear layers, and there's no loss calculation happening. The model's layers should be fixed to some expected format, so like for text classification you might just care about the \[CLS\] token's embeddings from the last layer from roberta-base, or maybe the last few hidden layers' \[CLS\] embeddings. If you just take the very last layer's \[CLS\] embedding for example, you know you can always expect a 768-wide vector coming out of RobertaModel to be fed into your linear layers.

If you're wanting to send in multiple strings like from a list, my understanding is the proper way to do that is by overriding LightningModule's training\_step method to define the batch, and use batched inputs and attention\_masks. But right now the way your forward method is set up, you're passing in a list of lists for input\_ids, and trying to convert it to a tensor, which you can't do because the two sets of input\_ids happen to be different shape. But even if you could do it I don't think this is right. I could be wrong though.",3,v00hi3,"I have a language model:

    from transformers import RobertaTokenizer
    from transformers import RobertaModel
    import torch.nn as nn
    import torch
    
    checkpoint = 'roberta-base'
    
    test_question = ['this is a string', 'this is another string but longer']
    
    tokenizer = RobertaTokenizer.from_pretrained(checkpoint)

I'm trying to change the head of the model to have 4 linear layers with 512 neurons each:

    class QModel(nn.Module):
        def __init__(self):
            super(QModel, self).__init__()
            
            self.base_model = RobertaModel.from_pretrained(checkpoint)
            self.dropout = nn.Dropout(0.5)
            self.linear1 = nn.Linear(12288, 512)
            self.linear2 = nn.Linear(512, 512)
            self.linear3 = nn.Linear(512, 512)
            self.linear4 = nn.Linear(512, 512)
            
        def forward(self, x):
            input_ids, attn_mask = torch.tensor(x['input_ids']), torch.tensor(x['attention_mask'])
            outputs = self.base_model(input_ids, attention_mask=attn_mask)
            # new head
            outputs = self.dropout(outputs[0])
            outputs = outputs.view(-1, 12288)
            outputs = self.linear1(outputs)
            outputs = self.dropout(outputs)
            outputs = self.linear2(outputs)
            outputs = self.dropout(outputs)
            outputs = self.linear3(outputs)
            outputs = self.dropout(outputs)
            outputs = self.linear4(outputs)                            
            return outputs
    
    model = QModel()
    
    model(tokenizer(test_question, padding=True))

But if I change the input size:

    test_question = ['this is a string', 'this is another string but longer', 'another input']

I get the error:

    RuntimeError: shape '[-1, 12288]' is invalid for input of size 18432

I understand that it arises from the 12288 value in linear1, but I'm not sure how to flatten it in the appropriate way to accept multiple inputs",MLQuestions,2022-05-28 18:06:52,6
"At this point, a “neural network” is almost any empirical highly parameterized model trainable by stochastic optimization.",1,uzndqu,"Hi, I have 15 years of experience in general programming and have been following the field of deep learning for a while, wanting to get into ML research. I get the impression that neural networks, at their core, might not be the best non-symbolic algorithm to use. There were enormous advances in this field, but even the best e.g. transformer models are not very data-efficient, have leaky memory and limited reasoning capabilities. Maybe they just need more data to train their ""common sense"", and everything else would follow. But (IMO) more likely some new general, non-symbolic, neural networks-like algorithms could be discovered, with better data efficiency and accuracy. Or completely different variants of neural nets at least, that have never been tried before. I wonder if any research institutes, universities, or companies are doing research in this area? I find this particular topic very hard to google, or maybe I just don't know the best terminology / keywords to use.",MLQuestions,2022-05-28 06:30:03,2
"Upload the csv to Google drive and mount gdrive on colab.

Split your csv into multiple and run it. Can use multithreading if you are just using the request API and some heuristics.",3,uzseen,"I am writing a ML code using python, and I'm still in the beginning phase of the project. It's about classifying URLs according to their content, so I need to be able to read the content of the URLs first.

I have a .csv file that contains over 26K URLs ([image of file in excel](https://imgur.com/nmC4jc7)) that I want to read, I am using this code ([image of code](https://imgur.com/69RR1uK)) running on Google Colab.

I'm facing some problems running it, the code is slow, the running code shows that it's completed even though the loop is not done ([Image](https://imgur.com/jf9B4bI)). Also, uploading the .csv file to Colab takes a long time, I think it might be related to internet speed (it's not so fast).

Does anyone know the possible cause of what's happening? And what is the best way to read the contents of these links? Any help would be appreciated.",MLQuestions,2022-05-28 10:46:31,1
"You say ""I want to train it myself"", which is  fine.   But you also say you want to use your own dataset.   While that is of course fine too, just note that you are mixing two nearly independent things.   ""I want to write all my own code to create and train the model"" is orthogonal to ""I want to generate all my own data"".

I would suggest at least trying the MNIST example so that some concepts become clear and then move on to something from scratch.   As another person said, this should take an hour and it's an hour well spent.",13,uzk93e,"The pixels will either be black (drawn) or white to simplify everything.  

I do not want to use an existing dataset such as MNIST, I want to train it myself (ie tell it that a 1 is a 1, a 2 is a 2 etc)

I would be using Python.  

For background I’m an Electrical Engineer (solid maths background) and an advanced Python user of 3 years.

What kind of Neural Network is best for this?

What kind of success rates are realistic to achieve ?",MLQuestions,2022-05-28 03:03:56,13
You can use image augmentation and add them to your data.,4,uzlg7d,"In other words, how can you make a NN robust to orientation changes?  

If I drew a number 1 upside down, or sideways, how could you train a neural network to still interpret that digit as the number 1?",MLQuestions,2022-05-28 04:33:19,4
"In the context of ""categories of data"", technically these are time series in that they have a time component, but no. ""Time series"" in a 'data' context really means ""forecasting"". The relevant subfield you're looking for here is called **""signal processing""**, and can be thought of as anything you'd want to throw a fourier transform at. This isn't to say that fourier transforms aren't useful for ""time series analysis"", but you don't generally take the ARIMA of a speech soundfile or talk about the octaves present in financial data.",3,uz9koa,"Hi everyone!

I'm trying to find the answer to my question but I can't really seem to find a direct answer to this question:

Generally speaking, when we have audio/sound data, can we classify them as time-series data?  Or are they their own ""category of data""?

I read that they would only classify as time-series when they possess a temporal dimension. But then I read some papers that generally see them as something separate even with temporal information. I'm especially confused about the tasks: when we talk about Sound classification, could we also talk about time series classification when the data possesses temporal information?",MLQuestions,2022-05-27 15:32:01,5
"Future proof? Be a software developer first, ML/AI specialist second.",10,uyw6j7,"What Basics skills should be cultivated for developing a career in AI field ?

What are the strategies for developing a future proof career in AI  ?",MLQuestions,2022-05-27 04:30:27,4
"show your classifier smaller contexts, so a single item becomes more training examples. this will reduce the probability that the canary is a subsequence of a particular view on the training data. 

More broadly: if you are classifying each document individually: segment the document into sentences and try to classify each sentence independently of it's neighbors, or classify a short context window like three sentences. 

You could also try thowing text augmentations at this. Randomly mask some percentage of words, like dropout applied to the input. use back translation or a summarizer or whatever to replace the actual input with a feasible and respectful rephrasing.",2,uz8kkl,"In a toy example imagine I have 20 classes and am fine-tuning a pre-trained transformer model like BERT / RoBERTa / XLNet with a classification head on top to solve the classification problem. The texts are from a variety of subjects, e.g. ""sports,"" ""politics,"" ""technology,"" but there are ""canaries"" artificially added to the documents. For example, the text ""SPORTS"" is randomly inserted somewhere in the string of all ""sports"" documents, and same for ""POLITICS"" / ""politics,"" and so-on. The trick is, I don't want the model to memorize the canaries, i.e. I'm not necessarily looking for 100% accuracy on the validation data. I want it to generalize to instances where the canaries aren't present, e.g. a document talking about ""football"" should be ""sports"" regardless of whether its canary is present. 

At present I am trying <mask>ing the canaries, since I know what they are. The model doesn't seem bothered by that and can still basically memorize what they are. Next I tried <mask>ing additional tokens in each sequence at random every epoch. That helps a little bit, but still, not great. To me this doesn't seem to be an issue of ""overfitting"" because memorizing the canaries will reduce both training and validation loss; rather, it's more like it's finding an ""adverse path"" to solving the problem, when what I'm really trying to do is get it to learn about the context around the canaries. Any advice you may have is greatly appreciated!!",MLQuestions,2022-05-27 14:41:43,3
"I suggest you try some AutoML library for anomaly detection, e.g.: https://github.com/datamllab/tods

AutoMl tools automatically try a large variety different models on the data and select the best one including tuned hyperparameters in the end. 

There is also a good GitHub page with learning material: https://github.com/yzhao062/anomaly-detection-resources",1,uz4snd,"Hello Redditers,

I am a student trying to do some project in ML. I asked my professor to give me some task and he gave me a Task related to Unsupervised Learning (I guess my professor wants me to learn with difficult stuff only :))

So basic task is to detect anomaly in unlabel time series data given by a monitoring sensor.

Each sensor gives 4 readings (speed, pressure, temp, volume) at regular time intervals. I have 350 sensors in all.

I have some sensors which are working fine in whole time period of 6 months and some sensors which have developed an error over time.

I have multivariate time series (speed, pressure, temp, volume) as they are dependent on each other and I have to detect anomaly based on all 4 features.

I applied z-score normalisation so that they are all become scaled down. Now I am stuck what to do. I tried SUOD library to get a trial run but its very vague and I have even no idea if I am applying library correctly.

I am looking for inputs what I can do and Is it even possible to achieve this task in one month.

&#x200B;

I am only good at coding but lack deep knowledge of ML so I would appreciate any leads or advice.

Looking forward to hearing from you all and I can provide more information if anyone needs it to answer it better.",MLQuestions,2022-05-27 11:39:47,2
"A powerful AI model, such as GPT-2 and 3, could be used for various malicious purposes. Consider deepfakes, which have been used to spread disinformation (e.g. making fake videos of a politician saying covid is caused by bananas), as well as slander (e.g. making fake porn videos starring someone you want to shame). To prevent this misuse, they only give the AI model to people they trust.

Now, the GPT models are language/NLP/text models, not videos. But these models are proving to be much closer to artificial *general* intelligence than specialized ML models like AlphaGo. And we've seen how powerfully these kinds of models can be hooked up to others to produce images, videos, and more. Consider DALL-E!

In addition to someone using these models to create things, it can also be used by governments to analyze data against civilians. Imagine a government inspecting every text message people send to try to identify people whose religion or sexual orientation is persecuted, or who might have misgivings about the government. Mass public surveillance and data processing at scale are a dangerous combination.

As for the reason someone can't train their own GPT-2 model, it's because GPT-2 took a LOT of computational resources, a LOT of training data, and a LOT of expertise to make. Most people can't do that on their own.

Personally, though, I think the real reason they don't want to release the model is that they probably have taken a lot of shortcuts, and the model isn't nearly as powerful as we're led to believe. It's good, but it may not generalize as much as we'd hope.",10,uxz929,"Specifically they said in [""Better Language Models
and Their Implications""](https://openai.com/blog/better-language-models/#sample1):

>""Due to our concerns about malicious applications of the technology, we are not releasing the trained model.""

What kind of danger were they concerned about?  Given the age of the code, were those concerns valid and are they still?  Since GPT-3 is purported to be much more capable, is it therefore ""more"" dangerous?

Could someone train their own GPT-2 model and replicate the results in 2019 that gave them concerns?",MLQuestions,2022-05-25 21:13:22,12
"Might be how you market yourself on your CV. First job is hardest to land. I got a job with only a simple ML cert. What got me the job is how I sold myself.  I was over 35 years old, no tech exprrience whatsoever.  I focused on a few things. 


Showcase ML skills on github. Since no exp. I downloaded a few take home tests from big tech companies. Posted my solutions on github. This really got me the job.

Sold my AWS cloud knowledge. Deploy an end to end.deep learming solution to production. 

Sold my math skills by explaining how I leveraged my newly learned stats and probability knowledge and applied it to my then sales job to give my customers an extra edge with predictive analytics.

Sold my soft skills. Emphasized on being a team player, self managing, self starting, able to speak tech and switch to speak business Explain all with examples

If you failed the take home, its a sign that you need more practise. Lotsa free courses to beat tech interviews

All the best buddy",3,uyhj4v,"So, I'll be graduating in about a month from now, even less, and I need an advice from you guys.
Throughout my degree I've come to enjoy and love the ML and DS  fields even though most of my friends hated it I liked it than the traditional programming. 

Most  of my courses were about those fileds ( deep learning included) so I have a pretty strong foundation on those topics compared to for instance Full Stack related languages and methodologies.
I've been looking for DS and ML job positions that I can apply only to be ignored and not have a chance at all.
I know most of the companies would like Msc which I'm not considering at the moment, Becuase I'd like more job experience after the academy years. 
As of today I'm trying to interview to every possible position related to coding( qa automation, full stack etc..)
I feel like that getting into ML job is impossible rn so I'm shooting everywhere, which it's sucks as I really don't know a lot of topics that in web programming etc..


What parallel jobs I might be able to apply?
I've almost made it to be a ML tech support, a job which involves understanding deep learning pipelines and debug customer code. I really wanted this job to evolve and learn.
Unfortunately I didn't pass the home assignment.

What would you guys suggest me do?
I can share my Resume if someone interested in private :) 

Thanks 🙏",MLQuestions,2022-05-26 14:06:08,5
"* keras, tensorflow, and jax are primarily maintained by google employees
* pytorch is primarily maintained by facebook employees
* huggingface is a vc funded startup
* pytorch-lightning is a vc funded startup (grid.ai)

volunteers can contribute, but most of the core of the deep learning tooling ecosystem is controlled by private interests.",26,uxxh10,"I'm wondering, do libraries (Keras, Huggingface, etc) make money to hire people, or do they all run by volunteers?",MLQuestions,2022-05-25 19:31:03,9
ML can help with identifying genetic factors for disease by comparing these factors to people with similar medical issues.  AI can help identify genetic disorders and thus help with treatment. Basic answer here but doing some research on Google is probably your best bet haha,1,uy8olq,What AI and ML can do regarding genetic data analysis sequencing and modeling provide for research of genetic based treatments and therapies?,MLQuestions,2022-05-26 07:19:16,2
"I'm not sure what you mean by confidence score. Can't the output of the softmax be seen as ""confidence""? Since they are all out of 1, you can look at it as a percentage. 

Regarding the ""none"" class, I'd add another node (ie 11 in total) and then probably do something similar to what you're suggesting with the noise",2,uy4wo6,"
I’ve trained my first classifier on sequential data. It performs well enough.

The last layer in the network is a 10 dense node layer with softmax. So the the network always picks one on of the classes, in the sense that it normalized the output to a total of 1.

But that’s not quite what I would like. I would like it to also have some sort of confidence score.

That or 1 of the classes should be ‘none’ in the sense that the data doesn’t fit any of the data from the classes.

How would I go about that? Do I create a bunch of noise data and train the model to classify they too?

Thanks.",MLQuestions,2022-05-26 03:54:22,5
" I found what I was looking for:  
oliverguhr/german-sentiment-bert is the most downloaded model for german sentiment analysis on hugging face.

[hugging face page about the model](https://huggingface.co/oliverguhr/german-sentiment-bert)

[paper about the model](http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.202.pdf)",2,uy2n97,"I’m very excited about a project I’ll do at uni about sentiment analysis of german twitter posts. I started to do some research and i couldn’t find a good answer to the following question:

would you rather translate german tweets to english and feed it to a ML model (RoBERTa) or is ther a model for the german language?",MLQuestions,2022-05-26 01:07:57,1
Check Kaggle or here https://paperswithcode.com/datasets,1,uxwcdy,"Hi, do you know of any data set for a multi-label classification task with binary features?",MLQuestions,2022-05-25 18:29:40,2
"Yes I would think that would be fine. A classic example for VAEs is face images which should have pretty distinct differences between men and women, which obviously have some overlap but are somewhat distinct. Another example is images of furniture which should be even more distinct.",1,uxdpby,"Hello! If the input data distribution is multimodal, is VAE effective for generative modeling? For example, my input data has two distinct Gaussian peaks, but almost no overlap. Can this kind of data be modeled using VAE? (By prior, I meant the original input data distribution)",MLQuestions,2022-05-25 02:40:07,1
Use simpler methods.  No reason to use a CNN to detect lines,6,ux4cwz,"Hey yall, I am trying to detect vertical lines in an image using a CNN. I basically followed this tutorial (https://tech.unifa-e.com/entry/2020/01/30/100451) and I also tried convolving the image with sobel filter and schar filter but I wasn’t able to detect the lines I was only able to detect some edges. Does anyone have a better idea about how I should approach this problem? It’s just a few niche images so I’m not sure if I should train a CNN (neither do I know how to train it to detect vertical lines)

EDIT: I included a link to the image [Here](https://imgur.com/a/uRKkGrm) if it helps and highlighted the lines I’m trying to detect. And it’s a tif image not sure if that makes a difference but felt that I should share.",MLQuestions,2022-05-24 16:47:13,18
"Just get the MacBook. That's what everyone else will have, and it will make troubleshooting faster since that's what everyone else knows.",15,uwsll4,"Hi everyone,

I am due to be starting a PhD in machine learning next year, and have been asked to pick which laptop I would like to purchase. I have been given two options.

My work is likely to be in Computer Vision, though due to the structure of my PhD I do not yet need to select a specialism - I'm quite aware this means most of my work will be done on a cluster, and this laptop will therefore mostly be used for prototyping/other work.

I'm also particularly excited about the recent work of PyTorch (which I primarily use) for optimising on the M1 macbooks. As such, I feel that is the way I am leaning.

The options:

M1 Pro Macbook Pro (10 core CPU, 16 core GPU) with 32 GB memory, 1TB storage

Dell XPS 15 (i7-12700H) with 32 GB memory and 1TB storage

As I say, I think I'm leaning towards the macbook. Do you think this would be the correct choice for my use case?",MLQuestions,2022-05-24 07:50:59,15
"What do you think big data is, and what do you think machine learning is?",3,uwkwih,"Hi,
I am working as a junior big data engineer and most of my projects are analytics. I want to ask if there is any purpose to use machinelearning in a big data analytics project. Is it worth it? Can you give me some cases of using ml ? 

Thanks in advance.",MLQuestions,2022-05-24 00:03:15,6
Use CTM [https://github.com/MilaNLProc/contextualized-topic-models](https://github.com/MilaNLProc/contextualized-topic-models) with sentiment labels to   built distribution of words over labels,1,uw9epe,"My company has around 60,000 reviews and I want to create a deep learning model that can extract words associated with each ratings (1 - 5). I have made several sentiment analysis models in the past (such as associating a given Amazon review with a rating using embedding layers and some RNNs) but I don't really know where to start if I want to extract or associate given words with a certain rating. For example it would produce words like 'late','back order','wrong' etc with low ratings. Is this possible and if os how to construct the deep learning model.",MLQuestions,2022-05-23 13:21:42,2
"Standardization or normalization is actually typically *not* required in linear regression, as any scaling to the inputs will be reflected in the weights. Where it is needed is in penalized methods like ridge regression. Without normalization/standardization, the coefficients may be on very different scales, and large coefficients may be heavily penalized only due to the scale of the data.

It could also help with numerical stability if the units of your data result in very large or very small numbers. See [this post](https://stats.stackexchange.com/questions/29781/when-conducting-multiple-regression-when-should-you-center-your-predictor-varia) for a more detailed discussion on the matter.",4,uw8d72,"Hi!

I'm aware that you have to preform normalization before preforming linear regression. But my lecturer is very confusing, he mentioned what standardization is after mentioning you should normalize before linear regression. This make's it seem like you can do standardization instead of normalization.

But since talking about standardization, he has never mentioned it again and whenever he talks about linear regression he says ""you must normalize the values between 0 and 1 before linear regression"". 

So my question is, can you standardize the data before preforming linear regression, instead of normalizing it?",MLQuestions,2022-05-23 12:35:34,2
"cp source.mp3 target.mp3


In all seriousness though, there isn't much to go by reading your description.",2,uw4knp,"Hello!

I have 2 audio files, first the source and second the target.

What I would like to do is to make system that generates the target audio file from the source. 

Ideally, the system should be able to take in any length of audio files and produce a same length target file.

Any pointers greatly appreciated!",MLQuestions,2022-05-23 09:46:43,4
"As a rule, Please don’t use R^2 to determine the strength of your predictor/evaluate your fit. Its an awful way to do so.",5,uvwx4a," 

I'm trying to create a tool for student performance prediction, but when I'm evaluating my model, I'm getting NaN for

    private RegressionMetrics EvaluateModel(IDataView data) {
          var transformSet = GetModel().Transform(data);
          var metrics = _context.Regression.Evaluate(transformSet); 
         return metrics; 
    }
      
    var metrics = EvaluateModel(dataView); 

I'm getting values for MeanAbsoluteError or LossFunction, everything else but RSquared, any idea why?",MLQuestions,2022-05-23 03:13:02,6
are you talking about normalizing flows? dataflow programming? optical flow estimation? physics-informed ML? it might help if you link us to an example of the sort of thing you are looking for an explanation for.,2,uw07zy,"I couldn't find any explanation about flow-based machine learning on the internet. if it's possible could you explain it to me?

a simple explanation is enough

thanks.",MLQuestions,2022-05-23 06:28:48,13
"You can try using classification as a first stage.
First stage determines if object is in top left, bottom right, or neither. 
Then pass this classification as an input to your object detection network. Add the classification loss to your object detection loss.",7,uvj9jb,"Given a set of images, I know that the object of interest is very unlikely (though not impossible) to be on the top left corner or bottom right corner of the images. Is there a way to incorporate this prior into a CNN? Current trained models seem to like to focus on these regions incorrectly.",MLQuestions,2022-05-22 13:19:39,5
"Firstly, if you care about the value it’s no longer classification only.  yes, that’s important as obviously zero vs non zero makes a difference.  make a classifier for zero or non zero, and a regressor for the log of nonzero value.

Can be a multi output model.  First is zero/nonzero binary.  The second is regession value.  Only put loss on second value when first is nonzero.

What is the physical meaning of the feature?  Is zero actually a finite time rounded to zero?  Or a different situation altogether?",2,uvi759,"Hi all

This should be a really simple question but I'm struggling to find a clear answer:

I'm building a supervised binary classification algorithm.

I have a dataset in which one variable is extremely skewed. It's an amount of time in minutes, and for about half of the instances the number is 0, most of the rest are very small, then there are a few of outliers stretching all the way up to 1,000. Do I need to transform this data in order to use the variable in my algorithm, and if so how? Is the answer different depending on the algorithm (e.g. Decision Tree, SVM, etc.)?

Searching for an answer most suggestions are to do something like a log transform, but this doesn't make sense when most of my values are 0. As things stand all that occurs to me is transforming it into a binary variable 0 or >0, but I'm not sure whether that is necessary/helpful.

Thanks.",MLQuestions,2022-05-22 12:28:06,6
"weighted cross entropy? assign lower weights to unique words 

most people just use temperature in their softmax to influence generation tho",2,uv2g5t,"I  have an autoregressive language model that generates words. I'm  trying  to minimize the number of unique words generated, and the only  thing I  could think of is having either python's set() operation or  torch.unique as part of the loss, to penalize for a large number of  unique words.  But both seem to be non differentiable. The error I got  from using torch.unique is

RuntimeError: the derivative for '\_unique2' is not implemented

I found [this](https://discuss.pytorch.org/t/torch-unique-derivative-not-implemented/118467) link which mentioned a similar problem, and that there is a similar tensorflow  
unique  operation that is differentiable. I'm wondering if I'm doing something   wrong or if there's a better approach to penalize for unique words",MLQuestions,2022-05-21 20:18:22,3
"Yes, that's the whole point of SHAP. Check out the force_plot(), it's right [there in the docs](https://shap.readthedocs.io/en/latest/example_notebooks/api_examples/plots/decision_plot.html).",1,uv1q0a,"Trying to learn about Model Explainability and SHAP values.  

First, I understand SHAP values are calculated my measuring marginal contribution of each feature, by  training multiple models on multiple combinations of features/data.

&#x200B;

Then comes the question.  You can output items like Feature Importance/Shap values for a trained model.  But, can you also output these for single samples?  ie. When making new predictions, checking which features have the highest influence for the new sample?",MLQuestions,2022-05-21 19:31:55,3
"You can find Colabs  for all of these. Pix2pixHD for example: https://colab.research.google.com/github/dvschultz/ai/blob/master/NFP_Pix2PixHD.ipynb

Time to learn some python.",1,uv3fpi,"Hey, thanks in advance!  


I stumbled these papers/articles where researchers made paintings mimic real photos. Got me thinking I could do the same with my renders. [https://webneel.com/painting-to-photo-app](https://webneel.com/painting-to-photo-app) [https://arxiv.org/pdf/1703.07511.pdf](https://arxiv.org/pdf/1703.07511.pdf)   


I looked around but I can't find any programs for Windows or Linux that are self explanatory to set up. UNIT, CycleGan, pix2pix, and pix2pixHD don't seem to have user friendly guides.",MLQuestions,2022-05-21 21:24:03,2
">a graph is created every time forward() is called

yep, your understanding is correct.

&#x200B;

>how much should I be concerned about the potential impact on training time?

you have to make your hands dirty to try it out.",1,uv2z7k,"As I understand, in Tensorflow, even in Tensorflow 2 where eager mode is the default, one can use static computation graphs but in PyTorch, dynamic computation graphs are the only way.

If I understand dynamic graphs correctly, a graph is created every time `forward()` is called. Is my understanding correct?

If so, it seems that the repeated dynamic graph creation will cause some additional CPU time consumption. In the case of training a large, non-toy, network using a fairly powerful GPU (e.g. 3080), how much should I be concerned about the potential impact on training time?

I'm wondering whether dynamic graph creation is a good reason to choose TF over PyTorch.",MLQuestions,2022-05-21 20:53:50,1
[deleted],1,uuxwie,"I  have an autoregressive language model that generates words. I'm trying  to minimize the number of unique words generated, and the only thing I  could think of is having the ""set"" operation as part of the loss, to penalize for a large number of unique words. But I'm not sure that it's  differentiable, nor if there's a better approach",MLQuestions,2022-05-21 15:46:26,2
Do you know how Shazam does what it does?,2,uuo7ls,"Suppose I have \~1000 custom (not available on the internet) music files. I am looking to identify the music file based on a short sample—similar to Shazam, but with custom music files.

I am unsure if this is an ML problem, but I am also unsure where else to find answers.",MLQuestions,2022-05-21 07:25:32,2
"I tried applying a variety of augmentations to audio data once with the same results.  Admittedly, this was not all tested on esc-50 so results may vary.

First, check the actual outputs and see if they are recognizable.  
If they are and the results don’t Improve, it may be that there is just too much variance even within classes.  Have you within class samples?",1,uuktpy,"I'm working on augmenting the esc-50 dataset using audiomentations , I split the data and I augmented the training set , but the accuracy got worse .

here are the augmentations I used :

    augment = Compose([AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=1),   
     PitchShift(min_semitones=-4, max_semitones=4, p=1),   
     Shift(min_fraction=-0.5, max_fraction=0.5, p=1),   
     FrequencyMask(min_frequency_band=0.2, max_frequency_band=0.6,p=1)])

I thought I would try applying this to some of my audio classes and see if they sound recognizable and then I would train ,Is this the wrong approach , should I try training with each augmentation on its own before chaining them ?",MLQuestions,2022-05-21 04:03:39,5
"If your goal is to generalize conclusions beyond the exact set of data that you're working with, then it's always smart to make inferences from unseen test data rather than training data.",1,uukqzk,"Hi,

When using SHAP to analyse the individual features, would you guys then evaluate it from the SHAP values generated on the training data or the test data?",MLQuestions,2022-05-21 03:58:31,1
Found it! It's this one: https://www.synthesia.io/,1,uuau4i," 

Hi guys, I found a video where this avatar was talking and it was surprisingly good. ([https://milanblack.harmonelo.video/?lang=en](https://milanblack.harmonelo.video/?lang=en))

Does anybody know which NN / service was used to create it?

&#x200B;

https://preview.redd.it/596h1wo5wp091.png?width=277&format=png&auto=webp&s=03e47af1a5c3fea1b4cb5816e0fa1b92798405a9",MLQuestions,2022-05-20 16:43:14,1
"You can use it and it'll work, but you're basically throwing out the advantages of population optimization (being able to evaluate in parallel) while taking on the disadvantages of population optimization (each evaluation is not optimally sample-efficient). It might still be better than random search since that is a baseline, but you're probably better off using a hyper-parameter optimization tool such as Bayesian optimization.",5,utzs5r,"I'm referring to this technique:

https://www.deepmind.com/blog/population-based-training-of-neural-networks

The blog post talks about utilizing multiple GPU's to train models in parallel to find good hyper parameters. Well, if I only have one GPU, does that mean this technique won't work? Or...be any better than random search, or bayesian optimization, or something? (Not super familiar with all the hyper parameter optimization techniques.)",MLQuestions,2022-05-20 08:39:53,2
"Microsoft's offerings are garbage outside of VSCode. Sometimes I'm shocked that Microsoft made VSCode, it's so user friendly, accessible and functional. 

From Windows to their Office Suite/Power BI to Azure it all sucks with a garbage UI.",5,uu2pts,"The background blurring on Microsoft Teams is pretty bad. And compared to Zoom's implementation it's terrible. 

Is the reason for this known? My best guess is that Zoom is using some (dubiously) patented method that leaves Microsoft with less good ways of doing it. But that's just a wild guess.",MLQuestions,2022-05-20 10:18:00,5
"I might be reading this wrong but kinda looks like this? https://en.wikipedia.org/wiki/Hungarian_algorithm

(EDIT: btw if above is what you want there's a scipy function for it I believe)

(EDIT 2: just looked it up: `scipy.optimize.linear_sum_assignment`)

Also this isn't an ML question. Maybe try /r/algorithms or something.",4,uty492,"  

 Hello, I'm trying to solve a certain problem here. In the table below,  there are headings from 1 - 11 and rows from 1 - 16.  I would like to  get a combination for each row and column such that  their grand total  is the highest. For instance, I could choose the  following combination  (1, 1), (2, 2), (3, 3), which means that I would  get my first value in  row 1, column 1. That value is 0.788297. I would  then get my second  value form row 2 column 2. The value in this case is  0.926258. The  third value is 0.510444. The aim is to maximize their sum.  If (1, 1),  (2, 2), (3, 3), ... are my choices, them my grand sum will  be 0.788297 +  0.92625 + 0.510444 +.... Obviously, there's a lot of  reasons to  suspect that the above combination may not result in the  highest  possible sum. So I would like suggestions on the best algorithm  to  arrive at the combinations to choose. I've tried a few including simulated annealing, but I'm not very satisfied with the speed.

https://preview.redd.it/9lnw4vbttm091.png?width=740&format=png&auto=webp&s=a322d6e81dab2234b28ebb57ccdb490680436c86",MLQuestions,2022-05-20 07:43:42,5
"Here is a github project doing that...

https://github.com/abhijitmajumdar/Mouse_tracking_predictor",2,utljo3,"I remember a few years ago running across a web site containing a demo that shows a dot following a user's mouse cursor. At first it lags behind the cursor but as it learns the user's movement, the dot gets closer and closer to the cursor and eventually sticks to the cursor throughout its movement. Does anyone know this website or where I can find a similar project? I thought it was pretty cool, and it taught how to build the application through tensorflow i believe.",MLQuestions,2022-05-19 21:42:45,3
"What I did was, uploaded my models on GitHub using git-lfs then from there i accessed and deployed my application on streamlit,
This was you can upload models having size around 1.5 gb",3,uttk0u," 

I tried to deploy a ML FastAPI Model on Heroku but the app size was 600mb after compressed so as a result the push failed, so I am looking for any free Heroku alternative that supports at least a max deployment size of 1GB.

Thank you",MLQuestions,2022-05-20 05:02:49,3
"It’s a bit unrealistic I’m afraid. AI “achievements” is fairly narrow and curated. 

I’m a fund manager, and like you, I was impressed by AI. Since ML has been making inroads into investing, I dived in. Spent the last few years programming ML. 

Here’s the catch. 

ML doesn’t exist alone. Once you step out of the carefully curated tutorials, getting and cleaning data is very painful. Also you need to do data pipeline, database. Deploying is a whole set of problems. 

The “stuff that works”, is carefully curated. Once you try to apply to your own domain, your fighter jet suddenly turns into paper plane. 

If you don’t want to pick a pre-trained model, and you want to roll your own, you need high level maths. Make sure you love calculus and linear algebra. 

I applaud you for willing to try but I do hope you are aware of the difficulties ahead",24,ut36w2,"I’ve been fascinated by AI from an epistemological and ethical standpoint since I read a paper on them in 2018.

I recently started learning Python for fun, and I quickly realized how much I love it, even though it is so left-field from anything I’ve ever really done.

I have a professional background in media production, copywriting and bartending. Currently an administrative assistant.

How reasonable is this pivot? Would I need to go back to school first or could I dip my toes in a more entry-level position and work up from there with experience + self-learning?

I’m not worried about being behind, just ready to get started.

Edit: Thank you all so much for the honest and thoughtful responses so far!

I like math a lot. I deleted the part about Neuro-Linguistic Programming since it’s not professionally relevant 😂",MLQuestions,2022-05-19 06:14:56,56
"If you haven't consulted, go work 1-2 years at a deloitte, McKinsey or DS firm like sfl scientific, mosaic, etc.

I've had my own DS consultancy and consulted (while under a larger company) for the majority of f500 companies. The reality is there are a lot more gaps/problems than building a model or recommending tooling. For consulting (at this point in company's AI Maturity) they need organizational change management, proper data architecture and pipelines, understanding of use cases, MLOps, productionized models, roi assessments, etc. And NLP is so niche that unless a company is using say an AI CSR bot, then your respective potential audience is very small.",1,utjhkk,"I have NLP expertise and want to found a startup, but I haven't reached conviction for any of my ideas. I'd like to spend some time talking to businesses and observing patterns in how they're using NLP tools.

Do y'all think it would be a good idea to start an ML consultancy first, in order to increase my exposure to real problems? I am slightly worried that it can turn into a time sink / opportunity cost.",MLQuestions,2022-05-19 19:39:51,2
"More train documents, and more diverse ones vs your current corpus.  Statistical topics are recognized by co-occurrences of related words in a document and their contrast vs others.

The grab bag words don’t have tight enough binding to one another in the train set to form compact topics.

Humans know what the words mean semantically but statistical topic models don’t. You could apply any random permutation between word index and word text, and the topic model is invariant, but the human semantic information is garbage.",1,ut4tg5,Any advice is greatly appreciated!,MLQuestions,2022-05-19 07:35:54,1
"In short: no. No one has (yet) found an unsupervised way to tell synonyms from just semantically similar words. Same between synonyms and antonyms, as they appear in the same contexts. 

For synonyms vs related I speculate that there might be some non-trivial property of their embeddings that could potentially be used to tell them apart.

For synonyms vs antonyms, there might be one or more dimensions in their embeddings that represent extent or similar concept, and if that was the case, you'd expect to find them at opposite ends of its distribution",1,ut7sv0,"Comparing the coverage and quality of the WordNet synonyms with the commercial options (eg. Oxford dictionary) and finding the former lacking I got to thinking - is there a good way to derive synonyms and antonyms from the more advanced word embeddings and language models? I played around with this with word2vec a while back with mixed results, but I'm guessing there are smarter people than I who have given it thought. Any pointers or ideas would be appreciated.",MLQuestions,2022-05-19 09:54:58,5
"X be data, Y be label, and z = Pr(Y | X) the conditional probability of getting the correct answer given data, aka the likelihood of getting the correct ans

then:

Pr(Y|X) = z = z\^(y)\*(1-z)\*(1-y)

therefore:

log(Pr(Y | X)) = log(z) = y\*log(z) + (1-y)log(1-z)",2,uswtm1,"I've summarized my question [here](https://stats.stackexchange.com/questions/575697/how-can-i-get-the-binary-cross-entropy-from-the-cross-entropy-function-for-gans) and [here](https://www.reddit.com/r/learnmachinelearning/comments/urd3t8/help_with_relating_maximum_likelihood_to_binary/) but I will reproduce my question but doing the question again, in Goodfellow Deep Learning book there is a relation to cross-entropy and log-likelihood it says that minimizing the log-likelihood of a probability model distribution estimated by some parameter theta:

&#x200B;

https://preview.redd.it/mc52m5zehd091.png?width=445&format=png&auto=webp&s=8f72048f94119892e886350a5be7264060bb2967

 and then dividing the log-likelihood by m we get the same as the negative of expected value of this model with respect to the empirical data of the distribution we are estimating: 

&#x200B;

https://preview.redd.it/xijpcr0mhd091.png?width=682&format=png&auto=webp&s=a770a29db99569a3cd2795494e85c96725fcefd6

Which is the same as cross-entropy function: 

&#x200B;

&#x200B;

https://preview.redd.it/y3ulwdkuhd091.png?width=429&format=png&auto=webp&s=c7410c259ee5a0ec12ccfd5694beed0c5facba9e

Well, I saw in one of the links I listed above, that the step where I get the expected value with respect to the distribution is in reality an aproximation using the Monte Carlo method which I don't know how to get this aproximation. And another problem is that the definition of empirical distribution is something like:

&#x200B;

https://preview.redd.it/9zrq9nugid091.png?width=516&format=png&auto=webp&s=8bb1dd2711c93742182d1c8d0d740a001728a4f2

Which is the empirical distribution to the distribution p data that I don't know but have some samples in the empirical distribution. The formula of Binary Cross Entropy is something like 

&#x200B;

https://preview.redd.it/giv0p3wuid091.png?width=548&format=png&auto=webp&s=32135f0a191214cd2cf6afbcd7cec87877f1bc51

Which makes me think that I need to change the definition of dirac delta to something like delta = yi or delta = 1-yi. But I don't know if it makes sense... So... Can someone help me with my cofusion?",MLQuestions,2022-05-18 23:07:38,14
"Since your model is small, why not pick a few settings in increasing size and see if a pattern emerges?

Have you simply tried 10-fold  cross-validation to see if you are fitting to a particular data distribution?",1,usvzck,"Hello, i am using Machine learning to conduct side channel attacks (for hardware security). The article that i am basing the attacks on is (https://tches.iacr.org/index.php/TCHES/article/view/7332). In summary, we use a simple MLP, with a single output, where the loss function is defined as Pearson's coefficient of correlation between  the labels and model's output. 
I was able to reproduce the results of the attack on the data that was provided online ( since everything was detailed). But i am having trouble recreating the attack, in general, when the device is unprotected, the attack goes smoothly. I just face trouble when attacking protected implementation. And i have a few questions.
1- does it makes sense to use dropout with an MLP that has less than 10 layers?
2- is there a recommended learning rate ( 1e-5 is what i choose, i change it depending on how fast/slow the model is learning or if it is learning at all)? Should I for example choose a learning rate, start training, change model's architecture and wait till the model performs well? Or it should be changed during the training?
3-increasing the training set size : i feel like i am always suffering from over fitting, so, do you recommend immediately increasing training set size, or again wait a little more and play with other params.
4-how do I really distinguish overfitting? Loss converges towards zeros, score on training data is extremely high, score on new data very bad, i say overfitting, but my supervisor isn't very convinced.
5-i saw a parameter research method where you fix a base architecture, choose a training par am (like learning rate), train on different learning rates, choose best performing param, move on to next param, and so on and so forth, is that recommended?
Thank you",MLQuestions,2022-05-18 22:11:55,11
"an attempt to answer my own questions...  
Looks like ""Population coding"" was what I was looking for.  
There are a bunch of papers when I search for that.   


But what it click for me was...

> if we pool together multiple neurons and count their spikes  
together, then it becomes possible to measure a firing rate for a  
population of neurons in a very short window of time.  
[https://snntorch.readthedocs.io/en/latest/tutorials/tutorial\_pop.html](https://snntorch.readthedocs.io/en/latest/tutorials/tutorial_pop.html)  


So these small population of neurons can act as a single entity, just like in the XOR. I think..  
Clearly I'm still learning... 

These papers/sites helped me too.  
[https://arxiv.org/pdf/2109.12894.pdf](https://arxiv.org/pdf/2109.12894.pdf)  
https://lcnwww.epfl.ch/gerstner/SPNM/node44.html",2,uslgg2,"I was reading a paper and saw they did XOR configuration of 2 inputs, 2 hidden, and 1 output...  
But each of those were actually hundreds of neurons. When I dove a bit deeper, Brian and Auryn simulators group the neurons together.   


So I guess the question is why? Is there a paper describing the benefits?  
Are they fully connected? (I can probably look at source code to see)

&#x200B;

Thanks!",MLQuestions,2022-05-18 13:03:37,3
"the model is generally stored in two chunks: a file containing the actual values of the models parameters (i.e. just a file with a bunch of numbers), and one or more files that describe the topology of the computation graph. That latter file just needs to know how to put the right values from the weights file into the right locations in computation graph. The weights component is (often) language agnostic, the topology component often needs to be described using the langauge. There are language agnostic file formats for describing the computation graph (e.g. ONNX).",2,usqiby,,MLQuestions,2022-05-18 17:06:31,12
Yeah markets change,1,usp3eq,"I'm trying to finetune *LayoutLM V3* [Base model](https://huggingface.co/microsoft/layoutlmv3-base) using the provided `dit/train_net.pyscript` on my own custom dataset that is similar to *PubLayNet*. The learning starts well but after reaching the first checkpoint (2000 iterations) and doing the first evaluation the loss starts going up and the accuracy keeps going down.

Anyone have ideas of factors that may produce this behavior ?

[Total Loss](https://preview.redd.it/29f3as5pdb091.png?width=1201&format=png&auto=webp&s=f9086cf6ed7c435b4ee65acbaa3790983c90da72)

[Bbox Average Precision](https://preview.redd.it/7v292n9qdb091.png?width=1198&format=png&auto=webp&s=e65b47a1adeff5c8d167adfd3554fbe15be82872)",MLQuestions,2022-05-18 15:54:48,5
Depends on why you want to do feature selection in the first place. What is the motivation?,3,usew6x,"Hi,

Do you guys have any recommendations when it comes to using SHAP values to make feature selection? Is it a bad idea or is there some kind of ""rule-of-thumb"" for when a feature should be removed when using SHAP?",MLQuestions,2022-05-18 08:00:07,10
"1. Are you normalizing your inputs?
2. Perhaps this is related to the weight initialization algorithm/distribution?",1,us9f5j,"I have been working with neural  Networks for a few months now and I  came across a little mystery I can't solve on my  own. I wanted to create and train a neural Network which can identify  simple  geometric shapes (squares, circles, triangles) in 56\*56 pixel,  greyscale  images. if I use images with a black background and a white  shape,  everythings work pretty well. Trainingtime is about 18 Epochs  and  accuracy pretty close to 100% (usually 99.6 % - 99.8%). But all  that  changes when I invert the images (aka now a white background and  black  shapes). Trainingtime skyrockets to somewhere around 600 Epochs  and  during the first 500-550 Epochs nothing really happens. The Loss  barely  decreases in those first 500-550 Epochs and it just seems like  something  is ""stuck"". Does anyone have an idea why the trainingtime  increases so  much and how to reduce it (if possible)? If you need any  code, let me know. I'll post whatever is needed upon  request.

Thanks in advance \^\^",MLQuestions,2022-05-18 03:04:27,3
I combine few shot learning with contrastive learning  to start train my model. But the performance isn't good…,1,us1nk8,,MLQuestions,2022-05-17 18:31:16,1
"1. Could you list the events
2. What do you mean by “weakly label”",2,urqnrk,"As  seen from the title, I severely lack the vocabulary to explain my  problem. Here however is an attempt to explain the issue I'm facing.

I  have an audio dataset with 10 events. It is a multilabel classification  problem. There are certain groups of events which are closely related.

For example, alarm and a doorbell.

I  have made multi-hot vector for the samples which weakly labels whether  an event occurs in the given audio clip. Now many times I have  false-positives that the the alarm class turns on if the doorbell is  present and vice-versa.

These two  events are highly correlated and I would love to hear your ideas, read   papers, ideas or any form of input to better tackle this problem.

Hopefully  I have been able to explain the problem correctly and if need be please  let me know of any more information that you would need. Thanks!",MLQuestions,2022-05-17 09:45:41,2
"I would apply pre processing, train the model, and select the features with SHAP.

Once the features have been selected, train one model with the raw version and one with the pre processed version.

The reason is that pre processing can affect the final weight of each feature, so do it before you look at the SHAP values.",1,uricry,"Hi,

I'm trying to make a classification model with a CNN model. Before training the model I use an XGBoost model to select features by looking at their SHAP values and also removing highly correlated features. 

Then I want to compare a CNN model which uses the selected variables, where the variables have been outlier treated, scaled and balanced, with a CNN with ""raw"" data. 

My question is then: Should I apply these 3 pre-processing steps before the feature selection phase or should I apply it after?",MLQuestions,2022-05-17 02:10:58,3
"I have no idea of what you are trying to do. Do you want to take the features A B C and if this is above 50, you pipe 50 into a ml method?? and it will use these numbers to retrieve values in ABC ?? Or do you want a machine learning method that takes one column of data in and retrives the ABC\_n columns ?

I think you would have to write a lot more in detail of what you want to do. Just say it exactly as is. By example, i have 3 columns with sensor data and a function that checks if its above 50. It takes this information to bla bla bla.

Or even better draw a map\\flow diagram of what you are going to do.",3,urnhic,"Hello,

so I have the following dataset:

https://preview.redd.it/drw96qk9n1091.png?width=2551&format=png&auto=webp&s=5158657bbe6f2ed2f3b4ff72b39039e97e7d8b34

Where the values for each Y is obtained by Y\_i = f\_i (a\_1, b\_1, c\_1) + ... + f\_i (a\_n, b\_n, c\_n) \*\*\* and each simulation has randomly assigned parameters a\_n, b\_n and c\_n.

So in a way the parameters a, b, and c are the inputs to each of the Y outputs.

What I'd like to do is, to train a ML algorithm, that, given a choice if Y, outputs all combinations of 3n outputs ( a\_1, b\_1, c\_1 .... a\_n, b\_n, c\_n ) that can give me a value of Y above a certain treshold.

So for example, if I wanted to find combinations of input parameters a, b and c that will give me Y10 above 50, the ML model will take an input of (Y10, 50) and will return \[\[5, 0, 56\], ..., \[41, 23, -6\]\], \[\[5, -5, 3\], ..., \[0, 0, 0\]\] etc. or something close enough that when input in the equation  \*\*\* would give me a Y\_10 above 50.

I am  not convinced this is possible to do with a ML model but my supervisor insists it is, even though he has no ideas about how to approach it, so I wanted to see what you guys think and whether you have any ideas.",MLQuestions,2022-05-17 07:18:03,5
"What do you consider baby level?

Data mining is pretty basic stuff in the realm of ML, it's kind of like learning to chop vegetables before you learn to cook.

The part that most people struggle with is the regex, but it's a lot easier to learn in a structured environment with some organized exercises and such.",2,urcbsr,"So I'm on baby level in CS in general, but my uni allows me to enroll in the data mining course this semester. Should I wait?",MLQuestions,2022-05-16 19:26:10,1
"From you images it looks like the parameters for the models are different:

R: (2,0,4),(0,1,0)
Python: (2,1,4)

I’m not familiar with the R package at all, however 

Judging by the extra terms in the R model it might also be that in addition to the above, the R model is also a seasonal extension of ARIMA, whereas your python model is just standard ARIMA",2,ur7zfn,"I have a series that looks like 

!\[this\]([https://gyazo.com/8a460fed032c8989b93cf26d8820e431](https://gyazo.com/8a460fed032c8989b93cf26d8820e431)). ([https://gyazo.com/8a460fed032c8989b93cf26d8820e431](https://gyazo.com/8a460fed032c8989b93cf26d8820e431)) 

\[!\[enter image description here\]\[1\]\]\[1\]

&#x200B;

&#x200B;

It shows very strong auto correlation !\[AC\]

([https://gyazo.com/4acd9b9bd32c70509bde1b8b874d6e33](https://gyazo.com/4acd9b9bd32c70509bde1b8b874d6e33)).

\[!\[enter image description here\]\[2\]\]\[2\]

I have differenced it and then it looks like this. !\[differenced\]([https://gyazo.com/ad860eea8eb921c2a5e822aa8c15c655](https://gyazo.com/ad860eea8eb921c2a5e822aa8c15c655)). However, when I build the ARIMA model and predict values it always predicts the same. 

&#x200B;

\[!\[enter image description here\]\[3\]\]\[3\]

\`\`\`

n\_predictions = 30

predictions = \[\]

rolling\_values = train.values

&#x200B;

for n in range(n\_predictions):

arima\_init = ARIMA(rolling\_values, order=(2, 1, 4))

model = arima\_init.fit()

prediction = model.forecast()

np.append(rolling\_values, prediction)

predictions.append(prediction)

\`\`\`

This is the summary of the model: !\[sum\]([https://gyazo.com/f91e7c643c638bd5755987fbe31fb505](https://gyazo.com/f91e7c643c638bd5755987fbe31fb505))

&#x200B;

And an image of the predictions for the 30 days after the series ends. !(end)\[[https://gyazo.com/f714ac90de2fd277f226d9b1083e2f3d](https://gyazo.com/f714ac90de2fd277f226d9b1083e2f3d)\] 

&#x200B;

However, when I do the same in R with the auto arima following the answer from another post here. I get the following:

&#x200B;

!(resultsR)\[[https://gyazo.com/c0a97992c1876048e0bd08e155ca50f6](https://gyazo.com/c0a97992c1876048e0bd08e155ca50f6)\]

&#x200B;

The code to generate this model is:

&#x200B;

\`\`\`R

fit <- auto.arima(ts(series\_ca, frequency=365), D=1)

\`\`\`

&#x200B;

How can I replicate the same with the python ARIMA model? And most importantly, what changes am I making to the model in R to perform so much better and not just predicting the same for every iteration.

&#x200B;

Sorry for maybe little but stupid question but really scratching my head over this! I appreciate any answers and am really eager to learn! Cheers :)",MLQuestions,2022-05-16 15:34:19,1
Andrew Ng,5,ur78vy,,MLQuestions,2022-05-16 14:59:33,5
"If your 17 values are all on a scale, such as the number of children a family has between zero and sixteen, then regression is better because it is more interpretable & will probably perform better",4,ur2sl4,"If I have a large dataset of numeric values that happen to consist of only 17 different values (discrete data so essentially could just be thought of as 17 classes) what is better to use, classification to 17 classes or a regression output? (the output of the regression values would all be rounded to the nearest discrete value). What are the advantages of using regression to predict these numeric values or is it better to use 17 class classification model? And why? Is one more accurate than the other? Is one more prone to overfitting? If my data is imbalanced, and there aren't many of some classes, does this make regression the better choice?",MLQuestions,2022-05-16 11:38:19,7
No,4,uqwshv,"If I am doing k-nearest neighbours algorithms, should I include the point itself on each iteration? (e.g. point itself + (k-1) nearest points?

(and what is the proper way to deal with equidistant points if say two are equidistant and this is k-1 without or k+1 with)",MLQuestions,2022-05-16 07:06:55,3
because they can potentially share information in their internal representations.,5,ur1eww,"I want to know what in terms of accuracy and inference time the difference between these two approaches are, and why (like parameter count/model complexity etc). I have been working on a project using a CNN to make two predictions, however one prediction is classification and the other is regression. Real time inference matters a lot too, as the project is being used for live work (a car driving itself around a track). What would be the benefits or drawbacks of going with two separate models (one for speed and one for steering angle) instead of one model with two outputs and why?",MLQuestions,2022-05-16 10:35:27,4
!remind me 6days,1,uqrr1m,"So I saw this [Picture](https://www.instagram.com/p/CcIyUBGO-Kq/?igshid=YmMyMTA2M2Y=) on Instagram which I could swear is generated by wombo.art. The artist earns money using those pictures and doesn't state anything. 

Is this the future that we're only gonna look at art generated by machines?",MLQuestions,2022-05-16 02:17:52,4
"""Training"", ""validation"", and ""test"" datasets all serve different purposes. The training dataset is just there to tune your model's parameters. The accuracy you get on the training dataset should be increasing while you train if you code everything right, for obvious reasons. The validation dataset doesn't directly affect your model weights; it is a partially-external set of data that you can use to tune ""other things"" (hyperparameters, stopping criteria). It can give you a bit of an idea about generalization since you're not directly training on it, but using its accuracy as a declaration of your algorithm's performance is still cheating, because you're still using the dataset to finalize your algorithm.

So you ask the question ""what accuracy should we consider?"", which is pretty general. All 3 accuracies tell you different things. But maybe you're really asking the question ""what accuracy best represents your model's performance in the real world"" or something of that nature (even that question has its caveats...), in which case it's the test dataset, which you have not let your model see at all while ""training"" it.",3,uqxkys,"Hello, 

I have a question concerning model accuracy in ML

When we build a model (CNN or whatever). What accuracy should we consider? do we take t**he last accuracy** shown during training (last epoch)? or do we take the accuracy when we predict on the test dataset ? or the accuracy of the validation shown also during training? or the avg of one of them? I'm a bit lost in here.

&#x200B;

Thank you for your answers.",MLQuestions,2022-05-16 07:44:28,2
"I trained an LSTM on your data. It outputs the mean when data is not scaled between 0 and 1, but works great when scaled ([https://prnt.sc/AsThxqeHH2h0](https://prnt.sc/AsThxqeHH2h0)). Not sure exactly where the error is arising.",9,uq6gjm,"I am losing my mind :( Been spending so much time on this but can't figure it out... maybe it is the data I don't know. All predictions turn out like this. It is a timeseries where the only features are previous time step values. I'm trying to do multi step forecasting but it always looks like this: [https://gyazo.com/1c11ff3f867b145ac41e70cb2009f4cc](https://gyazo.com/1c11ff3f867b145ac41e70cb2009f4cc). Anyone has any suggestions?

I have put the data and notebook here: [https://github.com/jrnkng/NN\_attempt](https://github.com/jrnkng/NN_attempt) I was having so much fun with ML but this one is just tough to swallow. Set my alarm at 8 AM this morning to get the most of my day but haven't gotten a step further. If anyone could skim through it to see if I'm making any errors, that would mean a lot to me! I have scaled everything between 0 and 1 so I don't think that is the issue. It is data about employee absence.",MLQuestions,2022-05-15 06:45:39,4
This isn't a question...,2,uqcy49,"Training a machine learning model locally is quick & easy to set up a new project on a local machine. This is sufficient for simple experiments (with reduced data subsets or small models) without paying to rent heavy cloud compute resources. A local machine is also deeply familiar — as opposed to the multitude of available cloud services, which can be intimidating even with a decent background in DevOps.

Once you locally set up and iterate over your data & code enough, you may reach a point where more powerful compute resources are needed to train a larger model and/or use bigger datasets with a methodology explained in the following guide: [Moving Local Experiments to the Cloud with Terraform Provider Iterative (TPI)](https://dvc.org/blog/local-experiments-to-cloud-with-tpi)",MLQuestions,2022-05-15 12:02:27,1
https://twitter.com/ak92501,1,uphucn,"I am an 'applied' machine learning researcher, i.e. about 80% of my time is on machine learning, 20% is applying it to physics problems.

The increasing breadth and depth of new machine learning research is awe-inspiring. I would like to be able to keep up with the newest developments in the field, somehow, without obviously having the time to read all the latest developments.

Is there a website, resource like weekly or monthly magazines, or community aimed at collating the newest insights and directions and publishing summaries/overviews in digestible formats?",MLQuestions,2022-05-14 06:59:51,2
[deleted],2,updj67," I want to build a product recommendation chatbot, and I was hoping that I could use an existing pretrained model or a library that specializes in this,

is there anything available like this?

Thank you",MLQuestions,2022-05-14 02:20:30,8
"this is typically called [one-class classification](https://en.wikipedia.org/wiki/One-class_classification), or positive-unlabeled learning. i don't know if your ideas would work but you can read about existing methods using those terms.",3,up0s6e,"Hi,

I have a dataset of words that I'd like to classify into one of two classes. 

The cases in which the outcome equals 1 actually belong to that case, while examples in which the outcome equals 0 are either mistakenly tagged as 0, or correctly done so.

\- I have a reason to believe that words that our closer to each other should have similar scores, even if belong to different classes. So I also have the edit distance between each pair. 

\- I thought first of trying to embed these words, and try to separate them using k-means, or something like that. Hopefully, similar words would cluster together regardless of class, and this would allow me to maybe create a synthetic tag and train a classifier/adding this cluster as a feature. 

&#x200B;

Thanks!",MLQuestions,2022-05-13 13:35:27,5
"Word2vec would be a good way to go.  For example you could create top level categories like ""bicycle"" and determine the vectors for those top categories.

The when you get a query, you would calculate the vector for the query and compare it (cosine similarity) to the top level category vectors.  The closest match would be the top level category like in your example.

This could all be done with just a page of python code.  And the time required to make a list of top level categories.",2,uou42f,"I would like to either let someone in bed or find a tool that can extract a keyword (searchable term) of a product description or product title.

Example:

Product Title: Woom Balance Bike 1 

Product description: characteristics about the product… free to your imagination

I need the keyword: Balance Bike

Back up Keyword: Bicycle (or Bike)

If really advanced, then the Programm could also tell that a balance bike -> is a bike.. 

If anyone knows how to either “filter out” the relevant part of the product title or description I would greatly appreciate it! 

People I ask with some knowledge about ML told me I should look into vectors, word2vec for instance? 

Thanks",MLQuestions,2022-05-13 08:20:57,1
"A/B testing is a very common way that people answer the question ""which of these two models performs best with users?""",1,uooxyq,"I want to build a recommender system to suggest similar songs to  continue a playlist (similar to what Spotify does by recommending  similar songs at the end of a playlist).

I want to build two models: one based on **collaborative filtering** (so I need something like user ratings about songs?) and another one, a **content-based model** (so I need song's features like tempo, bpm..) to compare their results and choose the best one *which returns the best songs that a user will like and certainly will add to his playlist .*

Now, I have two questions:

1. Where can I find a dataset with useful data for this type of work?
2. How can I measure the results to choose the best model? **How can I understand that** ***a specific song*** **recommended by my model is better than another one to continue my playlist?**  (e.g. in classification tasks I can use confusion matrix to have a  clear view of the corrected and wrong classifications, but with  recommender systems how can I evaluate the *""correctness""* of the prevision?)

&#x200B;

*For example, my CF model suggest me a song,  while CB model suggest me another different song as first result to  ontinue my playlist. I surely like the first song and add it to my  playlist, while the first song suggested by the second model is awful  and I surely wouldn't add to the playlist.* ***How can I objectively evaluate the prediction's goodness and so the goodness of the models?***

**IN OTHER WORDS:** If a model suggest me a song  that I like more, how can I evaluate the results from the two models in  order to say ***which model predicts better songs for me***?",MLQuestions,2022-05-13 03:47:22,1
See for example https://arxiv.org/abs/2008.12009.,1,uootuf,"Hi. I am looking for some code in python that gives me a % of how similar my generated extractive summary of a paper is compared to the original abstract of that paper. I want to know how good or bad my summary was. Preferably without using training, but I can use any pre trainned data if freely available. Any tips/recommendations would be amazing! Thanks",MLQuestions,2022-05-13 03:39:24,1
"- Basics of some math topics like linear algebra, calculus, probability and statistics
- Reasonable familiarity with some programming concepts. Better if also familiar with a programming language like python including working with libraries like pandas and numpy.
- Some exposure to data analysis (working with data in some form, being able to conduct EDA, cleaning data, feature engineering)
- Ability to ask questions
- Ability to google
- Ability to learn, apply, iterate
- Ability to build stuff/projects on your own without a frequent/constant need of a course or tutorial holding your hand
- Ability to learn new things as you go along instead of always wanting to learn everything before moving forward
- Understanding how to learn more effectively",10,uo6v6d,,MLQuestions,2022-05-12 10:52:41,4
"I don't see any benefit of this over including all data at the start and using a smaller batch size. Changing the data would also probably also change the epoch loss in weird ways. Although, it sounds like an interesting thing to try.",2,uo5g43,"As the title says. I was wanting to run an experiment with my data where, instead of just shuffling the same training data every epoch, I would randomly remove 50% of it and replace it with new instances it hasn't seen before (or, maybe generate synthetic data that varies every epoch based on the same raw training data). I was wondering if this is ""okay"" - like does it make sense to do this, or is there a specific reason beyond improving model fit to want to use the same training data every epoch? Thanks!",MLQuestions,2022-05-12 09:47:51,4
"Run Forrest, run!",1,unvahb,"Ive made my rf classification model, and got my results. However the variable importance change drastically when testing in sample vs out of sample.

What i don't understand is how this would happen. 

As the RF decision trees should be the same. I've tried googling it and read articles but most of them fall short of explaining why it happens.

Does the RF build a new model on the test set? Does it use the old trees? Does it have something to do with gini impurities? Why is the data sorted so different on the test?

Any one got a good explanation of what happens?",MLQuestions,2022-05-12 00:11:00,1
Checkers.,6,uo33to,,MLQuestions,2022-05-12 08:02:10,3
Iirc I’ve seen the reparameterization trick used with normal distributions in SAC the continuous space RL algorithm but I may be mistaken. I think it’s used there over Gumbel since extreme values aren’t as big a concern with robotics simulations so that might explain why the normal dist isn’t often used in other scenarios.,2,unoe5x,"Ignore the method name. Is there a reason why in the Gumbel softmax trick we sample from Gumbel distribution? Since we are doing something similar to a reparameterization trick, can't we just sample from a normal distribution? I know Gumbel distribution are use to deal with extreme values, is this why we use Gumbel distribution to sample and not other type of distribution?",MLQuestions,2022-05-11 17:39:41,1
"RNNs and LSTMs have been replaced by Transformers due to reduced training time (parallelization) and better performance. Transformers usually have a “fixed” size since it looks at the whole sequence at once instead of step-wise, so you’re actually right there is a benefit to using fixed length sequences!

However it’s also possible to use variable length sequences with Transformers with some easy workarounds: [https://stats.stackexchange.com/questions/488844/self-attention-for-variable-length-sequence-classification](https://stats.stackexchange.com/questions/488844/self-attention-for-variable-length-sequence-classification)",4,unbh6u,"A really noob question! I  saw most LSTM tutorials fixing the time steps to look back as if it’s like a hyperparamters to tune, similar to ARMA. 
For example, [tensorflow time series forecasting](https://www.tensorflow.org/tutorials/structured_data/time_series).

However, it seems that these kinds of models can deal with variable length input. 
I wonder are there benefits fixating the input length?",MLQuestions,2022-05-11 07:44:54,1
"Not sure about #1, but I'd assume that your GPU is more than sufficient. If it's not just use one of the free ones (google colab/kaggle/etc).   
For #2, I don't have any model in mind, but take a look at Kaggle. There are a ton of problems/projects that people show how to solve, their code, and model selection. I'm fairly certain you can find a similar problem over there. That will be a good place to start",1,unf5fl," Not a data scientist by profession, but rather a surgeon scientist who developed an interest in machine learning applications for healthcare outcomes research. Previously used Weka, now teaching myself Python using Anaconda/Jupyter notebook.

Anyways, I'm trying to create a predictive model for a rare minority binary class outcome (0.3% frequency). Large dataset (700k instances, 10-15 attributes). Using a PC with AMD Ryzen 5600X 3.7GHz, 16GB RAM, RTX 3060Ti. 

1. Is my GPU powerful enough to do training for this? Expected run time? How much upside using a better GPU/more memory (e.g. RTX 3090, 32GM memory)
2. Suggestions for model selection? First attempt I made with a Random Forest model I think was way overfit or simply just predicted the majority class for 99% accuracy. Is it possible to set AUC as the scoring metric instead of accuracy? Recommend boosting? Other strategies?",MLQuestions,2022-05-11 10:27:45,1
"Common question. Easy to do!

Step 1: Pick a name for the startup

Step 2: Register a url - www.<name>.ai

Step 3: And this is the most important step. Tell your friends and family that you just founded an AI startup. 

 That’s it. You just created your very own AI startup! That’ll be $500 for my consultation fee",20,unhg3c,any guide?,MLQuestions,2022-05-11 12:11:34,7
"  
 

from tensorflow.keras.preprocessing.image import ImageDataGenerator 
from tensorflow.keras.applications import MobileNetV2 
from tensorflow.keras.layers import AveragePooling2D 
from tensorflow.keras.layers import Dropout 
from tensorflow.keras.layers import Flatten 
from tensorflow.keras.layers import Dense 
from tensorflow.keras.layers import Input 
from tensorflow.keras.models import Model 
from tensorflow.keras.optimizers import Adam 
from tensorflow.keras.applications.mobilenet_v2 import preprocess_input 
from tensorflow.keras.preprocessing.image import img_to_array 
from tensorflow.keras.preprocessing.image import load_img 
from tensorflow.keras.utils import to_categorical 
from sklearn.preprocessing import LabelBinarizer 
from sklearn.model_selection import train_test_split 
from sklearn.metrics import classification_report 
from imutils import paths 
import matplotlib.pyplot as plt 
import numpy as np 
import os 
 

INIT_LR = 1e-4 
EPOCHS = 20 
BS = 32 
 
DIRECTORY = r""C:/Users/Umar/Desktop/Face-Mask-Detection-master/Face-Mask-Detection-master/project/model 2 categories/output"" 
CATEGORIES = [""N95"", ""Surgical"",""Fabric"",""No_Mask""] 
 
 
print(""[INFO] loading images..."") 
 
data = [] 
labels = [] 
 
for category in CATEGORIES: 
    path = os.path.join(DIRECTORY, category) 
    for img in os.listdir(path): 
     img_path = os.path.join(path, img) 
     image = load_img(img_path, target_size=(224, 224)) 
     image = img_to_array(image) 
     image = preprocess_input(image) 
 
     data.append(image) 
     labels.append(category) 
 
 
lb = LabelBinarizer() 
labels = lb.fit_transform(labels) 
labels = to_categorical(labels) 
 
data = np.array(data, dtype=""float32"") 
labels = np.array(labels) 
 
 
(trainX, testX, trainY, testY) = train_test_split(data, labels, 
 test_size=0.20, stratify=labels, random_state=42) 
 
 
 
aug = ImageDataGenerator( 
 rotation_range=20, 
 zoom_range=0.15, 
 width_shift_range=0.2, 
 height_shift_range=0.2, 
 shear_range=0.15, 
 horizontal_flip=True, 
 fill_mode=""nearest"") 
 
 
baseModel = MobileNetV2(weights=""imagenet"", include_top=False, 
 input_tensor=Input(shape=(224, 224, 3))) 
 
 
headModel = baseModel.output 
headModel = AveragePooling2D(pool_size=(7, 7))(headModel) 
headModel = Flatten(name=""flatten"")(headModel) 
headModel = Dense(128, activation=""relu"")(headModel) 
headModel = Dropout(0.5)(headModel) 
headModel = Dense(2, activation=""softmax"")(headModel) 
 

model = Model(inputs=baseModel.input, outputs=headModel) 
 
 
for layer in baseModel.layers: 
 layer.trainable = False 
 
 
print(""[INFO] compiling model..."") 
opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS) 
model.compile(loss=""binary_crossentropy"", optimizer=opt, 
 metrics=[""accuracy""]) 
 
 
print(""[INFO] training head..."") 
H = model.fit( 
 aug.flow(trainX, trainY, batch_size=BS), 
 steps_per_epoch=len(trainX) // BS, 
 validation_data=(testX, testY), 
 validation_steps=len(testX) // BS, 
 epochs=EPOCHS) 
 
 
print(""[INFO] evaluating network..."") 
predIdxs = model.predict(testX, batch_size=BS) 
 
 
predIdxs =np.argmax(predIdxs, axis=1) 

print(classification_report(testY.argmax(axis=1), predIdxs, 
 target_names=lb.classes_)) 
 

print(""[INFO] saving mask detector model..."") 
model.save(""mask_detector.model"", save_format=""h5"") 
 

N = EPOCHS 
plt.style.use(""ggplot"") 
plt.figure() 
plt.plot(np.arange(0, N), H.history[""loss""], label=""train_loss"") 
plt.plot(np.arange(0, N), H.history[""val_loss""], label=""val_loss"") 
plt.plot(np.arange(0, N), H.history[""accuracy""], label=""train_acc"") 
plt.plot(np.arange(0, N), H.history[""val_accuracy""], label=""val_acc"") 
plt.title(""Training Loss and Accuracy"") 
plt.xlabel(""Epoch #"") 
plt.ylabel(""Loss/Accuracy"") 
plt.legend(loc=""lower left"") 
plt.savefig(""plot.png"")",1,un18u4,"guys i am creating a model that has 4 classes, i have no idea what is wrong in the code, i am not an expert 
 
# import the necessary packages 
from tensorflow.keras.preprocessing.image import ImageDataGenerator 
from tensorflow.keras.applications import MobileNetV2 
from tensorflow.keras.layers import AveragePooling2D 
from tensorflow.keras.layers import Dropout 
from tensorflow.keras.layers import Flatten 
from tensorflow.keras.layers import Dense 
from tensorflow.keras.layers import Input 
from tensorflow.keras.models import Model 
from tensorflow.keras.optimizers import Adam 
from tensorflow.keras.applications.mobilenet_v2 import preprocess_input 
from tensorflow.keras.preprocessing.image import img_to_array 
from tensorflow.keras.preprocessing.image import load_img 
from tensorflow.keras.utils import to_categorical 
from sklearn.preprocessing import LabelBinarizer 
from sklearn.model_selection import train_test_split 
from sklearn.metrics import classification_report 
from imutils import paths 
import matplotlib.pyplot as plt 
import numpy as np 
import os 
 
# initialize the initial learning rate, number of epochs to train for, 
# and batch size 
INIT_LR = 1e-4 
EPOCHS = 20 
BS = 32 
 
DIRECTORY = r""C:/Users/Umar/Desktop/Face-Mask-Detection-master/Face-Mask-Detection-master/project/model 2 categories/output"" 
CATEGORIES = [""N95"", ""Surgical"",""Fabric"",""No_Mask""] 
 
# grab the list of images in our dataset directory, then initialize 
# the list of data (i.e., images) and class images 
print(""[INFO] loading images..."") 
 
data = [] 
labels = [] 
 
for category in CATEGORIES: 
    path = os.path.join(DIRECTORY, category) 
    for img in os.listdir(path): 
     img_path = os.path.join(path, img) 
     image = load_img(img_path, target_size=(224, 224)) 
     image = img_to_array(image) 
     image = preprocess_input(image) 
 
     data.append(image) 
     labels.append(category) 
 
# perform one-hot encoding on the labels 
lb = LabelBinarizer() 
labels = lb.fit_transform(labels) 
labels = to_categorical(labels) 
 
data = np.array(data, dtype=""float32"") 
labels = np.array(labels) 
 
 
(trainX, testX, trainY, testY) = train_test_split(data, labels, 
 test_size=0.20, stratify=labels, random_state=42) 
 
 
# construct the training image generator for data augmentation 
aug = ImageDataGenerator( 
 rotation_range=20, 
 zoom_range=0.15, 
 width_shift_range=0.2, 
 height_shift_range=0.2, 
 shear_range=0.15, 
 horizontal_flip=True, 
 fill_mode=""nearest"") 
 
# load the MobileNetV2 network, ensuring the head FC layer sets are 
# left off 
baseModel = MobileNetV2(weights=""imagenet"", include_top=False, 
 input_tensor=Input(shape=(224, 224, 3))) 
 
# construct the head of the model that will be placed on top of the 
# the base model 
headModel = baseModel.output 
headModel = AveragePooling2D(pool_size=(7, 7))(headModel) 
headModel = Flatten(name=""flatten"")(headModel) 
headModel = Dense(128, activation=""relu"")(headModel) 
headModel = Dropout(0.5)(headModel) 
headModel = Dense(2, activation=""softmax"")(headModel) 
 
# place the head FC model on top of the base model (this will become 
# the actual model we will train) 
model = Model(inputs=baseModel.input, outputs=headModel) 
 
# loop over all layers in the base model and freeze them so they will 
# *not* be updated during the first training process 
for layer in baseModel.layers: 
 layer.trainable = False 
 
# compile our model 
print(""[INFO] compiling model..."") 
opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS) 
model.compile(loss=""binary_crossentropy"", optimizer=opt, 
 metrics=[""accuracy""]) 
 
# train the head of the network 
print(""[INFO] training head..."") 
H = model.fit( 
 aug.flow(trainX, trainY, batch_size=BS), 
 steps_per_epoch=len(trainX) // BS, 
 validation_data=(testX, testY), 
 validation_steps=len(testX) // BS, 
 epochs=EPOCHS) 
 
# make predictions on the testing set 
print(""[INFO] evaluating network..."") 
predIdxs = model.predict(testX, batch_size=BS) 
 
# for each image in the testing set we need to find the index of the 
# label with corresponding largest predicted probability 
predIdxs =np.argmax(predIdxs, axis=1) 
 
# show a nicely formatted classification report 
print(classification_report(testY.argmax(axis=1), predIdxs, 
 target_names=lb.classes_)) 
 
# serialize the model to disk 
print(""[INFO] saving mask detector model..."") 
model.save(""mask_detector.model"", save_format=""h5"") 
 
# plot the training loss and accuracy 
N = EPOCHS 
plt.style.use(""ggplot"") 
plt.figure() 
plt.plot(np.arange(0, N), H.history[""loss""], label=""train_loss"") 
plt.plot(np.arange(0, N), H.history[""val_loss""], label=""val_loss"") 
plt.plot(np.arange(0, N), H.history[""accuracy""], label=""train_acc"") 
plt.plot(np.arange(0, N), H.history[""val_accuracy""], label=""val_acc"") 
plt.title(""Training Loss and Accuracy"") 
plt.xlabel(""Epoch #"") 
plt.ylabel(""Loss/Accuracy"") 
plt.legend(loc=""lower left"") 
plt.savefig(""plot.png"")",MLQuestions,2022-05-10 21:17:37,1
Find a survey paper. See what they say about future work or problems,2,umlkkx,"Hi! I have to do a research paper on HyperNetworks but I've only been reading and collecting papers so far, any idea on how to find a feasible idea to explore ? 

Thanks in advance.",MLQuestions,2022-05-10 08:39:15,4
Try here https://paperswithcode.com/datasets,2,umrrow,"Hello Guys, is there any dataset that contains moving cars at a traffic junction?
Or even a dataset of vehicles that can be used for vehicle detection and counting for a traffic congestion management project? 
Thank you
#MachineLearning #kaggle #kagglers",MLQuestions,2022-05-10 13:20:54,2
"we have 2 classes. In prod, we can get an image that does not fit into any of the classes, but the model will define it as one of the existing and with a high probability. How to filter garbage in the prod using the model?",1,umlyag,how to train the model and weed out images that do not fit the classes?,MLQuestions,2022-05-10 08:56:36,4
"Sometimes… it’s the imaging… camera not setup right, making lower contrast between features on darker skin.",12,ume0la,"This is not meant to start any debate, more so understand what the challenge is. - I think the biases of existing face datasets is well understood. I'm trying to understand if there is anything directly inherent about darker tones that will reduce the effectiveness of an identically sized model with similar training epochs compared to lighter skins.",MLQuestions,2022-05-10 01:37:23,4
"Try template/feature matching techniques.

https://docs.opencv.org/4.x/d4/dc6/tutorial_py_template_matching.html

https://docs.opencv.org/3.4/dc/dc3/tutorial_py_matcher.html",2,umgf32,"Hi guys im a complete noob at machine learning so I hope I’m asking in the right place. I’m looking to see if it would be possible to train Tesseract to recognise logos in photos of physical media and how I would go about doing so. 

For example I want Tesseract to be able to list off the names in a photo of a stack of Xbox games, but untrained it is quite inaccurate as logos obviously use different fonts. I was thinking to start my first goal should be to get Tesseract to recognise the logos of popular franchises such as Call of Duty and Lego games.

A friend of mine who has used tesseract before said I’ll probably need 100k tiff images to get it to do this but would like further advice, thanks!",MLQuestions,2022-05-10 04:23:02,2
"Man if you know good C++, some
python and have an Elect engineering degree you can apply to a lot of companies, maybe not strictly technology companies but ones related or in the “electrical” field",3,ulxvt3,"Hi All,

Need some of your advice . Let me give some background...

I am in my late 30s having worked in Semiconductor Testing for over 16 years. I have a BS in Electrical Engineering from Tier 2 university.. I recently came across AL/ML bootcamp from UC Berkley Extension. 

My question to you all is - 

1. do companies higher people with boot camp education on this topic?
2. Do you know of anyone on your team or acquaintance who successfully got a job after bootcamp?
3.  Do you have any recommendations of a boot camp? The only one I have so far explored is UC Berkeley Extension program. It costs \~$6k (6 months). Then I googled few others and they cost as much as $16k to all the way up to $30k. Why do some costs more and some less?
4. Bootcamp websites (UC Berkeley specific) states that starting pay is $140k. Today after 16 years of work I am at around $160k (if I add RSUs that is about $30k more). For sure I would loose RSUs even if I move to a different company in my same exact field - but what about the base? I expect loosing money on the base if I ever get a start in ML/AL engineering field - but would I be able to make it up in future? In other words - what is my ROI? 
5. What are the different job functions in this field? e.g in Semiconductor engineering field there are Design Engineers (equivalent to Developers  in SWE) , Application Engineers (who work with customers on their PCBs and provide company's best fitting silicon solution), Product definers (who work with customers and Marketing in defining new products which could see demand in future), Product Engineers (who work in operations, manage product flow, yields etc), Test Engineers (me =/ who test parts for production/factory testing before shipping). What has the most promising career path in ML/Al field?
6. I use high level C++ every day; have some basic Python knowledge and some statistics knowledge. Is this sufficient to build upon to learn ML/AI from these bootcamps? The way they have been advertising is most people can do it - but seriously I find it hard to believe. From some posts I read in this thread I cant follow a single thing. These bootcamp course folks are in the business of selling courses so I see their interests. But if something is so easy - then why arent more people doing it and why is there still skill gap in the industry? I just dont want to sign up and pay ($6k. or $16k) and on the very first day of instruction they casually mention One-Way Anova expecting me to know the details of it and this point I cant get my money back.

Thanks for your patience. I know this has gotten long - but would appreciate your feedback.",MLQuestions,2022-05-09 11:32:23,7
"For Yolo you need a frame from the video and you need the bounding boxes for the items you want to detect, along with the associated label. To do this manually is very tedious and time consuming. There are applications which can help but still require the individual frames.

IMO if you MUST label 100 videos by hand…look into Amazon Turks and crowdsource the data (this costs money) or extract either every frame from the various videos, every other, every 3rd, every 5th ( depending on the density of the objects in the videos) and use one of the labeling applications. They load your images and you manually add bounding boxes and the labels for each and every frame. 

IF what you are detecting is not very distinct, such as you are just trying to detect a specific type of car. Or a specific type of person. Leverage a pre trained Yolo model for those class labels. Create a relatively simple neural network (or other ML model) to only classify those you want from those you do not. For example let’s say you are trying to find a specific type of car. Run a pre-trained Yolo model on some subset of your videos. Extract the bounding boxes of All detected vehicles. Now you can extract the simple images of each vehicle. 

This gives you a dataset to work from. Now either manually or some other means find all images of the vehicle you are trying to detect. Once that is done you then have a dataset of positive and negative examples. Train your simple neural network ( or other model) on that dataset, with the single task of being able to say if a vehicle is the one you are looking for or not.

Now using that simple model you can easily process those videos and create a better dataset of what you are trying to detect. You can then retrain Yolo on your results.",1,ulzimw," 

I  have to label about a 100 videos to use them with YoloV5. What tools  should I use to do that? Do I take individual screenshots and then use  those or? Im new to computer vision so any help is useful.

Thanks for the replies.",MLQuestions,2022-05-09 12:46:06,1
ONNX never heard of the others,11,ullawg,,MLQuestions,2022-05-08 23:56:33,5
Start as simple as possible with your model first and print your data just prior to training to ensure it is what you expect it to be. LSTMs are notoriously tricky to work with.,1,uly0dt,"I am training an LSTM to give counts of the number of items in buckets. There are 252 buckets. However, I am running into an issue with very large MSELoss that does not decrease in training (meaning essentially my network is not training). I've tried all types of batch sizes (4, 16, 32, 64) and learning rates (100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 1e-5, 1e-6) as well as decaying the learning rate. In fact, with decaying the learning rate by 0.1, the network actually ends up giving worse loss. I've also tried gradient clipping (see code below) with various max\_norm values all of which results in the training getting stuck in some plateau. 

The network does overfit on a very small dataset of 4 samples (giving training loss < 0.01) but on larger data sets, the loss seems to plateau around a very large loss. Code, training, and validation graphs are below. I'm relatively new to PyTorch (and deep learning in general) so I would tend to think something is wrong with my model. I'd appreciate any advice, thanks!

    
    import torch
    import statistics
    from torch import nn
    from helper import *
    
    import os
    import sys
    import numpy as np
    import pandas as pd
    
    from torch.utils.data import Dataset, DataLoader
    
    maxbucketlen = 252
    
    # Number of features, equal to number of buckets
    INPUT_SIZE = maxbucketlen
    
    # Number of previous time steps taken into account
    SEQ_LENGTH = 2
    
    # Number of stacked rnn layers
    NUM_LAYERS = 1
    
    # We have a set of 144 training inputs divided into batches
    BATCH_SIZE = 4
    
    # Output Size
    OUTPUT_SIZE = maxbucketlen
    
    # Number of hidden units
    HIDDEN_DIM = 256
    
    is_cuda = torch.cuda.is_available()
    
    # If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.
    if is_cuda:
        device = torch.device(""cuda"")
        print(""GPU is available"")
    else:
        device = torch.device(""cpu"")
        print(""GPU not available, CPU used"")
    
    class BucketDataset(Dataset):
        def __init__(self, csv_file, input_length, seq_length):
            self.buckets_frame = pd.read_csv(csv_file, delim_whitespace = True)
            self.seq_length = seq_length
            self.input_length = input_length
    
        def __len__(self):
            return len(self.buckets_frame)
    
        def __getitem__(self, idx):
            if torch.is_tensor(idx):
                idx = idx.tolist()
    
            train = self.buckets_frame.iloc[idx, :self.seq_length * self.input_length]
            train = np.array([train])
    
            target = self.buckets_frame.iloc[idx, (self.seq_length - 1) *
                    self.input_length:]
            target = np.array([target])
    
            # Below can be used to reshape data to sequence data
            train = train.astype('float').reshape(-1, self.input_length)
            target = target.astype('float').reshape(-1, self.input_length)
    
            sample = {'train': train, 'target': target}
    
            return sample
    
    train_dataset = BucketDataset(file_name,
            INPUT_SIZE, SEQ_LENGTH)
    
    train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True,
            num_workers = 80)
    
    class Model(nn.Module):
        def __init__(self, input_size, output_size, hidden_dim, n_layers):
            super(Model, self).__init__()
    
            # Defining some parameters
            self.hidden_dim = hidden_dim
            self.n_layers = n_layers
    
            #Defining the layers
            # LSTM Layers
            self.rnn = nn.LSTM(input_size, hidden_dim, n_layers, batch_first=True)
            # Fully connected layer
            self.fc = nn.Linear(hidden_dim, output_size)
    
        def forward(self, x):
    
            # Initializing hidden state for first input using method defined below
            hidden = self.init_hidden()
    
            # Passing in the input and hidden state into the model and obtaining outputs
            out, hidden = self.rnn(x, hidden)
    
            # Reshaping the outputs such that it can be fit into the fully connected layer
            out = out.contiguous().view(-1, self.hidden_dim)
            out = self.fc(out)
    
            return out, hidden
    
        def init_hidden(self):
            # This method generates the first hidden state of zeros which we'll use in the forward pass
            # We'll send the tensor holding the hidden state to the device we specified earlier as well
            # Initial States
            self.hidden_state = torch.randn(self.n_layers, BATCH_SIZE, self.hidden_dim, device=device)
            self.cell_state = torch.randn(self.n_layers, BATCH_SIZE, self.hidden_dim, device = device)
            hidden = (self.hidden_state, self.cell_state)
    
            return hidden
    
    # Instantiate the model with hyperparameters
    model = Model(input_size=INPUT_SIZE, output_size=INPUT_SIZE, hidden_dim=HIDDEN_DIM, n_layers=NUM_LAYERS)
    # We'll also set the model to the device that we defined earlier (default is CPU)
    model.to(device, non_blocking=True)
    
    # Define hyperparameters
    n_epochs = 1000
    lr=0.001
    
    # Define Loss, Optimizer
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    
    print(""Training Started"")
    
    test_dataset = BucketDataset(validation_file,
            INPUT_SIZE, SEQ_LENGTH)
    test_loader = DataLoader(test_dataset, batch_size = BATCH_SIZE, shuffle = False,
            num_workers = 24)
    
    def predict(model, counts):
        out, hidden = model(counts)
        return out, hidden
    
    
    # Training Run
    for epoch in range(1, n_epochs + 1):
        for j, data in enumerate(train_loader):
            for param in model.parameters():
                param.grad = None
            output, hidden = model(data['train'].cuda().float())
            loss = criterion(output.flatten(), data['target'].cuda().float().flatten())
            loss.backward() # Does backpropagation and calculates gradients
    
            optimizer.step() # Updates the weights accordingly
    
        # Perform validation
        for k, data_test in enumerate(test_loader):
            with torch.no_grad():
                counts, h = predict(model, data_test[""train""].cuda().float())
                val_loss = criterion(counts.flatten(), data_test['target'].cuda().float().flatten())
    
        if epoch % 5 == 0:
            print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')
            print(""Training Loss: {:.4f}"".format(loss.item()))
            print(""Validation Loss: {:.4f}"".format(val_loss.item()))
    

Training and Validation Loss graphs for no clipping, no decay: 

https://preview.redd.it/q8zgyoogqhy81.png?width=640&format=png&auto=webp&s=51093575574511e1b59d504e0e22b17f272be521

The orange line is the validation loss and the blue line is the training loss. The loss function is MSELoss and the optimizer is Adam.

Error with decay and gradients clipped at 1.5 below. The value of the loss is smaller but it is still plateau-ing at a very high loss.

https://preview.redd.it/3abxa6wvvhy81.png?width=640&format=png&auto=webp&s=c8d2ed596572658acd5c9bd53285864dfa3af143

Any help and suggestions greatly appreciated!",MLQuestions,2022-05-09 11:38:12,2
"The mask enabled output will be [True,True,….,False,…]

False for the padding, if it’s enabled for that layer.",1,ulusvq,"I'm trying to create masks for one-hot encoded features with padding. An example of what I'd like to mask is:

&#x200B;

    [[0001][0100][0010][0100][0000][0000][0000]]

In the block above, the last three elements represent the padding. All of the tutorials I've found suggest that I use a mask to remove 0's, but how will that impact the one-hot encoded features? Is there a more general masking method that I could look into?",MLQuestions,2022-05-09 09:13:57,2
"There is likely a spectrum of performance improvement you would see with re-training on all the data available. If the new data is expected to be very similar in nature to the data the model is already trained on, and if the model was trained on sufficient data to begin with, the improvement will be minimal. If either of these conditions are not as true, the improvements could be more substantial.",1,ulsvx9,"Hi all, I'm looking into a classification problem with each observation being an aggregated geographical area, which I've trained some classifying algorithms on. The response variable is a binary indicator of poverty. Could people help me think through benefits and drawbacks of using my best model without re-training it on even lower disaggregated data or data for different time periods? Thanks for your thoughts!",MLQuestions,2022-05-09 07:47:00,1
"Optimizing the architecture or at least finding a good one is goal of the *hyperparameter optimization*. It's usually done around the actual net training because neural nets are mostly smooth functions where you can apply derivative based optimization, but hyperparameter optimization is black box and you need derivative free methods. Approaches vary from systematic search in the hyperparameter set (grid search) to using black box optimization like genetic algorithms. Latter approach is quite useful for the high dimensional search spaces that arise from all the possible combinations of depth, width, and other tuning parameters such as regularization, learning rate etc.",3,uljx38,"In an intro to ML class I'm currently taking, we learned how when training neural networks, we start with a pre-defined architecture suitable to the task and simply learn the weights of the network using stochastic gradient descent. Is there any type of neural network where the architecture i.e. which neurons are connected by edges is also learned by the algorithm? Is there a way to optimize the best type of architecture for a task? Intuitively, it seems to me like there are so many possible ways a network could be connected and that this would matter a lot in creating an effective network. Thanks.",MLQuestions,2022-05-08 22:16:58,5
"You're sort of both right and both wrong. the on/off perspective is definitely valid: ReLU is one of the most common activation functions, and that's basically all it does. Conversely, your sigmoid example is sort of moot because that's an activation that is basically never used anymore. But, to your point, models that use leakyReLU or other ReLU variants that are non-zero almost everywhere often exhibit performance improvements over models that use simple ReLU, so you're not wrong that there's a lot more going on here than just a simple on/off. So essentially... it's sort of situational. there are certain applications where the ""on/off"" interpretation of the behavior is going to be more valuable, and there are certain applications where the non-linearities are going to be the real value the activations provide. I think it might be worth trying to probe your professor further on why the on/off interpretation might be more applicable to the specific context you were discussing with them. If you want to bring an example to the discussion where the role of the activation function is clearly in the non-linearities, an example that comes to mind is SIREN networks, where the activations are sine functions.

NINJA EDIT: The important thing to keep in mind is that the way we paramterize neural networks with ""activations functions"" and ""layers"" is really a generic language for constructing complex sequences of mathematical operations as graphs. So discussing the role of activation functions is like discussing the role of an arithmetic operator: the role it serves in the broader model completely depends on the context.",5,ul9xg1,"This question is being asked here after i had a discord with a professor of mine. As far as i understood it, activation functions are used to make neural networks non linear but he believes activates and de activates individual nodes of a NN. 

I cant get my head around how it activates a neuron. Without using a threshold function. Taking the case of a neuron with a sigmoid activation the output will never be 0, though miniscule in case when x is a large negetive number, but still not 0.

I tried to find the answer online but found sites having different opinions.",MLQuestions,2022-05-08 13:04:51,1
I'm no expert but I think tabular data is where they shine,3,ulawze,,MLQuestions,2022-05-08 13:52:24,3
"I mean, you're either doing supervised or unsupervised.

If you're doing supervised, you need ground truth labels. If you don't have labels, then you need to annotate. 

Not sure if that's what you're asking, but this is why labeling is becoming increasingly important, as startups move into areas where there are no existing datasets. (It's also where there's most potential). This is why companies like Labelbox are doing so well.",3,ulalfu,,MLQuestions,2022-05-08 13:36:55,1
!remind me 2 days,1,ukc0x3,"Hello, I'm an Android developer and I want to know which techniques and   tools can help me to create an app that help the user to make up a face   picture. Here is an example of such app. [https://imgur.com/a/SkDW1nZ](https://imgur.com/a/SkDW1nZ)",MLQuestions,2022-05-07 05:14:06,6
>[Here you go](https://www.reddit.com/r/ProgrammerHumor/comments/8c1i45/stack_more_layers/),3,uk9q90,"Any good ""Designing Neural Network MOOC"" ?",MLQuestions,2022-05-07 02:32:50,3
"Yannic Kilcher has an excellent YouTube channel where he does explanations of the latest papers in AI and machine learning usually 20-40 min. A lot of content and sometimes does interviews with the authors which is great.

Machine Learning Street talk is also a really good ML podcast that talks about a pretty wide range of topics and has many interviews with top figures in AI.

Lex Friedman's podcast also has many interviews top figures in AI. From my POV it's more high level and talks about the big picture, directions in different emerging areas in ML, and career/research advice. Machine Learning Street talk is a little more technical and in depth.",3,uk4g3g,"I’m not a machine learning background and majority of the stuff I learnt from online courses, blogs and books. I wonder if there’s any sources or persons that I could look into to learn further materials reliably? 

For instance, when we talked about damodaran, he would be the credible source to look into for equity risk premium. 

Any help would be appreciated!",MLQuestions,2022-05-06 20:23:18,4
"(Another Beginner here) Can you provide examples of job posts that you are planning to classify? Let's say this is the type of job post-https://www.linkedin.com/jobs/view/3065072747 where everything after ""Roles and Responsibilities""  till the words ""Requirements"" is Roles and Responsibilities and later on everything is requirements till the next empty line. So if you have such formatted posts then I believe regex will be the best solution.",1,ujrqcp,"Sorry in advance as I'm a beginner in this area.  

My task I'm trying to do, which I've scoured examples for similar problems, is to classify segments of text in a larger text.  More specifically, given a Job Post Description post, I'd like to be able to extract the Job Requirements and Job Responsibilities.  At first I thought I should use NER for this, but of all the examples I see for this, NER is typically used for a few words and maybe phrases.  

Text classification also doesn't quite seem to work as that seems to analyze the whole text and classify it, but it doesn't necessarily label parts of text (though correct me if I'm wrong). 

Can anyone point me in the right direction of how I should approach this?  I appreciate it in advance!",MLQuestions,2022-05-06 09:48:00,5
"Jumping on the bandwagon because I'm having exactly the same issue! I found this post via a Google search for how to resolve the errors. I hope someone knows how to fix it! :D

Edit #2: Trying this now. (nope 😡)

EDIT: I found [this](https://github.com/kingoflolz/mesh-transformer-jax/issues/167#issuecomment-989905477), trying it out now. (nope 😡)",1,ujtzf5,"Hello,

I'm trying to use the following link: [https://colab.research.google.com/github/kingoflolz/mesh-transformer-jax/blob/master/colab\_demo.ipynb#scrollTo=8CMw\_dSQKfhT](https://colab.research.google.com/github/kingoflolz/mesh-transformer-jax/blob/master/colab_demo.ipynb#scrollTo=8CMw_dSQKfhT)

I copied the program over to my google drive and began to run the program, but then I got a few error messages that I'm not really understanding what I could do to fix them.

I get errors in two places. The first is in the first cell, where it brings up errors like this:

 ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu 2.6.3 requires grpcio<2.0,>=1.37.0, but you have grpcio 1.34.1 which is incompatible. tensorflow-cpu 2.6.3 requires tensorflow-estimator<2.7,>=2.6.0, but you have tensorflow-estimator 2.5.0 which is incompatible. 

&#x200B;

The second place is in the third cell, with the line ""import optax"".

That code causes different errors if I reload, but usually it's something like:

 AttributeError: module 'jax.random' has no attribute 'KeyArray' 

&#x200B;

Does anyone know why these errors occur?

Thank you",MLQuestions,2022-05-06 11:31:08,5
"For what task? In some subfields, e.g. dialogue processing systems, BLEU is completely uninformative and human evaluation (usually via Amazon Turk) is standard. It can be misleading in papers, because reviewers often still ask for BLEU if you leave it out, even though good dialogue systems often get like 2% on BLEU. I’d say COMET is generally accepted to be better for MT and Dialogue Systems, and I’ve been seeing it a lot more often lately.",1,ujaqgk,"Is anyone familiar with 1 resource with a list of evaluation methods to check the generated answer vs the ""gold answer""? I'm not looking for a survey with 30 pages, just something along the lines of:  
BLEU = output between 0 and 1. Measures n-grams similarity.  
ROUGE = ...",MLQuestions,2022-05-05 16:55:55,3
"What you essentially want is the inverse of the function which is approximated by your model. For certain kinds of networks which are invertible, computing this inverse is possible. Examples of these include flow models and iResNet. However, for most neural nets, the inverse is not analytically tractable.
One thing that you might be able to do is to search your embedding space, i.e. for a desired Chinese phrase, compute the embeddings which maximise its probability of being outputted. This can be done for example through gradient descent in the embedding space, though decoding algorithms which produce outputs given input embeddings might make it non-trivial. Once you get this embedding,  you can search through your input vocabulary to get a sequence of words which could have potentially produced this embedding. Due to the dependence of embeddings of a word on previous words, this might also not be very straightforward however. This paper [1] might be of interest for the second step.

[1] https://www.google.com/url?sa=t&source=web&rct=j&url=https://arxiv.org/pdf/2004.00053&ved=2ahUKEwj-zP3-r8f3AhVqS2wGHQPOCFcQFnoECAgQAQ&usg=AOvVaw1ihlzcXHePVn5bBHPWaYBZ",15,uimr0k,"I was playing around with Bert translation model which translates English to Chinese. I was wondering if there’s any way to use the pretrained embedding of the model to translate Chinese to English? 
Any help would be appreciated!",MLQuestions,2022-05-04 19:09:05,2
"Do you have a link to a publication or even blog where this has been proposed?

On the surface, if you have a set data where you have inputs that are smaller sets of data, and the known variances of those sets of data, you should be able to train a NN approximate that.  But it's hard to say more without more context.",3,uiopt7,"Recently, networks with heads that estimate variance have often been proposed. But I think it looks very strange. Is there anyone who can explain?",MLQuestions,2022-05-04 20:53:30,6
"> I'm particularly interested in 10+ hops

Just curious: what's an example where knowledge items that are so far removed from each other are still usefully related? Could you possible describe your graph and/or the kinds of queries you're playing with?",2,uifcj0,"Is anyone familiar with any papers / datasets that are doing more than 3 multi-hops? Seems like every paper / dataset I look at is either 1,2, or 3 hops at best. I'm particularly interested in 10+ hops",MLQuestions,2022-05-04 13:08:14,4
"There is no ""transferring"" models to other data. You either train it on the new data or get a model that was already trained on the new data",1,uif9tm,"I was exploring the pretrained reformer model in hugging face but it only provides English version. 

It seems to require a lot of gpu time to train the model from scratch. 

I wonder if I can transfer the model to another language version or is there a standard practice in dealing with this kind of problem? 

My preliminary thought is to combine a specific language Bert with the reformer model but I’m not quite sure how to proceed.

Any help would be appreciated! 🙏🏻",MLQuestions,2022-05-04 13:04:51,2
"Look through the documentation or read through the source code to determine where the .pt or .pb file is being saved. That is your model weights.

Edit: Sorry, I realized I just assumed you were working with deep learning. You are likely working with machine learning. Either way, the documentation of the library you are using should tell you where it is saved.",2,ui8rxx,"I trained the model using the !python http:\\\\github\\link command, how to save it so i dont need to retrain it again?",MLQuestions,2022-05-04 08:09:54,1
"If a product is ordered 20 times in a year , that’s not even twice a month. Is it reasonable to build a demand prediction model for such products ? Or is it better to leave such products outside the scope of the model and deal with products having a transaction volume above a certain minimum threshold ? 

This is the question you should ask yourself first.",4,uhizwx,"I've been looking around for a technique to predict a product demand with few data points. 

My dataset is on a daily basis, but I can aggregate it on a monthly or quarterly basis. The problem is that many products are only ordered 20 times a year meaning that: 

Either I will have a monthly time series for each product that contains many ZEROS, or I will have quarterly times series with few data points. 

&#x200B;

So:

My goal is to extrapolate, but my problems are either having a lot of ZEROS/ or having few data points. What can I do, please?",MLQuestions,2022-05-03 08:45:43,4
"You can always take a subset of the data and put it into a embedding projector to see weird similarities.

I guess its best to first tell us a few hypothesises about your data or project.",1,uh94rw,"Hi!

My data is comprised of different individuals for which we have monthly features over many years and for which the target is the monthly price of medical treatment over the same time span.

I know that in the classical approach we'd use panel data like linear models that take into account the differences between individuals but I'm expected to use machine learning models and I'm a bit stuck.

I guess if provided enough data the ML models (Random Forests, SVMs...) by default will learn the difference between individuals but I'm still wondering if it's a good approach to just feed that data over some years like 2010-2015 (correctly preprocessed) and validate on other years like 2015-2017 then test on 2017-2019 or is there a better approach for this. Or should I include the individual's index as a categorical feature? But I have some 20 000 different individuals.

I'm editing this post to include my approach so far:

Initially  the data I have is about 20m rows, some 50 000 individuals, for each  individual we collected a bunch of features (27 columns) over many years  from 2010 to 2019. We don't have data for all those individuals  sometimes we'd have one month in a year sometimes we'd just have 3 years  of data etc. The target is the price that the individuals did cost us  for insurance during that month/year (how much we paid for that  individual). We want to know if the next year for example the individual  will cost us more or less. We'd also have many rows in a month.

I  have many categorical features and sometimes there are just too many  categories like more than 2000 categories, and generally there's one  categories that takes over 95% data.

What  I did as preprocessing is first take the data from 2014 onwards because  I just judged that the data before is irrelevant. For each category I  one-hot encoded it into the categories that take over 98% data and put  everything else in ""other"". I aggregated data over the months like  df.groupby(\[""identifier"", ""year"", ""month"")\].sum() so each feature  categories has as value the number of times it did appear in the same  month. And the target is the monthly price. This gave me some 55  columns.

My cross-validation looks  like this take data over year 2014 (some 200 000 rows) as training and  predict on 2015, then train on 2014-2015 and predict on 2016 then train  on 2014-2015-2016 and predict on 2017. Without using the individuals'  identifiers. I trained 20 different random forests with 500 trees and  ""sqrt"" when selecting features to use, using RandomizedGridSearch where  max leaves went from 2 to 7 and max depth of the trees from 10 to 20.  All the models did extremely poorly (best RMSE is in the order of 500  000 while the mean of the data is in the order of 200, dollars), and all  had their best RMSEs when they were trained on 2014 and predicted on  2015. I'm talking about validation RMSE and not training.

All  the preprocessing (major categories etc) was taken from the data of  2014 to avoid data leakage (since in cross-validation I use other years  to predict on them). I want to keep 2018-2019 as test set.

I've also just taken the costs (the target) of the year 2014, without taking into account the individuals or features, and did the Advanced Dickey Fuller test and it gave a p-value of 0.0 indicating that the time series was stationnary, which was also supported by the plot of the time series and the ACF plot.

Thank you again for any ideas that may help me progress!",MLQuestions,2022-05-02 22:32:28,2
"I haven't done this one personally, but [this course](https://www.deeplearning.ai/program/practical-data-science-specialization/) or the MLOPs DL.AI would be a place to start. Can also ask on the mlops.community slack - they're super helpful there and def specialize in this kind of thing.
GL
C",2,ugrevs,Do u know any MOOCs for deploying a machine learning model on AWS ?,MLQuestions,2022-05-02 07:53:01,1
I don't think it hurts to reach out to other companies for part time / internships. Just make sure their employment agreement won't conflict with your current company's agreement.,2,ugxnxf,"I'm currently working as a deep learning/computer vision full time engineer, But I want to involve in more projects in practical AI applications in Robotics, embedded systems, autonomous vehicle etc.

Should I reach out to companies to work in part time in my free time ?  any idea? strategy ?",MLQuestions,2022-05-02 12:33:17,3
"short answer: it speeds up learning and convergence when using modern activation functions.

long answer: https://towardsdatascience.com/why-data-should-be-normalized-before-training-a-neural-network-c626b7f66c7d",6,ugjv7x,"Does it make sense to normalize the data even if the data is bounded from some min value to max value? Lets say we have image as input and we all already know each pixel value belongs to (0,255) range. I dont feel if makes sense to just normalize data by dividing complete data by 255. Please share your thoughts/experiences if any. Thanks!!",MLQuestions,2022-05-02 00:07:26,4
"Nah. Actually it's spelled Jupyter, where the py is for python. Jupyter definitely makes a lot of things easier, especially if you want to inspect intermediate values. So it could be very advantageous! But people in my cohort prefer to go with lower level hardcore programming, which I particularly find it to be faster, and for us, we used to use Jupyter wayyy back when we first started. But you can also be an advanced user and use Jupyter notebooks. Whatever floats your boat.

Also, note that Jupyter by itself is like an engine, and what people usually refer to are Jupyter notebooks, which are files that utilize Jupyter. You can also build other modules on top of Jupyter",5,ugavdx,"Hey guys I’ve learning with Udemy and YouTube videos, i think my college has a couple of electives but i cant take them yet. But in all the course I’ve watched they talk about Jupiter and how important it is and that pretty much everything in the ML field is done in it. My questions: Is it true? Do i actually need to learn and get familiar with Jupiter?",MLQuestions,2022-05-01 15:17:49,8
"I would just scan across a grid of values, and find the best combination at once.",1,uggfcn,"Is it okay to tune Gamma and Max Depth together ? Or should we first tune depth and then gamma, if yes, why ?

The reason I am asking this is because I believe - gamma is going to interact alot with Max depth. For some values of gamma - increasing max depth wouldn't mean anything. Since it would prune it back to smaller depth.",MLQuestions,2022-05-01 20:20:45,5
"I think the most common approach to this is td-idf.

And it is simple algo too.",1,ug56b6,"Hi Everyone,

I am developing one application in that I have multiple pdf files which user will upload then application will group those PDFs according to their similarity%(% entered by user) if user enters 80% it means documents with atleast 80% similarity will group in batch 1 and so on (similar pdf files will group into one batches i e. Batch 1, batch 2....). PDFs documents can be text or image or collection of both.

I am using .Net Core and Angular. I tried Kmean algo but results are different everytime as an Algo is picking random files for centroids. How can i implement this?",MLQuestions,2022-05-01 10:45:34,1
Quite likely they are talking about Kubernetes pods: https://kubernetes.io/docs/concepts/workloads/pods/,3,ug3ejf,"""Adding machine learning into new or existing data centers is commonly done to solve an existing known problem, like problems in large learning pods or network problems because of algorithms that were developed from received data. Most solutions today tend to be customized to match the size of the problem.""

I need to translate this to another language, but cannot figure out for sure what's meant by ""pods"" here.

Thank you!",MLQuestions,2022-05-01 09:21:43,2
As a regression problem. 0 is very bad to 1 very good.,3,ufzj85," I want to do multi class classification with five categories \['Extremly Negative' , 'Negative' , 'Neutral' , 'Positive' , 'Extremly Positve'\] so misclassification 'positve' to 'Extreamly Positve' is okay  but 'Extremely Negative' is not okay how to penalise misclassification differently",MLQuestions,2022-05-01 06:07:22,1
If the eval loss is going down then it should be okay imo. You may want to tweak the hyperparams and train it longer.,2,ufvz64,"I am training a binary classifier using an efficientnetv2 model with a  1M image dataset where I do a 60/20/20 split. Does this graph mean that  the model is over-fitting? I can see that the train loss is going down  much faster than the eval loss but the eval loss is still going down and  the accuracy is going up.

https://preview.redd.it/g81or40cytw81.png?width=1096&format=png&auto=webp&s=5da33bcd2f6ea1f7d18e0e7c2411a6881ca305ec

&#x200B;

&#x200B;

[Accuracy may seem to be low but it is actually a pretty decent amount for the problem I am working with.](https://preview.redd.it/d2avh1cfytw81.png?width=563&format=png&auto=webp&s=5aaac65fd2f19c163aadd176b786fe82f295ec1f)",MLQuestions,2022-05-01 02:06:13,1
"For the first one I would say ""yes"".

The second one in my opinion is about ""How do you decide which new components are key and which are not"". So it is about all that variability in components and so on.",1,ufwisp,"Hi everyone,

I'm learning PCA and face this problem in a textbook. In question b, I'm not sure about the answer, so please help me, thank you very much.

As far as I know, PCA should run on a normalized dataset, so is the answer to the question ""Should the data be normalized?"" always YES? Another question is ""Discuss what characterizes the components you consider key."", as I understand PCA, it creates new features by the linear combination of the old features, so what is the answer for ""key characterizes"" as it is a combination?

https://preview.redd.it/vz5g3bf36uw81.png?width=1234&format=png&auto=webp&s=be3f9e38d36a8daef979049b5a7f93f073056575",MLQuestions,2022-05-01 02:48:39,3
"Don't fit a second CountVectorizer on the test set.

After applying fit_transform() to the training set, use the SAME CountVectorizer object on the test set.  Of course, only use the transform() method (not fit_transform).

This way, the transformer will take care of dropping new tokens in the test set, and padding out unused tokens from the training set so that the dimensions will perfectly match.",2,ufobai,"Hello. We are trying to take court case summaries and their topic label (Civil Rights, Due Process, etc.) and predict the latter. As far as we understand, we have to label encode the labels and vectorize the summaries. We are getting an error ""ValueError: X has 9052 features, but KNeighborsClassifier is expecting 17687 features as input"" at the end when we try to predict the test features. Here is the link to the Google Colab Notebook: [https://colab.research.google.com/drive/1oD07yvKgDXAUHSIrWfggLJR9hKbDdmOg?usp=sharing](https://colab.research.google.com/drive/1oD07yvKgDXAUHSIrWfggLJR9hKbDdmOg?usp=sharing)

&#x200B;

The CSV we are using and uploading is from this Kaggle dataset ([https://www.kaggle.com/datasets/deepcontractor/supreme-court-judgment-prediction](https://www.kaggle.com/datasets/deepcontractor/supreme-court-judgment-prediction))",MLQuestions,2022-04-30 17:48:36,4
Jurafsky & Martin is the go to NLP resource https://web.stanford.edu/~jurafsky/slp3/ I used the 2nd edition when I was getting into industry over a decade ago and I reference this 3rd edition draft frequently,3,ufreo5,"1. If anyone knows of really nice resources that has NLP case studies ""explained in detail"", can you please share? 
2. You can also share any NLP resource that you found really nice to read. (blogs/books/etc...).
3. Also if you can share some nice projects that you have worked on and can explain problem statement, approach to solve and outcome, then it would be great. 

Thanks!!",MLQuestions,2022-04-30 20:52:33,1
Here's a go-to book on forecasting: https://otexts.com/fpp3/,3,ufbkhx,"Hi!

I am working on a POC for predicting certain KPIs over time (don't know yet how far ahead), with other categorical features also available in the data.

I would really appreciate references regarding the subject (or where I can find some), and also advice from your own experience if you can spare any.

I just want to get a basic model running which performs fairly ok, so the implementation doesn't need to be TOO complicated though I'll take what I can get.

Also, advice regarding common pitfalls to avoid would also be greatly appreciated.

Thank you🙏",MLQuestions,2022-04-30 06:54:31,7
"I have so many questions. Have you split your data into training & validation data? Which has stopped improving?
How good is performance when it stops improving? 

What optimiser are you using? Perhaps it's getting stuck in a local minima & an optimizer with momentum would help. 

What learning rate are you using & are you modifying learning rate as it trains? Perhaps the learning weight has ended up too low causing the lack of improvement. 

Are you training your model from scratch or are you using transfer learning? If you aren't already maybe you should consider using a model trained on a similar task. 

Also 115 classes is a lot, do you have enough representative data for each class? Is your dataset balanced?",1,uf6666,"Hi everyone! I've built a model for animal classification (115 classes) with keras. The model after 50 epochs doesn't increase in accuracy nor decreases in loss. I tried to modify the model in different ways, as adding optimization, dropout,  changed number of features, but it always stops to learn after a while. How can I improve it?


________________________________


More info:

Train and validation split at 80-20

Both training and validation accuracy stop at around 0.5

optimizer=Adam(learning_rate=1e-4)

Model from scratch, can't use pre-trained model",MLQuestions,2022-04-30 00:31:11,8
"Internships, connections (go to meetups, talk to professors), and start working on projects and showing them on github. Writing a blog about them (medium/towards data science) doesn't hurt either--my previous manager mentioned that she was impressed by it and that's a reason she took to hire me.",7,uemgow,"I have done a bit of research on various job boards and haven't really seen too many job postings for a junior MLE (<1 YoE), especially without master's/Phd. Even internships for this role are few and far in between. This is leading me to second guess my choice and I feel like I might have to pivot and learn something else like backend or blockchain. I am still a sophomore so I have time to learn new tech stacks, any advice would be helpful.

(not interested in pure data science or research positions and really don't want to pursue a master's degree)",MLQuestions,2022-04-29 06:55:19,9
"I mean, if you just used deciles or whatever, you would have adaptive bin ranges with equal numbers of points in each bin. That's probably not super useful, so you could try the [cut or qcut methods](https://pbpython.com/pandas-qcut-cut.html) in Pandas to play around with various ranges until you're happy with the distributions.",2,uezt5f,"Hi, 

I have the following setup: a list of integers, where some of them repeat more than others. 

I want to split them into bins, using a histogram, but such that, areas (on the Naturals) in which I have a lot of examples, will have smaller bins (and on the limit a bar per integer), while in areas which I have only a little information, the bar will include more integers, just like h in kernel estimations.

Any known method for such an example?",MLQuestions,2022-04-29 17:47:35,1
Is the outcome class imbalanced?,1,ueh554,"So I wanted to classify a dataset using Binary Classification but the accuracy on the training as well as the test dataset is coming out to be approximately 50%. Any ways I can increase it?

The data is 23999\*50.

https://preview.redd.it/2rypye7effw81.png?width=882&format=png&auto=webp&s=80cda0059b108ed5cd6d880ecd076b04b94a496e

I have tried preprocessing the data but it just decreases the accuracy.",MLQuestions,2022-04-29 01:15:09,10
Improve your software skills and earn a MS in computer science from an accredited university.,2,ueab6c,". I have a bachelor degree in English literature and linguistics. I took many courses in ml, Dl, and nlp. I have problems with writing codes. any advice?",MLQuestions,2022-04-28 18:16:59,2
Well written !,1,ue2f27,,MLQuestions,2022-04-28 11:59:23,1
If you pay they will play.,5,udzuim,"Hi everyone,

I am an associate at ProSapient, an expert network that connects clients with experts of different fields and industries. I have a client that is looking for software engineers without formal training but have  implemented a ML solution for a project or something. An example is a full-stack developer that gets a sudden update from their boss/client to add some NLP function to the backend, so they have to go online to find quick fixes.  


Does anyone know where to look for these people? Does anyone here fit the criteria? (DM if you do) Any machine learning search term gives me formally trained professionals, so it has been a tough search.",MLQuestions,2022-04-28 10:05:15,20
Yes,1,udvtwx,,MLQuestions,2022-04-28 07:07:05,2
"This is not enough data points to determine that accurately. E.g. there's way too many factors at play than simply having yes no based on aggregate. 

You're better off doing a beat line fit in excel and figuring out your growth based on a segmented time frame. E.g. analyze watch months growth or decline and use the best line fit to predict that. Or do a linear line.",2,udtcpl,"I Want to build a machine learning algorithm from 2 years data from (2020-2021) and also the 3 first months of 2022 , its about number of companies i've done a partnership with t them also i have a potential partnership : here is sample data frame , it looks like this

    Date     Company       Contrat          Contact  
    10/1/2020   comp1       signed       I contact  
    11/1/2020   comp2       Potential    I contact  
    13/1/2020   comp3       signed       They made contacted  
    17/1/2020   comp4       signed       They made contacted  
    17/1/2020   comp5       signed       I contact  
    17/1/2020   comp6       signed       They made contacted  1/2/2020    comp7       Potential    I contact  7/2/2020    comp8       signed       They made contacted  8/2/2020    comp9       signed       They made contacted    9/3/2020    comp10      Potential    They made contacted     9/4/2020    comp11      signed       They made contacted      9/5/2020    comp12      Potential    I contacted   9/6/2020    comp13      Potential    I contacted  ...                                 

I was wondering if it's possible to predict 2022's number of companies that i will sign a contract with, also the number of the interested companies which they are potential , which model i should use and how efficient the result is going to be ?",MLQuestions,2022-04-28 05:01:17,1
"Besides replacing with a mask you could reformat the sentence to a grammatical sentence that replaces the instance of the class word with the relevant article like “it” or “they”, might be pretty doable in an automated way if most of the sentences can be made grammatical but just replacing the class word with one of a few possibilities.",8,udego8,"I have a dataset with articles discussing either dogs, cats, or fish (mutually-exclusively). There is a ""giveaway"" keyword in each article class; dog, cat, and fish deterministically identify the articles. Is there a way to train e.g. a transformer model to learn a more ""nuanced"" understanding of the articles? I want it to be able to generalize out-of-sample where dog, cat, or fish might not be mentioned. In other words, I was hoping it could learn about the context, e.g. discussing ""water"" for fish, ""scratching posts"" for cats, or ""fetch"" for dogs. Even a transformer model does quite poorly on this, as it ""catastrophically forgets"" that the context of words surrounding dog, cat, and fish can help, because it never needed to use context during training. Any ideas on this, or something I could read up on, would be much appreciated.

At present, I am just applying `<mask>` in place of the keywords and training on that, and also randomly masking 15% of transformer-model tokens during training. I'm also using dropout = 0.35, and layer normalization. Should I add weight\_decay to an AdamW opitmizer? Any other ideas to force it to learn things beyond dog, cat, fish?",MLQuestions,2022-04-27 14:31:10,5
"As the neural network is basically a math function, the gradient descent calculates the derivative, that is the slope of the function at a point (the input values). So the slope is the steepest ascent. Then you put the minus to make it descent. 
That is the main idea, but it is a bit more complicated as you have a batch of inputs, so need to minimize the error for all inputs and not just one. And in order to improve the convergence with the whole dataset you use things as momentum which takes into account past gradients to apply the new one, etc.",3,udldlf,"Fact: We all know of a fact that Gradient Descent marches towards direction of Steepest descent. That is the reason we do W = W - alpha(dW) where dW = dJ/dW => Gives us new W in ""steepest"" direction in which we will decrease (-ve sign) the cost function (J) with respect to W(weight). 

Question: How come slope at a given point is the direction of ""steepest ascent""?  I dont know two parts to it: How come its ""Ascent"" and How come its ""Steepest""  guaranteed?",MLQuestions,2022-04-27 20:13:34,2
[deleted],7,ud0ot3,"This is probably a very dumb question, but I have no background at all in this area, and I've never really understood how research papers work. I watch almost every Two Minute Papers video and I see so many amazing papers, but I never actually see any of them available as a released tool, even a year or two down the line. The only way I see some papers can be used is by setting up the code from GitHub on your own computer, but it's not really accessible to anyone that doesn't exactly know what they're doing. And I see mentions of training your own models for the provided code, but doesn't that require thousands of images and a GPU that can crunch them over a couple weeks?  

[This OpenAI paper from over a year ago](https://youtu.be/-6Xn4nKm-Qw), for example. I couldn't find any released product/tool that would allow you to use it for your own images. Why's that?",MLQuestions,2022-04-27 03:49:15,4
"Swarmplots, boxplots, etc.",1,udegu5,title says it all I think. Thanks for the help.,MLQuestions,2022-04-27 14:31:22,1
"If I understand well, you want to solve a regression task. (C)nn models used for classification can be used for that with small modification for output layer that would be linear instead of softmax.",3,ud8o3p,"I’m new to image processing/computer vision. I have a data set of tabular data with an image file associated with each row of values. I can find plenty of examples of image classification, segmentation, etc using CNN’s, where a model is trained to determine which pre-defined class an image belongs to. However what I would like to do is to create a model that accepts an image and train it to predict one or more of the numeric values from the data row associated with that image. Basically something similar to those video-based thermal scanners that output a person’s body temperature. 

Im wondering what would be the best approach to implement something like this, and if anyone can recommend any related reference materials?",MLQuestions,2022-04-27 10:14:37,1
I assume the paid API based options are out of the question?,1,ud5irq,"Hello, I'm a layman interested in speech synthesis (tts) based on deep learning. I need a tts functionality at a program I would like to make, but currently I'm having a difficult time since I do not know about speech and machine learning area.

Since the last few days, with 2k recordings which I had made, I have trained Tacotron2 + WaveGlow model (implemented by nvidia) by 20k steps and 50k steps, respectively. After the training, I listened to the resulting tts sound and it was a delight that I could hear some pronunciations which could be understood. However, unfortunately, in case of a little bit lengthy sentence, tts took approximately 5 seconds to infer the waveform from the text and I couldn't accept it because I need a real time tts functionality.

Meanwhile, after a little bit of internet search, I got to know a model named FastSpeech 2 and I tried to read the paper, but there was a problem that I couldn't understand it due to my lack of background knowledge. Of course, because there are already some implementations on the internet, maybe I would manage to use them somehow superficially, but I think it will be difficult to use them deeply since I don't know the inner mechanisms.

So what I would like to ask here are (in terms of tts):

1. What I need to learn in order to become of a level that read a paper and implement a model? Or this is hard, a level that read a code of an already implemented model and fix/add something I need? (I really would like to know what is 'alignment' and what is 'attention')
2. How can I know research trend and state-of-the-art technology?

Thank you very much for reading this long question.",MLQuestions,2022-04-27 07:57:45,1
"Hey, have you solved it? I NEED to get the audio files...",1,ud4fvd,"it seems the officia link is not avaliable.

How can I ge the videos...

thx",MLQuestions,2022-04-27 07:08:12,1
"Linear algebra, multivariate calc, probability",1,ud3w41,"Hi, I've studied courses of maths in uni, now I wanna start to study ML on the surface level but I do not know which topics/ concepts to review before getting into it. Can you give me a summary of what I need to review? Thank you.",MLQuestions,2022-04-27 06:42:52,1
you can simply use regression decision trees in this.,1,ud8913,"Hey there

I have a small dataset with a few hundred rows. An example of the rows are as follows:

|dogs\_name|dog\_breed|dog\_size|dog\_color|
|:-|:-|:-|:-|
|woofy|golden\_retriever|large|golden|
|scruff|labrador|medium|black|
|wolfie|poodle|large|brown|
|barky|golden\_retriever|medium|golden|
|trevor|golden\_retriever|medium|golden|

&#x200B;

My goal is that a user will have anywhere from 1 to 5 existing dogs in their collection (ie, Woofy, Scruff, Barky) and then it would recommend Trevor and some other similar dogs as it could see that the user typically likes medium to small sized golden retrievers.

So a sample input might look like:

\['woofy', 'Scruff', 'Barky'\]

and a sample output might look like

\['Trevor', 'another\_dog\_the\_user\_might\_like', ''another\_dog\_the\_user\_might\_like', 'another\_dog\_the\_user\_might\_like'\]

I think it's quite simple to build however I'm unsure of how to approach and break this down.

Any general direction or specific steps for how someone would approach building this would be appreciated.

Thanks",MLQuestions,2022-04-27 09:56:43,2
It will depend on your problem. Which variables exactly are the inputs of the ranking and what is your goal for the ranking. Without more information I would only suggest to average the variable and sort the students by that score.,1,ud1nya,"Hi! I don't know if there is a simple answer to this question but if somenone could help me I'd really appreciate it. Let's say I've got some metrics (i.e metrics about students performance) and I want to use them as variables in a formula/function to build a ranking that allows me to sort them from top to bottom (at the top you've got the best and at the end, the worst). My question is which criterions should I use in order to build that ranking formula. Are weights assigned arbitrarily? Should a variable be the power? May be someone could recommend bibliography for me to read about this or can guide me in some way. Nevertheless, thanks for reading!",MLQuestions,2022-04-27 04:48:15,1
"First things first: if you take a large sample of ML papers published in the last ~6 months, there will be a broad spectrum of mathematical rigor / sophistication / relevance. The reality is that our mathematical tools for a lot of deep learning are just not up to the task to really describing how or why our models work the way they do, so there is a lot of valuable and successful research that is done without it. On the other end, there's also plenty of valuable and successful research that's done trying to add it. This plays out in (sometimes bitter) fights about whether ML is science or engineering, but for you all that matters is what sort of work you find exciting and that you want to do.

All that aside, let's say you really want to get your math up to speed. The first thing you should do is ask your advisor. At my school there was an intensive grad course that did probability and statistics with full rigor, but very quickly and with an eye towards applications. Find someone who knows the options at your school best.

If you want to self-study, I recommend books like __Linear Algebra Done Right__ by Axler, which is a good book for a proof-based approach to linear algebra and Spivak's __Calculus__, which is a very rigorous approach to calculus. That you (likely) already know a lot of linear algebra and calculus is precisely the point--it will be much easier for you to focus on the rigor when you already have a solid intuition for the content.

Where you go after that is tough to say, because it will depend a lot on your research direction. It is also really easy to get lost down mathematical rabbit holes. I studied math in undergrad and have a pretty solid math background, but I constantly get tempted to learn about some new corner of math when I see it used in some cool paper. Eventually we all need to focus and actually try and do some machine learning. Again, it makes sense to talk to your advisor or your peers instead of randos on the internet. Good luck!",7,ucebd3,"Good day to everyone.

I am a first-year PhD student in machine learning. I have a background in Electrical Engineering. I've studied probability theory and statistics in the past, but the level of proficiency required to even read (let alone write) some of the maths in DL papers is extremely daunting. I've been reading a lot of papers recently, but it seems like most of the math is going over my head. I was looking to re-read all of the fundamentals of probability and statistics from the ground up, so I decided to give Prof. Tsitsiklis' lectures and book a try. I haven't finished it yet, but it appears that even after coving a lot of it, the amount and level of math found in ML papers is staggering, and to be honest, it scares me a little. However, I am not going to give up, and so I request the masterful people of this community, that is you, to kindly provide me with some guidance. I need to know how to raise my level of mathematics so that I can eventually contribute ML papers myself (through theorem and proof writing).

Thank you.",MLQuestions,2022-04-26 07:51:07,12
"You can use blob storage on your cloud provider of choice and retrieve those files via your fast api.

Edit:
Also with less than 5 gigs that may be fine for docker image to handle. I’ve used one before that was closer to 50G to hold NLP libraries. It’s obviously not ideal, but it can work.",1,ucfbcq," 

so I have a text classification model saved along with the variables, and I have imported it to a fast api server(local files ofc) and dockerized the whole image which is like 4.46 gb

so what my question is, is there anyway I could keep the model files separately where i could just use the link or some import code to the fast api main file and then just dockerize the api files

I am a complete noob in this department, any explanation would be helpful

Thank you",MLQuestions,2022-04-26 08:34:26,4
"If you work though https://mml-book.com/ first and then https://www.statlearning.com/ (both freely and legally available online), you will have a solid basis on par with BSc AI students. Make sure to do the exercises.",13,uc3l6t,"Hello there, university student here. 

I am interested in ML and would like to pursue grad school on something related. As of now optimization sounds the most appealing and in line with the math I like. I will be doing some research over the summer and taking math courses (Multivariate Calc, Lin Alg II, Stats) in hopes of getting a better understanding of the fundamentals. I would also like to start reading academic papers to:

1. See if this is a field I actually like, and,
2. If it is, what subfield am I interested in the most

I plan on self learning the majority of the math concepts, and I believe I have the motivation to do so (from past experiences with other topics). I am aware that different subtopics require different types of math, but what would be a good starter?",MLQuestions,2022-04-25 20:44:46,6
"Have you tried PCA (principal component analysis)?

What you have done is basically apply some knowledge of the domain to feature selection. It applies to your problem but probably won't apply to others.",1,uci0uf," I am currently doing some research and as part of one of my projects, I had a dataset comprising of approx `200x200x200` data.

The research is partly to do with computational resource and so dimensionality reduction and feature selection is something I need to write about.

To reduce the cost of the data, I applied a weighted average across the third axis, using a quadratic weighting such that the outer slices have weight ≈ 0 and as you progress to the center, the slices are quadratically increasing in weight. This is because the outer slices usually have less relevant information and the central slices are where the main features that the model will be able to learn from are located.

The reduction worked and my model achieved sufficient accuracy but I was just wondering if the process that I applied has a name of any sort? Can I class this as having applied feature selection / dimensionality reduction - and does it come under a type of feature selection?",MLQuestions,2022-04-26 10:34:31,3
"This is due to a misnomer between intelligence as we intend it for humans, and artificial intelligence. The latter has to do with logic and maths more than with the way in which biological brains function, even though admittedly the human brain has always been a source of inspiration for the development of AI systems.

A neural network is essentially a function of the form y = W X + b. Would you ask yourself the question as to whether this mathematical function is capable of emotions?",4,uch08j,"I'm confused. An AI  can't feel emotions. Yet, they're made up of a bunch of nodes and some electricity.

So are we. What's the difference between the neural networks in our brains And that of an ai?

Why cant an AI feel emotions? Why isn't it conscious? If electricity moved around a structure in the right way, why can't it be considered 'alive'",MLQuestions,2022-04-26 09:49:04,14
With what kind of data?,1,uc8gzf," I was having a few problems with this topic (I'm doing my PHD) and my advisor couldn't help me. He suggested to look out in the internet, as no one is working on this in my university. So, here I am Reddit, anyone willing to help a little with this?",MLQuestions,2022-04-26 02:22:14,3
"The data is text. Your input is text and your output is text. So you can set as an input a partial test and try to predict the rest. The difference between your prediction and the real words (or vectors) is the loss. 
That is the basic idea, then you can add special tokens to your input to decide what type of prediction you want, whether to complete the sentence, translate it (if your dataset has the translation), change the gender of the sentence, or the emotion, or whatever you have.

In order to have a metric for your data you need to use some information about your problem to create that metric. Then you can use it for early stopping to prevent overfitting. This metric depends only in the problem you want to solve and you have to think about it by yourself.",1,uc1fsl,"1. What is the training loss in this case? The data doesn't have any label nor the task is a classification one, yet how does the model calculate this loss?
2. I have a text dataset of around 300MB (or 600k samples) for finetuning. I have only trained for one epoch at a learning rate of 1e-3. The generated text of the finetuned model was pretty good however I'm not sure if it could be even better by training for more epoch, or use more new data (not iterating over the old data), or lowering the learning rate.",MLQuestions,2022-04-25 18:51:11,1
"MLops, or just ""software engineer"". Sample job posting here:

https://careers.twitter.com/en/work-for-twitter/202011/34b91e84-fbc0-44b6-993c-575faf989e37/f0424ea3-06eb-4b80-a98d-883bab61d7e6.html/sr-software-engineer-cortex-platform-infrastructure.html",3,ubipl6,Basically Title. Thanks!,MLQuestions,2022-04-25 04:15:36,8
"HDBSCAN's library has an entire section in their docs about extracting hierarchy/ cluster information: [https://hdbscan.readthedocs.io/en/latest/advanced\_hdbscan.html](https://hdbscan.readthedocs.io/en/latest/advanced_hdbscan.html)

But why do you want median cluster values? The big benefit of hdbscan over more traditional clustering, along with the explicit assumption of noise, is that density and boundaries can vary.",2,ubtzrs,"Good day,

I once again get in contact because I still haven't figured out how to solve this problem that I have with HDBSCAN. As I stated before, I'm currently working with a dataset of stars located in a certain star-forming region. The region and the details aren't of importance here, what I'm trying to accomplish is identify stars clusters using HDBSCAN. I can identify them by sight but I want to verify this selection using algorithmic methods.

I've already identified said clusters but wanted to standardize my data so as to have a more robust selection of them. So I wanted to know if it's possible to determine the number of points of each cluster and as well get the values for each point, so to determine the median of each cluster.  

I'm pretty new to HDBSCAN and will gladly take your advice and help.

Thank you very much in advance to all of you.",MLQuestions,2022-04-25 13:00:48,1
"Looks like the command line argument is `--data`, not ""dataset"".",2,ubtohz,"So recently I started to do computer learning and this message popped up, I looked it up and found no useful information. Does anyone know how to fix this?

https://preview.redd.it/l3fq3xpmbqv81.png?width=2560&format=png&auto=webp&s=43c7c55daca0bb8645e2a3fc4b05e80dc7dc93e0",MLQuestions,2022-04-25 12:47:56,1
"It's a little hard to say without knowing the details of your project. But you should look into models like BERT and GPT 2 to generate synthetic text based off your prompts.

It might take some time for your to get familiar with the models but it may be worth the invested time for your project.",2,ubmz6c,"I am working on a multilabel classification problem where there is significant class imbalance. I've augmented the loss function accordingly to account for class weights, but I also wanted to explore generating synthetic data for the under-represented classes. To this end, it seems like most people recommend back-translation (e.g. translate English -> German -> English) and use those as new instances.

What's recommended here? After I back-translate a document once... that's all I can do in terms of back-translation, right? If I wanted to go the route of back-translating, is there some way to create more than one back-translated version of a document?",MLQuestions,2022-04-25 07:54:28,4
"The classical approach to solving exactly this problem was to use Markov chains at the syllable level. Make a set of common two or three letter combinations, and cycle through them with a Markov chain.

Let's say there's a language with three syllables:

Sa/Ma/To

And for each, the probability of that syllable being followed by X syllable is:

Sa /  followed by Sa: 10%, followed by Ma: 20%, followed by To, 20%, followed by word end 50%

Ma/ Followed by Sa: 15%, folllowed by Ma: 25%, followed by To, 10%, followed by word end 50%

To/ Followed by Sa: 20%, followed by Ma: 15%, followed by To, 15%, followed by word end 50%

And the odds of a word starting with any are equal. We let it run for a bit and get, say:

Samasa sato toma mamamatosa

This is actually what Markov chains were invented for. The guy who invented them codified probabilistic rules like this for Russian, and than used random number generation to write poetry. **Because syllables in the generated text have the same frequency in Russian, and the same conditional probability of following other syllables, the words look like Russian without being Russian.**

Language generation- at all levels, letter, syllable, word and phrase- is a very common application of Markov chains, and would probably be perfect for what you're talking about.",8,ubnwz9,"I would like to create a model that is capable of generating gibberish based on one language. Not the gibberish like 'skdjfbnskj', more like words that are ""near"" real ones, but arent actual words.

&#x200B;

Would a GAN be able to do this? would i use embeddings somehow? 

please advise",MLQuestions,2022-04-25 08:36:03,4
"So in the context of convolutions, deeper layers get a few benefits.

The first, you mention, is that it is easier to express complex shapes as a function of more fundamental ones - a ball is a function of a curve, a house is a function of lines, etc. Sure, you could express that as a function of a singular convolution but 1) it’d be overfit to whatever a ball or house looked like (imagine American football vs soccer ball) and 2) there are far too many shapes to describe them all individually starting from the ground up each time (you spend a lot of capacity in those convolutions defining a line over and over again).

Second, (when using pooling), later convolutions can “see” more of the image. A 3x3 => max pool => 3x3 setup, for example, allows that second convolution to, effectively, see a 9x9 patch of the image since the pooling reduces the input size from the first convolution to the second. Again, this helps build an understanding of features that aren’t all just local to each other (a nose pixel is > 2 pixels away from an eye pixel, for example).

Combining the two, you can get a sense for what the increasing number of filters allows you to do: the lower layers optimize to find features common to all images while later ones combined those features to identify more complex structures (a face, a building, etc. - gross simplification here).

Hope this helps!",19,ub7t57, Online resources say that first layers detect edges and all which are very much finite and then as we go deeper and deeper we learn complex patterns so we should increase filters but I just dont get that? Can someone please explain it in a more detailed fashion please. **Also would love to see if there is any visual explanation/proof/article that clearly indicates that increasing filter as we go deeper is a clear winner.**,MLQuestions,2022-04-24 16:58:16,1
"Okay I think I've figured it out. In this formulation we're trying to solve a maximum likelihood problem to find the right distribution for t with additional constraints in the form of a prior.

Reducing the variance of the distribution t (by increasing beta) forces the means (our estimate of the target y(x,w)) to be as close to the target as possible. This essentially leads to our model trying to fit exactly to the training data and in a way lowers the regularization.

Edit: [This diagram](https://ibb.co/KFgyGVQ) should help in visualizing the effect of beta",1,ubf5ro,"I was reading up on Bayesian curve fitting in Pattern Recognition by Bishop and came across this formulation. Beta is the precision parameter for the model of the target t and alpha is the precision parameter of the prior distribution of w.

https://preview.redd.it/gnoo6sh1jmv81.png?width=928&format=png&auto=webp&s=b006c21d2b7e18461ecc48a2d7d70f5aeb0bfa6b

I'm trying to get an intuition for what it means to vary alpha and beta and the subsequent effects on regularization. Here is what I understand so far:

* Increasing alpha leads to a more precise and restrictive prior. This reduces the range of parameters w which would be considered and therefore increases the regularization. 
* Increasing beta leads to a model for t which has lesser variance and this *reduces* the regularization weight. This is what I can't make sense of because according to what I know **lower variance -> less flexible model -> greater constraints -> more regularization.**

I'd be glad if someone helps me out with this, thanks!",MLQuestions,2022-04-25 00:09:19,2
"The model actually won't cheat even though you show it the answers. The reason is you're not training it on the unmasked tokens, you're only training it on the masked tokens. So you should randomize the masks every time.",1,ub785l,"I want to add additional pre-training to a BERT model. Currently, I have it so that the PyTorch Dataset's `__getitem__` method randomly masks 15% of tokens for the instance. This means that during training, every time the model looks at an instance of training data it is observing a different random masking. I thought this would be useful, but now I'm worried that it might just be allowing the model to see the ""answer key"" for which word belongs in the <mask> position. 

So, is it preferable to instead apply the masking to the input sequence *before* training the model, and hold it fixed throughout? For the validation set - is it okay to randomize the masking still? I figure this would lead to nice properties in the sense that it's tested on ""new"" validation data every epoch.

Thanks for any thoughts you might have!",MLQuestions,2022-04-24 16:26:59,1
"[U-Net](https://arxiv.org/abs/1505.04597) should do what you want, but you'd need to train it appropriately. Finding datasets is going to be hard, building one will be time-consuming.",4,uakus9,,MLQuestions,2022-04-23 19:33:56,6
"It's not really about parameters and usage costs. 

Google's revenue stream is ads. They make money on all their services that show ads, which they use to fund their AI development and free access to it.

OpenAI's revenue stream is from licensing the AI models they develop directly.",10,ua17qi,,MLQuestions,2022-04-23 01:57:10,1
"If BERT does your job, why not use it? I’d suggest exploring a bit around Huggingface and check if they have fine tuned bert models for QA.",1,uaa1q9,"The title says it all. For little more context, I am not so advanced in NLP nor have I done a project something like this, but one methodology that I've found is using BERT. For someone who is just an beginner in ML like me, should I try using BERT, or if not, feel free to give me suggestions on what can I use for making the QA system.",MLQuestions,2022-04-23 10:10:51,2
according to [this article](https://medium.com/@selfouly/r-cnn-3a9beddfd55a) you basically use the second to last layer of your network to get a vector for each region proposal and then train an SVM classifier on these to get a confidence score for each class.,2,u9wart,Hey so I have build and made a functioning CNN but I don't know how to turn it into a R-CNN. It's my own custom model so no ResNet50. I wondering if there are any articles that could help me. Also if they document how they format there data would help as I think that maybe one of my problems. Thank you.,MLQuestions,2022-04-22 20:31:41,1
Study Pandas library in python.,1,u9xw98,"Hi, I'm very new to ML and I'm currently studying the fundamentals such as maths, programming and some basics concepts of ML but there is a problem. Many times I got tired or bored of what I was studying after hours of studying but I always feel like I still had the energy to continue studying but I had no idea what should I do next. So I think I can can learn some important tools or programs which don't have many requirements of initial knowledge to work with ML in the future. Could you please suggest me some of them? Thank you.",MLQuestions,2022-04-22 22:05:57,4
"Few or single shot means a model can be trained on very few or just one sample respectively. k denotes an arbitrary number of samples needed to train a model for a new class/category. As such, most models are trained „many-shot“. 

For an initial understanding Wikipedia gives you an overview: https://en.m.wikipedia.org/wiki/One-shot_learning#:~:text=One%2Dshot%20learning%20is%20an,or%20only%20a%20few%2C%20samples.

A more scientific take on this: https://proceedings.neurips.cc/paper/2016/file/90e1357833654983612fb05e3ec9148c-Paper.pdf",8,u9js6l,"Can anyone explain at a high level what the difference is between the different “-shot” designations of learning models? i.e., k-shot, few-shot, single-shot. Also, how are these different from other ML methods that wouldn’t be called “-shot”?",MLQuestions,2022-04-22 10:18:46,1
"Maybe an active learning pipeline?

(1) calibrate your model so that its reported confidences are akin to pseudo-probabilities (you could use temperature scaling)

(2) set some threshold of confidence, under which your model flags the instance and queries the user / oracle for a label for that instance

(3) incorporate the new ""gold-standard,"" hand-coded instances into the training data and retrain the model (or if you need it to be quicker, train from your latest checkpoint, maybe with a 50-50 mix of the most recent training data & the new hand-coded data)

&#x200B;

Edit: Also, random sanity checks are good to include, I think. So maybe also randomly select from among the model's most confident predictions to be hand-coded. If the model is well-calibrated this should be redundant and kind of a waste of time; if humans are disagreeing with what's supposed to be your strongest predictions though, you know that the above loop is not protecting you from a shift in your domain",7,u9cc5v,"I have a made toxic classification model with 6 labels, and let's say this model successfully predicts and has an accuracy of more than 90%, but however an unfamiliar input is passed to the model how can we assure this particular model would successfully predict and keep improving via the active pipeline?",MLQuestions,2022-04-22 04:23:20,5
"The most informative logs will be on the server itself and not on the responses from api. If you run the server in one terminal and hit it with a request in a second terminal, the first terminal will be the one that gives you a stack trace, the second terminal will just say 500 internal server error. 

I'm pretty sure any response other than 500 internal server error, has to be explicitly programmed into the server error handling yourself.",1,u9kuwv," `from gc import callbacks`  
`from urllib import response`  
`from fastapi import FastAPI`  
`from fastapi.middleware.cors import CORSMiddleware`  
`from uvicorn import run`  
`import os`  
`import transformers`  
`from transformers import TFBertModel,  BertConfig, BertTokenizerFast, TFAutoModel`  
`from tensorflow import keras`  
`from tensorflow.python.keras.models import Model, load_model`  
`from tensorflow.python.keras.layers import Input`  
`from tensorflow.python.keras.callbacks import Callback`   
`from pydantic import BaseModel`  
`from fastapi.encoders import jsonable_encoder`  
`class ModelOutput(Callback):`  
 `def on_predict_end(self, logs=None):`  
 `keys = list(logs.keys())`  
 `return keys`  
`model_name = 'bert-base-uncased'`  
`max_length = 128 # max 512`  
`# Load transformers config and set output_hidden_states to False`  
`config = BertConfig.from_pretrained(model_name)`  
`config.output_hidden_states = False`  
`# Load BERT tokenizer`  
`tokenizer = BertTokenizerFast.from_pretrained(pretrained_model_name_or_path = model_name, config = config)`  
`bert = TFAutoModel.from_pretrained(model_name)`  


`app = FastAPI()`  
`model_path = './Bert_Dcnn_model/'`  
`new_model = load_model(model_path)`  


`def tokenization (input):`  
 `xy = tokenizer(`  
 `text=list(input),`  
 `add_special_tokens=True,`  
 `max_length=max_length,`  
 `truncation=True,`  
 `padding='max_length', # padding=True initial value,`  
 `return_tensors='tf',`  
 `return_token_type_ids = False,`  
 `return_attention_mask = True,`  
 `verbose = True)`  
 `return {'input_ids': xy['input_ids'], 'attention_mask': xy['attention_mask']}`  


`async def makePrediction(text):`  
 `if text == """":`  
 `return {""message"": ""No text provided""}`  
 `tokenizedValues = tokenization(text)`  
 `results = new_model.predict(tokenizedValues,batch_size=32, callbacks=[ModelOutput()])`  
 `return results`  


`origins = [""*""]`  
`methods = [""*""]`  
`headers = [""*""]`  
`app.add_middleware(`  
 `CORSMiddleware,`   
 `allow_origins = origins,`  
 `allow_credentials = True,`  
 `allow_methods = methods,`  
 `allow_headers = headers`   
`)`  


`class UserInput(BaseModel):`  
 `comment: str`  


`u/app.post(""/predict/"")`  
`async def root(comment:UserInput):`  
 `text = [UserInput.comment]`  
 `results = makePrediction.predict(text)`  
 `return {""prediction"": str(results)}`  
`u/app.get(""/"")`  
`async def root():`  
 `return {""message"": ""BERT Boi is up!""}`  
   
`if __name__  == ""__main__"":`  
 `port = int(os.environ.get('PORT', 5000))`  
 `run(app, host=""0.0.0.0"", port=port)`  


Any idea why this is happening? 

everything I run up the server and pass in a string through post, it says internal server error! 

I am new to fastapi, and I am still learning, I might have done a noob mistake, if so please let me know

and the model file I am using is saved in .pb format

Thank you!",MLQuestions,2022-04-22 11:08:35,1
https://stackoverflow.com/questions/38423387/why-does-word2vec-use-cosine-similarity,1,u9c78b,"

I've implemented the word2vec algorithm according to its negative  sampling architecture,using a shallow neural network that performs  binary classification on word-embedding vector pairs. The network is  expected to output 1 for pairs that occur in the corpus and 0 for the  random negative pairs. In the final neuron, my implementation calculates  the dot product of the two vectors before passing it to a sigmoid  activation and later down the line, cross entropy is calculated and  averaged over the batch.

My question is, should I use cosine instead of dot? I'm well aware  that they differ only by the normalization of the vectors, however, I  was unable to find a clear answer. Does this actually affect the quality  of the embedding vectors? Or the use of cosine vs dot only matters when  similarity is calculated for among the trained embedding vectors?",MLQuestions,2022-04-22 04:15:33,1
"That's a CMD command, and you are putting it into a python shell.

Also, remove the < and > tokens",2,u9bxmd," Hi all,

Hope you're all doing great and you're happy with your progress so far :)

I'm  trying to train a neural network in Pytorch to do a style transfer,  I've made a file with the code snippet found here to train it

[https://github.com/pytorch/examples/tree/main/fast\_neural\_style](https://github.com/pytorch/examples/tree/main/fast_neural_style)

However I'm getting this error, does anyone know why this may be?

python  neural\_style/neural\_style.py train --dataset  <C:\\Users\\callu\\Dropbox\\Programming\\pytorch\_styletransfer\_webapp\_streamlit3\\neural\_style\\images\\style-images\\postlightroom\\style1>  --style-image  <C:\\Users\\callu\\Dropbox\\Programming\\pytorch\_styletransfer\_webapp\_streamlit3\\neural\_style\\images\\style-images\\postlightroom\\style1\\style1\\style1.jpg>  --save-model-dir  <C:\\Users\\callu\\Dropbox\\Programming\\pytorch\_styletransfer\_webapp\_streamlit3\\neural\_style\\saved\_models>  --epochs 2 --cuda 1

\^\^\^\^\^\^\^\^\^\^\^\^

SyntaxError: invalid syntax

Many thanks in advance",MLQuestions,2022-04-22 04:00:06,2
"Def read the link the other guy posted. It's super helpful to derive vanilla Gaussian processes from scratch, and really not that hard if you just take the formula for a conditional multivariate Gaussian as-is. That said, here's a couple quicker comments to maybe give you a shortcut to getting more pleasing results while you study the theory in the background:

1. The ""length"" (`k` for you) of your SE kernel controls how correlated the measurements are, or conversely, how ""fast"" your prediction will regress to the prior. If you did want a zero prior (I know you don't, but for argument's sake), you could increase that `k` number to make the measurements more correlated and your data would have more ""momentum"". I know I'm using a lot of hand wavy terminology here, but that's sort of my goal, as this is a reddit comment lol.

2. It's trivial to include a non-zero prior. Take a look at the conditional Gaussian formula here: https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Conditional_distributions. You can plug in a non-zero mu1 and mu2 easily in your `kernel_posterior` function. I didn't like your x + 1 prior, and calculated my own linear prior from the data with `np.polyfit`. So something like this:

`prior = 0.15*X + 2.24`

`prior_p = 0.15*Xp + 2.24`

`mu_f = prior_p + K_fsf @ K_ff_inv @ (y - prior)`

(ugh I can't get stupid code formatting working, whatever)

Hope that helps.",3,u8opi8,,MLQuestions,2022-04-21 07:19:05,6
"Top one because the validation accuracy looks a little higher and it hasn't converged. So, it could be even higher. Although, from these graphs, it doesn't look much different, just maybe the learning rate is higher on the bottom one.",11,u8nb16,,MLQuestions,2022-04-21 06:09:44,8
"You might want to look into multilingual models like XLM-Roberta. Although they might not cover your language, they might perform slightly better.

I think the ultimate performance will depend on whether your sentence can be tokenized correctly, and what task you are trying to achieve. If it's a classification task then you will generally need fewer sentences compared to pretraining (MLM).",1,u8paip,"Hi!

Im a total beginner, but I want to train a BERT model (or similar) for translation. I speak a language with around 40'000 speakers, so the resources are really scarce. I started accumulating as much data as possible and I even have a few 100'000 translated sentences (from our gouvernment and I have yet to format them), but im wondering if the effort is even worth it. How much data do i need to make this work?",MLQuestions,2022-04-21 07:47:17,2
[deleted],1,u8pnej,"What's the best way to deal with this.

I got a lot of data in Excel/CSV format related to healthcare.

The columns or rows generally change the position based on the vendor or use case. I want to extract the key-value pair from the CSV.

Has anybody worked on these types of projects?

ETL is hard to do on this type of data due to the varying nature of the headers and the header names.

Edit - adding link

[https://docs.google.com/spreadsheets/d/1oiZEVaVqisZfHADgvbchVxzJr2nvCiyh22fG5QyEkqA/edit#gid=0](https://docs.google.com/spreadsheets/d/1oiZEVaVqisZfHADgvbchVxzJr2nvCiyh22fG5QyEkqA/edit#gid=0)",MLQuestions,2022-04-21 08:03:30,3
Apparently it’s a VSCode indexing problem. I get the same message without pylance.,3,u8irjb,,MLQuestions,2022-04-21 01:30:09,3
"not really a ML question, but if that `np.array` is x, y (2 dimensional), then you should index with `[rect_min_x:rect_max_x, rect_min_y:rect_max_y]`. difference is in using : instead of , between your min and max y values.",3,u8hsjs,"I am making a ML project along with Pygame

I have an IndexError which I do not know how to fix. Can anyone help me with this?

https://preview.redd.it/hhu60kjt1uu81.png?width=1131&format=png&auto=webp&s=ab21c70a7f67b8f860250f803647c0cca51b143d",MLQuestions,2022-04-21 00:16:57,1
"I usually use miniconda but anaconda is fine if you are not low on storage. Anaconda has like a 30 gig footprint which I think is absurd. 

You could install python3 directly and use virtualenvs but it's not as smooth of an experience as using conda.

There's no right or wrong approach here, just whatever you're more comfortable with.",3,u82zga,"Might seem naive, but have been using Anaconda and started following this article and saw that they recommended  Python3 directly from Ubuntu packaging rather than Anaconda.  I do know many packages are slow to reach anaconda and are found quicker on Ubuntu but just wondering what you use for your in-house needs (or personal research setup)",MLQuestions,2022-04-20 11:24:28,18
"Just think through exactly what you said. What do you start with, what's the process, what do you end with in both these things?

If you're doing unsupervised clustering, you've got a bunch of datapoints you know nothing about and you're going to cluster according to some criteria, e.g. k-means. You end up with a bunch of different groups, your classes.

Now please explain to yourself how unsupervised classification is different from this.",3,u7wwhe,"Hi.

I'm asking this question for my thesis, looking around i've read that saying unsupervised classification is the same as saying unsupervised clustering.

Can you help *formalizing* why this can be said that is true?

Thanks.",MLQuestions,2022-04-20 06:46:34,4
With polinomial regression you could discover relationships between features too,1,u7liut,"I'm relatively new to ML.

I'm learning about regression models. When I read about polynomial Regression, this doubt arose in me.


¿Does every feature has to have an exponential relationship with the label? ¿Is it possible to use Polynomial Regression other way?",MLQuestions,2022-04-19 19:00:07,2
"If you have enough data, then the validation set in split #1 should be enough to represent your training set and real world data. I therefore think you still should stick to split scenario #1, since it would give you a better estimate of how good your final model will be when you eventually train in on the full dataset.",4,u794pf,,MLQuestions,2022-04-19 09:26:02,1
"I use the joy cons but that works too ig, u would probably get less drift I would assume",0,u79x8p,"Hi everyone! I am currently trying to make a bot that can complete a lap on Mute City in F-Zero for the SNES. I am using gym-retro as my environment to do the training. As of right now, I am using PyGAD as my GA to train a multi-layer PyTorch CNN to control the bot, instead of using a reinforcement algorithm like PPO2. The output layer of my CNN uses softmax as it's activation and outputs array of length 4, representing the CNN's confidence it should perform specific actions (accelerate, break, turn left, turn right). Each individual's fitness is scored by their total reward after its session is finished. The bot is rewarded when it stays on the track, and is making progress towards the finish line. I want the bot's only input to be the current frame of the game.

I thought that since the bot will only be running on the same track, this could be seen as a classification problem. The CNN will learn which actions are good for what frames. However, I am not having good results.

The problem I am having is that the bot is stuck making the same move and not really exploring other possibilities. For example, the bot will drive forwards (getting rewards and increasing it's fitness) until it reaches the first turn, and then will continue driving forwards, missing the turn and going off the road. Does anyone have any ideas on how I might fix this? Or if they've run into anything similar? Please let me know if any more info is needed",MLQuestions,2022-04-19 10:01:00,1
"I've been studying data science, math, and machine learning for about 1 year now, and have put about 500-1000 hours in (large range since I also spend a lot of time studying for my role as a resident physician and measure hours in the same tool). You don't just need to learn the math and algorithms, you need to learn multiple entirely new skillsets; but, start with the math and algorithms :)

- If you can do basic python (numpy, pandas, loops, if/else, build a class with methods/attributes) then skip computer science and come back to it at a later time otherwise do it first.

- Start with Ng courses they are very good and cover everything you need. Expectation is to get an initial grasp of a lot of different things. This doesn't make you an ML engineer, it gets you started. A lot of this stuff takes many repetitions and projects to understand well. Using Octave in the first course is kind of weird, but it's not a big deal and the language does show matrices cleanly which is good for learning linear algebra.

- Math is a slow burn, linear algebra is a must, but the rest of it depends on your life goals. If you really want to know math, then do a proofs book (Chartrand) along w LA. Get a Chegg subscription so you have answers to all the questions in the chapters of whatever books you use. 

Finding ways to apply what you learn and building adjunct skills is essential.

Slowly work on

- Effective pandas (Harrison)

- Learn SQL (DeBarros book + CodeSignal practice problems)

- Learn regular expressions (regex101.com questions are good)

- Read book on how to visualize data

- Learn matplotlib. Not a lot of great resources on this, I literally just remade all the graphs from the book ""Better Data Visualization."" I'll say, it was a STRUGGLE - but now I got it :)

- Sign up for AWS and Google Cloud Services and learn how their services work. There are some good course courses I've been looking at to get better at this myself.

- Listen to a bunch of ML/DS podcasts 

Life goals really matter here. Without background you're in for a long haul here. I'm about 1 year in, and have grown tremendously, but I still have so much to learn. I'm expecting that it'll take about 3-5 years of constant work on this (probably about 2500 hours) to be competent. My definition of competent is: able to develop and deploy multiple different model types along with evaluation, production monitoring, and iteration.

Studying online courses for hours per day can be hard, it's very active engaged learning. I've found 6 hours on days off and 2-4 hours on work days is a nice middle ground. I usually read 2 hours, work on math for 2 hours, work on ML courses for 2 hours. I've had a couple of nice work related data science projects that I fully commit time to when they come up. I always apply methods to my own datasets and build my own implementations alongside the coursework. 

8 hour days were not working out well for me from a balance/guilt perspective. I've done this will being a resident physician working many 80 hour weeks, so you can definitely fit this in with the rest of your life. The caveat is, it really must be a priority. I think it's actually a great idea to start slow and tickle away at it for a few months. Then, if you like it, you can ramp up.",12,u6l4bn,"Hello! Machine learning sparked my interest, and I'm ready to dive in. I have some previous programming knowledge but I basically start at zero in data science. So naturally, I don't really know where to begin this journey. I've researched for resources and roadmaps to learn machine learning and created my own basic roadmap just to get started.

**Math - 107 hours**

* [Single-Variable Calculus - MIT](https://www.youtube.com/playlist?list=PLE2215608E2574180) \~ 29 hours
* [Multi-Variable Calculus - MIT](https://www.youtube.com/playlist?list=PL4C4C8A7D06566F38) \~ 29 hours
* [Linear Algebra - MIT](https://www.youtube.com/playlist?list=PLE7DDD91010BC51F8) \~ 28 hours
* [Statistics & Probability - MIT](https://www.youtube.com/playlist?list=PLl8XY7QVSa4aUyZAtL2Hlf_mx3LaSix9B) \~ 21 hours

**Programming - 135 hours**

* [Introduction to Computer Science and Programming Using Python](https://www.edx.org/course/introduction-to-computer-science-and-programming-7) \~ 135 hours

**Machine Learning - 200+ hours**

* [Machine Learning Specialization (Andrew Ng)](https://www.deeplearning.ai/program/machine-learning-specialization/) (release June)
* [Deep Learning Specialization (Andrew Ng)](https://www.coursera.org/learn/neural-networks-deep-learning?specialization=deep-learning&irclickid=Xa1wyCQHuzYeRMESFSRiQXN3UkGXVIyn4RCEVk0&irgwc=1) \~ 142 hours

Please give comments on it and or advice on better/more efficient ways to learn. Thanks!",MLQuestions,2022-04-18 12:20:04,28
"didn't we already do this yesterday? if you're going to repost your question the least you could do is link to the previous conversations. kind of disrespectful to the people who contributed time and effort to trying to help you already.

EDIT: Also just so this doesn't come off as butthurt, you're also depriving people in this new thread of valuable context.",6,u781t1,"&#x200B;

If I do the loss is not going down, I think it is because different data in my set could have large difference in loss?",MLQuestions,2022-04-19 08:38:35,16
* do u have 'training loss' vs 'validation loss' graph?,6,u6v2ow,"&#x200B;

https://preview.redd.it/ont79x1ajeu81.png?width=424&format=png&auto=webp&s=edfb08992eda9ce8a49a4f5cb50356af100aa03d",MLQuestions,2022-04-18 20:06:02,6
"""keeps lingering forever at values around 2""

Sounds like convergence to me",2,u729tx,"I am working on NAS training code.

May I know why [the loss](https://github.com/buttercutter/gdas/blob/b7f53c1d7d5c51597f1b8ed06aa3a7d329a4ff0d/gdas.py#L897) keeps lingering **forever** at values around 2 ?",MLQuestions,2022-04-19 03:50:21,3
"That will be no problem. I finished my Master’s program with a 6 year old Mac mini. You create what you want on the computer you have, and run the models on a cloud service",4,u6zlf8,"Hello everyone, I'm wondering how far can I go on ML with that limited computing power I have in hand. Would you please tell me what I should expect to be able to do with those 2 devices? Thank you",MLQuestions,2022-04-19 00:44:07,7
"You're probably looking for this: https://madewithml.com/#mlops

some more targeted resources: 

* https://docs.pytest.org/en/7.1.x/getting-started.html
* https://pre-commit.com/
* https://pre-commit.com/hooks.html
* https://cookiecutter.readthedocs.io/en/1.7.2/
* https://kedro.readthedocs.io/en/stable/",2,u6w8k8,"How to incorporate software engineering ideas into DL/ML code to improve code quality? 

any MOOCs ? 

training code, evaluation, deploying models etc should always use OOP ?",MLQuestions,2022-04-18 21:09:06,1
"can you show some code or explain a little better? not sure what you are saying. normally what you will do is shuffle your data and then batch it out. you'll train on each batch sequentially and at the end of these batches that will be one epoch completed where you can calculate some performance metrics. then you'll reshuffle the data, re-batch, rinse and repeat.",1,u6jlsj,,MLQuestions,2022-04-18 11:14:16,13
"- Step 1: only use dow and no other variables.  
- Step 2: verify you have dummies and not one hot coding. (i.e. realize your reference category is encoded in the intercept). 
- Step 3: validate that your model can exactly predict back the b_metric per day.",1,u6o3ht,"I'm currently using a Logistic Regression model to build a basline framework for a classification problem that I have. My data consists of many features, but the relevant ones for this problem are a certain binary metric (i'll just call this b\_metric for now) and day of week (dummy variables for each dow). My goal is to predict an accurate probability for b\_metric.

 The issue that i'm seeing is that b\_metric has a mean in the 90s for Sunday, but very low for every other day of the week. However, it looks like my model isn't catching this, and the coefficients indicate that its basically just learning that the b\_metric is usually low (basically, its predicting a low probability for Sunday, I think because the b\_metric is low for every other dow). What would help my model gather an understanding of separation based on day of week so its able to predict higher (and more appropriate) probabilities for Sunday? Interaction variables? A different algorithm?",MLQuestions,2022-04-18 14:31:02,3
"This is not a book, but a repo with “ML from Scratch” https://github.com/eriklindernoren/ML-From-Scratch#implementations",3,u6o2m7,"I'm looking for a particular book written in Python that takes a less black box approach to ML. One that walks through popular ML algs (linear regression, logr, kmeans, etc) without using the ML packages like Scikit/Sci-Py and instead uses a more algorithmic approach with linear algebra and numpy.  


Any suggestions?",MLQuestions,2022-04-18 14:30:00,1
"How about:

>  KMAE: Profiling network users traffic behaviour using K-means (KM) with ad hoc feature selection and autoencoder (AE) with clustering optimization.

Everything doesn't have to be a catchy brand-name. Especially in the context of academic research, there's absolutely nothing wrong with naming your thing just in accordance with what it does. 

Also be advised, the acronym I suggest here is actually a bad acronym and you shouldn't use it. ""MAE"" is already an overloaded acronym in ML: masked autoencoder, mean absolute error.",2,u6ie0x,"Hi everyone I'm doing this project for a paper in which I profile network users traffic behaviour.

I have used firewall logs, and the folding machine learning site I've been using K-means with ad hoc feature selection and Autoencoder with clustering optimization.

I would like to find the some distinctive name, like ones I've read from papers around in which the name is at the same time a cool word and an acronym that indicates the research work I've done.


Can you give me some suggestions?

Thanks.",MLQuestions,2022-04-18 10:21:26,2
"How much more work is it to write a few manually engineered rules and use that as a baseline?

Why is it necessary to commit to a solution before starting work",1,u6hpsz,"The question is quite straightforward, I have a list of emails and a binary target variable by which I want to classify them.

One option is to manually generate new features, that is, for example, how many numbers or unique characters each email includes.  

Another approach that comes to my mind is to use an existing architecture that knows how to handle one word. I guess we could look at characters in an email as a time series, but as the number of these characters is small, it wouldn't make any sense of using transformers/lstm or even the simplest RNN, right? 

On the other hand, looking at these names as time series has some reasons too, they could be a part of a vocabulary, the numbers between letters could be meaningfully in some way, I don't know. I just don't want to be restricted to hand-written rules as I couldn't probably account for all of them.",MLQuestions,2022-04-18 09:51:01,6
Decision trees train very quickly. There are so many factors which impact training time. Some of the more impactful are tree depth and number of leaves. You need to provide more details on what you are doing if you want someone to provide more insightful suggestions. 10 second could be incredibly long if you are training on a tiny dataset of tiny images or could be incredibly fast if training on imageNet. All things are subjective and need more perspective to make more constructive suggestions.,2,u64nak,"First time doing decision tree, I'm using Spark MLlib, the training now takes like 10 seconds and I don't feel I'm doing this right, I know a lot of factors can affect the training time, but could somebody share some estimates on this based on your experiences?",MLQuestions,2022-04-17 21:27:30,2
"On policy you are updating the policy with actions, observations, and rewards gathered while following the current policy and off policy it can be from previous versions of the policy.",3,u5yfos,Can somebody provide a simple explanation of the difference between “on policy” and “off policy” learning?,MLQuestions,2022-04-17 15:54:20,5
Perhaps https://en.m.wikipedia.org/wiki/B-tree,1,u5xwa0,"I stumbled upon a problem that I'm not sure what is the best approach to solve and wondering if anyone has a suggestion.  
I have a very large list of items (each item is a word or a sentence). I need to find one or more items that are the most similar given some similarity metric, to some query (imagine that the list is too large for brute force search). The issue is that the list is changing over time (items are removed or being added), so I'm not sure that the common embedding method would work.",MLQuestions,2022-04-17 15:27:31,1
"I would check out NeRF, it appears to be more adapted to your use case than vanilla GANs. Part of the neural radiance field literature works towards doing pretty much this: generating images of a scene from a different point of view.",5,u5mw1e,"I have a collection of images all different angles of the same object.

I would like to input the angle values to the generator and then make it generate the specific corresponding image.

Having tried this with just a basic cnn network I get pretty blurry results, so I thought a GAN similar to pix2pix might be able to do it.

However I find that a. the GAN just produces pretty mutant blobs and b. It starts to ignore the inout angles pretty much.

I am doing this experiment to learn a bit about loss functions and try different things but so far none of what I’ve tried makes much of a difference.

What I am trying to do is use a GAN to make crisp realistic images but specific ones based on a specific input vector.

Is that even possible?",MLQuestions,2022-04-17 06:18:24,6
"Pure MCTS without any heuristic does not need any training.

However, there can be a 'next-move' heuristic that helps guide the decision which move to explore next. Another trick that is often done ist to do a rollout until the very end, but to stop early and evaulate that state using another heuristic.

These two heuristics can be anything. They could also be a trained model. AlphaGoZero is a famous example where a heuristic is trained.",1,u5vnli,"Hi everyone. I'm a maths student working on a research project which for me involves the monte carlo tree search. For the most part I understand the way this algorithm works, but I can't find a straight answer on some of the specifics. How does this algorithm require training? From what I understand the position forms the root node and then following the algorithm a tree is constructed and the most visited child is taken as the action. This child then becomes the new root node and repeat. It also comes across like this needs to be trained prior to use somehow. Is this by saving the entire overall tree each time it plays rather than completely discarding what isn't used each turn? I hope this makes sense.",MLQuestions,2022-04-17 13:37:41,3
Throwing it online within a Flask (old and busted) or FastAPI (new hotness) framework is probably easiest.,7,u5iicc,"i created a model and i have saved .pb file and the variables, from here on, how i deploy my model in a server? lets say i want to send inputs from a front end and get predicted data outputs from the model, how do i establish this? 

Thank you",MLQuestions,2022-04-17 01:11:07,8
"nvm it's called emotion recognition. never thought of using different words.

i got square brain :)",3,u5d04f,"I can see that specific sentiments can be extremely subtle and subjective, but it's surprising that every single paper/dataset I read only use positive/negative sentiments.

Maybe I'm missing something here? I'm still new to this topic.",MLQuestions,2022-04-16 19:10:56,3
"They're called ""open-domain"" chatbots and they have a lot of problems- they're certainly not useable in most business applications and the webtext they learn from can cause a lot of issues for hosts (https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist). FAIR released a great 2020 paper with a few variants and common recipes: [https://arxiv.org/pdf/2004.13637.pdf](https://arxiv.org/pdf/2004.13637.pdf). Open domain chatbots require a ton of data and compute, so you should almost certainly just use a pretrained one.",3,u5ias0,"
What frame work make a chatbot talks in any domain?",MLQuestions,2022-04-17 00:56:15,3
Hahaha. I have experienced this. Reduce the weightage of regularized variables or reduce the dropout rate.,3,u57fyw,"&#x200B;

https://preview.redd.it/c5vjvmx9jyt81.png?width=880&format=png&auto=webp&s=16239b1e29030560ae48931d6a37d6d9e325d74a",MLQuestions,2022-04-16 14:17:16,8
"So it looks like you are using something called [finite differences](https://en.wikipedia.org/wiki/Finite_difference) a common way to get derivatives in numerical methods.
There are some drawbacks, such as the fact that the precision of your derivative estimate depends heavily on the step size. 
I'm not aware of any rules that tell you what a ""good"" step size is, because it depends on how much error you can tolerate and the function you are using.
As it stands, finite differences is probably a good starting point, but knowing if it is a good way to go can be difficult. 


There are a couple of alternatives.
First, neural network libraries such as pytorch or tensorflow will have a way of evaluating the exact derivative. 
This is called autodifferentiation and is a key part of any NN library.
Second, if you don't want to use a NN library, there is an approach to autodifferentiation called dual numbers that may also work for you. 



Note: with the step size of 1, the line
```
slope (loss2 - loss)/1
```
can have the division removed. 
I don't know if the python interpreter cleans that up automatically or not.",5,u57m4b,"&#x200B;

https://preview.redd.it/qnhonmmqkyt81.png?width=2566&format=png&auto=webp&s=b32d293414d1018d779e528b1d18a94d2502b26a",MLQuestions,2022-04-16 14:25:51,1
That is literally what they are for,32,u4rtot,"The captchas filled out for websites online often require picture identification or pattern recognition. Is it possible this could be turned into a method to affirm pattern recognition in machines to help find a 'measure of good fit' for data? If so, then what all could show up in captchas to help researchers?",MLQuestions,2022-04-15 23:35:40,17
Try and change it to .xlsx or similar. I think I had to change the file type before.,1,u5143q,"Hello everyone,

I'm using Weka to implement a unsupervised algorithms like LOF to detect outliers within a given dataset but of course the dataset has to contain binary attributes as LOF is dependent on this (according Weka compatibility information) and it does (0,0,1,0,1). However, when I try to import the dataset from the preprocessing section on Weka I get back an error outling this:

File: C:\\Users\\JohnDoe\\Downloads\\labelled\_Dataset.csv

Reason: index 13 out of bounds for length 13 Problem encountered on line: 293

Does anyone here have any ideas on how to overcome this, I've never encountered this error and a google search doesn't particularly help in this context too as it comes back with different results",MLQuestions,2022-04-16 09:13:31,3
Subsequent epochs should not increase the ram unless you are storing intermediate values. Its a good idea to dump the intermediate data to logs or  files.,2,u4uo4h, Is there a way to run this notebook without exceeding the memory usage? [https://colab.research.google.com/drive/14Ea4lIzsn5EFvPpYKtWStXEByT9qmbkj?usp=sharing](https://colab.research.google.com/drive/14Ea4lIzsn5EFvPpYKtWStXEByT9qmbkj?usp=sharing) any help would be much appreciated! thank you so much,MLQuestions,2022-04-16 03:06:14,9
"I made a little algo I use for problems that involve non-differentiable loss functions. The general idea is that we estimate the gradient by scoring noise in weights. Each step, instead of starting from scratch we start from near the previous gradient estimations, and hopefully only calculate as many samples that are needed to re-""saturate"" the estimate. Although it's a reinforcement algorithm, you can just score the model via your own loss function. The usage is very abstract such that you supply your own model and get/set param functions. The algorithm itself doesn't really care about any of that. It's worked pretty well for my use cases, feel free to give it a try- [https://github.com/ben-arnao/OnGrad](https://github.com/ben-arnao/OnGrad)",6,u4hw02,"I am now using evolution but it is slow.

https://preview.redd.it/a4tw7vd0drt81.png?width=2096&format=png&auto=webp&s=fc5d38d3afe203b7604f7be060573c393eb5db24",MLQuestions,2022-04-15 14:09:28,3
You should rely on precision and recall in this case,13,u4bznn,"Like if I only has 10% of positive data, if I calculate based on randomly selected val data, I will likely to get a high accuracy. Should I choose 50% positive val data and 50% negative val data?",MLQuestions,2022-04-15 09:34:27,12
Are you including the distance from the sample to itself? You need to exclude the instance you're evaluating from being its own neighbor.,3,u4bvrl,"I built an knn model predicting credit card user type, and I got 96% accuracy on val set, this is so crazy and too good to be true. Am I wrong on anything?

Here is my code:

https://preview.redd.it/6u6k10g1zpt81.png?width=894&format=png&auto=webp&s=0d9ee62120c18a0d149f1d298c4ba402378bb1bc

Edit: When I increase the sample size, it is lower to 80% which makes more sense. But why when it is 20 samples it is so high sccuracy? I tried to shuffle val set every time.",MLQuestions,2022-04-15 09:29:26,2
I think there is a MOOC by deeplearning.ai on Coursera or some other institute offering the same. Not completely sure though.,3,u40tvi,What is a good way to start learning MLOps ? any MOOCs ?,MLQuestions,2022-04-14 22:26:05,3
I know someone who does this. They were a corporate data scientist before the word was commonly used. About 20 years experience before they made the jump. Buckle up.,2,u43sj3,Any advice for Getting into AI technical consulting?,MLQuestions,2022-04-15 01:57:03,2
"You're probably asking the wrong question. The way you're thinking makes sense, because having the data-generating equation/model would be very, very helpful. However, for most problems we don't know it. Even worse, generally we don't even know whether there exists a nice closed-form expression that fits our data: it's very likely that that expression just doesn't exist.

Of course, we can try to guess some candidate function. For example, we can say that your data looks close to a Poisson distribution or rational function. We fit it, results are close enough, and we're happy. The issue with this scenario is that the result depends mainly on our initial guess. Was our guess any good? Are there better guesses? How do we test it? If we collect 2000 more data points, do we guess a new function? This is strongly related the the [bias-variance tradeoff](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff) problem.

All these problems are why we generally only distinguish between two families of functions: linear and non-linear. Linear is easy and your case is clearly non-linear. So what do we do? For line-fitting, generally we try n-degree polynomials, because they are easy to work with and can approximate many functions (see e.g. [here](https://en.wikipedia.org/wiki/Approximation_theory), [here](https://en.wikipedia.org/wiki/Taylor_series), or [here](https://xronos.clas.ufl.edu/mooculus/calculus2/approximatingFunctionsWithPolynomials/digInApproximatingFunctionsWithPolynomials)).

So if you want to continue what you're doing now, you can try polynomials of increasing degree until you are happy with the fit. Make sure to also observe the behaviour of your fitted function outside of the domain of the data, because you will see it behave increasingly erratically as you increase the degree.

A second option is to use non-linear machine learning models that do not fit a closed-form formula, but instead model the data some other way. Tree-based methods like random forest regression are a common example. These are typically extremely useful when you have a larger number of inputs/features. For your current problem, this may be overkill.",2,u3y8x9,"So for context, I am trying to predict an algorithm's runtime complexity from empirical data generated.  I have the data as a dataframe and I wrote the following in attempts to find the curve of best fit:

    import matplotlib.pyplot as plt
    import scipy
    import sklearn.metrics
    
    def func(x, a, b, c):
        return a * (c * x ** -b)
    
    popt, pcov = scipy.optimize.curve_fit(func, bellard_rate_df[""nth Digit""], bellard_rate_df[""Digits/sec""])
    fig = plt.figure(figsize=(20, 10))
    ax = fig.gca()
    ax.scatter(bellard_rate_df[""nth Digit""], bellard_rate_df[""Digits/sec""], label='data')
    ax.plot(bellard_rate_df[""nth Digit""], func(bellard_rate_df[""nth Digit""], *popt), label='fit', color='red', linewidth=5)
    
    fig2 = plt.figure(figsize=(20, 10))
    ax2 = fig2.gca()
    ax2.scatter(bellard_rate_df[""nth Digit""], bellard_rate_df[""Digits/sec""], label='data')
    ax2.loglog(bellard_rate_df[""nth Digit""], func(bellard_rate_df[""nth Digit""], *popt), label='fit', color='red', linewidth=5)
    
    ax.legend()
    plt.show()
    print(f""Coefficients: {popt}"")
    print(f""Determination Coefficient: {sklearn.metrics.r2_score(bellard_rate_df['Digits/sec'], func(bellard_rate_df['nth Digit'], *popt))}"")

What does the above do? Well it would best be described by a picture below:

[The top graph is just a regular evenly spaced plot, the bottom is a log-log plot.](https://preview.redd.it/xc0ryk4jwlt81.png?width=1167&format=png&auto=webp&s=6ac43d38a896704c3dd5bcc1b239d746452a764f)

So I am not getting what the shape of the raw data fits in. I tried to plug in my own model, but as you can see by the red line, I am off by like alot. So the question is, what in the world is the blue data? I don't even need a precise fit, just something like a times b + x minus c type of thing will do. Or even better, just the name of the model (such as polynomial, exponential decay, etc.) will help me google better. Rn I am just searching slightly curvy log-log data that straightens out, and would u look at that: no results.",MLQuestions,2022-04-14 19:53:40,4
"136GB is not a big dataset when we are talking about images. I have dealt with datasets of more than 1TB in my 6 years old desktop. I would say more than 100TB is starting to be big data in this field.

If you want to work with deep learning for such datasets you need a dedicated Nvidia GPU, either a desktop computer or an AWS instance (or other cloud provider). But cloud is usually much more expensive than a desktop, 6 months of cloud is what you could spend in a good desktop computer.
For starting I would suggest you to try an AWS instance with one GPU (with as much ram as you can find, but only one GPU), 32GB of ram should be more than enough, and a SSD disk (your dataset fits without issues I'm a ssd, and you need the speed to load very quickly the images to feed the GPU). And probably when you have learnt the basics you would know better what would you need for your problem and decide whether to stick with cloud or move to a desktop or server or whatever.",1,u3wrai,"I’m in a small startup that’s doing some ML research, and I was wondering what recommended hardware and infrastructure we should have.

For some context, our research mainly has to do with ML and medical and telemedicine research. Currently I’m working on a project to detect Gaze (basically what location or direction a person is looking based on an image), and I tried using the Gaze Capture dataset but had issues due to how big it is (136 GB!). I’ve tried using Google Drive + Colab but am having trouble untaring all the files (data for each patient is in a tar.gz file and when I try to untar them not all of them save). How do researchers do projects like this? Do they have computers/servers with lots of GPUs and memory or something? I tried using AWS before but had trouble untaring the file. Is there a way researchers do this stuff on cloud platforms?

Btw currently my computer is the MacBook Pro with the M1 chip.

In summary: Want to do ML with large datasets, what hardware/infrastructure do I need (server? AWS? GCP? Azure?)

Btw if a cloud service is recommended, the name of an online course for it you’d recommend would be great, as I’ve tried in the past to do cloud ML but have had trouble with where to start.",MLQuestions,2022-04-14 18:32:46,3
"I'm sure there are other examples, but [nilearn](https://nilearn.github.io/stable/index.html) + scikit-learn, [PRONTO](http://www.mlnl.cs.ucl.ac.uk/pronto/) toolbox, and [Neurominer](http://proniapredictors.eu/neurominer/index.html) all do ML + neuroimaging. Maybe some inspiration to be had there.",2,u3smne," So... I kind of started my machine learning journey with my thesis on Parkinson's disease classification on brain MRI images. When I ran my model with very basic processed images the performance was very low. which is...obvious. But, when I try to run the model with specific medical image-based processings steps, it cannot read the data while training.

I am really at a loss right now, and I need a proper guideline on starting the work from the scratch I guess; some suggestions will be really a great help for me!

P.S. The images are in nifti file format, both are structural mri, and fmri for resting states are available.",MLQuestions,2022-04-14 15:03:21,1
"Have a look at ""MLOps"".  

As a SW developer I like the book ""Clean Code"". (Cheat sheet: [https://www.planetgeek.ch/wp-content/uploads/2014/11/Clean-Code-V2.4.pdf](https://www.planetgeek.ch/wp-content/uploads/2014/11/Clean-Code-V2.4.pdf))  Be aware that some stuff is more for Object-Oriented coding.

Consider using a style guide for the programming language you use and try to automize (the format, naming and such) as much as possible.  


Have a look at SW Engineering ""Anti-Patterns"" and ""Best practice"".",3,u3dkpu,How to improve knowledge about software engineering techniques as an AI engineer? Any good MOOCs?,MLQuestions,2022-04-14 02:42:13,4
"Not a ton to add except that Meta emails me almost weekly for machine learning positions in software engineering, though I am Data Science leadership by trade. They specifically are looking for deep RL folks. If you can get a recruiter from any perspective to respond to you, and explain to them what you are explaining to us, they will find a path for you to interview towards. Meta has been far and away the most enjoyable (albeit difficult) Interview experience I’ve had in my 15 years.

Edit: Worth pointing out I don’t work for Meta, and most likely won’t",5,u30fry,"I'm in an awkward position. I got my bachelors in Mechanical Engineering and worked as a mechanical engineer for a few years, then got my masters in data science and got a government job as an analyst for a few years. I haven't used any ML concepts from my masters degree in the past few years but I have been working on deep learning as a hobby for a while now, specifically synthetic media (voice, image, text, and video generation). So I have a couple questions.

1) My real interest is deep neural networks and not so much standard machine learning, are there many positions out there? Something with generative models would be ideal, or CV, or NLP possibly. What are some of my options? I looked around Indeed a few months ago and didn't see much.

2) I feel like I am in an awkward position because I have about six years professional experience, but nothing for the field I am trying to get in. My masters degree can help I'm sure, and I'm working on a couple DL projects that I can post to github in a few months but that's it. I know enough to be able to finetune and modify prebuilt models to fit my needs but I'm not at the level of the researchers that can build a brand new architecture from scratch.

What are my options and what steps should I take to get to my end goal? I am hoping to get a new job in about 9 months time. I appreciate any responses. Thank you.",MLQuestions,2022-04-13 14:08:39,9
"* transformers
* self-supervised learning
* contrastive losses
* denoising diffusion
* codebook quantization
* batch/layer/group/AdaIn normalization
* scaling laws (e.g. efficientnet)
* equivariance/invariance (geometric DL)
* multi-modal latents
* stylegan, progressively growing gans, projected gans
* NeRF, implicit representations generally",16,u2p1jp,"Sorry if this is a dumb question. I've sporadically taught DL at masters level for a couple years, but I've realised that most of my theoretical and practical knowledge comes from that 2016 book and I might be teaching concepts that are now out of date - I've just been assuming that training convolution/maxpool blocks with ReLUs and Adam is still relevant, and that dropout/L2 norm are still widely used. Is there a good review or summary somewhere of the chief *practical* developments (not research trends) in model training that have become commonplace over the last few years?

As far as I know, people aren't pre-training stacked denoising autoencoders anymore, and RNNs have really fallen out of favour compared to convolutional models and transformers. Unrolling conv layers also seems to be a dated practice compared to global average pooling. I think batch normalisation has fallen out of favour too, though I'm not certain why. Certainly you don't hear about deep belief networks or Boltzmann machines anymore.

What else? I think there's been a greater focus on pre-training, fine-tuning and domain transfer compared to training models from scratch, but I think that's just an artefact of the increased availability of large, open-source models. Data augmentation? More emphasis on latent spaces and modality-invariance?",MLQuestions,2022-04-13 05:19:53,4
Just learn business. The highest paid people in tech are usually the ad sales bros.,2,u35wfr,Any Ideas how to learn business side of AI product/service development ?,MLQuestions,2022-04-13 18:33:16,1
"I don’t know if this can help, but the least confidence query often query datapoint at the classifier boundary, which often neglect the internal structure. This is solved by using greedy algorithm e.g. core-set approach, but using this alone don’t really preserve the boundary structure

In the end, IMO you should use a mixture of both least confidence, and greedy method. But the exact ratio is up to debate",2,u2crkj," I am training random forest to do multiclass classification on a pretty small dataset of embeddings (from BERT).

I'm using active learning (with a least confidence query strategy) to iteratively label more datapoints to retrain the model using newly extracted embeddings. The labeled dataset only contains approx 600 labeled datapoints, and I'm planning on doing 3-4 iterations of labeling, adding 300 datapoints each time. My question is, is it best practice to do hyperparameter tuning (using randomized grid search, grid search, etc) before, after, or during the active learning iterations? Doing it after would entail using the default hyperparameters of sklearn's random forest until the active learning is done.

First time posting here, thanks!:)",MLQuestions,2022-04-12 16:49:33,1
No.,3,u2a0dd,,MLQuestions,2022-04-12 14:31:03,2
"I was actually just looking into evaluating free response text and stumbled upon semantic similarity (literally 5 min ago). [This](https://www.deepset.ai/blog/how-to-evaluate-a-question-answering-system) was useful (the [paper](https://arxiv.org/pdf/2108.06130.pdf)). They have some examples using Bi-Encoder, BERT, etc. 

Regarding clustering, after you move the text to an embedding space any unsupervised clustering can work (obviously each has pros/cons)",1,u2c0en,"I have over 2000 of different texts of different lengths and contents. 
I want to see how close they are semantically to each other. 
I don't have labels, so it needs to be unsupervised. 
So far, it seems to me that I need to create a measure of semantic similarity using some model and then apply an unsupervised clustering algorithm on it. 
I see that HuggingFace has lots of models related to semantic similarity and think that I could apply k-means after. 
I am not really sure if that's the right way to do it and perhaps there are better approaches. I'm also not sure which model to take on HF and which model to use for clustering. 
If anyone has suggestions or could share a link to a towardsdatascience blog (that does exactly what I described) that I missed, I'd be really grateful.",MLQuestions,2022-04-12 16:12:04,3
"> the model is exploiting the prior class densities to load weights onto its predictions. Again, the model still learns the under-sampled classes, but it is far more confident about the larger ones

Could you elaborate on this?",1,u21m4e,"I have *n=1,000,000* observations, spread unevenly across 10 classes. The class imbalance is not so bad as to prevent the model from learning about the under-sampled classes, but there are two features of the data / model that, for out-of-sample testing purposes, are not desirable: (1) the model is exploiting the prior class densities to load weights onto its predictions. Again, the model still *learns* the under-sampled classes, but it is far more confident about the larger ones. (2) the model is exploiting the joint distributions between classes to know which classes tend to go together in the multi-label setting. This makes the model resistant to accepting that the joint distribution can be different out-of-sample.

My thought was to draw a small sample from the overall pool of 1,000,000, but if I do this randomly, law of large numbers says that I will usually just recover the true underlying distribution of classes, which I am actively trying to avoid. To make use of all of my data, I was thinking I could draw a new training sample every epoch, and in some way artificially enforce that the draw of instances tends to *not* follow the true underlying distribution. Are there any procedures for encouraging a model to disregard priors like this? Thanks for any help you can provide!",MLQuestions,2022-04-12 08:22:38,8
"Not sure if you came across this yet but this article goes pretty in depth and has some sources

https://medium.com/@luis_gonzales/a-look-at-mobilenetv2-inverted-residuals-and-linear-bottlenecks-d49f85c12423

The mbconv seems to have originated from mobilenetv2 so potentially looking into that models architecture can help out.

From reading I think they are both convolution layers just do the function in a different way.

Hope this helps even if I don't have much experience",2,u25tzw,"I am working on a machine learning project to learn more about this  field. 

The project is about image classification. I want to use the  EffnetB0 architecure and they mention in this architecure they use in  the fisrt stage the following layer: ""Conv3X3"" and the following layers  they use ""MBConv1"". I tried to understand the difference between these two layers but I  can't seem to find the answer. These two layers are both **convolutional**  layers right ?

But what exactly is the difference between ""Conv"" and ""MBConv""?

Thank you for helping me!",MLQuestions,2022-04-12 11:25:51,3
"It depends. Some deep learning models, like Generative Adversarial Networks (GANs) build a distribution from which you can only sample and not derive an analytical representation of.

Some methods like Naive Bayes, or Logistic Regression, directly parametrize a distribution that you *could* represent analytically. For all intents and purposes though, it's better to think of them as methods of finding conditional probability (e.g. P(y|x)).

But really I think it's important to be able to wield both perspectives freely: to swap between the mathematical ideal of a model describing exactly a probability distribution, and a model *inducing* a probability distribution. Both perspectives have different situations where they're useful.",2,u1wjx2,"Hi everyone :)   
I've le

arned that some (or all?) ML models work by approximating an underlying probability distribution (unsupervised ML approximates P(x) and supervised ML approximates P(y | x)). 

My question concerns the relationship between a machine learning model and probability distributions. Given that the trained model approximates a probability distribution can we think of the machine learning model, itself, as a probability function or do we have to understand it as producing such a probability function? 

&#x200B;

Hope my question makes sense :)",MLQuestions,2022-04-12 04:11:50,3
"The decision you are enforcing on the NN are discrete in nature, the loss cannot propagate on discrete rulings (assuming calculus based back propagation).",2,u1cmjv,"I'd like to create a neural network that can generate a prediction by picking parts of the input data

More precisely, I have a 2D array of integers, that's for instance `[[0, 3], [9, 4, 5], [0, 3], [1, 2, 6, 7]]` here, and I'd like to feed this 2D array into the NN so it can pick an integer in each of the second dimension array, with one specific rule: if there's (and there will be, just like here) several arrays containing the same sequence of integers, it has to pick one that hasn't been already selected, so in the end the output sequence is composed of only unique numbers, for instance `[0, 9, 3, 1]`

About the relation the NN will learn to emulate during the training process, it is simply to predict a sequence of integers so another component, being fed this sequence, will return another of different nature (not relevant here) that has to be the shortest

Is it possible to make a NN in such way it follows all those criteria? If so, what would be the best model and topology for that? I'm guessing an RNN, but I'm not sure how I can introduce the multidimensionality in that configuration and how it's supposed to output another sequence. A precision in case it helps, the second dimension arrays are all of variable size, but the first dimension containing them shall be always of fixed size, e.g. 50. (Perhaps Seq2Vec could help?)

I'm also wondering about the scalability: for the training part I can generate a dataset automatically but it's quite a lengthy process, since I'll have to iterate through all the possible combinations to find the one of least function output size. So to accelerate the process I thought about reducing the size of each second dimension arrays, but won't it decrease its accuracy?",MLQuestions,2022-04-11 10:11:53,5
"Object detection is the term you are searching for.

A possible network you could be using would be the CSP (center-scale-prediction), it is end-to-end trainable and anchor-free.",1,u162q6,"I come from a software background and am building a fun project to learn a new technology. I want to extract items from a bill to find patterns in my spending. However, all my search results so far just end up showing me how to implement image classification in general, i.e. whether the image contains a dog or what type of flower it is, or in my case whether the image contains a receipt or not.

However, my use case is the following: I wanted to get coordinates of the bounding edges of the section of a receipt that lists all the items. I have a dataset with 100s of receipts that are annotated like this ([https://gyazo.com/65b9eb2b5296a7f4885601c3b0bf0c90](https://gyazo.com/65b9eb2b5296a7f4885601c3b0bf0c90)). How would I go about building a model that extracts the coordinates of this bounding box? Is there a more specific name for this than 'image classification'? Any answers pointing me in the right direction are much appreciated! Cheers",MLQuestions,2022-04-11 04:59:40,2
I think if you define the ML algorithm as using data to fit a model and make predictions then yah Prophet could be using a ML model.,1,u1eyx4,,MLQuestions,2022-04-11 12:12:31,1
Forgot to add that there is a \\@tf.function decorator at the top of the function,1,u11lxt,"Hello I am trying to create a custom tokenizer for a multilingual MT following the Transformer tutorial from Tensorflow. My first token in the string is the language tag indicating the direction of translation. I managed to extract the first token, but can't properly combine the remaining tokens into a single sentence.    


I get this error from the *tf.unstack(text,num = num\_tok)* part of the code. Is there a way to pass an 'int' variable during graph execution? Or any other work around for my problem? 

  
*TypeError: Expected int for argument 'num' not <tf.Tensor 'strided\_slice\_2:0' shape=() dtype=int32>.*

    def mult_tokenize(tokenizer, strings):
      tokens = tf.strings.split(strings, ' ')
      ID = tokens[0][0]
      text = tokens[0][1:]
      num_tok = len(tokens[0])
      text = tf.unstack(text,num =num_tok)
      text = tf.strings.join(text,' ')
      enc = tokenizer.tokenize(text)
      enc = enc.merge_dims(-2,-1)
      enc = add_lang_tag(enc,ID)
      enc = add_start_end(enc)
      return enc,ID,text",MLQuestions,2022-04-10 23:56:38,2
Kinda hard to tell without knowing what we're looking at. Maybe no?,9,u0rzk9,"# 

https://preview.redd.it/33fpul90wrs81.png?width=251&format=png&auto=webp&s=d9e24bd1c54bf4773dfe8fa30b1fcbd0c74287fb

https://preview.redd.it/6ybl6n90wrs81.png?width=251&format=png&auto=webp&s=8cdca9adb4a2a718c860cd3efe7cebe812d8a2e4",MLQuestions,2022-04-10 14:51:48,7
Pandas has the rolling window function. Scipy also has windowing functions as part of the signal sublibrary,2,u0rxi8,"Hi. I am working on a school project and we are using a paper as a reference. In the paper they say the following: “sliding window needs to be added to the signal before the pre- processing step. The time window used in this paper is 12.5 seconds with no overlap between the windows.”

I am working in Python but am unable to find a resource on processing my data frame using the above method. Any help? 

I should mention this is on some time series data

Thanks!",MLQuestions,2022-04-10 14:49:07,4
Check out mne and moab python libs. They have datasets themselves but have tools for creation with custom data and I think can do what you want.,2,u0gk3a,"Hello,

I have a bunch of EEG data and behavioral task results from Go-NoGo tasks with correct and incorrect answers. EEG dataset is recorded as EDF + files and task results as csv. I need to import these datasets which is somehow ok separately, but how can I combine them because each EEG data has values based on the tasks including event-related potentials. Should I learn SQL for this or can I do without it? I will conduct something basic like SVM classification. 

By the way, I am using BrainVision Analyzer for data preparation. This part can be skipped. 

Any kind of **introductory** paper suggestions would be appreciated. I am very new.

Thanks in advance!",MLQuestions,2022-04-10 05:26:51,8
[deleted],1,u0pdue,"As the title suggests, I am kind of overwhelmed with the amount of models there are so finding the one that best suits my dataset is proving kind of difficult. The Dataset I have is as follows, its produced by a Radar, which outputs a row of values for a signal that it detects for a target.

Plotting across the row gives me the following wave, and as we go below the rows, we get the translation in the x-axis suggesting movement of the target, my dataset that I want to feed the model will have the following features: The average value of the two peaks for signal strength, the x-axis average value (multiplied by .77 meters), this will be the case for all of the waves for each target as each # of target will have these waves associated with it, tracking the movement of these waves shows the distance change, the change in amplitude and so on

I am currently working on a script that will try and get all the times when the radar detects something and get the amplitude column values and fill in the dataset, if I can't get the script to work I will just do it manually:

Link to what the Radar outputs

[https://drive.google.com/file/d/1IJOebiXuScjLPytemulcXph7ZB1X65wU/view](https://drive.google.com/file/d/1IJOebiXuScjLPytemulcXph7ZB1X65wU/view)

Reference images to what I referred

[https://imgur.com/a/AMDfUvY](https://imgur.com/a/AMDfUvY)

I might also add another column that gets the average velocity of those points since we have time, distance given by the Radar.

What model would best fit this sort of data, a top 5 list would be super appreciated!

Thank You

EDIT: Sorry I completely forgot to add this, but I will have a similar dataset for pedestrians as well, the first dataset was for vehicles, but there will be one for pedestrians as well, I want a model that can predict, once the training is done if the target was either a pedestrian or a vehicle given the features.",MLQuestions,2022-04-10 12:47:27,2
"Some common advantages of KNN algorithm include:

* Simple to implement and intuitive to understand
* Can learn non-linear decision boundaries when used for classification and regression. Can come up with a highly flexible decision boundary adjusting the value of K. \* No Training Time for classification/regression: The KNN algorithm has no explicit training step and all the work happens during prediction.
* Constantly evolves with new data: Since there is no explicit training step, as we keep adding new data to the dataset, the prediction is adjusted without having to retrain a new model.
* Single Hyperparameters: There is a single hyperparameter, the value of K. This makes hyperparameter tuning easy. \* Choice of distance metric: There are many distance metrics to choose from. Some popular distance metrics used are Euclidean, Manhattan, Minkowski, hamming distance, and so on.

For more information, do visit:

[https://ml-concepts.com/2022/01/26/6-knn-step-by-step-guide-on-k-nearest-neighbor/](https://ml-concepts.com/2022/01/26/6-knn-step-by-step-guide-on-k-nearest-neighbor/)

&#x200B;

\[Full Disclaimer: I am a part of the [ml-concepts.com](https://ml-concepts.com) team\]

Feel free to reach out to me for any help!",1,u0suja,"I am trying to do this as my undergrad research topic: a better kNN algorithim. My model DOES require training, so the ""training-free"" advantage of traditional knn is not applicable here. What are some other advantages of kNN?… Read more",MLQuestions,2022-04-10 15:33:14,1
"Pre-averaging the features is a bit hard to reason about generally because that behavior might be quite different depending on your distance function for KNN. 

In practice we can probably say that averaging the features increases bias and decreases variance. There  is more compressed information in each datapoint to match on. 

Going from K to L > K would increase bias and decrease variance as well. 

For just your averaging question I would say increase in bias decrease in variance.",1,u0g6us," 

Hi,

I would like to ask - how will the bias & variance will change increase or decrease) in the following case

* Moving from model with K=3 & 2 features ---> to a model with k =3 & 1 feature which is the mean between the 2 old features

Thanks",MLQuestions,2022-04-10 05:02:54,1
"Hi. I'm interested in this. Can you provide some further details about this program? Things like recommended audience, timeline, and activities.",4,tzyene,"I’ve had the opportunity to work with collaborators from Meta, Google, and IBM in their personal capacity, researching various domains of machine learning such as Reinforcement Learning Frameworks, NLP, Time Series Forecasting, and Computer Vision. It’s been quite a journey multitasking these collaborations, I’ve been fortunate enough to have some of these projects be covered by [Analytics India Magazine](https://analyticsindiamag.com/how-oscillatory-activation-function-overcomes-problems-with-gradient-descent-and-xor/) and discussed in forums by leading AI practitioners.

After the overwhelming response of the first batch, I aspire to extend my learning by enabling early career professionals to kickstart their research journey. This is something I’ve been thinking of doing for a very long time. Initiatives like these throw light on the peer learning side of research and yet manage to bring out the best in ourselves is what my personal experience says.

In support of the above, please fill out this form or reach out directly if you or anyone in your network is interested in learning more about the mentorship program.

Form Link: [https://forms.gle/NCwuz3iETdwJYsFp9](https://forms.gle/NCwuz3iETdwJYsFp9)

Thanks for your time, have a great day!",MLQuestions,2022-04-09 10:49:32,2
"To train an ML model you will need to generate a lot of these examples. Can you do it without stating those rules first? If yes, then you'd better choose a model that is not overly complex, otherwise it will just remember the examples without learning any patterns.

Technically, you can use linear regression with only the data that you have already mentioned.",1,u00iuo,"I have a problem whereby arrays represent rules that result in a highly predictable number out the other end, I’d like a machine learning algorithm to analyse the pattern and predict a result. Example patterns to their results:

[[0,0,0],[0,1,0],[0,0,0]] = 4
[[1,0,0],[0,1,0][0,0,0]] = 8
[[1,0,0],[0,1,0][0,0,1]] = 12
[[1,1,0],[0,1,0],[0,0,1]] = 14

As you can see the pattern is complete repeatable. What I’d like is an algorithm that can learn this pattern for an arbitrary number of arrays and an arbitrary number of array values. 

Naturally I could solve this simply by defining the formula/rules in code, however the rules get more complex the larger the grid is, but yet still highly predictable and thus “learnable” - I’d like to avoid having to hard code the rules. 

What would be the best approach for this?",MLQuestions,2022-04-09 12:35:06,3
"Hi! I know it's tough sometimes to choose courses, as you can't be aware of how they are and how much you actually like them, you only find that after some time studying. 

Both are very useful, interesting and widely used in the industry, so either one you choose is a good choice. Think of what appeals to you the most, what you want to focus on in your CS career and which one of them would be more suitable. But if you realize that you don't like it at the end of the semester, it's still an experience and you are going to learn a lot from either of them.

Personally, I like cloud computing more and it's easier than ML, I like system-related courses more. To really understand what happens in ML you must have solid math knowledge, but it's quite interesting knowing what happens behind al the ML hype.

I am sure that you will make a right choice! Good luck!",2,tzvjbz,,MLQuestions,2022-04-09 08:29:18,2
"A MacBook Air won’t be able to do any serious ML locally, but it will be fine for remotely accessing servers for serious training of models.

Don’t get a super-powered laptop for anything ever. The high performance laptop market is a wasteland. Get a tower (half the price and double the performance) if you think you absolutely must have the power locally. Otherwise, do what everyone I know in industry does which is get a laptop just good enough for email and small spreadsheets, and expect to do the rest in AWS.",4,tzp0dx,I'm joining MSc AI & ML this September. I want to buy a laptop. Is MacBook Air sufficient for this? If not what would you recommend to someone like me?,MLQuestions,2022-04-09 01:38:18,4
You can start with the Coursera Andrew NG course. Afterwards you can just look up for a topic online and read resources on it.,3,tzfdo8,"Hey all, I hope this is the right place for this question! I have an IT (operations) background with some big data (NoSQL) and distributed systems experience, but very little development experience. I can read Python, Java, JSON etc and am working on learning more, but would definitely say I'm still a beginner. I have not touched math in years.

Anyway, with the above background, what courses or resources would you recommend I begin with in growing my Machine Learning skillset?

**My goal is not to become a practitioner, but to be able to talk intelligently about the subject, common tools, and reference architectures etc**. Eventually I would like to pivot from my current role as a pre-sale Solutions Architect in the big data sector, towards a similar role in the ML and/or MLOps sector. Thanks in advance!",MLQuestions,2022-04-08 15:44:46,3
I don't know if this will be real time or funny. But you can use Netflix's movie recommendation system as an example.,4,tyyfyg,"I am organizing an event for a high school. The purpose of that event is to illustrate machine learning application in a funny way.

Ideally, the applications should be interactive (real time would be nice).

Do you have any ideas in mind? :)",MLQuestions,2022-04-08 00:59:04,8
"You can try top2vec

https://github.com/ddangelov/Top2Vec",2,tz2vob,"Hello!

I am working on a project where I am trying to find out what vulnerability is trending in security blogs. I gathered some articles about vulnerabilities and I want to cluster them based on their similarity and then extract some information (keywords or maybe use a q&a model to get more info) from the clusters (cve ids, affected products etc.). 

I tried using sentence-transformers to create embeddings (I am using all-distilroberta-v1 pre-trained model) and then cluster them using agglomerative clustering, as I don't know the number of clusters. I tried different distance thresholds for the agglomerative clustering, sometimes I get okay results, sometimes it's crap. However, I am not sure how to choose the best threshold for my task.

Some other ideas that I was thinking to try is using a different sentence embedding technique (e.g. InferSent) or maybe a different technique for clustering (e.g. hdbscan), but I have to do a bit more research before applying them.

Do you know better methods that I can use to do this kind of text clustering?",MLQuestions,2022-04-08 05:53:35,2
You can look into using YoloV5?,1,tyy2qe,"Hey guys
I'm a beginner in CV and ML. I want to learn multi object detection and tracking so I request all to please drop some video links that might help me. Also, drop any project links that have code implementation in python.",MLQuestions,2022-04-08 00:31:01,1
There will be an open source one ai am sure.,2,tyhtcv,"Just based on the estimated running costs of GPT3, and then whatever profit gets applied on top of that, are there any estimates for what openai will eventually charge for image generation?",MLQuestions,2022-04-07 10:14:59,1
"Did you try image/video processing libraries? Opencv-python? You should be able to get what you indicated with some basic rotations and transformations, but you might have to process each image manually.",1,tynfiq,"I am using hand X-ray images (12000 images) of the hand (from RSNA Bone Age Dataset from Kaggle), to which it will be fed into the neural network for automatic bone age estimation. I used TensorFlow's ImageDataGenerators to tweak image features (As shown in the code below).

The images do get processed according to the ImageDataGenerators (Figure 1), but:

* How do you get the images to look like Figure 2 from X-ray inputs?
* How do you center and align images like figure 2 unlike figure 1 when some of the images are misaligned like rotated?

Code used to get result as Figure 1:

`from keras.preprocessing.image import ImageDataGenerator`

`from keras.applications.mobilenet import preprocess_input`

`IMG_SIZE = 256`

`core_idg = ImageDataGenerator(samplewise_center=True,` 

`samplewise_std_normalization=True,` 

`horizontal_flip = True,` 

`vertical_flip = False,` 

`height_shift_range = 0.10,` 

`width_shift_range = 0.10,` 

`rotation_range = 5,` 

`# shear_range = 0.01, #consider... default`

`fill_mode = 'nearest',`

`zoom_range = 0.05,`

`preprocessing_function = preprocess_input)`

&#x200B;

[\(Figure 1\), so this is the result from ImageDataGenerators of TensorFlow. As you can see, it looks meh and bone features are not that visible as well. I also want the bottom images to be aligned accordingly.](https://preview.redd.it/2x7twa53c6s81.png?width=1180&format=png&auto=webp&s=63a9013d8254e75e676f396bd438ff94315046ea)

[\(Figure 2\), This is the result I want from X-ray inputs. I want the bone structures to be clear and visible, and even aligned straight. ](https://preview.redd.it/pzp4363cc6s81.png?width=802&format=png&auto=webp&s=beaa94850e144d977ca2472e953b8be338ce398f)",MLQuestions,2022-04-07 14:33:11,3
"""Introduction to Statistical Learning"" by Hastie et al is very accessible.",1,tyjywr,"What is your book recommendation for beginners in ML? I have previous knowledge of Python and CC in general (currently working in InfoSec)

Thanks for your answer.",MLQuestions,2022-04-07 11:53:48,2
"If the input to the RL algorithm is some brain scan (fMRI, EEG, etc) and the output is a prediction of whether the human is felling pleasure then there is no connection to what they are viewing. You would have to have the RL agent that has actions related to making a video for it to be able to learn how to make a video, not surprisingly.",1,tye1ef,Say an entirely hipotetical program get pics of a bunch of human brains while they feel pleasure (dopamine for example) viewing videos generated by an algorithm randomly. If a reinforcement learning algorithm watch how this people's brain react to the generated content...could It learn in a reasonable time how make an pleasurable video?,MLQuestions,2022-04-07 07:16:45,2
"[Here’s an old post](https://reddit.com/r/MachineLearning/comments/ljgako/project_i_created_a_fourpage_data_science/) I saved that I reference from time to time. I also have an final technical on Friday afternoon. Good luck to you!

Edit: original post content got deleted, but someone commented the content a little lower in that post. Direct repo is [here](https://reddit.com/r/MachineLearning/comments/ljgako/_/gneur4d/?context=1)",1,txxk1c,"Hey, so im writing these post asking help from the experts about how to prepare well for a machine learning interview, like what usually are the things people tend to focus more and stuff",MLQuestions,2022-04-06 15:04:06,2
"If I was you I'd ask myself 3 questions to decide whether a project is feasible:

Are there any existing techniques and approaches I could apply to solve this problem?
Is their suitable data available to help me solve this problem?
How would I measure success?

It might take a bit of research, but if you can answer those 3 questions for any of your potential topics then you have something that is a viable project that you know how to get started with. You don't want to pick a topic to later discover the relevant data isn't available or that you just have no idea how to solve it.

These questions don't really apply to curating a dataset, which is a project that would provide lots of value and is definitely feasible, but personally wouldn't excite me very much.",5,txhf4b,"Hi everyone, I have almost finished my CS undergraduate degree in Germany and am now trying to narrow down the topic of my thesis as it is the last remaining. After talking with my potential supervisor, we have agreed that a more practical than theoretical topic, possibly in the areas of Natural Language Processing, Data Engineering or Software Engineering for Data Science seems like a good trajectory.

I'm racking my brain for what data sets, problem sets, or thesis titles would be a good fit, but I just can't come up with anything good. The next meeting is in two days and my potential supervisor is expecting suggestions.

So far I have looked into topics like:

* **Automated information extraction** from job postings (potentially to see how much of a match it is to a given resume)
* **Misinformation detection** in textual data (potentially ""just"" a way to see if a textual piece is opinionated or holds any kind of contradiction)
* Automatic **text summarization**
* **A translator** that takes simple english text and turns it into ""corporate"" text (much like a translation engine to make sentences sound more professional)
* Scalable Architectures for **parallel data processing**
* **Describing/defining data standards** as in fields like agriculture or health care, there is no data standard for collection or format
* **Curating a dataset for real-world problems** and reporting on the method and approaches, possibly obstacles that arised during that process
* Sentiment analysis on Twitter data (so far everyone told me to do it, but I can't wrap my head around it)
* Trying to come up with a **styleguide for Software Engineering in Data Science** and related fields, much like the style guide for programming that Google has on its GitHub profile

Do you have any ideas or tips that can help me formulate a more specific topic in relation to Natural Language Processing, Data Engineering or Software Engineering for Data Science?",MLQuestions,2022-04-06 01:30:37,2
What grade do you want?,2,txknia,What are the possible areas of application of Graph Neural Networks in healthcare domain?,MLQuestions,2022-04-06 05:07:20,1
"There are so many possible reasons why. Check that your DQN is actually capable of producing a different output. Second check if the data you are training on are provided the ~best action. Third check if you are also using an epsilon greedy approach, if so make sure you have implemented the greedy choice correctly. Epsilon greedy ~ should have the network produce various actions if implemented correctly.",1,txn7wb,,MLQuestions,2022-04-06 07:18:00,1
"In any case, I'd say having top conference publications can never hurt your resume, but how much they'll benefit you depends on what you want to do. AI researcher job descriptions often explicitly ask for candidates to have first-author publications at top-tier conferences. But if you want to go into management, work in government, or specialize in a niche subfield, top conference publications might be less important; like if your focus is IR research, SIGIR would probably be a fine substitute for EMNLP/NAACL.",2,txr9sm,"I'm currently working on a paper in NLP that can be submitted to several conferences (EMNLP/COLING/etc). I know that different conferences are being looked at differently in the industry and was wondering if it matters which one I'm submitting to if they are in the top 4 for example, vs the next 4 (I'm using the index factor from [here](https://scholar.google.com/citations?view_op=top_venues&hl=en&vq=eng_computationallinguistics)). Or even between the top 3 and COLING (the 4th), as there is a high index difference",MLQuestions,2022-04-06 10:20:56,1
"You start with a dataset that has depth information for images or you have some 3d software that can render 2d perspectives along with the correct depth information.  

From there any method that takes your image vector as input and produces an output vector of the same length, but containing a depth encoding can be used.  Encoder-decoder is a good start.",1,txnass,"Hello all,

Idk if I can post this kinds of questions here, but I just wanted to give it a shot.

I want to write a supervised model to predict depth in an 2d image. I have read research papers that said to use an UNet with the encoder part being a pre-trained ResNet or DenseNet. But I'm not sure how to start with this.

Any help will be greatly appreciated.",MLQuestions,2022-04-06 07:21:43,1
"You have to use encoder to reduce the high frequency component. Consider the output as latent space variable whose dimension is much smaller than the input size. Use latent to create graph nodes and apply gcn. Adj matrix can be created by first splitting the audio into non overlapping chunks using some window size, forward pass into the encoder, choose similarity/distance func of your choice (depends on problem statement) then compute the matrix values.",1,txh1tx,"Hi, I am interested in applying gcn to speech/audio data.

Any advice for structuring an adjacency matrix for such data?

thx",MLQuestions,2022-04-06 01:01:18,1
"> When we talk about large language models, are we still talking about this but built in a different way (ie. a larger corpus?). 

Honestly the ""large"" could both refer to the size of the models (they are ginormous compared to even the largest vision models), and the sizes of the corpuses (terrabytes of text data, which is a lot considering a huge book like War and Peace is only 3mb, roughly the same number of bytes as a single hi-def photo). I tend to believe that most people are referring to the sizes of the models though when they say large language models. 

> Second dumb question, what's the relationship between language models & transformers? My intuition is that it's a generalization on language models to operate on tokens which are not necessarily part of a language, but other domain. Does this makes sense?

Transformer is an architecture component, like a convolutional layer in a ResNet. Language models need not have transformers in them (they don't even need to be neural networks). The tokens encodings also I believe predate transformers, but they are what seems to work now so we use them. They are language models because of the way we train them (specifically with masked language modeling) not because of anything about the models themselves.",3,tx0fxe,"Hey folks,

&#x200B;

Sorry if this is a really dumb question!

&#x200B;

In machine translation, a language model is P(T) - the probability of a given phrase to appear in the target language. When we talk about large language models, are we still talking about this but built in a different way (ie. a larger corpus?).

&#x200B;

Second dumb question, what's the relationship between language models & transformers? My intuition is that it's a generalization on language models to operate on tokens which are not necessarily part of a language, but other domain. Does this makes sense?

&#x200B;

A couple pointers to review papers would be lovely, I'm really lost.",MLQuestions,2022-04-05 10:37:27,2
"I don't remember exactly where I read it, but another intuition is that when using activation functions like sigmoid, you end up with a lot of tiny inputs with close to zero gradient, which are not so useful for training. With ReLU you are kind of enforcing some sparsity, i.e. many activations will be zero with no gradient, as if the neuron is turned off, but some neurons will have positive activation and a non-vanishing gradient, so you end up training a small number of meaningful activation pathways rather than a lot of almost useless pathways.",9,twtou9,,MLQuestions,2022-04-05 05:21:54,14
If its just two predictors try the most basic algorithm. Logistic regression or LDA. Complex computations are helpful with high dimensional data. If you want to try out non-parametric try CART,3,tx3zbz,"What algorithm should I use for the following case:

Value x and value y result in a classification of class 1 or 2. After some amount of records, I would like to predict one record or a set of records to be of classification 1 or 2. I thought Naive Bayes would be what I am looking for, but I have not found any code examples that read how I would expect. Can someone point me in the correct direction? Thanks!",MLQuestions,2022-04-05 13:15:20,4
sounds to me like you want some sort of natural language processing model if you are to do this the right way. depending on the format the specs are in it might not be a trivial task.,2,tx0bk2,"I'm going to use a car example. Lets say I have 3 different Properties that I want to look for:

* Manufacturer: Toyota, Ford
* Color: Red, Blue, Green
* Size: Sedan, SUV, Truck

I want to know what the Manufacturer, Color, Size is if I give you a huge body of text. In order for this to happen, I also want to provide a dictionary of possibilities for each term used for each property.

I want system to spit out the first value if it finds the following, examples:

* Manufacturer = ""Toyota"" if it finds
   * toyota, lexus, lex, most reliable
* Color = ""Blue"" if it finds
   * blue, sky blue, ocean,
* Size = ""SUV"" if it finds
   * suv, big family, Family Truck <-----(notice that it has the word truck in it, since it says ""family truck"", it should not make the size ""truck"")

What is the tool, system, etc I should be using to solve this problem? Looking through Microsoft's Azure services, some phrases stand out are:

* Analyze Sentiment
* Extract Key Phrases
* Recognize Entities
* Custom Multi Category Classification

Are any of these what I'm looking for? The main purpose for something like this is we are a custom manufacturing shop that receives specs sheets from designers, architects, and business owners with floor plans, etc. And I want to be able to parse these word documents, pdfs, etc. (obviously it will never be perfect, but would provide a good starting place).",MLQuestions,2022-04-05 10:31:52,4
"You (and/or your university) might want to reach out to all the major cloud vendors (Amazon, Azure, Google, and the smaller guys) and ask if there are any educational partnerships/discounts/promotions/etc available.",2,twwimi,"Hello guys, I'm a grad student outside US trying to train a vanilla transformer for machine translation for my thesis, I was able to follow the Tensorflow tutorial with a smaller model and a  small portion of my own dataset in Google Colab and it all went well, BLEU score of around 20+ with just a few epochs.


So I tried training a full transformer with the original hyperparameters and around 6 million sentence pairs with max sentence length of around 260 and a vocabulary size of 32k, total trainable parameters of 93 million. One epoch was around ~15hrs Im not sure since i got disconnected since I used up my whole 12 hrs of the day with a GPU(Nvidia K80?),  Colab Pro/Pro+ isnt available in my country.


My dilemma right now is I have a research grant of around 2000$ (converted). The max Desktop PC I can build with this is with an RTX 3080 and a R5 5600 with the prices here in my country. Should I go with this or just go with something like Amazon Web Service or Google Cloud Service? In google cloud I can get around parallel T4 GPUs 12hrs/day for 5 days a week at around 300$ a month.


Much appreciated if you guys can help, thanks!",MLQuestions,2022-04-05 07:41:23,4
"To put it simply, nothing like that exists as an open source project. Unless something has changed in the past six months, because I have been out of the loop on speech synthesis for a bit. 

An alternative option for you is to use free text to speech websites like uberduck or vo.codes or 15.ai. The emphasis of certain words and sentences may seem off but it may be your only option for this type of project.",1,twl9qc,"Not sure if this is appropriate to post here, but in case it is: I am looking for something to change my voice in a way that is more satisfactory and more convincingly varied than what simple voice modulation software can  achieve and as cheaply as is possible (preferably free).

Use case: I have been working on an animated movie to which I am the sole contributor. Though I have been putting it off while looking for an appropriate solution, the time has come to voice my various characters, who are a range of ages, both male and female. For several reasons, I am  interested in voicing them all myself while doing the facial motion captures as well. What I am in need of is, essentially, something that does exactly what Respeecher does, but without the $200/month sub fee. I would love to be in a position to simply pay them what they are asking  for in exchange for what seems like exactly what I need, but it's too much. The alternative solution doesn't have to be easy, it doesn't have to be fast, it doesn't  have to be real-time, and it doesn't even have to perfectly emulate the 'target voice.' What I would like to be able to do is, when I hear a voice that I like for any given character, feed in samples and get back something that sufficiently changes my voice in that direction in such a way that marries my delivery in inflection, tone, volume, and emotional affect with the target voice in such a way that it is convincingly a unique and human voice. It doesn't have to fool anyone into thinking it actually is that voice, just be enough to not be able to tell it's the same person behind each character, while also not being able to readily tell it's computer generation at play. i have tried 'normal' voice  modulation and find that it lacking for my purpose. perhaps it would be  good enough if the range of characters were not so numerous and varied,  but I feel that before I settle for that I should do due diligence to  all the work I'm putting into this thing and try to find the best possible option. I suppose this just reveals my naivety, but I had no inclination that, once it came time, I simply wouldn't be able to find a tool suited to this task (at least, one w/o such a steep price).",MLQuestions,2022-04-04 20:14:14,1
I don't think it gets much more interpretable than CLIP. It gives you a zero shot classifier that can also be used to generate captions for images or even generate images from text.,2,tw43jx,"I’ve been looking at interpretable algorithms and the information that can be drawn from them. Classical algorithms like decision trees are very simplistic and as such can obtain graphs that are interpretable and show how an outcome was decided. 

Are their any algorithms that are interpretable in this nature considered state of the art? I know of XAI tools such as LIME and SHAP, however I’m specifically looking for machine learning algorithms that are interpretable out-of-the-box.",MLQuestions,2022-04-04 07:59:17,5
"with example 1, the input (X) and the output/target (y), are in the same object (`housing`). if `housing` was a dataframe, you would have to take out your target before training, something like

    y_train = trainset['target']
    X_train = trainset.drop(['target'], axis = 'columns')

whereas in the second example X and y are already separated. in this case it assumes `X` and `Y` contain input and output, with the same order row-wise. this is generally used if you have Numpy arrays instead of dataframes.",2,tvz40d,"Here are two examples, I havent seen this explained thoroughly so hopefully someone can help clarify

    train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)

And the other example:

    X = df.drop(['variable'], axis = 'columns')

    Y = df.variable

    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=0)

So what is the difference between one and the other?",MLQuestions,2022-04-04 03:54:04,6
"If you are running out of memory on the forward pass (as it appears from your stack trace, though I can't be sure) then that's a problem. You need more RAM or a smaller model.  Can you just sample from the model normally?

If it is the backward pass that's troubling you, then you have more options.
1) How much of the model are you finetuning? It might be worth seeing if you can get away with just finetuning the last few layers. Really and truly, you should try this before doing anything else. It's very compute-cheap and if it's enough for your problem you'll be glad you didn't spend the time or cash scaling up.

2) Train with torch's AMP. As long as you're careful when doing grad scaling / clipping, then this shouldn't cause any issues, though keep an eye out for noisy gradients. I've used this with diffusion models and it has always been fine.

3) Train using a more memory efficient optimizer. Optimizers like Adam use 3 copies of each parameter, which adds up. There are other options, though in my experience it is problem specific whether or not they work, and in particular with diffusion models I haven't had much luck. Even when it has worked, the gradients are massively more noisy than when I use Adam and it can really mess with the training dynamics.

4) You can also try using the `bitsandbytes` repo and their implementation of 8 bit Adam, though I have never used it.

5) You can try to use something like Deepspeed to do CPU offload of the optimizer's parameters, though it might (probably will) take some careful engineering work.

6) Yes, you can split up the model on different GPUs. This is called ""model parallelism"", and is also something you could do with a framework like Deepspeed or OSLO. This would also in all likelihood be a lot of work though.",1,tvltlh,"Hey folks,

I'm trying to finetune this model:

https://huggingface.co/lowlevelware/512x512_diffusion_unconditional_ImageNet

My command line and error message are here:

https://pastebin.com/30fcTPLs

Note that I'm running on a 3090 and I'm running out of ram at about 21 gigs or so. I already have batches set to 1. Is there anything else I can do that might reduce ram usage a bit more?  Or, failing that, is it possible to pool ram between GPUs?

Thanks!",MLQuestions,2022-04-03 15:25:21,2
Does the loss actually decrease over time when you run it?,2,tvhiqd,"**Edit to anyone who find themselves in a similar issue:** The main problem was that the dimensions of the problem I set is 1510 while the number of particles is 200. PSO optimization works when the number of particles is at least equal to the number of dimensions (in my case it only worked with particles= 3\* dimensions which I got by making the middle layer of the network smaller to reduce dimensions and increasing the number of particles) after this the loss continue to drop instead of getting stuck after a few iterations. Thanks for all your comment and the idea for this solution came from [u/random\_guy00214](https://www.reddit.com/u/random_guy00214/)  
**Original post:**  
The usual way to optimize a neural network for a specific task is using gradient descent where we iteratively try to improve the parameters using gradient descent. This is not the only way, another technique that is widely know is using genetic algorithms like NEAT (NeuroEvolution of Augmenting Topologies). For a project I'm trying to use PSO (Particle Swarm Optimization) to create a neural network for a specific task. Before I started working on the actual task I want to solve I wanted to test this approach on a simpler problem like MNIST. I tried to optimize a network with one hidden layer using PSO with the help of this [library](https://pyswarms.readthedocs.io/en/latest/index.html) which has an example for how to use it for neural network optimization ([example](https://pyswarms.readthedocs.io/en/latest/examples/usecases/train_neural_network.html)) where it works well for the IRIS classification problem. Yet when I try the MNIST problem it failed to learn anything significant. it achieved a loss of 2.310028314590454 and an accuracy of 0.06111111119389534 compared to the same network architecture trained using regular gradient descent achieving a loss of 0.19050493836402893 and an accuracy of 0.9388889074325562. I don't understand why is this happening so I wanted to know if anyone has used PSO for NNs before and If they found out that it not capable of doing this for complex problems or is there a problem with my code.Gradient descent code:

    import numpy as np
    import matplotlib.pyplot as plt
    from sklearn.datasets import load_iris
    from sklearn.datasets import load_digits
    from sklearn.model_selection import train_test_split
    import pyswarms as ps
    from tensorflow import keras
    from tensorflow.keras import layers
    from keras.utils import np_utils
    data = load_digits()
    X, x_test, y, y_test = train_test_split(data.data, np_utils.to_categorical(data.target,10), test_size=0.1, random_state=42)
    print(X.shape,y.shape)
    num_classes = 10
    input_shape = (64,)
    model = keras.Sequential([keras.Input(shape=input_shape),layers.Dense(20, activation=""tanh""),layers.Dense(num_classes, activation=""softmax""),])
    batch_size = 8
    epochs = 15
    model.compile(loss=""categorical_crossentropy"", optimizer=""adam"", metrics=[""accuracy""])
    model.fit(X, y, batch_size=batch_size, epochs=epochs, validation_split=0.1)
    score = model.evaluate(x_test, y_test, verbose=0)
    print(""Test loss:"", score[0])
    print(""Test accuracy:"", score[1])

PSO code:

    import numpy as np
    import matplotlib.pyplot as plt
    from sklearn.datasets import load_iris
    from sklearn.datasets import load_digits
    from sklearn.model_selection import train_test_split
    import pyswarms as ps
    from tensorflow import keras
    from tensorflow.keras import layers
    from keras.utils import np_utils
    data = load_digits()
    X, x_test, y, y_test = train_test_split(data.data, np_utils.to_categorical(data.target,10), test_size=0.1, random_state=42)
    print(X.shape,y.shape)
    num_classes = 10
    num_hidden = 20
    input_shape = (64,)
    n_inputs=64
    model = keras.Sequential([keras.Input(shape=input_shape),layers.Dense(num_hidden, activation=""tanh""),layers.Dense(num_classes, activation=""softmax""),])
    model.compile(loss=""categorical_crossentropy"", optimizer=""adam"", metrics=[""accuracy""])
    def forward_prop(params,d):
        W1 = params[0:n_inputs*num_hidden].reshape((n_inputs,num_hidden))
        b1 = params[n_inputs*num_hidden:n_inputs*num_hidden+num_hidden].reshape((num_hidden,))
        W2 = params[n_inputs*num_hidden+num_hidden:n_inputs*num_hidden+num_hidden+num_hidden*num_classes].reshape((num_hidden,num_classes))
        b2 = params[n_inputs*num_hidden+num_hidden+num_hidden*num_classes:n_inputs*num_hidden+num_hidden+num_hidden*num_classes+num_classes].reshape((num_classes,))
        model.layers[0].set_weights([W1,b1])
        model.layers[1].set_weights([W2,b2])
        return (model.evaluate(d, y, verbose=False)[0])
    def f(x):
        n_particles = x.shape[0]
        j = [forward_prop(x[i],X) for i in range(n_particles)]
        return np.array(j)
    options = {'c1': 0.5, 'c2': 0.3, 'w':0.9}
    dimensions = (n_inputs * num_hidden) + (num_hidden * num_classes) + num_hidden + num_classes
    optimizer = ps.single.GlobalBestPSO(n_particles=50, dimensions=dimensions, options=options)
    cost, pos = optimizer.optimize(f, iters=500)
    def predict(pos):
        W1 = pos[0:n_inputs*num_hidden].reshape((n_inputs,num_hidden))
        b1 = pos[n_inputs*num_hidden:n_inputs*num_hidden+num_hidden].reshape((num_hidden,))
        W2 = pos[n_inputs*num_hidden+num_hidden:n_inputs*num_hidden+num_hidden+num_hidden*num_classes].reshape((num_hidden,num_classes))
        b2 = pos[n_inputs*num_hidden+num_hidden+num_hidden*num_classes:n_inputs*num_hidden+num_hidden+num_hidden*num_classes+num_classes].reshape((num_classes,))
        model.layers[0].set_weights([W1,b1])
        model.layers[1].set_weights([W2,b2])
        return (model.evaluate(x_test, y_test, verbose=False))
    print(predict(pos))",MLQuestions,2022-04-03 12:28:21,12
"I'm currently in a situation like that - I tell people this could take 1 person 1 month, or it could take 10 people 2 years and I truly won't know until we get some data. So I use the message to ask them to help get some data sooner, even if it's a sample. But also that's an unusually broad scope like ""deliver novel business value""

In other cases, if the scope is unclear I like to time-box the research effort. And I set the time-box based partly on the range of outcomes the project might have. For instance, if the best case is going to save us $200k/year, I might say that's worth a $100k investment of time to see if it's viable. The best case is rarely the actual outcome and hopefully we can learn whether it's viable early on and cut our losses.  I don't have exact ratios like that, it's just an example of how I try to reason it out.

Some projects are pretty similar to something we've done before. For those I have a good sense of about how long it'll take and whether it's likely to work or not. I typically communicate a rough estimate of the whole effort up front but start with a short research investigation to double-check that there aren't any major surprises, then communicate any information as soon as I know.

Getting buy-in is difficult and I'll share some advice I was given: Some people are more convinced by data, and some people are more convinced by storytelling. It's important to understand the stakeholders before trying to convince them.",3,tvamku,"When you start any ML project, how do you communicate project timelines with stakeholders?

1. How should we decide if it is a 2-month long project or a 4-month long project if we have not explored the data yet?
2. Is there a better way to present tentative timelines with stakeholders for each phase of the ML lifecycle?

We are in the early phase of ML adoption in my company and getting buy-in on a project from stakeholders without well-defined timelines is difficult.",MLQuestions,2022-04-03 07:34:11,3
"I am not sure on what are you trying to do. But if your purpose is (lossy) compression I would suggest VAEs model would suit you more than GANs. For sparsity you can add L1 regularization on VAEs latent space

But if you just want to convert word inputs into vector representation, just use random embedding layer should be enough (you have to map your word list to one hot vector though some dictionary), then train your model end-to-end

But if you aim to generate a sequence of “words” which match your input pattern, I think you should look into text-generation scheme. A simple way to do that is to use word embedding + RNN (LSTM/GRU)

But if your words have meaning (like a sentence), it might be better if you use knowledge transfer on larger model like BERT (or distill BERT if you are on a budget)",1,tv5sfv,"Hi there, sorry if this is an overly-simple question, but I have done **zero**  work on generative models. I have plenty of experience with different  types of supervised classification models and autoencoders, for NLP,  bioinformatics and signal processing applications, but I'm only now  becoming familiar with GANs and VAEs. Just making sure I can use the  right keywords to look for the ideal model, if it exists:

If  I would like to generate a one-dimensional vector output given an input  embedding (e.g. a word embedding, but not necessarily), would a regular  GAN be enough? Imagine for example the output would be a sparse vector,  which represents configuration parameters for a machine, and the input  are word embeddings. What I'm trying to get to: after training on input  embeddings of ""rain"", ""grey"", and ""hail"", and given the ""thunderstorm""  embedding as an input, to generate a new sparse vector that follows  similar patterns of the ones I trained on.

Would a sample size of \~500 be way too small to train a generative model  like these, or is this still somewhat reasonable? (can increase some, maybe to 1000,  but training data is expensive to produce)

Thanks for any hints! \[EDIT: grammar\]",MLQuestions,2022-04-03 02:53:28,1
"If you don't have a balanced dataset (not having data for all labels in close proportions), it's quite obvious that your accuracy scores will be bad. Your model won't be able to learn the discriminant space. 

In this case what you can do is, use a text generation model (from huggingface) and generate enough examples for the other labels. This isn't perfect but since you don't have enough data, this can help for now.",3,tuoybc,"I need to build classification models for about 20 categories. But the problem is I have data for just 2-3 of those categories. I have no historically available data for the rest, but I could type out 200-300 sentences manually for each category. But it doesn’t seem to be working well with such a small corpus. 

Things I’ve tried are:

1. Using different encoders (google encoder, sentence-Bert, infersent) etc and building Naive-Bayes or LR, RF models on top of the sentence embeddings obtained.

2. Deep learning with 1 hidden and 1 dropout layer. 

3. Transfer learning by fine tuning one of the pretrained sentiment classification models from huggingface.

4. I’ve also tried out zero shot models from huggingface (specifically: facebook/bart-large-mnli), which gives decent enough out-of-the-box performance, but the moment I try to fine tune them using the premise-hypothesis approach, their accuracy seems to fall drastically.",MLQuestions,2022-04-02 10:57:28,20
"[This is the paper for the MOMPO algorthm](https://arxiv.org/pdf/2005.07513.pdf)    

The link in the OP is a Torch implementation of the MPO algorithm. Also, [here is a Tensorflow implementation fo the MOMPO algorithm. I don't know Tensorflow. I was hoping to be able to find an MOMPO algorithm in Torch, or to get some help learning the difference to be able to modify the MPO code.  
If you don't know what either of these algorithms are but maybe have the math and coding skills to learn, they are Reinforcement Learning algorithms. I believe they are a type of Q-Learning. I am specifically using them to train an agent to have multiple objectives. i.e. conserve energy and score points.",1,tu7m2b,,MLQuestions,2022-04-01 18:57:02,1
"In my previous work, I did a combination between pdf2png, pdf2txt and tesseract and worked very well (https://revistaeletronica.pge.rj.gov.br/index.php/pge/article/view/205)",5,ttow90,"I want to extract texts from multiple pdf files with complex structure (contains lots of tables and logos) . What kind of model should I use for this task?

What I have already tried :

1. PyPDF2 : didnt extract meaningful texts as the PDF files contains lots of complex medical names.
2. EasyOCR : converted each PDF page to image and used EasyOCR to extract texts from it. Works mostly fine, but I would like to remove the image conversion step and extract from PDFs directly. If not feasible, image is also fine, but would like to increase the text extraction accuracy.

Any suggestion is highly appreciated. Thanks.",MLQuestions,2022-04-01 04:56:23,5
"Sorry, don't have much experience with transformers to answer all your questions. Generally, I'd say the learning rate is more related to batch size than model size. I've not tried it myself, but from what I've read cycle learning sounds like a good approach to quickly find the best learning rate

https://arxiv.org/pdf/1803.09820.pdf",3,tt6vh1,"Hello everyone,

I am trying to train a self-supervised audio transformer based on the data2vec paper. I am relatively new to training transformers and have a few questions:

1. I've heard that transformers need large datasets in order to generalize well. I am currently using the maestro dataset which consists of around 120 hours of piano music. Is this considered a large (enough) dataset?
2. Should one increase or decrease the learning rate with the model size (#parameters)
3. Is it reasonable to have a bigger model dimension (width) than sequence length? Or does this lead to overfitting?

I have one V100 GPU available. However, in order to be able to efficiently test different models in reasonable time I can't use the model size from the paper (with proper batch size) and have to reduce my model.

4. There are several hyperparameters that can be tuned (hidden dimensions, number of layers, number of heads, sequence length). How are these hyperparameters correlating with each other (are they orthogonal)? What do you recommend for decreasing the model size in order to effectively train the model (biggest contraint is probably GPU memory)? What is the effect of the different hyperparameters?

5. Do you have any experience with linearized attention methods? Are they any good? (e.g. Nyströmformer, Performer, Linformer, ...)

&#x200B;

Thanks in advance!",MLQuestions,2022-03-31 11:55:19,1
"Missing data and anomaly detection don't go very well together, in the sense that we often deal with missing data by imputation, but imputation will fail to fill in rare, anomalous data. However, since you have ~30 features, it would also be shame to discard those other measurements just because one feature happens to be missing. If all features are missing, things may become difficult unless, as you suggest, the missingness itself is a useful feature.

One simple suggestion is this: for every feature series `X`, create a second time series `X_is_missing`, that is 0 if not missing and 1 if missing. Then deal with the missingness in the original feature as best you can, probably some form of imputation. You can even see whether you can predict anomalies using only the `is_missing` features.",1,tt7es0,"I'm trying to figure out a good starting point for my problem. I have a dataset that's composed of a timeseries with \~30 features, and I would like to predict the chance of an anomaly at each timepoint as a scalar. The dataset includes an initial epoch where I am sure there is no abnormality, which I was planning to use to train my model. This epoch should include all of the ""normal"" variability.

The complication there are some timepoints where some or all of the features are missing, and I'm unsure of how to deal with them. Ideally, I want to incorporate the presence or absence of a feature as a feature in and of itself - because the temporal pattern with which they go missing could be significant.

Does anyone have any recommended tools to start with? Ideally, it would be something relatively simple and well documented.",MLQuestions,2022-03-31 12:19:57,1
"I am not sure why you would normalize the segmentation values. Think about those results as classification. Your model is saying these pixels belong to this class.

If you want to visualize the results you can create colors for the various classes and set pixels to the colors associated with that label.

Also not sure what your last question is asking. “do I somehow quantize the output of my model with just the amount of segmentation types”.",3,tt7c8b,"My dataset has the segmented images stored as integers. Normalizing those images results in small floating point values for each segmentation type.

Is that how that is done? Or do I somehow quantize the output of my model with just the amount of segmentation types?

Thanx.",MLQuestions,2022-03-31 12:16:43,9
"A decision tree/forest or similar type of model would be the best choice here, especially since there are relatively very few features. No need to overcomplicate things, and you’ll be able to see which features are the most important.",1,tt4qcu,"Hello everyone ! :) I wanted to practice ML, and I came upon an idea to create a model that will predict future prices of collectibles. But just as I was creating a dataset(Dataset will have 15 000+ entries but the prices are random entered by me), I came upon a problem as I am not sure how should I tackle it and what model should I use for it. So I wanted to ask for help on this subreddit. The ink to the imaginary dataset is here https://imgur.com/a/ONheBwh and I would really appreciate if anyone can explain me the thought process of finding the solution. (I am not looking for any accuracy 70+%, as I know that it wouldn't be possible with this random dataset ) Thanks in advance :) !",MLQuestions,2022-03-31 10:19:02,7
"No this is not overfitting. Your training loss and validation loss track each other throughout training and are almost identical on the last epoch. If anything you might be underfitting. You could try a more powerful model (more layers, or higher width of layers if using neural net).",29,tshhvk,,MLQuestions,2022-03-30 13:04:13,12
Yes. Your model is overfitting. Try to generalise your training data.,1,tstt9g,,MLQuestions,2022-03-30 23:45:06,5
"I can tell you something from my experience. For me it is almost 90% of the time doing data preparation (cleaning, understanding, modelling), using Excel and SQL mostly. The last 10% I work on my visualizations (right now at Apache Superset) and analysis. I work in a small company so there is that, not a data team (data engs, data scientists... only me)  


Hope this gives you some insight, but as I said, it's only my personal experience on a small company and data roles may vary A LOT",14,xi1qj8,I want to become a data analyst. I am currently studying computer information systems in college. I just wanted to know what is the day to day like as a data analyst and what skills you need and use in your job.,dataanalysis,2022-09-18 21:24:27,11
"Ask what they need help with. Get some data sets. Play around with them. 

Find out how they evaluate performance. They may be so busy they haven’t had time to plan. At the same time, check out the data and understand the business and show the value you can bring to them. They’ll be impressed with the initiative.",3,xii9mf,"I just started a remote job as a contract Data analyst (my first tech role) and so far it’s my second week and I’ve done next to nothing.

I have two managers that are very busy, and I find that most of my days have just been me waiting around for them to be free between meetings so they can walk me through the next steps of the one or two things I’m working on.

I’ve been using the extra time to learn technologies the job requires that I’m not familiar with, and studying up on my data analysis skills overall, but I feel useless in the role, and scared they’re just gonna let me go.

Is this normal? Should I be concerned they over hired, and try to find a new role before I’m left completely jobless? Any guidance would be appreciated. I have a family, so the stress of being laid off is always weighing on the back of my mind.

I know nothing is guaranteed, but is this a red flag?

tl;dr: I’m doing nothing at my new job and afraid I’m gonna get laid off.",dataanalysis,2022-09-19 10:06:56,1
These are questions you should ask in the interview,1,xigapj,"Has anyone taken the Greensboro college and silver tone analytics apprenticeship? I have an interview but I don’t have much information on it and I can’t find reviews online. I’m wondering if a job placement is garnered, if it’s geared more towards data analytics or science, and how much it’ll cost. Also- if it’s worth it overall.",dataanalysis,2022-09-19 08:57:12,3
Are you trying to be a data analyst or a data scientist?,2,xi0676,"I feel very comfortable using Python, Pandas, Seaborn, Matplotlib, Jupyter Notebooks, Excel, and intermediate statistics. The thing is that I learned everything separately, and I don't have any idea about how to apply it in the best way to an actual project, meaning, the characteristics that the model should have, where should I implement it (I heard that Jupyter notebooks is not a good medium to show to the clients). I neither have learned what to do with NaN values when there are lots of them and dropping them leaves a table without data. 

I have some Jupyter notebooks on GitHub, making use of the frameworks I  mentioned but they look awful. 

If you don't have a learning path or book recommendation, can you give me some advice? 

This is my github with the Jupyter Notebooks:  lordaris/Data\_Science\_learn",dataanalysis,2022-09-18 20:06:16,1
"Bar (stacked),  quantitative data ( real sales numbers over time by category)",55,xhiv7u,,dataanalysis,2022-09-18 07:58:33,12
https://youtu.be/y3TxcejHw-4,2,xi8ij2," Data Analytics and Data Science are the buzzwords of the year. For people who are looking for long-term career potential, big data and data science jobs have long been a safe bet. This trend is likely to continue as AI and Machine Learning become highly integrated into our daily lives and economy.  Data analytics focuses more on viewing the historical data in context while data science focuses more on machine learning and predictive modeling. Data science is a multi-disciplinary blend that involves algorithm development, data inference, and predictive modeling to solve analytically complex business problems. 

[https://youtu.be/62ERGL-BdJs](https://youtu.be/62ERGL-BdJs)  


https://preview.redd.it/1i4lnqh5pso91.jpg?width=640&format=pjpg&auto=webp&s=c7d2356995d26c736bde07b3fdf00d951df01f67",dataanalysis,2022-09-19 03:47:36,3
"Maybe ask whats the goal of the campaign?
Which ads show success and which demonstrate failure?
Is there a seasonal trend where there is success or failure( day of week. Month. Hour. etc )",2,xhzy3s,What type of analysis can I do with the datasets of Google and Facebook ads? most of the ad guys don't do advance analysis of their data which I think could be useful for them to get insights from their data. So If anyone can share advice and their experience :),dataanalysis,2022-09-18 19:55:22,1
Sure,2,xhs54o,"Hi, 

For a school project it is required for me to interview someone in a field I am interested in pursuing, which decided to be data analytics. If possible, I would like to do the interview over zoom, but I will completely understand doing it over PMs. Also, if it is not too much to ask, I would like a LinkedIn or anything of the sort to be provided, just so so can be sure of certifications and the like. If this was the wrong place to send this request, please let me know. Thank you to anyone reading.",dataanalysis,2022-09-18 14:08:22,3
"SQL. Just learn that. Maybe some Python if you find yourself motivated. 

After that, jobs don’t usually care. They want experience. So you need to focus on gaining experience solving real life problems, preferably in your existing job. Adding tools to your tool belt won’t be relevant.",2,xhwzd4,"Hi! I’m trying to get a feel for the “order of operations” for learning programming languages and tools.

I have a list of tools I think I need to learn, but don’t know what to do first and next?

From reading, I think the current tools I need are:
SQL, Python, Power BI, Tableau, MongoDB

What’s the order I should be learning as a beginner? And are there any missing that you would suggest to make myself be a better analyst?

I appreciate any help or tips :)",dataanalysis,2022-09-18 17:36:32,4
Can you better define strong LinkedIn profile?,28,xgzlhg,"It is SO important to have a strong LinkedIn profile and turn on the setting that allows recruiters to be able contact you. 

I’m currently at my third position in this field, and I have gotten all 3 of my jobs via LinkedIn.",dataanalysis,2022-09-17 15:28:50,9
Are you trying to be a data scientist?,2,xhgy9a,,dataanalysis,2022-09-18 06:40:04,7
Does myData already have a structure? If so... what is the create statement for it?,1,xhd2ti,,dataanalysis,2022-09-18 03:24:05,2
"Congrats!

Always remember where you came from and pay it forward when you’re the ceo",5,xh26w5,My company had me in a rotational role as a junior analyst for claims for a little over a year now. Originally was an adjuster for claims for a Fortune 100 insurance company. Now got promoted to a permanent role as a Product Data Management Analyst and a huge pay bump. Got some advice from this subreddit and followed others suggestions and just wanted to thank the whole lot of ya. Pretty darn stoked.,dataanalysis,2022-09-17 17:27:22,3
https://corise.com/course/sql-crash-course,8,xgyont,"We are experimenting with a new SQL cohort-based course and will be doing the first run for free. The class is taught by Sourabh Bajaj, the Co-Founder of CoRise and former team lead at Google Brain.

It's project based, had live lectures, support from a TA, and an awesome community of other professionals. If interested I am including the link below. It's our first time running it so would love feedback.",dataanalysis,2022-09-17 14:49:09,1
"Depends on the company and role. 

The JD you’re looking at should give you relevant details.",3,xhicc1,What is the responsibility for the data quality analyst role? Can someone explain to me? Thanks!,dataanalysis,2022-09-18 07:37:29,1
"My experience is kinda lucky. I was approached by hr of an ad-tech company and was offered an internship position. It happened on my last semester and I didn't have a single post back then. So few points to take out of it:
- LinkedIn profile page should be a better representation of your CV.
- ""about"" section should have 'keywords' and be similar to what they write at job descriptions. For example : I'm s problem solver and my investigating skills are....
- github repo with at least some ipynb notebooks from school projects. Preferably some kaggle competitions stuff. That can sometimes cover up for lack of industrial experience. 

Back to your question:
I think those can't harm. Being proactive usually pays off.  In that way you may discover yourself  and show others what you are passionate about.

Last last advice - best way to get a job is by referral from a friend who works at the company. So be relentless and sit on your friend's heads (if needed) and ask them to throw in few good words for you. They tend to get bonuses for that too.",3,xh67e1,"I'm seeing a massive spike of people posting theoretical knowledge on LinkedIn. I have also heard that recruiters reach out to you for job opportunities (it never happened to me, I always read it somewhere or the other).

Now, My doubt is that me being a fresher with no work experience, Will posting theoretical content help me in getting a job in Data Analytics or Data Science role.",dataanalysis,2022-09-17 20:46:48,1
so it begins,1,xh7cgi,"I've been working on a WhatsApp parser for DoubleText, my messaging analysis tool on the side.

It works for my WhatsApp chats, but sometimes it breaks when there are global users and the time formats aren't supported.

For my pre-launch I'm giving away all the WhatsApp features for free to reddit users, just so I can gather feedback on what doesn't work, to make it more resilient. You can turn off the internet before uploading if you're wary about privacy, but I don't store the messages anywhere.  


Use this link with the coupon code:  
[https://doubletext.me/?coupon=whats-app-reddit-pre-launch](https://doubletext.me/?coupon=whats-app-reddit-pre-launch)

Thanks everyone :)",dataanalysis,2022-09-17 21:47:44,1
How did you like the Tableau 2022 course? I'm just about to start it. Do you feel like you know what you're doing after completing it?,3,xh1d5e,"Hello all,

I am looking for a switch in careers by gaining some Data Analysis skills in my field. I'm looking to start learning from courses in Udemy since they are free for me.

I have completed Tableau 2022 A-Z: Hands-On Tableau Training for Data Science by Kirill Eremenko

I am also almost done with The Complete SQL Bootcamp 2022: Go from Zero to Hero by Jose Portilla

I want to get an introduction into Python next. Naturally I am looking at Jose Portilla's Python Bootcamp for Data Analysis, however the comments says that it is outdated and is mostly Python 2 code.

Any recommendations for free courses I can begin to learn Python for Data Analysis?

Also I will appreciate if there are some resources where I can learn more and practice SQL. Jose's course is a good introduction, but I am clearly still unable to write syntax on my own without much referencing and need more practice.",dataanalysis,2022-09-17 16:48:28,7
"Excel is a must; focus on learning about creating tables on it, formulas such as vlookup, sumifs, countifs, concatenation, remove duplicates, etc. I highly recommend learning Power Query in Excel; it has powerful and easy-to-use features that can do amazing work without having to delve in SQL or more complex data extraction processes. Just my two cents.",8,xh3z86,I've already started to learn sql. Wondering if there's anything else I need to learn.,dataanalysis,2022-09-17 18:54:14,6
"Im not sure, but i believe the first step is to find the attributes with highest correlation with the number of customers",1,xh8ywf,"Company is looking to set new KPI metrics target based on a goal of increasing number of customers by 10% over a period of 2 years. How best can I decompose this goal (10% growth). I have 8 metrics that I measure and monitor. How best can I decompose this goal (10% growth) on the KPI metrics?

Thanks",dataanalysis,2022-09-17 23:19:31,1
"Why do you want to change the title? I don’t see anything in the job description that includes handling analytics? Sounds more like a marketing generalist role. 

As for incorporating data analysis into your role, you should be able to analyze the social media and email content to see what performs best. But that’s a typical part of many marketing generalist roles.",3,xh66sy,"Hey, I just got a job offer. The title is Marketing but also include some support for international trade/sale logistics. 
I have experience in Excel and some sql, power bi. But the company is not large enough to use sql, power bi. How can i use data skills to support the process and do marketing research for finding new clients in other regions.

Also, i am thinking of telling my boss that i want to change job title from marketing to marketing analytics, but i don't know what marketing analytics can do in this particular position. Can you guys help me?

Here is my job jd:

Working hours: M W F from 9 am to 12noon.

Working checklist:

Argentina Food Export:
- Post news and branding on our social media RED IN and FB.
- Market research: new markets clients in SE Asia and Vietnam.
- Sales support: help to send emails, and calls to our clients and new
ones.
- International trade supporting: helping to follow our sales documents
by imports and exports companies.

Del Plata Argentina Market:
- Design flyer for posting in our social media: FB and IG.
- Help to send mkt materials to be printed.
- Promote branding.
- Sales support: HORECA (hotels and restaurants) market research",dataanalysis,2022-09-17 20:45:56,7
"Congrats on the job offer! I learned SQL up to an intermediate level and started the Google course last week and already on course 4. I’m surprised at how dull it is as I thought it was going to actually be much more in depth. I’m 27, currently a truck driver for FedEx. Being able to actually switch careers to data analytics one day sounds like a complete fantasy unfortunately but seeing posts like this gives me hope!",67,xg0c4u,"Basically I got lucky. I finished the course on August 27th. I then applied for 10 jobs. The next day I posted my resume on r/resume for some feedback. Turns out my resume was terrible. After changing my resume I applied to 40 more jobs and updated my LinkedIn. Out of those 50 total applications I put in, I got two interviews. I made it to the final interview for both jobs, but didn’t get an offer.  But since I updated my LinkedIn, a recruiter reached out to me. For context, I was an accountant and I have a degree in accounting. He was looking for someone with strong accounting knowledge and SQL/Python skills. I was the perfect fit. I nailed the interview process and signed my offer yesterday!

EDIT: Many have asked to see the resume that got me the job, so I've added it. I'm aware, it is not fancy. I've whited out all of my personal info. 

&#x200B;

https://preview.redd.it/k1zxg34pkbo91.png?width=1227&format=png&auto=webp&s=4f37d4c039c7569cf4f982eba91fd4d8c741ebc8",dataanalysis,2022-09-16 12:19:32,79
"I swapped out python for a manual task at work that has always been done in excel in a data cleansing and organizing task and I didn't tell anyone. It increased my productivity and reduced chances for errors. I ran it along side the manual way a few times and the results were the same so I felt comfortable relying on what I wrote in python to do the task. Now, after about a year, people are learning I've automated this portion of the job and want the whole department wants to use it.

So, up to you. If you can prove the worth and show that it's valuable, your manager would be a fool not to use it or allow you to use it.

Edit: adding the caveat of its worth asking, but if your company or client has strict requirements to use excel bc xyz, then that’s that. If not, though, it’s usually worth asking *if* you can prove it works and that it’s more efficient for everyone. Not just you.",10,xgivsv,"In around 2 weeks I'm finally starting my first job as a junior data analyst and I'm super excited about it. Tools used in this company are SQL (of course), Excel and Power BI. My only worry is that I'm not super comfortable in Excel (I know all the most important functions for this job etc.) and doing EDA in Python feels a lot more native and straightforward with many more options. Would it be too rude, as a complete newbie, to ask my manager whether it's possible to use Python for EDA instead of Excel (with all of the pivot tables/cleaned data saved to xlsx file if needed ofc). Or should I just go with the flow and dont't bring it up at all?",dataanalysis,2022-09-17 03:24:10,19
"Sounds like you clicked the add database connection button in sqlitestudio instead of import. Import should allow csv as an option.

Edit: you may have to create a database first. You can create a blank database by creating a text file name it anything then delete the .txt extension and replace it with .dB extension. It will then recognize it as a database and allow you to import and create a table.",3,xgxruq,"Hi there,

Long story short, I'm trying to upload three CSV files into a RDBMS and then use SQL to clean/process, then import it to Tableau for visualizations. I'm having issues getting a free RDBMS for mac to cooperate.

**What I've done so far**

Found Iowa Liquor Sale data in BigQuery's collection of datasets. Unfortunately after cleaning and processing data to my liking, BigQuery was unable to export the full CSV file I was trying to extract(I'm only using data from years 2019-2021).

No problem, I'll just have to use a different program. I found the datasets online, downloaded the sales from 2019, 2020, and 2021 as three CSV files and have tried two different RDBMS's. 

When I tried MySQL, it downloads, I install and then nothing happens. No interface pops up and I can't even find it on my computer. Google told me to run a script in terminal that shows Hidden Files and that didn't help to the best of my knowledge.

I next tried SQLiteStudio. Download, install. App opens, I go to upload/import the csv files and its seems to only recognize .db, .sdb,.sqlite, .sl3, etc. 

I feel like I'm out of the loop. Any help or suggestions? Just trying to upload three CSV files to then use SQL queries to process/clean Running macOS 12.5 if that makes a difference",dataanalysis,2022-09-17 14:10:31,8
"If I were you I would get a month free trial of LinkedIn premium. Take assessments to get skill badges, they’re pretty easy if you’re intermediate-proficient in a specific skill. LinkedIn premium let’s you know where you stand compared to other applicants for a job. When job searching, apply for jobs posted in the last week that you’re a top applicant for. You can actually see your percentile amongst other candidates and compare your skills. You might want to also look at jobs posted for longer than just the last week- and apply to those that didn’t receive a lot of applications and you’re a top applicant. Also look for jobs like Quality Engineer, business analyst, operations analyst, research associate, production analyst as they all need those skills that you have.",12,xgfjc1,,dataanalysis,2022-09-17 00:04:53,4
"Maybe I’m misunderstanding the context, but I’m not sure it makes much sense to incorporate the opinions of people who are professionals in the field to determine the public perception?",1,xgqu5j,"Hello everyone, I’m conducting a study on how statistics and data visualisations shape our opinions. Please help me out by taking part in this survey. Link to the survey : [https://forms.gle/9XNCxqo2HPe1Vdja6](https://forms.gle/9XNCxqo2HPe1Vdja6)

It’s a short 1-2 minutes survey which has 8 compulsory multiple choice questions & 1 optional text-based question.",dataanalysis,2022-09-17 09:23:41,1
"What do you mean?

Like, what tools I have used?

Projects I have worked on?",2,xgs5v8,Assuming we are talking 2-5-years later as well.,dataanalysis,2022-09-17 10:16:21,5
"It probably isn't 'missing' any columns, it has keys that allow it to be joined to the other tables that *have* the columns in the appropriate way - retailer code to go_daily_sales, and product number to go_products.

I would recommend doing a bit of reading or looking at SQL basics - joins are a fairly integral part.",1,xgo6le,"Can anybody explain these two tables for me? ""go\_1k"" table is missing 3 columns ""order method code"", ""unit price"", and ""unit sale price"" of table ""do\_daily\_sales"". I'm doing EDA in mysql and confused what to do with the ""go\_1k"" table as it is missing some columns.   
dataset link:[https://relational.fit.cvut.cz/dataset/GOSales](https://relational.fit.cvut.cz/dataset/GOSales)",dataanalysis,2022-09-17 07:36:55,3
"This gets asked a lot

Data is three fields: analytics, engineering, and science 

Data is still a growing field and there really isn’t a “starting point.”

Depending on what you do now, you could start applying. 

It’s okay. I’m actually looking to do soemthing else.",3,xgt64t,"Wanting to switch careers, where do I start and how much can I learn in single year? Also, if you don’t mind, what’s the job like?",dataanalysis,2022-09-17 10:58:09,6
When are you going to be doing this workshop?,5,xfj0e4,"Hello there. I have been in the data analysis field for 10+ years. Based on posts, I noticed a lot of new and interested individuals who don't know where to start. I'm offering a **FREE** basic introductory 1-hour workshop. I will share some tips and techniques I have used in the real working world. 

This workshop is **ideal for people:**

* With **zero experience in data analysis**
* Who wants to **understand the concept/theory** 

If you're interested please view the **following criteria:**

1. Have **access to Google Sheets and Google Data Studio**
2. Some **basic knowledge of Sheets/Excel**
3. Able to use **Google Meets (audio)** 

**Agenda:**

1. Cleaning Data - tools and techniques
2. Pivot Tables/graphs/charts - when and which to use
3. Google Data Studio - reports vs dashboards 
4. Writing - summarizing and recommendations 

**What do I want in return?**

* You to be interactive during the workshop - please **bring questions and good vibes**
* Your **honest feedback** on my workshop- I'm trying to help people and hopefully can pivot this into something useful
* **Don't be a creep** 

**Disclaimer:**

The tasks and roles of a data analyst may vary from company and industry. I'm sharing my professional experience and what I believe a typical jr data analyst or entry-level data analyst will be doing. I cannot cover every possible tool or software. Therefore, I decided on using Google because it's free and allows learners to get their feet wet without spending money. I will provide other useful sites and tools as well. Please note that I'm not a data scientist nor do I claim I know everything but I  am willing to help others to learn.  

At this time, I am taking **only a handful of students** because this is a trial run for me as well. Please PM me if you're interested. Again, this is FREE and I only want your feedback on my workshop. Thanks!

-UPDATE 1-

**WOW thank you so much for everyone’s interest!!! At this time I have reached max capacity. However if you are still interested, please leave a comment and if there is another FREE workshop, I will contact you!! Thank you again!**


-UPDATE 2-

**THANK YOU SO MUCH EVERYONE. When I made this post, I was hoping for 2-3 interested people. I am truly flattered by all your interest. I have to close the waiting list. As you can see there are many interested individuals. I am going to work on trying to add all of you guys to a workshop. My personal goal is to keep these workshops live and in small groups (5-7 max). I will keep you guys updated! Thanks again!**",dataanalysis,2022-09-15 22:50:17,129
"Nice, thanks. Usually I hate these ""Day in the life"" videos because they start off so cringy and are more about somebody's routine and life and not much about the job or what they do. Nobody needs to see someone wake up to an alarm (as if they didn't just crawl back into bed after setting up their recording device), check social media on the phone, shower, dress, brush teeth or exercise. lol.",1,xfvdsr,,dataanalysis,2022-09-16 09:06:43,2
"It seems like you know the problem but you don't understand the problem. 

I find that structure is pretty much everything. So start by breaking down the problem into smaller problems. Then start working through them.  So for example. 

Overarching problem: Create process to extract, filter, analyze, and respond to customer emails. 

Smaller problems

1. Accessing email data
   1. What email service are you using?
      1. Do they have api connectivity? 
      2. Is this a common use case? Does alternative infrastructure (in house or external) already exist that pulls email data?
      3. Can you scrape the data utilizing a python script?
   2. Once you have the data...
      1. Where will it be stored?
      2. Do you have a cleaning process in mind? What does your data need to look like? 
   3. How will email formatting impact your analysis? Do you need company specific formats? 
   4. What information do you actually need? 
2. What type of analysis is being conducted? 
   1. Do different companies require different analysis? Or, does each company get a similar plug and play report? 
      1. What are the most common business needs?
      2. Do you need to identify company specific factors? 
      3. Does your data ACTUALLY align with your problem? 
   2. What are the primary questions you need to answer and do you need to combine datasets to get to those answers? What does automation success look like? 
   3. How will you know if a manual review is required? 
3. What does your response look like? 
   1. Are you outputting to text? Will you need visuals? How will your analysis interact with your end clients? 
   2. What level of granularity do you need to have an accurate response?
   3. Are your clients more interested in speed, quality, or depth? 

So let's say that these are the hypothetical questions and sub-questions you have. You could then take each step and work through the complexities to better understand your overarching problem. We've taken the bigger problem and broken it down to a smaller question. 

You could then use these questions to seek out specific answers. So if the first step is accessing the data itself. Explore the pathways of access and consider their tradeoffs based on your needs. 

Once you have the answers to these questions, you can work towards implementing them based on current work flows.",4,xfvp0s,"Hi all, 

I have 100k emails that I need to extract data from, analyze, and respond to based on a company mandated workflow. They’re from all different businesses, and most wouldn’t require manual review if we can automate the process. 

I’m a new data analyst (first week) and I would appreciate any advice on how to solve this problem. I know this is a “do your own research and figure it out” type of situation, but if someone can give me a good solution, I will Cashapp you my next quarterly bonus. 

Cheers!",dataanalysis,2022-09-16 09:18:39,3
Snowflake is basically cloud based storage for databases - it uses SQL as the query language so you shouldn't have any issue transitioning over to using the Snowflake platform other than a little orientation of where things are.,5,xfxqa8,"I have interview coming up in regards to a data analysis position. They require SQL which I am proficient in but they also require SSRS and Snowflake which I have never used.

If I know SQL specifically MySQL how easily can I learn to use SSRS and Snowflake? Do you guys think this will be an issue during the interview?

If it is not an issue how do I let the interviewer know that it is something that I can pick up easily?

&#x200B;

Edit: I'd like to add that I know Tableau and data visualization generally.",dataanalysis,2022-09-16 10:37:23,7
"Just start applying 

A specific certificate is not going to make you more marketable",2,xfye4a,"Hi all!! 

I’m hoping I can get maybe a little guidance or advice here. I graduated with my B.S. in finance this past may and have been looking to break into this space. I’m pretty confident that I’ll be getting my masters in data analytics starting this upcoming November. 

I completed the Google Data Analytics Professional Certification a few weeks ago and have been debating between doing either the UC Davis SQL certification, or the IBM DA certification. I’m open to any others as well, I’m just looking to learn and grow my skill set and become more hire-able! My goal is to get some type of analyst role ASAP. Thanks!",dataanalysis,2022-09-16 11:03:18,4
Tableau is strictly a data viz software while Power BI is a end to end BI tool with Python integration. Honestly not sure why these two software are  compared.,6,xfneff,"I’m a MacOS user and I understand Tableau is the one one that runs natively on my machine. I want to invest in a specific tool to begin, so I’ll be happy to read your insights.",dataanalysis,2022-09-16 03:16:02,8
"I am also in the same boat as you are. I have started with individual projects for now, and will definitely look in the hackerrank challenge.",4,xfl6n7,"Hello All,

I am trying to steer my career into data analytics. I learned SQL from Udemy and YT. After that I am practicing on Hackerrank. I recently found out the **8 week SQL challenge by Danny Ma**. It has 8 different scenarios and datasets to practice queries. The challenge has business case and you have to write queries for questions given. I wanted to show my skills in SQL on my resume. I am currently working through it. Can I put this 8 week SQL challenge in resume under projects? Since I don't have any other project which I have done using SQL?

Or should I specifically do separate project?",dataanalysis,2022-09-16 01:00:17,1
Its all “Business Intelligence” these days….,3,xfy7d7," I joined as a data analyst.Now I got a little confused about what I am doing here. Because a colleague of mine told me what I am doing is DEMAND PLANNING.  Let me explain a little bit about my job kindly suggest this is related to data analysts or not.

1- Doing costing for the PRODUCTS
2-  Creating a sales forecast based upon the sales planning workbook which is entered by sales team
4- Based on the sales planning workbook, I am planning the production as well
3- Making a report based on the Production  whether we achieved the target or not in a regular basis.

Kindly tell what I am doing is analysis or not. Moreover data scientist isn't comes in IT, Is it??",dataanalysis,2022-09-16 10:56:22,3
"You're going to want additional excel / SQL / Python on top of either of these two courses.  

Excel skills for business is good.  

Adding a Datacamp subscription can help with SQL/Python/VizTool",3,xfn6xe,"I've watched comparison videos for the two courses and all of the guys/gals recommend the google course + a python and an excel course as an option. From what I see as job requirements, the majority of data analysts / scientists use python and excel and the google course doesn't teach either. The IBM has an excel and python modules included in the course, but not tableau like the google one. I believe the right approach in this case should be the IBM course + a tableau course maybe. What do you guys think?",dataanalysis,2022-09-16 03:04:12,4
"Business Intelligence Consultant

Took me a year. Was in 2 separate bootcamps for 8 months.",3,xftruc,"What was your first job in data? How long did it take you get into it? Was it to you expectations or a complete surprise? Looking for any sort of insights to the the field as it’s new to me.

Thanks!",dataanalysis,2022-09-16 08:04:45,5
An answer is Excel has functionality to connect to external data sources such as sql databases.  As data is updated in the remote data source excel can fetch that updated data by refreshing connections.,1,xfskx8,"So I once had this interview where they said ""so if you have a sql database or an excel sheet, is there a way in which, say you create a dashboard on excel, and then you update your sql database, how can you can your excel dashboard update dynamically""

So in my current role I've never dealt with anything like this and my answer was ""personally the data I work with is mostly static, however I understand that there are various ways of data being updated dynamically especially through new cloud technologies etc etc and I am very eager to learn""

I didn't get the job as I clearly didn't have experience in this area so can someone give me the correct answer to this question?",dataanalysis,2022-09-16 07:18:40,2
Use the search function to research would be a good first skill,32,xfcjhm,How do you get a job in Data Analysis with 0 experience?,dataanalysis,2022-09-15 17:27:29,25
"1. What are you being paid now? 
2. How long have you been at your current company?
3. Are you mostly an Excel Pro, and do some powerbi/forecasting/SQL on the side? Or a pro at all of those? 

To be honest, best thing is to apply elsewhere and see what they offer you. With 12 years' experience, it might be worth trying to get into a management role at this point vs just being an IC.",22,xf3ldh,"I work from home for a 50-person HR company. I am their sole data analyst. My boss knows nada about data and just tells me to do magic. 🙄. I support the entire company when it comes to data needs, excel help, urgent reporting needs, analytics, etc. 

With 12 years of heavy excel experience from a financial background and three years of prior data analysis experience in the same industry where I was underpaid, this job asked what I was making prior and simply bumped it up 5k…which I accepted, desperate for a WFH position. I was not told at the time of interviewing/accepting that I would be the sole analyst for the entire company. So every team almost constantly needs my help to understand/visualize their data. In addition there are compliance reports and audit requests all the time. 

Some of the data I have to work with requires me to see HR-related info. It’s disheartening to see people I helped interview as potential coworkers making substantially more than I am with less experience. 

So my fellow data analysts, what is the US salary range I should be at as a 15 year excel pro with powerbi, forecasting, SQL experience etc (and limited R and Tableau experience)?",dataanalysis,2022-09-15 11:09:08,20
Look for linear algebra class on edx.,2,xfl60m,"Hi 

I need an advice. Where I can get hold of a good Linear Algebra course or a book? I am learning Python for Data Science in [DataCamp.com](https://DataCamp.com) now. Looks like they omitted this aspect in courses. I want to learn some linear algebra required for profound understanding of the field. 

Thanks in advance!",dataanalysis,2022-09-16 00:59:36,1
What exactly r u trying to put in the columns?,2,xfjmnc,"Does anyone have any pointers with sorting word documents in python. I have extracted the data using docx2txt which was straight forward. Now the text is extracted and looks similar to a .txt document within python how on earth do I sort this data into columns so it can be exported into excel?! 

(Chemistry grad taking on a data analyst role) help lol.",dataanalysis,2022-09-15 23:25:38,6
"Bro

None of us know what we’re doing lol

We just know enough to get by",33,xeysy5,"Hey fellow data enthusiasts,  


It's been 6 months since I started my internship, and my first project (dashboard) was built solely in Power BI using DAX. I learned a lot: especially for a guy who's coming from a completely different background (civil engineering). And I did it, the project is complete, BUT:  


There were times when I genuinely didn't know how to solve a problem, and I found myself posting about it for someone to help on forums like Microsoft or even Reddit.  


Because of that, I feel that I am not a real DA and everything that can come with thoughts like these.  


Is it okay?



UPDATE:

Thank you all for your words of support, really. There are times when you know it is the imposter syndrome, still can’t help but ask for engaged peoples’ opinions, and I glad that I did it!",dataanalysis,2022-09-15 07:52:14,14
"I would seriously look at ""Analyst"" jobs of any sort. Data Entry is a pretty low position. Get something that says ""analyst"" (even if it pays lower than data entry). Your next job offer will thank you.

Take a look at [Meetup.com](https://Meetup.com) for data groups. My area has a bunch but I'm in a pretty populated zone.",2,xfcsrb,"Hi, I’m new to data science. I have a BA in Econ and I’m looking to get into data analytics and eventually upskill to data science. I’m considering taking a data entry job to pay the bills while I build a portfolio. Does anyone have any experience working in data entry or general advice? 

Also, I would love to expand my network and potentially work on projects or talk data with ppl in a smaller setting. Lmk if that interests you! 

Thanks : )",dataanalysis,2022-09-15 17:40:05,3
"1. I feel there's too many projects that seem similar there,  why not put three distinct projects and then add a link to your portfolio site in the cover page, that'd get you more attention.

And with that new space use it to give a pitch summary of those projects, that'd make it look better",14,xeosyb,,dataanalysis,2022-09-14 23:01:19,7
"Upload those streamings to youtube please, it would be very helpful.",6,xexbiu," ([Xposted](https://www.reddit.com/r/datascience/comments/xex9m2/curious_to_see_how_an_industry_data_scientist/) from r/datascience)

Hi! I am [u/ar\_t\_e\_m\_is](https://www.reddit.com/u/ar_t_e_m_is/), a senior data scientist and member of this sub :)

About 2 months ago I did a live machine learning stream of a Kaggle classification problem and a lot of folks reached out mentioning it was helpful -- so, we're back!

I am hoping to offer an opportunity for aspiring and junior data scientists or analytics professionals to see what data science and data analytics is all about, by doing a live-stream of a data science project :).

On **Wednesday September 21 and Thursday September 22 around 9pm EST**, I will be doing a livestream on Twitch with a dataset I have never analyzed, and working on a machine learning solution while live streaming :)

I will analyze the dataset, prep it for clustering or segmentation from a business perspective, and try to build and optimize a model while also unlocking business-driven insights :) And, yes, this does include searching Stack Overflow and debugging along the way! During the stream, I will be talking about my career path, how I got to where I am at, and offering insight into the successes and failures of my career, as well as answering any questions you all have!

If you'd like to learn more about my background, I've included a redacted version of my resume. The link to the channel is in my profile, or I can include in this post so long as it doesn't break rule #3 for the sub!

Would LOVE to see you there, and will be very responsive with answering all questions about the process, my career, and the data science field in general.

If you have any questions, feel free to post below or DM!

Hope to see you there :)

[https://drive.google.com/file/d/1EhqMsfUVCYWUa-Sjb9aUrIih2RmpotqM/view?usp=sharing](https://drive.google.com/file/d/1EhqMsfUVCYWUa-Sjb9aUrIih2RmpotqM/view?usp=sharing)",dataanalysis,2022-09-15 06:48:14,5
"The first thing is to clean and transform your data, so that every column has the same format. Make sure that the duration column is all on minutes or hours, no both.

To know how many trips (1, 2) you can use COUNTIF.  
To get the how many male drivers are, you need to nest the condition of (2) AND gender column in a COUNTIF.  

How to remember the formulas, there is no trick for me, I just look for functions that gets me the specific task COUNT, SUM, MATCH… and search in the official documentation if needed.",1,xf7tab,"Doing sample projects through excel . 
How do you guys remember the formulas  to apply ? 

Here are the questions below , assist me with formulating . 

The data consist of bike riders trips like duration , from to where , gender & birth year . 

1. How many trips are less than 4 minutes ? 
2.  How many trips last longer than 5 minutes ? 
3. Of the trip lasting longer than 5 minutes , how many were male drivers ? 
4. Enter a formula to sum the total time bikes were used in hours .",dataanalysis,2022-09-15 13:59:32,4
The certificate only will not help to getting interviews.. and it's pretty basic. You need to do your own projects besides that. Also I am also completing the certificate. And in my opinion the course is pretty basic. I mean I already had a course of sql before so what I learned from that course was really basic information. But it gives you a better picture about what data analysis is and what data analyst do etc.,11,xeum3p,"To anyone who has completed this course, has it helped you get a job in both interviews and skills?",dataanalysis,2022-09-15 04:40:30,16
Congrats!,35,xe2uo9,"First job in the field. Before this I had only worked in retail/kitchen service jobs. I'm guessing I got lucky considering how competitive people say the field is, and it's in a F500 company. My resume was also not the greatest compared to others people have posted in this sub. I just listed my skills and I did not have any projects on it. A phone screen and two interviews later I got the job!

Not sure what the actual position title would be considered as the role just lists it as analyst, but I'm on the business intelligence team, so I'm assuming a BI analyst. Job duties will include SQL, tableau, Big Query, and power apps. Pretty excited as this opens up some opportunities.",dataanalysis,2022-09-14 07:04:02,37
Be warned that it never loves you back,10,xekygy,"I started by taking the google cert. This lead to the py4e class. And now I’m onto in-depth excel classes. And I’ve started the more in depth python classes that are prerequisites for a masters of applied data science. 

Now I’ve discovered guided projects on coursera. And they are so helpful. I am getting so comfortable and confident with tools like power BI and tableau. 

Is a master’s degree worth the price? I can’t stop taking classes. I’m obsessed 🫠",dataanalysis,2022-09-14 19:43:25,12
Sometimes that’s all you need.,4,xe5jih,,dataanalysis,2022-09-14 08:55:19,1
"You could and should google this....

Descriptive analytics is what you do to understand the basic gist of the data. How many rows/records are there? How many columns? What kind of data do you have? Is it child data? Car data? Flower data? What is your population right. Then what kind of info do you have or want to explore? For cars, maybe the company? Seat size? Color? You look at a quick peak of how many colors are represented in the color column, what does the distribution look like?

So effectively, lots of bar charts and other graphs to show density/distribution. Just showing simply what you are working with, describing the data.

Predictive analysis can be regressions using some combination of the columns and fields to predict an output. So you test, does the color of a car help predict how much mileage the car will have? What about seat size? So regressions and other significance tests to verify which column fields are useful.

Edit: https://online.hbs.edu/blog/post/types-of-data-analysis",3,xf14oc,If you could explain yourself in depth I would appreciate it very much.,dataanalysis,2022-09-15 09:27:05,10
"Look at ARIMA and TBATS for well-studied forecasting models. 

Also Excel has Exponential Triple Smoothing (ETS).",3,xexizg,"Hi so I have a task where I have to project/forecast future end dates for different categories based on past data, so I have the “start date” and the “end date” and have already calculated the days in between that. But now I need to predict future “end dates” for future “start dates”(using excel or tableau) but I’m not quite sure which statistical methods I should use. Please help or redirect me to a diff sub!",dataanalysis,2022-09-15 06:57:11,1
"you should make a discord and add a link, id join",2,xejmeb,,dataanalysis,2022-09-14 18:40:55,3
You could always learn SQL and Python and explore other analyst jobs. You could learn more about data engineering or big data analysis and modeling or data science. You could go down the strategy and process improvement route and look at IC roles like a technical business analyst or product owner. You could be a people manager and own a team. You could be a project manager.,2,xeb8t8,"A bit of background, I graduated in 2020 and got a role as a buyer in a mid size manufacturing company until mid 2021. From there, I am now a procurement data analyst.

I have a lot of free time in this role and feel underutilized. I create reports & fetch needed data, clean messy data from acquisitions, etc. but am looking for more. I’ve brought some side projects to my boss that I’ve done, and have been commended for them, but still feel like I want to push myself further—whether with a new role at this company or transitioning somewhere else.

Most of my work is out of excel. I don’t know any programming languages, SQL, etc. and I figure I should change that.

What is the career path for someone in this position? I’ve searched around, but kind of feel like there’s not an easy answer to that. Any tips or help would be greatly appreciate :)",dataanalysis,2022-09-14 12:42:51,4
"Your question prompted me to search, as I was having the same thoughts a while back. A podcast would be great! I haven't heard it, but found one called The Analytics Power Hour, and based on the description, it sounds like a good listen for us aspiring data/business analysts. The author of the podcast is Michael Helbling.",3,xee6zd,"I’m trying to squeeze some learning in wherever I can with a full time job and busy family life. Have you guys found any podcasts that have genuinely helped you learn more about certain topics in data analytics? I have a 30 minute commute, so would love to utilize that time better.

Edit: I’m not so much looking for learning technical tools. I feel like those are best learned through practice. I’m more looking for analysts describing their approaches to business questions, or advice for improving data literacy, etc.",dataanalysis,2022-09-14 14:41:43,7
"Kindly recommend YT channels to follow and get tips from, I’m trying to understand how modules work concisely",19,xdni9n,,dataanalysis,2022-09-13 17:34:23,20
"You’re likely to see different opinions especially if you do a search of this sub for the same question which is asked daily. 
I suggest evaluating what your preferred industry is as this can greatly influence the choice of workflow. Ie. I work in Python and excel mainly, but most of my industry is MATLAB oriented so I have to focus on MATLAB instead of my preferred methods or convert recognised analysis into my own scripts.",4,xendlq,"Hiya Everyone! I want to transition into data analytics, however, I have literally NO IDEA how to go about this! I've dabbled in the past, but nothing really deep. What certs are worth it? Will having a General Studies degree work against me if my emphasis and minors deal with business & marketing? Is this even the right industry for me? I love, LOVE Excel and am happiest with large amounts of raw data where I can create understanding out of what seems to be chaos. I've been told to get Power BI & SQL certs, but I want to make sure that's actually right from people in the jobs/industry. I really appreciate all your suggestions, input, direction, etc. Thanks!",dataanalysis,2022-09-14 21:43:20,7
You should know excel and basic statistics. I would bump PowerBI or Tableau ahead of R and Python. I would also recommend just learning R or Python and not both at the same time. Python is used more than R outside of academics.,14,xe48m5," 

1. I've been working as a Data Entry Associate for 8 months for a medium sized company
2. Currently getting my Google Data Analytics certificate through Coursera (Which gives you a portfolio of projects to present to employers as a bonus)
3. I know SQL at an intermediate level, and I'm only getting better
4. I'm currently learning R as a beginner
5. Also learning Python as a beginner
6. I am looking into learning PowerBI and Tableau, although I'm not sure which one I should do

Am I missing anything, or should that be enough?

Also should I learn PowerBI, Tableau, or both?

Thanks so much!",dataanalysis,2022-09-14 08:01:54,5
"Create a filter, that makes the person choose between different years or better still he wants to see all products together",1,xek6mh,"I have 5 lists with all the sales from the past 5 years.

Do I combine the list and analyze the top sellers this way, or only focus on the most recent year?

I combined all lists together and got the top 20% of items that sell the most.

The sale of these items accounted for about 81% of total sales, so I thought that was a good indication because of the pareto principle.

Should I try a different approach instead?",dataanalysis,2022-09-14 19:07:44,5
Lolol,1,xe5mx3,"Hey, I'm working on the Google Data Analytics Course. My working on the Capstone, and I'm having issues importing the bike share data to Google Sheets. I keep getting an error message saying the dataset is too large. Any recommendations?

Also, if you are in data analytics (any level of experience) and would like to share and work to together on other data projects (or really anything data), lmk! I'm looking to expand my network a bit as I work on my DA skills.

Thanks!",dataanalysis,2022-09-14 08:59:17,5
"Check out our blog article for more information ([https://blog.dataddo.com/free-data-integration-forever-with-dataddo](https://blog.dataddo.com/free-data-integration-forever-with-dataddo) ) and you are also more than welcome to join our upcoming free webinar about [How to Connect Data Directly to Dashboards for Free](https://my.demio.com/ref/UMr7Q3dL30k5faof) on Sep 21 hosted by our CEO, Petr Nemeth!",1,xea7wx,"Hi!

In full disclosure, I work for Dataddo, but I am happy to announce the launch today of our **Free plan**, which lets you sync data from any online services directly to Google Sheets or popular dashboarding apps like Google Data Studio, Tableau, and Power BI.

The Free (forever) plan includes:

* Access to any of Dataddo’s 200+ connectors
* Weekly synchronization/extraction of any volume of data to up to 3 dashboarding applications and/or Google Sheets

Happy to answer any questions in the comments or DMs!",dataanalysis,2022-09-14 12:01:47,1
"I’d say your chances are pretty high. Just start submitting applications. If you don’t already know SQL, you should learn it.",1,xe9f2q,"I'm currently doing the Google Professional Certificate Course in Data Analytics. I have watched several tutorials and have a decent foundation and knowledge in Tableau, Numpy and Pandas. Worked 2 years as a front end and QA. What are my job chances and any advice to help me land that first job",dataanalysis,2022-09-14 11:29:20,2
"Hey there, you’re probably not going to find a lot of help here for a couple different reasons. The first, and probably most important, is that it’s for an interview. If you can’t figure this out and is a requirement for the position, you’re going to have a hard time. Not to mention that you’d be misrepresenting your abilities by saying you know it when you actually don’t understand the requirements. Second is there is absolutely no context around what the actual metrics mean in relation to the business. I highly recommend giving your answer to the business around what you think it means and asking for feedback if you’re not sure.",1,xe8u1z,"Would like to know what Actual, Forecast, and Sales orders mean in this chart?  The context is supply chain of a business. What does these metrics mean? What does connection mean?",dataanalysis,2022-09-14 11:05:38,1
"I hope you mean "".xlsx"" files, not xls.

Exporting to CSV exports the data as it's written. Excel will change data nilly-willy, that's why you don't use Excel.",0,xe8a9o,"At my job, we use Excel to create and modify data transfers. The vast majority of the transfers we send out are csv files to be loaded in to client databases. We use Excel to create the data transfers, then save as a csv for ""production"". 
 
I've noticed that when saving from xls to csv, the additional worksheets and formatting are removed, while maintaining whatever formatting state it was previously in (9/14/2022 was formatted in to 14Sep2022; csv has 14Sep2022 instead of what was initially entered/formatted from).
 
My question(s) is, when I save an xls file as a csv, is the formatting actually removed and ""saved"" as if it was initially entered in that format? What about the actual data changes as a result of the conversion?",dataanalysis,2022-09-14 10:43:27,2
What was your career progression and how did you did you end up in that industry!,6,xdkztu,"I'm hosting an AMA for [Nadine Merheb](https://www.linkedin.com/in/ACoAAAn-pggBawBvGTIXzjEUuUQFG1lxun6BdS0) this Thursday!  


As the Sr. Manager of Data And Strategy at the Chicago Bulls, Nadine is responsible for:  
🎯 Helping various business units become more data driven  
🎯 Building reports for sales, marketing, customer service, etc  
🎯 Building a data product that increases speed to insight  
🎯 And much more!   


If you'd like Nadine to answer your question, or if you'd like a recording, go 👉[**here**](https://www.operationalanalytics.club/events/ama-w-nadine?utm_campaign=Parker%20Rogers%20-%20Organic%20Social%20Campaign&utm_source=reddit&utm_medium=social&utm_content=post-oa-club-scd-all-sessions)👈  
**Disclaimer**: She is *not* on the player analytics side of the business. She's on the business side (merchandise, tickets, etc.)  


Here are several questions I'm already excited to ask Nadine:   
🤔 What changes have you made to the data product (or data team operations) since you arrived at the Chicago bulls?  
🤔 How do you help your stakeholders take action on the insights you deliver?  
🤔 How do you say ""no"" to projects from your stakeholders?  
How do build/ improve relationships with stakeholders?  
🤔 What new challenges have you faced joining the sports and entertainment industry?",dataanalysis,2022-09-13 15:41:36,2
It's very interesting that you've chosen to not include the mean QA failure rate of the different products....,1,xe4s16,"Hello, I have no idea where else to find counsel for my problem. I basically have to determine the frequency of the quality checks of some products where I work. Products are inspected 2x per shift, if in one of the checks a product is deemed with bad quality, the the production since last check up is scrapped. Now I have to come up with a system to weigh all factors and assign classes of priorities, e.g. some products will have 3 checks per shift while others 1, or even 1 per day. Depending on history there is variation between products counts that have been scrapped, their aggregated cost, the number of parts. I sort of want to do a matrix score and determine these priority classes, but I am trying to come up with a weight and don't know how to approach it. Any suggestions? 

&#x200B;

https://preview.redd.it/gu0gwfpvdun91.png?width=821&format=png&auto=webp&s=3aefcd471b36e08738a9d70338b4eb8c1ecda235

Sorry if I was not 100% clear",dataanalysis,2022-09-14 08:23:59,1
Tell me if you find it,1,xe0txx,Does anyone maybe have a link for the total gpu sales for the last 5-10 years I've been looking online and haven't had much luck in finding any resources that I can use.,dataanalysis,2022-09-14 05:34:45,1
"As a noob can I ask how you learned all this?

>My skills so far include searching public data bases, cleaning the data, applying some analysis pipelines (using some libraries in R), and using machine learning algorithms to predict a target feature.",3,xdj4fy,"I've been given mixed opinions over this one. Some told me it's useless content, and some told me it is absolutely worth it. The reviews over the internet are positive for the most part, but most of them are outdated, I want to know  where does this course stand today. I'm having some trouble deciding since it's a big course and will cose lots of time and money. What do you guys think?

I have a bachelor degree in medical science and currently I'm a MSc student of bioinformatics. My thesis includes some data analysing, but I have a feeling that I need more improvment in those skills. Biological data is cool but downloading and cleaning data does not take a very big part of my work.

Bioinformatics is a great field and I love it, but there just isn't enough job opportunities out there, (at least where I'm from there isn't ), and that is why I want to be on the safe side and be able to become an analyst if I want to. The need for analysts is spreading pretty rapidly.

My skills so far include searching public data bases, cleaning the data, applying some analysis pipelines (using some libraries in R), and using machine learning algorithms to predict a target feature. I don't make new algorithms. I use R and I'm still a nooby in python.  
So currently I'm working on my python skills, and my statistics skills. 

I'de love to hear from you! Thanks!",dataanalysis,2022-09-13 14:24:57,13
"There was no feedback drawn from your analysis?

There needs to be a tab for findings",1,xdrepn,"I am doing my MSc in Computer Vision, but I am more biased toward Data Science and Data Analytics, and I started studying data analytics.

Before this, I did two projects, but now I feel like I need expert feedback to do better than what I already did. I would really appreciate it if you Could take a look at my Kaggle notebook and give me your constructive feedback.

I look forward to your constructive feedback.

[You can find the notebook here.](https://www.kaggle.com/code/betizazualemu/ford-gobike-eda-visualization/notebook)",dataanalysis,2022-09-13 20:38:20,7
"I would also look at Business Intelligence (BI) roles. 

If you know Python, it should be pretty easy for you to pick up SQL.",1,xdvs6i,"I'm a recent graduate with a masters degree in physics with astrophysics and I've been job searching for a while and not sure what job titles I should be searching for what I'm looking for. Mostly been searching for ""Data Scientist"" and ""Data Analyst"" jobs on Reed and Indeed UK that are fully remote partly due to social anxiety (obviously wont bring this up in an interview). In my degree I really liked working with data, writing python code to process data and create plots so I've been looking for something where I can utilise those skills. I do have experience from my master's project where I developed software in python to process data and find best matching models to real data. And after doing some more research into data analyst roles I'm not even sure if they're what I'm looking for as you need experience in some coding languages that I've never used, namely SQL and JavaScript (although I have been looking into some of the Linkedin learning courses to learn). So what I want to ask here is, is there a job title is more appropriate or better suits what I'm looking for than ""Data Scientist"" and ""Data Analyst"" and are there any other job seeking websites where I might have better chances than Reed and Indeed UK, I've heard gradcracker is good but from what I saw there were no remote based jobs. Also any advice that will help me through my job search is very much appreciated.",dataanalysis,2022-09-14 00:49:16,11
Sure needed for projects for portfolio,3,xd2f06,"Hi all, in my few months as Redditor I noticed that a lot of people that are entry level in data science/analytics are struggling to put the theory into practice in their projects or assignments from university, courses or work, especially when they have to ''go'' from the problem statement to actually implement a ''data work''.

I started to offer some mentorship for few Redditors, and I am happy to offer data science and analytics mentorship to help anyone who is struggling to get from problem statement to solve the problem with data science or analytics. This can be because of a lack of skills, expertise, or time. If you're interested in learning more about Data Mentorship, then contact me or DM for more information!",dataanalysis,2022-09-13 01:57:14,4
"I'm a Information systems degree graduate that secured a job in startup that specializes in a business intelligence platform. I was hired as a generic analyst title but my main projects mainly include automation of scripts to scrape more data sources, automation of scripts to do data cleaning of the database. 

&#x200B;

1 major issue that I am struggling with is my lack of data or an analytics background. I'm more confident in an engineering role, so I'm comfortable writing logic and implement architecture and data. So say if I was tasked to clean a particular column in a table in a database, I would need to do some form of analysis first before I can implement the logic to clean it up. The gap comes during the analysis part and I feel paralysed and unable to make any progress because I honestly don't know HOW to tackle the issue. I lack the analysis problem solving workflow and I wish to fill up the gap. 

&#x200B;

i am the only one in the project and Mentorship in the company is out of the question since it's a startup and my supervisor does reports and looks excels but doesn't understand anything technical. Anyone else also from an engineer perspective that has troubles with the work of an analyst? What do I do?",20,ogzy7l,"Here's a thread to post all of your burning questions regarding r/analytics career advice. Remember, there are great subs like r/careeradvice , r/careerguidance and r/CareerSuccess to cover the basics - so please limit your advice requests to analytics specific questions.",analytics,2021-07-09 10:07:09,230
"Not sure sbout other countries but you may well struggle with only a diploma, especially if you want to get into a well-known consultancy. The job makret is ultra competitive at the moment for entry level jobs. I guess it’s the same in Canada.

However you can only do what you can do. A diploma would still give you some basic skills that you can then build on yourself. As always a degree shouldn’t really be about what job you can get afterwards, but learning something you want to learn, and which you find useful.",6,xi5qri,"So, I am looking at Canada so I can pursue education in Business Analytics after my business administration undergraduate degree. The problem is that the Master's degree in Business Analytics in universities operating in Canada is expensive for me. But I looked at colleges offering a post-graduate certificate/diploma in Business Analytics and I can only afford these colleges. 

I want to know: Does it matter if I have a Master's degree in Business Analytics or a Post-graduate certificate/diploma in Business Analytics? After my studies, I want to gain an experience in a consulting firm and later wanna be an independent business analyst consultant. 

Thanks for your help and opinions!",analytics,2022-09-19 01:09:30,4
"Market is getting saturated with Analyst. Self learning is easy and technology readily available. 

I harp all the time about becoming a SME (subject matter expert) in your domain.",39,xhl96e,"Hello all,

I've been on the job hunt for around 5 months now, essentially since my current employer required everyone to be back in office 4 days a week. Well, I quite liked working remotely and it looks like everyone else does too, because I've seen some insane competition for any remote position posted online. I'm mostly going off of LinkedIn numbers - but it seems that the competition has gotten more steep, maybe because of layoffs that are going on right now, especially in the tech industry? 

For reference, I'm under 4 YOE in analyst roles working mainly with SQL and PowerBI, although I do have experience and certifications in Tableau, Excel, Alteryx. I was so close to getting a remote position last week with a big payroll company, but after getting all the way through the interview process, the HR recruiter called to let me know ""the team really liked me and thought I had great experience, but unfortunately they've decided to put the role on hold for now as they work through strategic planning blah blah blah"". Really sucked because I was super excited, did all the the interviews and then they yank it out from under me. Now, I have an offer from a pharma company that is a relatively good offer, except I'd have to be in office 3 days a week. I just don't understand why employers around here still necessitate the in-office thing for essentially dashboard development roles.... but anyways, I'm not that excited about it. I'll probably counter and ask for a lot more money, and if they agree then I can probably suck it up, but I highly doubt they will. I'm planning to ask for something crazy, like an additional 10-15% off the already generous base they've offered me. I figure if they're crazy enough to accept, then I win - even though I am sure the money will not outweigh the suck of the office over time.

Maybe I'm completely out of touch with the job market, but is it normal to see hundreds of applicants for not even F500's in a matter of days when a job is posted? I feel like you have to pretty much be overqualified for the role to actually land a remote job at this point. And yes, before anyone asks I am looking outside of LI for jobs, but it's the only job board I'm aware of where you can get an idea of the number of applicants. Anyone have any tips or thoughts about this? Thanks!",analytics,2022-09-18 09:32:36,47
"Yes - I might recruit people in the near future and it’s definitely a big plus if they have a portfolio of data visualizations. It’s also showing immediately their level and weakness, so be careful. I’m not interested in diplomas/certifications but more about talent/skillset and if you know some basic analytics tools used by top 100 large corporations: Power BI / Tableau Server / Oracle Analytics / Qlik.",15,xhw0je,I don’t have a bachelors or any experience only the analytics google certification and want to land a job but can’t find anything,analytics,2022-09-18 16:53:18,6
"We had a similar task back In my orevious place where they wanted to calculate the roi for marketing campaigns ( 95%+ of them didn't have an cta so calculating a direct revenue per campaign was not possible)

We had broken it down to Roi from activations ( when an inactive merchant becomes active after receiving campaigns in a period of time) + roi from growth ( spend increase in merchants receiving mkt campaigns vis a vis dthise who didnt)

As long as you can break down your roi into meaningful business components you will be good. I can think of
1) customer growth from marketing _- you can segment this via different channels
2) growth due to new product Launh if any- new product ,wont need any benchmark
3) growth from existing customers -yoy comparison of growth  from existing clients
4) organic growth-growth you would have expected anyway from last year if no new initiatives took place. This is usually a simple linear CAGR projection from last 3 to 5 year data.

Setting up the analytics for the first point might be difficult since that requires A/B testing. Keeping of controls, so if you dont have that already it might not be feasible this year. But make sure you don't force it if you don't have the data. My previous company forced it, and we were made to do the grunt work of deriving insights from wrongly setup controls. Got numbers all over the place and had to do 40+ reworks amd data manipulation to get numbers which looked ""pleasing"".",6,xh8y6j,"Company is looking to set new KPI metrics target based on a goal of increasing number of customers by 10% over a period of 2 years. How best can I decompose this goal (10% growth). I have 8 metrics that I measure and monitor. How best can I decompose this goal (10% growth) on the KPI metrics?

Thanks",analytics,2022-09-17 23:18:21,20
"You can use third-party analytics services:

1. Use Google Analytics: You can use Google Analytics to track user journeys, events, and conversions on your MyBB forum. Simply create a Google Analytics account, then add the GA code to your MyBB forum. There is also a GA plugin.   
  
2. Use a third-party analytics service: There are a few different third-party analytics services that work with MyBB, such as Clicky and Piwik. These services will give you detailed information about your users and their activity on your forum.",1,xhhnkr,"Good morning everyone!

How would you setup analytics for an Internet forum(mybb) to track user journey, events such as posting threads, and replies? Not necessarily the technicalities but what would this look like? What platform is the best free or paid? 

My goal is to track where new HQ users are coming from and track their journey to create funnels to obtain more, efficiently.

A HQ user is defined as creating and replying to threads that has been active for over 90 days.

I appreciate the help and hope you all are having a wonderful Sunday!",analytics,2022-09-18 07:09:32,1
[deleted],-3,xgej9d,,analytics,2022-09-16 23:06:30,1
"Job descriptions are wish lists for employers. A starting point. 

After that, hiring managers (and I am one) are willing to be squishy on requirements for the right person. 

I check for curiosity, lack of ego, a learner, etc. If someone checks all the boxes but is an asshole that will ruin my team’s chemistry I’ll pass on them. 

Curious why you were told you talked your way out of job opps - can you expand on that?",52,xfrlyw,"I’ve been interviewing for a bit. 

I’ve learned that I have “talked my way out of job opportunities”. 

I have no family that’s attended university before me and the internet says that these job descriptions ask for way more than they actually need on the job. 

Could anyone elaborate on this?

Do hiring managers ACTUALLY hold back on their work culture, role responsibilities (huge shock to me if so) in order to attract better talent?",analytics,2022-09-16 06:40:04,32
"You can only obtain that data in Google Search Console, it has been obfuscated for a long time with (not provided) as the value in the keyword dimension within Google Analytics (UA).


After you've verified your website in GSC through one of the various methods you can go in the platform and look on the left hand side of the screen and under performance there is a header ""Search Results"".


Click that and it will show top queries, pages, countries, devices, search appearance & date data. You can filter by queries or landing pages to determine what keywords are generating traffic to your customers' website.",5,xg776q,"Hey there reddit, I am trying to find how to acknowledge which organic keywords generated traffic to the website of my customer (in **UA**).‍ I'm pretty sure it's possible, because I remember someone once showed me but I forgot how. Do I have to connect the Search Console to Analytics? I have tried going to Acquisition > Campaigns > Organic Keywords, but it's only telling me ""not provided"". Any idea? Thanks!",analytics,2022-09-16 17:05:30,1
">	is a online certification course good enough to land an entry-level position?

No. And a degree isn’t generally good enough, either. You need experience and most people get it on-the-job in their current role before transitioning to data. 

>	could I be making close to what I am making now at $25/hr in a entry data position?

Entry level data positions don’t really exist, so this question is somewhat loaded. And very much depends on where you live. Most first full time data jobs for people make between 50-80k depending on a host of factors. 

>	in a remote/at-home setting does the position  allow me the flexibility to also be a stay-at-home father? 

Largely depends on the role, the company and the amount of time you want to spend wearing your dad hat versus your worker hat.",7,xfxd4n,"I am considering learning about data analytics and transitioning my career as a cable/security technician to Data Analytics. I have a few questions and I hope some wonderful people in the field can help.

I don't really have the option to go back to college at the moment (I need to be the bread winner while my wife is in medical/pharmacy school), is a online certification course good enough to land an entry-level position?

Speaking of being the provider, could I be making close to what I am making now at $25/hr in a entry data position?

Lastly, in a remote/at-home setting does the position  allow me the flexibility to also be a stay-at-home father? We want to have a child, but don't see a possibility of raising one without paying ungodly amount for daycare.",analytics,2022-09-16 10:23:01,8
"It took me two years to learn the skills to pass the stupid interview and coding assessment in my company (internal transfer)

Do you know how many times I have used those skills since then?

Zero.

The way they assess these things makes little sense sometimes. What they should do is test your ability to google an answer to something. 

Anyway, keep trying. You will get better. I know it sucks. I've been there. But you can learn the skills to pass these things. 

Good luck. And reach out on here if you need advice on anything.",25,xfgir9,"Took 2 coding assessments with companies I was very interested in working with .

First one actually wasn't too bad, but I probably worked a little bit slowly. 2 SQL data modeling questions and 2 Python data analysis questions. Got through 3.5 of them and got the first 3 to pass 100% of the tests. Didn't get through the 4th one in the allotted time. Looking back, even though I prepped with HackerRank, I wasted too much time having an issue with the STDOUT interface. Probably won't get a call back here, but at least feel good about being able to answer all the questions.... albeit probably too slowly.

I knew the 2nd one would be harder, but I'm gonna go ahead and say I bombed it. 4 Python questions. DS&A type questions, with after looking some up probably equated to LeetCode mediums. I finished and passed the first one, but it took me almost 25 minutes and it was definitely the easiest one. Counting the number of steps while traversing keys on a keyboard. The time limit was 1 hour, so I started a bit on the others but didn't really get much done. I think I was pretty much resigned to failure since I had already spent half my time and could tell I wasn't going to be finishing the other 3 in 30 minutes. Definitely will not be getting a call on this one.

First time doing these full on coding challenges and interviewing like this and figured I'd share my experience. ",analytics,2022-09-15 20:40:07,14
"I think it depends what you want to do.

I started off as a data analyst and transitioned into a DE role (went from 70k to $350k) in 4 years. This isn't meant to brag but more to show you the opportunity cost off getting your MS-> time and money.  


Now of course you may not want to be a DE and thats totally fine, but the advantage you have of working at a company is that you can talk to your manager and start guiding your path towards whatever role you want. 

**Pro Tip:** Look into your companies employee educational budget -> you should be learning on the side and building up your data engineer / data scientist / whatever position you want skillsets in parallel with learning on the job 

Best of luck my friend!

Christopher Garzon

Author of Ace The Data Engineer Interview",7,xfu4kz,"Hi yall!!

Currently living in Northeast FL and just graduated college 3 months ago. I started out at 49k and now 55k after 3 months review (probably realizing that I’m being way underpaid). I’m trying to go for a full-time MSBA and hopefully getting 70k after this degree. Right now, I’m pretty much just working with SQL Server, doing stored procedures, SSIS packages, some Power BI here and there. 
I’d appreciate any career advice!!",analytics,2022-09-16 08:18:26,17
"It varies from company to company. But here are some best practices:  


* Keep events organized by customer lifecycle stage. This will help you track and analyze customer behavior at each stage of the lifecycle.
* Track events by both time and frequency. This will help you identify both one-time and recurring events.
* Use event tags to categorize events. This will help you track and analyze events by type.",1,xfmw2d,Our event structure has gotten messy over the years and I'm interested in refactoring it and cleaning it up. There are lots of ways to organize the event data and I'm curious what patterns others like for keeping events scalable and intuitive.,analytics,2022-09-16 02:46:42,1
Apply to some jobs and see what kind of offers you get.,75,xf2jh9,"DA, 3 YOE, 75k all in, F500 insurance company

Work specifically in Power Bi. Have heard I can clear 100k if jump",analytics,2022-09-15 10:25:33,49
"According to Google, it has “negligible impact” so you should be good",1,xfmlut," If I create two variants of a page on the same URL through A/B testing on Google Optimize, will the variant affect search engine results?

Lets say my original page uses the word ""dog"", which I then replace with ""pet"" on the variant page, will the word ""pet"" count towards search engine results? If the split is 50% 50%, how would this affect the search engine results- given the variant has different keywords than the original?

&#x200B;

What would be the past way to create 2 landing pages with different keywords, which are for the same product/offering?",analytics,2022-09-16 02:29:47,2
"Do you really need a master's degree? I only used udemy courses to break into my real analytics job. My first analyst job only required excel,  and i learned programming and BI tools at work. I kinda doubt you need a master's degree for that",9,xfbwzs,"Hello everyone,

If I want to become a very experienced data analyst, and expand my career opportunities and salary as much as possible, which Master’s degree would be the best option:

1) MS in Business Data Analytics
2) MS in Data Science

I want to focus on creating visualizations, such as charts, graphs, and dashboards. I would like to work in finance, healthcare, or technology. I also know some machine learning, so I think it would be nice to do some predictive modeling, but that is not absolutely necessary for me. Also, I would like to work with Python, Excel, Power BI, Tableau, and SQL.

Which degree would be the best option?

Thank you in advance!",analytics,2022-09-15 16:58:05,14
"Personally, I'm just pulling datasets of stuff I'm interested in or have questions around. Throwing into Excel, cleaning, then visualizing there. Haven't stepped into the Tableau world but most likely will.

Overall, I take notes at every step, issues I run into. I'm at the very early stages though (just Datacamp, StrataSratch, what have you)

I'm not sure if this is the 'right' way. Some of the analyst YouTubers detail what should be in a portfolio. Would be interested to see good examples from anyone else.",6,xf0btf,"Hello everyone,

What are some popular resources for making portfolio projects? Should I use datasets from Kaggle? Do I need to use a combination of Excel, Python, SQL, Power BI and Tableau for every project? What are some popular project ideas?

Thank you in advance!",analytics,2022-09-15 08:54:22,5
Asking never hurt anybody.,26,xevv16,"Found a position im super excited for. 

It’s in a state I’ve always wanted to visit. 

Can I still ask?",analytics,2022-09-15 05:42:18,16
Utms are useful for webmasters on a site. I’m not aware of any way to use them to get info from discord. I’m a mod on a server and I don’t see this kind of data being made available.,2,xf9ljx,i have my discord server invitation link on my website and on my instagram. i want to know where most of my discord server members are coming from. how can i use UTM tracking or 301 redirect to do this? thank you,analytics,2022-09-15 15:12:39,3
You need a website for UTM's to function. The GA snippet of code needs to live in the header of a webpage before anything will know how to digest the values you attach to a link.,1,xf3zkq,Is there a way to setup UTM tracking to track traffic going to/joining a Discord server?,analytics,2022-09-15 11:25:28,1
"This is a tough question - pay depends on where you’re located, what kind of jobs you’re applying for, and how well you can sell your experience. 

I suggest you look for roles doing “people analytics” aka analyzing hr data. It would be a great marriage if your current knowledge and new skills. 

Then check Glassdoor for companies you’re applying to to see what the ranges look like. 

Good luck on your journey!",4,xf8m98,"I have a Master's in HR and 5 years experience, some of that in Project Management and some as Head of HR for a company. I'm currently doing a boot camp where I'll learn SQL and Python, and would like to know what expectations I could realistically set for myself in terms of pay after I complete the course. I'd be looking for remote roles, if that makes a difference at all. 

Thank you!",analytics,2022-09-15 14:32:06,8
"Computers are stupid and humans are error prone. If page B is showing traffic from page A, either they are coming from page A or the way you’re capturing the traffic from page A isn’t exclusive to page A.",2,xf2l30,"Report is showing users are coming from page A to page B, but there are no links directly to page B on page A. How would you explain this?",analytics,2022-09-15 10:27:23,1
"My opinion is that you will be spreading yourself too thin/casting too wide a net, whatever metaphor you choose. SQL is key. Always sql. Then, choose either Python or R. That decision is based on where you see yourself in the future. Python is more “all-purpose” and R is used more by data scientists/statisticians. 

Same for your viz tool. I have a experience in both. And I’m personally biased towards Tableau. 

Power BI has much more extensive data modeling/transformation abilities, while Tableau has significantly more functionality and options for creating attractive, super interactive dashboards and charts, and you can do all kinds of cool visualization related stuff. 

Think about it like this, as a hypothetical, let’s say exactly 50% of jobs want Python and 50% want R. And let’s say every one of the jobs wants you to have 2 years experience in their language of choice. 

Would you rather have 1 year experience in Python AND 1 in R? Thereby alienating your chances at any the jobs? 

Or would you rather specialize in one of those languages and have 2 years experience and although you can only apply to 50% of the jobs, you are qualified for those 50%. 

If that makes sense. 

In conclusion, good job, you’re on the right path doing the right thing, google analytics cert is probably not bad. 

Do more research into the fields that use R and Python, see what aligns with you more, or where you see yourself in 5-10 years then specialize in one. 

For Tableau vs Power BI, it’s not as cut and dry. Both are used in lots of companies and companies also switch between them when they’re figuring out with tool is best for them. 

So do you think you like the visualization side more? Are you interested in the more front -end design and creating unique dashboards? Or are you more interested in the back-end data modeling concepts? That will help you choose.",52,xe472q,"1. I've been working as a Data Entry Associate for 8 months for a medium sized company
2. Currently getting my Google Data Analytics certificate through Coursera (Which gives you a portfolio of projects to present to employers as a bonus)
3. I know SQL at an intermediate level, and I'm only getting better
4. I'm currently learning R as a beginner
5. Also learning Python as a beginner
6. I am looking into learning PowerBI and Tableau, although I'm not sure which one I should do

Am I missing anything, or should that be enough?

Also should I learn PowerBI, Tableau, or both?

Thanks so much!",analytics,2022-09-14 08:00:16,56
"Who handles the data onboarding? Are there any data engineer team?

How mature is the analytics ecosystem? To what extend data silos happen? 

How are people making reports on the teams? Manually, semi automated, or fully automated?

What tools do you use? R? Python? Excel?

Is there a wiki that detail the tables?",4,xe2f8j,"I'm currently working in project management for my company, and have been learning DA for the better part of this year (SQL, Tableau, Power BI) in an effort to pivot my career into a full-time DA role. I have some experience pulling reports from this company and my previous one, but no real direct DA experience.

Recently, I had the opportunity to meet one of the Analytics / BI Managers for my company, and after reaching out to learn more, we set up a lunch meeting.

I'm wondering what sort of questions I should bring to the table. As I don't really have much exposure to the world of DA/BI, it's a bit difficult to even know what questions to ask.

Any advise is greatly appreciated.",analytics,2022-09-14 06:45:47,5
"You can ingest multiple files from a folder and process them in PowerQuery to wrangle them into one big table. 

Set up a PowerAutomate flow to refresh the data every time there’s a change in the folder. 

Boom done.",5,xe1njk,"Hi, what would be your approach to do automation + ETL pipelines as per the case below?

What I'd like to do is pushing flat files into shared-locations (can be transformed before or later, depend) and visualize it via PowerBI.  


Generally, I'm manually doing this by putting the data in my Onedrive and transform it with python before I connect the data in PBI. This issue happened as the data source is new and it's not in our company's database yet.   


Tools I currently use: SQL Server, PBI, Python/R  
P.S. We got Azure Databricks as well but I have never used it.",analytics,2022-09-14 06:11:59,10
"A/B test a campaign that offers 30% to one cohort and 35% to another, possibly with a negative control with a much lower percent discount (maybe 5%)? If you send an email and tie discounts to recipients, you can at least monitor click rates and purchases. 

Make sure you do a power analysis to establish your cohort sizes with consideration for how many purchases you expect based on historical data, relative to the number of emails. Also, set your prior expectations and methodology in stone so you don't commit any of the common sampling errors (e.g. the goal is not to let the test run until you hit significance).

Also, you can never prove your hypothesis, nor is the goal to prove your hypothesis. You are testing your hypothesis. This isn't about semantics, but rather mindset.",5,xea3q9,"Hey guys, I am working on proving value of campaigns we are working on next year and the team is looking to determine the optimal offers. 
Granted that discounts are not a great strategy for quality customers, todays competitive landscape is filled with offers. 
That said, if you worked on proving value of a 30% off vs 35% off, how would you go about it? I feel like I am concentrating too much on existing data but the reality is, we have never ran a 35% off. Any help on how you would quantify results would be helpful.",analytics,2022-09-14 11:57:24,4
"Depends on what you mean by Data Analyst versus Data Scientist. Those job titles mean different things at different companies. My title is Data Scientist, but since I don’t build ML models, some folks don’t feel that I’m a “real” Data Scientist. So it would help more to know what exactly you want to do on the job.",15,xdoyum,"Hello everyone,

I was previously in graduate school for a health profession, but I decided to pursue a data-related career, and I have been debating between Data Analyst and Data Scientist. I would love to work in finance. I really enjoy creating visualizations, and I really enjoy learning about finance and the stock market. I also love working with data. I have been accepted into a MS Data Science program, and I’m not sure if Data Science is the best option for me. Here are my skills:
-Completed Udemy courses for Data Science/Machine Learning and SQL
-Experience with Python: general Python, NumPy, Seaborn, Matplotlib, and Plotly
-SQL, specifically PostgreSQL.
-Machine Learning: Scikit-Learn and Kaggle

I’m almost 30, and I don’t have a lot of professional job experience. I’m worried that my age, limited professional experience, and difficulty with understanding advanced math concepts and topics will make it very difficult to finish the program, or even find an internship for Data Science.

I’m wondering if it would be better for me to pursue a Data Analyst career path, create projects, apply for entry level jobs, and get the Google Data Analytics certificate.

What are your suggestions?

Thank you in advance!",analytics,2022-09-13 18:41:35,48
Harnham and BurtchWorks both specialize in hiring for data roles. I’ve never worked with either to land a job but they’ve both approached me and I’ve attended some of their online events (around industry trends) and they seem like they know what they’re doing.,3,xe3nbt,"Hi All,

I am currently in consulting on the analytics side and want to transition to an in-house analytics role as soon as I find the right opportunity. Although they can be hit or miss, I did like using recruiters when I was in the accounting world because it at least ensured my resume would get read and would sometimes help me find jobs that aren't posted publicly yet.

I'm based out of the west coast and would like to stay out here if possible (Seattle, NorCal/SoCal are all fine with me), but I'm pretty open to moving for the right opportunity as well. If anyone has good recruiters or firms I could contact for analytics that they'd recommend, I'd appreciate it! Feel free to DM me if you understandably don't want to make someone's email public.

Thanks!",analytics,2022-09-14 07:37:40,5
"In general and from my experience, any company with its core work based on any type of engineering (software eng, petrochemicals, data eng,….) like to recruit their sales with people that have technical background. So that they can well understand the customer and market requirements that will also match with their abilities. Also, technical background helps the sales to well address the company/product strengths. 
To whether apply or not for this type of jobs will only be based on your preferences. You should look to your own skills and compare them to what is required. Also, there should be an increase in your total income.

You will be happy if you are into sales and have the required sales skills, building relations, prospecting, closing deals.

The pros should be the income increase and that you will be out of the 100% technical job if you are bored of doing this stuff.

On the other side, you will have probably a target that you should achieve, a big proportion of your income will be based on commissions (no sales low income, good sales good income).

I hope this could help.",3,xe06n5,Been working in analytics for ~5 years. Recruiter reached out about a sales engineering type position at a data viz software company. Has anybody made a similar switch? Are you happy with it? Biggest pros/cons?,analytics,2022-09-14 05:03:38,1
"In short, probably not. I presume each half year period has 6 data points? That’s likely not enough, which is why you’re coefficients are varying wildly. 

It really does depend on what the data is and what you’re trying to find, but with such a small dataset I would be quite conservative and only look at the data as a whole and not split it up. Even then 20 datapoints is not high, so again a correlation coefficient of 0.59 suggests some sort of link between the two, but either it’s not v strong (other variables are important) or you don’t have enough data. Hard to say which, although with common sense hat on the conclusion “paid advertising has some influence on paid visitors” seems valid enough. I would not think you can say anything more quantified though (e.g. you wouldn’t be able to say half of paid visitors come through paid advertising).",3,xdznj5,"Hey all, I'm by no means an analyst but work in finance and have been fiddling around with using the correlation coefficient to compare some variables.

I have visitors to our website and paid advertising, visitors can be split up into various categories such as paid. So I can see over 20 months (Jan 21 - Aug 22) I get 0.591 for paid visitors Vs paid advertising.

Now what I've done is look at it over 6 month periods and H1 '21 I get 0.94 H2 '21 0.95 but for H1 '22 I get -0.11

I can't see any outliers in the data and I was wondering is this a justifiable way to look at the data and compare the coefficient over different periods?",analytics,2022-09-14 04:37:38,2
/r/dataengineering is more what you're looking for,3,xdziwk,"Hi folks,

We are looking to change our data infra and wondering what are some good ELT tools. We are looking for a no-code ELT tool that has decent support. I would really like it if you can provide a range of tools (best, moderate, good)  
One-time load: 20GB & Daily batch loading of up to 200k rows per day.  
Sources: S3, MySQL, Mixpanel, Stripe, Google Analytics.  
Destination: BigQuery

Thanks!",analytics,2022-09-14 04:30:59,3
"Honestly - if that was the only qualification someone had I’d be very wary. Depending on your other experience I might be willing to interview, but I would interrogate you til the pips squeak to see if you actually knew anything useful!

Right or wrong, a Masters from a good university in US or Europe is still a good shortcut to identify those who have the baseline quant skills needed. If there are many other applicants (and currently competition for entry level jobs is fierce) you may well struggle to make the cut.",1,xe2zvj,"So I am looking to change careers. I work in a small niche area of Healthcare that is not in high demand so I can't easily change jobs locally. I already have a graduate degree in my area, unrelated to business or analytics. I make about 80k in mid America but have a second job that pulls in another 20k. But I hate working with patients and I love the idea of working with Excel, python, and R all day. I recently came across the MBA program in Business Analytics on Coursera for only $6500. This is great because I don't think Id be comfortable spending a lot more since I just got rid of all my students loans.  How would employers view this MBA program from an Indian school? I'm from middle American and Caucasian so no one would even think I could be from India. Is this is an opportunity with good ROI to change careers or is this a big mistake?Obviously there is no right or wrong answer here but hoping kind analytics redditors will share their honest opinion.",analytics,2022-09-14 07:10:18,3
"We’ve run interviews like that, and as you’re a new graduate I don’t think they’ll be looking for lots of experience. What they are looking for is you asking the right questions.

They’re going to have invented a scenario and it’s up to you to find out as much useful detail as possible. Ask for existing analysis, particular pain points, who the end user will be, what are there expectations, priorities, timescales. Who else is working on this, how is the team constructed.

Specific KPIs they want to see, how are they validated, what is a good result. What does success look like. When they’ve discussed the vertical ask some good open questions- how are the sales going? Is there growth, new clients, products, acquisitions etc.

On the tech side, where is the data, how clean, how timely, volume- how far back does it go. Most clients don’t really talk about the tech, but as this is an interview they may expect it.

Also as this is a mock client ask a few ice breakers. How’s your day, did you see the match, what’s xxxx city like to live.",13,xdlvsd,"Hi, so I recently graduated from an data analytics graduate degree program and have been applying to some entry level data analyst positions. I have an upcoming zoom interview where I will be acting as if it’s a live consultation as an  employee and the hiring team will act as internal customers, providing a data set and a request.

Was wondering if you guys had any experience in interviews like this and whether you had any tips on what to expect and how to prepare?

&#x200B;

Edit: Thanks for the advice so far everyone. I was not expecting such extensive answers. I'll take them to heart in my preparations, as I was not expecting nor do I have any experience in an interview process like this at all and was at a loss of where to even begin.",analytics,2022-09-13 16:19:16,5
product. need to be product focussed. currently im in a role where I go across multiple products (but don't own/responsible).,7,xdgt4z,"It seems project management for analytic projects is falling out of fashion (deadlines and deliverables)  and being replaced with product management approaches (features and iterations). 

Is anybody else seeing this and what do they practice at your place of work?",analytics,2022-09-13 12:52:16,6
"Cost?
Please mention here instead of asking everyone to dm.",21,xd2bdz,"Hi all, in my few months as Redditor I noticed that a lot of people that are entry level in data science/analytics are struggling to put the theory into practice in their projects or assignments from university, courses or work, especially when they have to ''go'' from the problem statement to actually implement a ''data work''. 

I started to offer some mentorship for few Redditors, and I am happy to offer data science and analytics mentorship to help anyone who is struggling to get from problem statement to solve the problem with data science or analytics. This can be because of a lack of skills, expertise, or time. If you're interested in learning more about Data Mentorship, then contact me or DM for more information!

&#x200B;

Cheers",analytics,2022-09-13 01:50:34,14
"If they’re going to teach you something about Excel/databases, then maybe. But data entry is data entry - I would not consider it relevant experience for data analysis. Sorry.",16,xdf5ew,"I'd like more relevant data work on my resume. At the same time I'm taking the Google Analytics course to get the certificate and building a portfolio during my free time. The job pays very little and I'd give up my current job as a server (which pays more) so I was wondering if the pay off was worth having something, *anything* data related on my resume so in the long rerm I could find something more analyst related. I'm just not sure if these two occupations are seen as not very closely related.",analytics,2022-09-13 11:45:43,9
Are you looking solely into the business analyst role?,2,xdbm5h,"Hi there! I am part of a research project from a university based in Finland, and as part of my research, I'm trying to get some insight into the current job practices of business analysts. My team and I came up with a method to visualize multidimensional data into a 2-dimensional space, which is super cool!

We'd love to understand how we could provide value to analysts in the future by first understanding your pains at work and the kind of tasks you deal with daily. Can you please explain what a routine day at your job looks like? What kind of EDA do you perform? Do you ALWAYS use dashboards and if so, what characteristics do you find most useful when using them?

Huge thanks to anybody willing to give us their time to contribute to the project!

If you are curious or have any questions about this, let's just start a conversation below :)",analytics,2022-09-13 09:09:55,12
You need to find the work with them. A lot of people in other departments aren’t trained in analytics and don’t know how they can benefit from it. Knowing what projects to start instead of just doing what’s requested is a huge difference in career growth.,69,xcgs9y,"I'm on my 3rd company in 5 years. I've been able to get promoted between companies along the way. 

However, at all of these companies along the way it seems like there's almost no work for me to do. I can never tell if they're expecting me to go out and find things to do, or if they're truly just paying me for 10 hours of work per week. 

I'm a data analyst, and today I had a ""meet and greet"" with a team that I'm supposed to support. How I am supposed to support them has never really been clearly defined to me (only on my 2nd full week of the job). They asked me what I'll be working with them on, and I was just like ""uhh, whatever data/analytics needs you guys have"" and they all just gave me a distant stare. 

Am I the problem? Do I just need to work on my communication skills to find work to do? Or do a lot of people in analytics have easy/quick work?",analytics,2022-09-12 09:08:31,18
"Ripe for opportunity. You could take some extra initiative, read some books and tutorials to figure out how to improve the process within the controls the business has, time and cost. Lead an improved process. Reap the rewards of leading such a process.",36,xclof7,"Accepted a new job a while ago as an analyst for the marketing analytics department of a decently sized company  - while I understand I have to wrangle and clean data, it is just flat out unusable due to a high amount of turnover on our data team/analytics team and lack of upkeep/zero documentation. I feel like I’m in a position to fail - things are mislabeled, or made into different data types when they should be another, poor attribution. can’t tell if it’s me or if the way everything is set up is keeping me from doing my job well and if it is me, is there any way to mitigate it?",analytics,2022-09-12 12:23:49,25
If you ask for data analyst project you can watch Alex abalyst on you tube he did videos step by step,7,xcw66t,For someone with no experience,analytics,2022-09-12 20:00:39,10
"Are your queries and tables optimised for performance? MySQL performance tuning is a big area. We've had success using views, summary tables, and of course, correct indexes.

If you have some budget, I can recommend a guy who can help you with this stuff.",2,xd58ks,"Hi folks,

We have a virtual machine (t3.medium, 2vcpu) with 4GB RAM on which we have installed the MYSQL server and have 2 DBs of size 2GB (1.5GB+1GB).

When we run bigger queries(>1 million rows) it takes more than 5-10 mins to run the queries. What are the ways we can improve this time? Should we change the memory/RAM size or CPU or something else? Any ideas are welcome.

Thanks!",analytics,2022-09-13 04:41:29,3
"Have been researching this exact topic on here, noticed quite a few people recommended Web Analytics 2.0 by Avinash Kaushik. It’s more web based but there’s no abundance of data quite like in web analytics. Just started reading it and has been helpful!",3,xcrp2c,"For examples topics surrounding 

- LTV Lifetime Value
- LTV / CAC Ratio
- ARPU Average Revenue Per User
- Churn 

The topics above are the ones I know off, but I want learn more at a deeper level. Maybe I should be looking at business Analytics?

Any books, videos, or even university courses that teach this stuff would be greatly appreciated. 

Thanks!",analytics,2022-09-12 16:33:06,6
"Just by way of followup, the aforementioned visitor has returned this morning, just after midnight.  As far as the blog itself goes, they seem to be just hitting the main page, but this morning's visit included them clicking an exit-link to my Facebook group. Again, the URLs related to my blog are different, as mentioned in my OP: The long string of characters, followed by the website's main location, followed by ""? show Trashed = true"".",1,xcglxa,"(I attempted to post this once, but the system deleted it immediately. Trying again.)  


I tried finding something online regarding this but couldn't find much, so I'm hoping someone here can decipher what's going on:

Visits  to a blog of mine registered in my stats has having come  from ""India,  Google Corporate Office"".  The pages of my blog that were visited by  them don't appear in those stats as they normally would, but rather as a long string of numbers and letters, then my  blog's URL, and then ""? show Trashed = true"". When searching for answers, I  did find another blog from 2017 where the  author complained that during  the course of getting similar results in his visit-stats, his blogs got  shut down by Google India, and he ended up going to the BBB to get it  fixed. Both his blog and mine are on Blogspot.

Anyone  know what this data is indicating? I downloaded a copy of my blog to my  computer just in case it vanishes.  Thanks in advance!",analytics,2022-09-12 09:01:52,1
"Work up to it through another role. Internships or just an analytics adjacent office role.

1) Learn Excel and get really good. Check out Chris Dutton's Udemy courses.

2) Start learning how to get data for yourself using SQL. Learn joins, aggregate functions, and CTE tables.

3) Start learning basic python. Automating basic reporting tasks, emails. Check out ""automate the boring stuff with Python""

4) Tableau or Power BI for reporting or exploratory analysis.

That's the basic gist of it. The important part is not just doing tutorials, but applying the things you learn in your own projects. That's how the knowledge stocks and gets integrated into your skills set.",18,xc04c0,"What's the best way to get a data analyst job with no exp, are any courses/ certs worthwhile?",analytics,2022-09-11 18:50:56,19
"As an immediate next step, I’d suggest identifying a project that involves capturing/pulling data from a source and transforming the data and modeling it for consumption.

For example, instead of using powerquery to source the data, use python to create a connector.   Instead of transforming using VBA, use pandas or equivalent. I’d also get comfortable with cloud based file systems like s3 (sourcing and landing.) also would be good to get used to landing and sourcing using DBs like redshift or Postgres via python. 

It may seem daunting but you can chunk it into smaller steps such as start with power query first before moving to the next step. If you get used to capturing and transforming data, it’ll greatly help you in most analytics related roles.",21,xbofdo,"I have been an accountant for the last 2.5 years and I would like to make the career jump due to various factors. A sizeable chunk of my time (at least in the last year) has consisted of automating different processes in excel with VBA and power query, along with writing custom views/reports in SQL. 

I know I need to start learning python and either BI/tableau but I am not sure where to start. I started a python course a couple of months ago (from Udemy) and got through some of the basics, but I am finding myself at a crossroads. I plan on continuing to work at my current job for another year before making the switch — what is the best way to learn python during this period of time? I think it would be beneficial if I could use python to do some of the analytics work I already do in my current role, to at least get some practice applying what I learn. 

Any advice, insight or words of wisdom would be greatly appreciated.",analytics,2022-09-11 10:31:38,9
"Your submission looks to be asking about industry tools. If so, you are not the only one asking this question, try the search, the sidebar (lots of resources there), and [check out the resource collection on our community site](https://lookingformarketing.com/tools?utm_source=r_analytics&utm_medium=ai)

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/analytics) if you have any questions or concerns.*",1,xbrjhz,"Specifically interested from the perspective of trying to post the most useful, relevant text posts on my preferred subreddits.

For example, I want to ask, ""What are the best Reddit analytics tools for Reddit text post creators?"" One application I found was ""Social Rise"", which is used often by OF creators. However, I post text not image / video content, so the tool isn't ideal for me. So I wanted to discuss with others what tools do / don't exist besides Social Rise.

Thanks in advance for feedback!",analytics,2022-09-11 12:37:54,1
"Why not just start applying to roles that seem more interesting? Or even see if there are internal opportunities -- which is fairly likely in large organizations.

While I am not a data scientist, I've been in the analytics/data field for some time (and I transitioned from a non adjacent internal role). A LOT of the work is still checking if the numbers are correct. Every so often, you might come up with some useful insights, but that doesn't mean business will take action. 

Build a new data model? Great, but not before writing the unit tests and monitoring to check expectations. New analysis? Loads of fun, but you have to test and validate your methodology, and go back and refactor the logic if you intend to put it into production. New dashboard? Whoever builds it, maintains it, so better make sure the back end is in shape so maintenance doesn't become a full-time job.

Not saying it's not fun/challenging still, but the grass isn't necessarily greener. There are still plenty of weeds, gopher holes, and bare patches -- and you still have to cut the grass.",2,xbqwpg,"Hey guys, I’m new here, this is only my second post.
So I studied accounting and passed my board exams 2.5 years ago, and have since worked as a data analyst for 21 months in KPMG and now almost a year in EY.
Throughout my time in both firms, I had an opportunity to work with tools like SQL, Excel, PowerBI and Alteryx, but because these are accounting firms, I had to perform IT audit as well.
Here’s the issue:
The pay is low and the work is boring. Most of my analysis revolves around performing calculations for financial instruments with a large amount of data (100k-5m rows), which the audit team isn’t capable of calculating because they don’t have my set of skills. At other times, it’s either performing conversation projects, in which I have to validate that the data has been transferred from one system to the other (for example, from Priority to NetSuite) or from one business to another (for example, from mother company to the subsidiary or vice versa). Sometimes I’ll have an interesting project like fraud, validation of historical data for evidence in a lawsuit, or maybe even validation of transfer account balances for a debit card business process.
Bottom line, it’s always the same idea:
Make sure the numbers are calculated correctly-boooooringggg…..
Honestly, the only benefit I feel I got was that my SQL is pretty high level now.
I hear so many stories from guys here working with Python for statistical analysis and ML algorithms, analysis of business that actually leads to insightful findings, and more. I also have interest in the world of data science and studying it right now in Udemy. 
I kind of regret that I studied accounting and became a CPA because I could’ve gotten more from a science degree.
Any other people here that went through the same process that I’m going through and could give me some advice on how to transfer from where I am right now to a career in data science?
I would highly appreciate it.",analytics,2022-09-11 12:12:39,5
"Salesforce Trailhead had a ton of resources. Not sure if they directly have raw data, but you can get access to dummy Salesforce instances for free.",10,xb6mh3,"I’ve downloaded plenty of Kaggle datasets, and while I like the range of topics like Premier League stats or stock prices... None of these are particularly helpful when applying for roles were the job position will entail tasks like reporting email marketing campaigns. Etc…

Anyone know of were I could find examples (I’ve been creating fake data in excel until now).",analytics,2022-09-10 19:16:54,3
"I copy so many peoples work. Programming and data visualization are so similar in that respect. Why reinvent the wheel.

(I use my own data but use their effort as guidance on how to represent it)",17,xb83r6,"I found a healthcare dataset I'm super interested in. I want to analyze it. I looked up other's githubs and found two other people who have analyzed it using Python. I'm an R user.   


Would employers not like this? Do they care? Of course, I'm going to try to find things that I didn't see on their stuff.   


If it's okay, would it be good to say something like ""I did so and so and that's something you won't find on the two projects you'll find online for this dataset in Python""?",analytics,2022-09-10 20:32:43,10
"Idk anything about boot camps. I went the degree route. 

If you already have a bachelor's degree (in ANYTHING) don't waste your time going back to school. Start self studying A LOT. 

I self studied + got my bachelor's in IT management. But people have degrees in finance, communication, advertising, etc at my job.",23,xay33w,"Personally I'm coming to this from scratch and want to try to learn it. 

I've heard plenty of different opinions about pursuing a computer science degree, self teaching and doing a bootcamp that can help you fast track it and get hired and work on your resume after completing the camp.  

Anyone have any opinions or insights on what could be a good way to learn and breakthrough in the field?

Thanks",analytics,2022-09-10 12:40:46,24
"East coast could be NYC and 80k is severely underpaid, you could be South Carolina and 80k is very good. Where I am, 80k is good for first year grad.",47,xag113,"I am currently a data analyst for a tech startup. I live on the east coast in the US, but my job is remote. My official title is Data Analyst/Developer. However as of right now I’m doing more of Data Engineer work which includes designing data models, writing Python scripts for ETL or data validation testing, working with AWS tools (DynamoDB, Redshift, S3) other cool DE stuff. However, my real passion lies in Data Science and I will transition to a Data Science role once our product goes live. 

I just graduated with my MBA and MSc in Data Science in May 2022. I was offered the position for $80,000 a year right before I graduated in January 2022. I have 5 years of IT experience in corporate America prior to the job in more business focused roles i.e. IT Business Analyst. I knew that after going back to school to redirect my career, I could potentially make less than what I was making. However, I sometimes I feel as if I'm being under payed considering my previous experience and educational background. I  know that eventually I will earn more as I continue to grow in the field, but I also want to make sure I am not settling and the manager/company knows my worth. I am considering asking for a raise at the end of the year, but not even sure what I should negotiate considering it is a startup. Is this salary average for my role and responsibilities and background or am I being lowballed?

Edit:
To elaborate on my previous experience. I was a Project Analyst for 2+ years. In terms of the data aspect of that job, I did basic data management/quality control using Excel. Data cleaning, Creating simple reports or pivot tables. I basically managed every point of data for the company’s IT projects. Then I was a business analyst for 2.5 years. This was more excel data reports, leading agile ceremonies,  lots of requirements documentation…blah blah. I also had a 3 month internship during Masters in a data analytics role where I did data science projects and data analysis. I’m in no way saying that these roles were even closely related to what I do now, just that I did a lot of entry level work (basically an excel monkey) when I was fresh out of undergrad. All in all, these experiences have really helped in my current role in one way or another.

Also I appreciate all of the feedback!",analytics,2022-09-09 21:32:16,31
I am not even consulting and it's still like that internally.,159,x9zfdg,"**Client:** Hi, we would like to hire you to analyze our data.

**Analyst:** Ok. I will need access to your data.

**Client:** Oh geez, oh no. Oh, gosh, I had never considered that.",analytics,2022-09-09 09:08:18,23
"I think for many (myself included) people want to have these kick ass projects that show all of the skills they know. An imperfect start is better than no start at all. 

I think developing your understanding with projects like this in R are just as valuable as projects created on the fly. Use available datasets (that are already cleaned for you) to develop a greater breadth of understanding of the software is important. In the future, you can interactively build on these concepts by adding other parts of the data analysis process.   

Most of the analysts and engineers I've talked to have highlighted a single thing, they want to understand how you think and how you work with others. Projects are just a way of showing interest and showcasing your learning.",4,xaez7u,"I'm using R. I'm looking at the titanic data. Would it be a bad idea to perform analysis on this dataset despite it already existing within R? 

Should I only stick to data found elsewhere?",analytics,2022-09-09 20:36:51,9
"I've been in management for around 15 years, and I've discovered a little trick which makes the job much easier:

Get rid of the poor performing staff, and only hire great people.

This takes effort (I'm constantly on the lookout for people, including hiring people for short projects to better understand their skills and attitude), but the payoff is huge.

When you have a team of responsible, intelligent, skilful people, **they manage themselves**. Sure, we have meetings where we discuss things, but their input is usually great and the team does things by consensus. 

If the junior analyst and data engineer are problematic (sounds like they might be), you can talk to your boss and HR about coming up with a plan to replace them. Don't keep lazy or toxic people on your team.",22,x9t3h6,"I'm hoping that some people here have been in the same boat. I feel at my company though I've hit a ceiling and the only progress is business intelligence manager, which would make me responsible for managing our junior analyst and data engineer. My manager (he's head of our business intelligence and finance teams) is supportive of this promotion. I really don't like managing, but I also feel like hitting the top of my career in my early 30s is too soon and I want to keep developing. Some details about my profile:

\- 5 years in (big four) corporate finance consulting

\- 2 years in a business analyst role in an SME (current position)

\- Master's in finance, technical experience all from on the job

\- Expertise leaning towards the business-side of my current role, technical aspect is primarily front end (Power BI & Excel) and the conceptual side of data engineering.

I still really like what I do but I feel like if I don't take this opportunity I might regret it, as I don't see any other career paths. The only one I can think of are becoming a freelance consultant. 

Is there anyone here who's been in a similar situation?",analytics,2022-09-09 04:33:46,6
I use a Sankey to represent lead-opportunity lifecycle but I am interested in learning what others are doing.,7,x9alsz,"I'm working on revising an executive deck for a monthly operating meeting. A big focus is the marketing and sales funnel, and a challenge is how to reconcile monthly metrics with the sales cycle. So I want to show how things are performing, but in a cohort view, looking at August leads in August doesn't tell you much about their success if they take a couple of months to convert.

I'm thinking about a waterfall view to show conversion over time, but curious how others have approached this challenge.",analytics,2022-09-08 13:19:17,8
You would just calculate the correlation coefficients like normal.,9,x99dil,"I am working on some KPI metrics and want to determine two things:

1. The relationship between these metrics. Does increasing pressure on one put pressure on the other? 

2. See if there's any statistical difference in the relationship between both of them. 

How do I go about this please? 

Also, the metrics are in different units and each of the metrics numbers are significantly higher than the other. Is there a need for a normalization to make all metrics numbers similar? For example, metric 1 is a max $2000, metric 2 is 0.3 day, and metric 3 is 100 miles.",analytics,2022-09-08 12:32:15,6
"Yes we saw this for some clients, I think it's processing.",1,x9i5q8,"All metrics (events, sessions, users) report zero since yesterday 9/7/22. Was working fine up until Tuesday.  Anybody else?",analytics,2022-09-08 18:38:11,2
"Probably depends on company but having worked in 3 different companies, loading a dashboard shouldn't take much longer than 15 seconds to at most 30 seconds. Anything that consistently takes above a minute is extreme and basically unusable. Analysts usually make a dashboard that can be used by everyone in the company who needs it and most of them are not analysts so speed is of utmost importance. You shouldn't worry much about it though, the databases connected to dashboard creation software like Power BI, etc are fast and have summarized data so they usually run pretty fast. If it takes time, most you can do is make your query or code to pull data efficient but most of the responsibility lies with data engineers and database admins.",11,x8sfu7,"Of course both the nature of the viz (and the underlying query), resultant number of queries, filtering etc. all impact this, but I'm keen to get your views on the general perception around what constitutes ""fast""? 

Also - at what load time would a dashboard be considered ""unacceptably slow""?",analytics,2022-09-07 23:13:24,10
"I’m not quite understanding what you mean by “the addition of aggressive divided by the total number of states” and “the aggressiveness per state”. As near as I can tell, you may want something like

SELECT

State

, CASE WHEN SUM(Aggressive) > 0 
THEN SUM(Aggressive) / COUNT(DISTINCT State) 
ELSE NULL END

AS Aggressiveness

FROM Table1
LEFT JOIN Table2
     ON Table1.col = Table2.col

GROUP BY State",1,x9dkom,"I have this SQL project I'm busy with. I'm having difficulty trying to isolate parts of a column and dividing it by the total of a different column.


I have table 1 that has the column (Aggressive).
This column has two results (either 1 or 0)

I have table 2 that has the column (states). This column contains more than 5 different states in it


What I want to see is the addition of aggressive divided by the total amount of states there are, given that the aggressive column has a ""1"" in it.

I don't know if I should alter the integer and convert it to a string to make the division easier


I created a CTE to join the two columns into one table but I still can't divide them properly. I've used count, sum, having count >0 it's not really working.


I also want to make the conditions such that I can see the aggressiveness per different states in the states column.",analytics,2022-09-08 15:17:45,5
It’s good for doing stuff with Power BI administration. I myself am just scratching the surface on what it can do but it helps streamline a lot of tedious tasks with a click of a button/key on your keyboard.,8,x8dri9,How have you used it or how is it currently used? Is it worthwhile learning for analytics?,analytics,2022-09-07 12:16:20,11
Is the original event actually being emitted from the site/app to begin with?,1,x8t1gw,"I am trying to create custom event for my purchase page with GA4. I read many blogs regarding on that and I created custom event for my page but when hit the save button after it will not showing up in the list of event.

It is showing in real time section but not in the list why?

Please help me on this.",analytics,2022-09-07 23:47:24,1
Never heard of it and I’ve been in BI work for years. I suspect it’s garbage but you’ll have to let us know.,8,x85uri,Anyone here ever used Oracle BI Publisher? What are the pros and cons? They are implementing it now at my workplace.,analytics,2022-09-07 06:59:13,12
Quantum metric,1,x8hq24,Client uses inspectlet but it's heavy on speed. Any recommendations?,analytics,2022-09-07 14:54:11,3
"Computer Science, Information Technology, Management Information Systems, Business Administration, Applied Math, or Engineering are all great picks.

I have a BA in English Literature.",34,x7twrg,"Hello, I am starting my senior year I Highschool and I am heavily looking into data analyciric positions for the future. I would love it if someone could give me some insight into what majors I should be looking into as to purse a career in this field. Any advice or resources or help me would be appreciated !",analytics,2022-09-06 20:04:32,23
You need to give more information than that to go on!,11,x7eas6," I have my interview tomorrow for the aforementioned post tomorrow, I am an undergrad, currently in the final year of my course (Mathematics Hons) and was shortlisted for this role. As a fresher, what are the areas that I should focus on?",analytics,2022-09-06 09:02:14,9
"Both courses sound good, and not until I got to work as an analyst did I find out that it really depends on which type of analytics you'll end up working, but whatever you choose you will inevitably be better off position than most graduates to apply for an analytics role.",2,x7q3sy,"To give some context my courses are mainly qualitative so far. (had a stats introductory course but it was a while ago so rusty, will have another econometrics introductory course this year, know some basic excel with pivot tables etc). Courses are only available in one semester, so restricted to these two combinations - cannot do A+B or C+A etc. 

**Course A + unrelated.** 

The primary focus of this course is to help students view various problems from business, economy/finance, and social domains from a data perspective and understand the principles of extracting useful information and knowledge from data. Students will also gain  hands-on experience using R -- a programming language and software environment for data analysis and visualisation. Learning basic data analytic methods and techniques is combined with real-life examples. 

The core contents of the course include data cleansing, data transformation, data visualisation, R-programming,  classification, regression, clustering, over-fitting avoidance and model evaluation. The course also covers a subset of the following topics: illustration of R-access of databases and big data platforms,  illustration of parallel computing in R, similarity matching, market-basket analysis, link prediction, text mining, network analysis, causal modelling. 

**Course B + C** 

The primary focus of the course is to cover principles of computer programming with a focus on data science applications.

The topic covered will include variables, basic data types, data structures and sequences, control flow structures, modularisation, functions, variable and function scoping, errors and exception handling, and data input-output operations using file systems and operating system standard input-output; use of multi-dimensional arrays via NumPy, data processing using Pandas dataframes; principles of object-oriented programming including objects, classes, methods, encapsulation, inheritance, and polymorphism; principles of functional programming languages such as use of immutable data, flow control using functional calls and recursions; practical aspects of algorithmic concepts such as searching.

The course will primarily use Python programming language, but will also discuss and provide references to how the fundamental programming concepts are implemented in other programming languages, in particular, R. 

A second course in statistics with an emphasis on data analysis with applications in the social sciences. Students will gain hands on experience using R-- a programming language and software environment for data analysis and visualisation. The course contains five topics, including (1) principles of statistical analysis, including data preparation, statistical models, regression and classification, inference, prediction, and bias-variance tradeoff, (2) multiple linear regression, including its assumptions, inference, data transformations, diagnostics, model selection, (3) regression tree method, (4) logistic regression, including odds ratios, likelihood, classification, and ROC curve, and (5) Bayes rule for classification and linear discriminant analysis.",analytics,2022-09-06 17:06:57,5
Maybe use that keeness for Analytics to search through the many similar questions asked in this subreddit before to find your answer!,15,x7glmx,"What are some good courses, especially the ones that welcome students from non statistical/ technical backgrounds?",analytics,2022-09-06 10:34:35,14
"I would start with

- funnel: how many people start, how many drop out at each step, how many convert 
- quality of each channel/campaign. What’s the conversion rate? What % are quality? What’s the ROI?",3,x7gdpp,"Hi guys,

I have joined a company recently, i'm a Paid Campaigns manager and i've been asked to run some analysis on our Google analytics4/matomo & Dsps that we're using and come up with conclusions on what we should to improve, and honestly, the only thing that i can see is that we need to improve the SEO..

I wanna know if there are some practices and common knowledge between data analysts, some basics that i should know of, concerning organic & paid traffic.

This might sound a little bit lazy from me but i have to come up with a conclusion asap so i'm trying everything i can

thanks all",analytics,2022-09-06 10:25:33,4
"Generally yes, data analysts don’t do machine learning. Hypothesis testing and exploratory data analysis and dashboarding are the common tasks.",33,x6wkv3,"Title.  


I'm trying to avoid Machine learning at all costs.",analytics,2022-09-05 17:56:07,23
More >,1,x7ec06,"Are there any benchmarks for the duration of data which should be considered for analysis to come up with a CRO strategy like 4 weeks, 8 weeks, etc?",analytics,2022-09-06 09:03:35,2
"Hi,
I'm looking for the opposite switch having worked with Supply related analysis the past 7 years. If you'd like we could perhaps give each other some tips?",15,x6bl5c,"I am currently a Data & Analytics Consultant in a company that only serves Marketing departments.

In general, I am very well versed in the Google Marketing stack and other marketing-specific products, but since the beginning of this second career of mine (I studied Neuroscience and was a PhD student researching in a hospital), I tried to keep myself as general as possible, learning to use SQL and Tableau during my first job in a payments company and working with those to analyze purchasing behaviors and product analytics in general.

Now I can officially say I grew very tired of Marketing, plus I don't believe it is a stable domain to invest my learning hours in and I want a change.

Ideally, I would be most interested in working in the Supply Chain industry at any level, since I have a fascination with logistics, but other domains I am considering are banking/payments and general business operational analysis.

Problem is, I cannot jot down a clear path to take myself to any of those and I am therefore asking those of you who made the jump, how did you end on the other side, what skills and habits were useful for you to make the transition.

Thanks in advance for the suggestions.",analytics,2022-09-05 02:09:07,23
Huh?,1,x6aeib,How Anblicks helped the world’s largest commercial real estate services \&amp; investment company build a self-service driven analytics platform and enable the customer to achieve faster insights and improve profits.,analytics,2022-09-05 00:52:10,1
Maybe an Apple or iPhone related sub would be more helpful,3,x6psfh,"{""crashreporter_key"":""4a28f145c0ffa189e25c7f197540f45942a04b91"",""bug_type"":""211"",""timestamp"":""2022-09-04 17:00:03.00 -0700"",""os_version"":""iPhone OS 15.6.1 (19G82)"",""incident_id"":""720BFC10-CE30-486F-A0E6-D358FBD978AB""}
{""_marker"":""<metadata>"",""_preferredUserInterfaceLanguage"":""en-CA"",""_userInterfaceLanguage"":""en"",""_userSetRegionFormat"":""CA"",""basebandChipset"":""mav21"",""basebandFirmwareVersion"":""1.70.01"",""configUuid"":""0ca68bd1-f288-438d-b6f4-3a72fd15fdb8"",""currentCountry"":""Canada"",""deviceCapacity"":128,""dramSize"":5.5,""homeCarrierCountry"":""Canada"",""homeCarrierName"":""Public Mobile CA"",""isDualSim"":false,""rolloverReason"":""scheduled"",""servingCarrierName"":""TELUS CA"",""startTimestamp"":""2022-09-04T03:24:08Z"",""trialExperiments"":""0"",""trialRollouts"":""2"",""version"":""2.3""}
{""_marker"":""<end-of-file>""}


Is there anything that is fishy or is this a completely normal report, please let me know if you are able to read this :) 

Thanks and have a Great Day 
Respectfully, Mayo",analytics,2022-09-05 13:03:58,1
"If they take the time to give you feedback even if you get rejected, it's a pretty good signal from the company. If you don't get hired, ask them what were your weak points and work on them. Good luck all the same!",28,x6dfug,"Does this mean that I am rejected? I really want to work here but the call ""will only take 5 minutes.""",analytics,2022-09-05 04:04:21,10
"Not more than basic SQL and excel. I’ve known people with no experience at all learn on the job.
Just be curious, motivated and try to learn as much as you can.",12,x5kz03,"I am in my third year and starting to build up my skills for a data analyst position. My career counselor offers me no advice because I can’t contact her. I am confused about this job, what skills do I need?",analytics,2022-09-04 04:43:13,11
"Sure, Ive just jumped to my 2nd role as DA. I'll be happy to take a look",1,x5mgt1,"I’ve been working as a data analyst since 2020 in KPMG and now EY. Looking to take my career to the next level.
I would like a CV review.
I can’t post it here so if anyone can PM me, it would be nice.",analytics,2022-09-04 06:01:53,3
"> My new boss had already announced that I will get the Senior title and that she’s trying to get me a raise too

It's good for your career that you've got the title change, but they're not yet sure if they're going to increase your salary, even though they want to change your role to have more responsibilities? Not cool!

Is the company profitable? Roughly how many people work there?",29,x4sxaw,"Hi,

Hope to hear your advice on this. 

My boss left the company leaving me as the sole person on our marketing analytics team. My company had mentioned that they want to promote me to the Senior Marketing Analytics role at the beginning of our fiscal year (in less than 2 months). They have already transitioned some of the reporting to me and I will be taking on even more responsibility.

My dilemma is the following- I had talked to the interim boss about a raise and senior role, particularly because in my country the inflation rate this year is about 12%. My new boss had already announced that I will get the Senior title and that she’s trying to get me a raise too. 
With that said, how much of a % increase is reasonable to ask for or agree to? To me it would be whatever base salary + inflation rate + new role percent increase. What do you think?

P.S- I should mention the company is giving inflation raises at the end of the year but I assume it will be less than the inflation.",analytics,2022-09-03 05:10:54,28
"I’m a data scientist on a product analytics team and I use Python a lot for data exploration and visualization and some predictive modeling. 

What kind of job are you aiming for?",30,x49okq,"I do game dev as a hobby but cannot see myself doing it professionally. I analyze manufacturing processes for a chemical plant and help optimize them, make business decisions, prevent issues before they occur, etc. It’s mostly Excel with SQL data sources (Oracle, Microsoft or local SQLite storage).

The most complex things I write in SQL are probably joins and sub-queries. I do a lot of logic in Excel after I do the bulk volume on the server side.

Excel is a great framework and it works just fine but… Is this as good as it gets in terms of programming?

It feels like stagnation…

Should I just pick up new tools and look for a new job? R? Power BI? I am good with C#. I have a 2-year college degree with 3 years of experience in process analytics.",analytics,2022-09-02 12:05:36,28
What other analytics skills do you have other than Google Analytics? Because that's really important too for getting a job into the analytics field. If you know SQL and R/python then you can already start looking. If not then I would highly recommend learning SQL. I don't have experience with Google analytics but you would be limiting your options but not learning other skills which are really important in the industry.,8,x4cpih,"tl;dr - almost done with google analytics program and looking for tips on how folks handled their next steps!

After writing that title, I realized these Qs are just as applicable to folks who weren't successful. I want to hear from both, ONLY if it isn't inconvenient for you. I really do appreciate it. 

About 5 months ago, I got married, and also decided that I am going to make a career change from client support/quality assurance ---> data analytics (a broad statis point for my career move, i know but I'd accept a job in cleaning, analysis, database architecture, whatever at this point). 

After having the Google and IBM program recommended, I went with Google (pls feel free to tell me if you think I should've gone the other way!) 

I am almost finished with course 6 of 8 (the visualization one focused on Tableau), and I need your guy's help. I don't want to jump the gun and become overtly confident (v much dislike that personality), but I don't want to sit at my desk if I am able to, theoretically, acquire a job. I learn with practical knowledge better. 

So the Qs: 

1. At what point in the course did you all begin looking?
2. What felt most critical about the resume? 
3. What would you say is the biggest difference between an interview to become an analyst, and an interview to become a client support person (using this example because it is familiar and feels like the main distinction given the non-technical nature of those interview) . I believe I'm very good with people, and I have success in conversational interviews, but I'm a little scared of the tech interview, so any help would be absolutely awesome. 

I may have more Qs, but regardless, thank you to anyone who can help!",analytics,2022-09-02 14:16:34,15
"It's day one...

On my day one i had to google program requirements and create a manually made crosswalk...

Data gathering is step one. You explain to ETL guys what they need to plug in. They run the data. Then and only then do you get to analyze. But it is cyclical",68,x38n5d,"Has anybody run into data analyst positions that don't hold true to that title at all? 

I recently accepted a position and today was my first day. I was under the impression I'd be pulling and analyzing data. 

I spoke with my boss's boss today and he said he doesn't want me involved in any dashboard building or ETL. He just wants me to document business requirements and give them to the ETL guys. 

Is this a data analyst? Seems more like a generic business analyst type of role. I'm concerned I won't be able to leverage any of the SQL and Python skills I'm trying to continue growing. 

I'm in a bit of a panic because I don't know if I'm supposed to quit my first day or stick around and waste time doing things I'm not interested in.",analytics,2022-09-01 07:09:28,18
"I don't like that your FAQ section has answers in video form. I want to quickly read an answer, not watch a video.",3,x3arns,"Here is the link to the original post: [Original Post](https://www.reddit.com/r/analytics/comments/j3wujw/i_created_a_look_up_any_companys_suppliers/), [V3.0 Beta](https://www.reddit.com/r/analytics/comments/lg52mb/after_25_convos_with_ranalytics_redditors_giving/), [V4.0 Beta](https://www.reddit.com/r/analytics/comments/t0e0n7/after_160_convos_with_ranalytics_redditors_giving/)

You can find an example visualizations here: [Lululemon's Supply Chain](https://www.importyeti.com/company/lululemon-usa)

If you missed the earlier posts, ImportYeti searches 90,000,000 public shipping records to find a quality supplier 1,000x times easier than Alibaba alone. You answer questions like:

* **Who makes American Barbell's epic dumbbells?** Answer: Top Asia Sport Industrial
* I'm thinking of buying barbells from Nantong Leeton Fitness Co., the #1 ranking company on Alibaba for the term ""barbell"". **Is Nantong Leeton Fitness Co. the right supplier for barbells?** Answer: No. They are a big company but primarily sell resistance bands. Thus, they likely outsource their heavy metal work creating a more costly and potentially worse product.
* **Who are the top companies & suppliers who import/export under HS Code 62.04.19 -- women suits and ensembles?**

Here are the BIG changes on this release:

* New country graph
* New vendor table
* HUGE improvements to the site speed
* and hundreds of small changes

**I'd love any and all feedback (love or hate)** on how I can improve existing visualizations or create new ones... no matter how brutal, small or crazy : ) I only want to create things that people really love. If you enjoyed this tool, have any ideas for how to improve it, or found a bug/usability issue, I want to hear from you. Please PM me or comment below anytime",analytics,2022-09-01 08:38:08,2
"Why not create a dedicated unlinked video and send it only to the email list?

You cannot run JavaScript within emails to send calls back home from email. Gmail and Apple Mail will shut that down fast.",2,x3pije,"Hi, I'd like to know if it's possible for me to analyze if users finished watching the video embedded (self-hosted) on an email I sent.  I want to be able to analyze what's the view analytics of the video from the specific campaign I'm sending.  


Currently it's a Youtube Embedded Video, where users click to Youtube. Unfortunately, youtube doesn't provide in-depth data on specific UTM performance such as How much of the video did viewers from XX specific email watched.  


Any help/clarification is much appreciated.",analytics,2022-09-01 19:15:20,1
"ETL is extract transform load. Basically taking data from one place, performing some action like adding context or enriching it and then loading it somewhere else (another DB, table, etc).

A pipeline is a link for data to follow from one source to another, normally the DB to a reporting tool like Power BI or Qlik.",18,x39ppc,"Some jobs ask for familiarity with ETL and building pipelines. 

What on earth are pipelines? Im a new grad seeking my first role.",analytics,2022-09-01 07:55:00,22
"First you get the data. That can be api, web scraping, PDFs, any weird way the data comes, you have to be able to handle it. 

Then you can clean it and format it correctly, data is so dirty and standardizing it is important. Get those date formats correct! 

Then you load it to your database so it can be modeled. 

So it is the start of the process. Garbage in, garbage out. 

This is the garbage in portion",90,x2y5sr,Many jobs ask for ETL? What is that?,analytics,2022-08-31 21:25:59,27
It’s terrible.,2,x3e7i2,,analytics,2022-09-01 11:01:33,4
Looking at the platforms included the only one I see that may have a cost associated with it is tableau (assuming you’re using tableau desktop). However there is a web-based free version called tableau public that you can continue using post certification.,3,x37c39,I just have a simple question: Do you get to keep the programs included in the certificate or do you only get access while being a subscriber?,analytics,2022-09-01 06:11:45,2
Can you rephrase the question? I really want to help but have a hard time understanding your situation.,1,x3eedv,"I am currently 9 months out of graduation. Started applying 3.5 months ago. If for whatever reason, I got let go from a job in 3-6 months, would I have a hard time finding another job after?",analytics,2022-09-01 11:09:47,5
Depends on the problem. Check the admissions requirements. Some will teach you to code.,4,x3e6jp,Is it possible to pursue and be successful in a MSBA program without any coding/programming experience?,analytics,2022-09-01 11:00:31,7
"That sounds like a massive undertaking. Are you the only one working on this? Also, do you have any senior resources that are able to take a look?

Is there a possibility the legacy metric was flawed itself?",7,x2t2k0,"Hey there, I’ve been taking about 2+ months to complete a project, so hoping to get your perspective here as this is the first time I’ve experienced a project of this scope. 

A little bit of a background, 
I work as a BI Analyst 2yoe at a manufacturing company with a high product mix. Our systems are home grown, so underlying data structure and reliability of the Warehouse is gnarly. 

I’m tasked with helping develop new version of a legacy key quality metric. The legacy metric is calculated from a 800+ line SQL script composed of multiple nested temp tables and fed into Power BI that has additional logic to produce the metric. The script was developed about 5 years ago, and the original author left the company 2 years ago. This presents a huge tribal knowledge problem, and stakeholders no longer trust the legacy query/dashboard results. 

Because the legacy query was so hard to unpack, we decided on developing a new query/Power BI from scratch free of any old/incorrect assumptions and dev patterns and fits the current business context better. 

Now the future state is done and loaded into Power Bi. The result was a 3x offset from the legacy data. That triggered a massive effort / experiment of digging into the legacy query to identify logic deviations, then conflate that logic into a modified version to-be state to attain data parity, so I can prove the break points and impacts to leadership. We know the results won’t match but we need to prove why and if those reasons are acceptable, by working backwards in a  sense. 

This is what has been taking me 2 months, and counting. It’s very daunting. So far I have identified 6 impactful deviations and I’m still about 1.5X off in my experiment group (which is the Jerry rigged to-be state script and dashboard). I’ve crawled through rows in the temp tables, traversed through model relationships, checked every filter, DAX formula and still need to figure out why director filtering is off. Still no telling if when solved it gives the result we want - which is matching values. 

Am I way off base here ? Should I be concerned about how long this is taking relative to my experience ?",analytics,2022-08-31 17:17:18,10
"Yes? Like anywhere. You'll have people at school doing the bare minimum to pass, while you'll bave others busting their ass to get straight As.

It's the same at work. Some people will work to live, some will live to work, and some will be somewhere in between.",6,x3aate,"My friend says it’s great that I “care”. I’m a first gen and most of these analytics people on the internet sound super smart. 

I went to a good school but don’t have industry experience yet so I’m terrified even though I’m learning as much as I can. 

So… are there a lot of people in industry who do the bare minimum? Not posting to judge anyone. Just want to know.",analytics,2022-09-01 08:18:42,1
"I’ve done technical/logic assessments during interviews, but usually part of the second round. However I did work for a company that had the tech screening first as part of the intern/new grad hiring process.",1,x3a6se,"Received this morning, for a job with just the title “Data Analyst” 

An email with ~5 math/logical-ish puzzles. Same concept of “if a train is going 75mph and it’s 3 pm, and another train is going 50 mph etc etc” 

I don’t want to post the screenshot of the email in this post because I do want the opportunity to give them my honest answers. And if there’s the possibility this could come back to me, avoid any negative repercussions.",analytics,2022-09-01 08:14:08,2
"Yes just keep it short and thankful, not apologetic",6,x2whm7,Had a call with a recruiter for a super great company. Don't feel like I did the call justice because I had just gotten off with a video interview with a manager for another company and it went past the intended time.,analytics,2022-08-31 19:59:54,5
"If you have joins covered (left, inner and full) and group by and aggregations like sum etc. Then you're probably good to go for the basic positions. From there I'd suggest learning about subtleties and CTEs (common table expressions) and window functions. That is about the extent of what I use as a data analyst that is pretty hands on in the data department of things.",8,x34xko,"CS grad here. The college study done was mostly based heavily on ML, DL, python, and R-based visualization. 

Realized most data-centric roles require SQL. I know the basics of SQL(like any fellow CS grad), but clueless about what they would want from me in the interview. 

A guy the other day told me they asked about window functions him in an interview, I don't even know what hell is that. 


What should I focus on and learn? 

I started a udemy course by Stephen Grider just to get on track. 

Any advice, please?",analytics,2022-09-01 04:16:07,2
"Data is data and where you start has little impact on where you end up. 

Ecological inference has nothing to do with ecology but everything to do with political science. 

So don’t think you have to pigeon hole yourself. 

Right out of school you can go anywhere, and your first job, that industry, is super important so pick it with care. 

Or be like the rest of us and just stumble into the first job that was hiring.",15,x2d7ww,"I am a current masters student in the ecology field. Recently I have started to develop a disinterest in the fundamental draw to our field, and dont really have much desire to pursue a job in the field. During my undergrad I worked on a project where I was forced to learn R, and really loved it. So far in my MS program I have continued to develop my skills, take a few stats courses, etc. and have started to consider data analytics as a job path post-masters. This semester I am taking a stats course in big-data analytics which I hope will help me understand the field a little more and how I compare to stats students in terms of raw skills.

I consider myself to be close to proficient (but also maybe not depending on expectations) in excel and R. I also have a bit of GIS knowledge. I am hoping to get into SQL and probably Python a little this fall/winter and anticipate picking them up at a working level relatively quickly (side note: any resources to share would be great). I am fairly comfortable with applied statistics and interpreting and visualizing results but definitely lack the mathematical background that most stats students have. I am very comfortable with technical writing and will have publications/thesis to support this. 

I am wondering what the job outlooks for a candidate like me might be. Sifting through ""data analyst"" job postings is overwhelming as I have no real grasp as to what I would/wouldn't be cut out for. A few questions I have

Does it sound like my skillset has potential to successfully transition into the field?

Right now finding an analyst job working with ecological data seems like the best in for me.. but is there any hope to landing other sorts of analyst jobs outside of the biology/ecology realm?

Will the jobs I am competitive for be entry level analyst jobs, even with a masters in a quantitative science field?

Has anyone heard of transitions like this and have advice/suggestions/warnings?

What kinds of things can I be doing now to better prepare myself as a job candidate?

Thanks in advance! All advice is welcome",analytics,2022-08-31 06:08:49,7
At least on my end I’m confused about what you’re looking for. Are you wanting former grads to give you info on the schools or are you wanting general info about these programs?,3,x2u7eh,"Looking for any information (advice, feedback, tips, experiences, etc.) related to the following graduate schools, particularly related to the degrees listed. I am looking to get into Data Science, Data Analytics, Business Analytics, Consulting, or general Analytics. I am interested in all things data, analytics, and programming.

I am a senior studying engineering, minoring in statistics in the United States, been fortunate enough to do very well in school and obtain internships and research experiences. I have exposure and coursework with Python, SQL, R, SAS, Java, Tableau, and Excel.

List of schools:

1. MIT Sloan - M.S. Business Analytics
2. **North Carolina State University** \- M.S. Analytics
3. **Wake Forest** \- M.S. Business Analytics
4. **Georgia Tech** \- M.S. Analytics
5. **University of Virginia** \- M.S. Analytics
6. **University of Georgia** \- M.S. Business Analytics

And before you type it, yes ik that MIT is the most prestigious of these programs, but really looking for general info on all of the schools if possible. TYSM",analytics,2022-08-31 18:10:38,23
What does this have to do with analytics lol,29,x2r9p6,"Title. For Data Analysts, specifically.",analytics,2022-08-31 15:55:59,18
"What analytics system are you using? What kind of site/app is it? Did anything else change other than the traffic? Session duration, bounce rate, etc. Are you sure nobody applied a filter or segment to your view of the traffic?",1,x2mbvc,"We saw an 80% decrease from these cities. We’ve always known some of the Ashburn traffic is due to aws server located there and have referred to it as bot traffic. 

I’m not sure why there is such a steep drop from July to August. Anyone seeing the same thing in their website? Any ideas on why we’ve seen this traffic for years and it’s suddenly gone. 

It makes our numbers look really lopsided even though we’re still getting good traffic",analytics,2022-08-31 12:28:59,1
Not in Universal Analytics. GA4 has a data deletion request.,1,x2io3z,"Hey all,  


Does anyone know if there is a way to delete data by user ID within GA? I can't find anything other than the data deletion requests which looks like it would be all data for a time period not data attributed to a particular user ID.  


Thank you!",analytics,2022-08-31 09:58:45,2
Bounce rate is the inverse of Engagement Rate and the latter can be adjusted @ Admin > Data Streams > \_\_\_ > Configure Tag Settings > Adjust Session Timeout > Adjust Timer for Engaged Sessions. (default is 10 seconds).,1,x2ghk3,Is there a way in GA 4 to not include users with a dwell time above a certain threshold in the bounce rate metric?,analytics,2022-08-31 08:27:59,4
"Update your channel grouping in the settings on both your default as well as a retroactive grouping. That's much easier than working around Google's built-in classifications.

Edit: just a quick addendum. If you look at your source/medium report for social, you should be able to identify the actual combination that Google will automatically recognize. The only exception being if all your social traffic is fucked.",1,x2f570,"Hello everyone,

I've been trying different codes on my UTM in order to fix the conversions being assigned to ""direct"" instead of paid or social.

UTM for regular ads:  

    ?utm_campaign=fb_paid&utm_medium=social&utm_source=facebook 

UTM for dynamic ads:

    ?utm_campaign={{campaign.name}}&utm_medium=social&utm_source=facebook&utm_term={{ad.name}}&utm_content={{adset.name}} 

After reading some articles I've come to the conclusion that my medium should be 'social' and 'facebook' as source, am I right on this and it's only a matter of time until the issue is resolved? if not i'm happy to learn the correct method.

Additional information:

* UA account
* Implemented via tag manager, all tags firing correctly
* All conversions are being tracked, no information loss, just not assigned correctly
* WooCommerce based website

Any information would be greatly appreciated, thank you.",analytics,2022-08-31 07:32:38,1
"You can encourage both, but I'd also be more careful about trying to keep someone with strong technical skills versus someone who had industry/domain knowledge because those strong technical skills are more easily marketable and transferable if the person gets pissed off enough to leave.",44,x1n25z,"Say your analyst has very strong technical chops (ETL, finishing requests fast, automation, dashboarding, learning new tools quickly), but is lacking in presenting domain specific analysis and insights. 

Or the other way around - great industry knowledge and awareness, but needs handholding when it comes to technical stuff/takes a long time to pull requests.

Would you try to push them to learn what they’re lacking in, or encourage them to expand on what they’re already good at?",analytics,2022-08-30 09:09:46,36
Yes with the use of audience. You set it up so it's one time per user/session and then have an event fire when a user joins the audience. Count that event for unique conversions.,2,x288mq,"Is this possible? We're currently tracking conversions based on pageviews, so if they visit the page again they're counted as a conversion.

&#x200B;

Is it possible to count conversions uniquely by user? Or some view/exploration window we can use to see this unique metric?",analytics,2022-08-31 01:34:16,2
">Recently, I switched industries and landed a new job 6 months ago in the  
healthcare industry as a Senior BI Engineer/Analyst. Now my career   
track is tending towards Analytics/Data Engineer

This is a bit confusing, since titles often have lots of overlap across and within industries. If you're not pursuing management of any kind, then you'll just have to be aware of reaching the upper limits of salary ranges and potentially move companies accordingly when maxing out.",3,x1qo8e,"My career right now has been great and I am exactly where I want to be. 

I graduated with a BA in Mathematics 3 years ago and have been working in industry for 3 years now. I was doing business intelligence work (analyst / developer) across two teams (corporate finance (FP&A) and data warehousing/business intelligence) in the media industry. 

Recently, I switched industries and landed a new job 6 months ago in the healthcare industry as a Senior BI Engineer/Analyst. Now my career track is tending towards Analytics/Data Engineer and comp is above market (especially for my 3 YOE) in a VHCOL city (based in the US).

The hospital I work at encourages higher education and has a generous tuition reimbursement program, so I would not have to take out a loan to pursue a grad degree. I'd have to do grad school as a part-time student in order to keep working in my role, and avoid going past the max reimbursement amount. 

With that said, I'm not really sure if grad school is really necessary for this field and my career trajectory. Typically I see people pursue the following grad programs:

* MBA 
   * I personally wouldn't pursue this unless I wanted to go down the management route. I feel like I wouldn't really learn much from an MBA either, since I did take accounting/finance/econ courses in undergrad and worked in FP&A. Easy credential/salary boost for me, but it wouldn't make me happy (I left the finance/business side of things world for a reason).
* MSBA / InfoSys
   * Less business-y and more technical than an MBA, but not as technical as STEM. I feel like this would be a waste of time since my math degree and work experience trumps anything I would learn from it. People tend to pursue this program in order to break in to analytics. Another easy credential/salary boost, but I feel like I wouldn't be learning anything.
* STEM (Computer Science, Statistics, Informatics)
   * Definitely intellectually stimulating, but I don't see this being worth it unless I wanted to become a data scientist, or produce research (although that sounds interesting down the line). Salary/credential gain is also negligible--I could arguably make more by staying in industry, learning/implementing new tech, and maybe job-hopping a couple of times. I also have the ability to self-teach myself grad level math/cs/statistics. 

I've heard that grad school can be like an ""intellectual vacation/break"" from industry, but I also recognize the opportunity cost of time associated with it. There are creative pursuits that I am also actively working on outside of my 9-5 that I would have to put a pause on. I am still young, no family/kids so I'd be willing to put that on pause if I had a big purpose to do so. 

Grad degree holders, or others thinking about this--would love to hear your thoughts.",analytics,2022-08-30 11:32:58,24
"I appreciate coworkers who are eager to learn from their mistakes and don’t make the same mistakes…also coworkers who own up to their mistakes…we can’t always be perfect, we’re all learning and the WORST is when someone won’t admit that maybe they made a mistake.",13,x1zcbw,"I made a lot of mistakes on my calculations for a project after sending it to my boss. My boss says it's okay it's what she's there for (to check my calculations) but fucking hell I've never felt so embarrassed. 

This may come off as an entitled rant and shit but I've always prided myself for being detail oriented especially since getting out of uni. 

The presentation were for our major client too. I'm in the toilet right now ranting to reddit.

I need advice on how to be a great analyst in the future.... And to minimise mistakes in the future. Thank you reddit",analytics,2022-08-30 17:33:04,19
I look through data studio templates and replicate them in ppt whenever I get stuck,5,x1vst8,"I am good at building my models and doing analytic work. However, I am finding it hard to make good powerpoint slides to show case my work especially to higher management without being too technical.

Are there any PowerPoint templates tailored towards analytics out there that I can follow or use to make it easier? 

Thanks",analytics,2022-08-30 14:59:12,5
"Disclaimer: I’m definitely very partial to Tableau but have used both. Some considerations I would keep in mind:

-Tableau visualizations are definitely better than PowerBI. Both can look great but you’ll run across more limitations with PowerBI. Things like being unable to hide column names from tables or setting a chart to auto fill by width may seem trivial but may make your life harder if appearance is important to stakeholders.

-I’ve heard it said and my experience seems to confirm that PowerBI data models are significantly more inflexible and harder to update than Tableau. You need a equivalent single key to join tables on vs being able to use multiple keys using multiple logical comparators in Tableau. If you have a well defined list of tables with unchanging schema this isn’t a problem, but if you expect to need to add in additional columns or data sources later as needs evolve this will be a weakness of PowerBI.

-PowerBI does make it easier to bring in images via url as data. If your use case requires displaying images as data from your tables this is a point for PowerBI.

-Obviously PowerBI requires a Windows machine to get full access to the desktop app. Depending on your org this may be 0 issue but consider everyone who might need access to desktop editing power.",3,x1ubct,"We are bouncing between Tableau and PowerBI. I'll be working on visualizing our metrics and KPIs and will want a central location for our stakeholders to view these visuals. 

Beyond the visuals, any insight as far as data cleaning goes as well?

Both seem to be solid options. Anything specific I need to be mindful of? Power Query seems to be pretty powerful from a data cleaning perspective.

I'm new to analytics in a formal capacity so just looking to get some ideas to think about.

Thanks!",analytics,2022-08-30 13:58:45,8
"https://support.google.com/analytics/answer/9191807?hl=en

Having recently been on a large project implementing GA4 and BigQuery reporting, all I will say is fuck Google. They are - without a doubt - the most awful company to work with, when it comes to their suite of tools and how they handle updates and transparency. Everything is a black box, they regularly release updates and change their algorithms without informing their customers, and have no consistency between their offered tools and services.

Don't rely on their numbers, track only trends and patterns. You can't trust anything else they might show because they ""improve"" the shown data without telling you how, why, and when.",2,x1jju2,"Quick question -

My one client consistently sees more GA sessions than clicks (pulled via their ESP). 

A few weeks ago (reporting period of 7 days), they had a click total of 3,650 and 5,675 sessions. This stood out to me, as this click total was pretty average, but sessions were WAY higher than usual. 

I've tried researching the difference between clicks and sessions in specific relation to email, but I can only find info on how this pertains to google ads or just straight up junk articles. 

Can someone enlighten me on this?",analytics,2022-08-30 06:45:22,1
"Depends on the type of role. But generally you need to be able to do

- basic arithmetic. Addition, subtraction, multiplication, division.

- basic stats like mean, median, mode. 

- determine when to measure the volume of something and when to compare the percentage or rate. 

- what is the difference between percentage and rate 

- how to calculate “lift”

In some advanced roles, if you’re doing A/B testing, then you need to understand 

- Normal distribution 
- sample size
- confidence interval
- p-value",65,x0oe2n,"I have been learning Python, on-and-off and I am comfortable with the basics. 

I have a poor background in math. While I am comfortable with arithmetic/pre-algebra, algebra itself has always been an issue. 

I am learning progressively and hoping to make it up to calculus/ statistics. 

But is math required for an entry level position? 

In addition, how is algebra even used in real world analytics?  

Thank you so much for all your answers.",analytics,2022-08-29 06:10:11,32
"I was lucky enough to get another offer. These companies should be your last resort. You have little control over where you will work. Pay is also low. Depending on where you will be working it could be possibly the 2 worst years of your life. Mentally, physically and financially. You are as good as golden afterwards though. If you are below average in terms of skill they can patch you up and ship you off to clients. Again, if you have no options left and I mean nothing you could turn to these consulting companies.",3,x0vnyo,"So I've been looking for work for a couple months now and the only 2 places that have reached out were Revature and FDM Group. Both companies seem designed in almost the exact same way, ""paid"" training for x weeks, then put into a 2 year work contract. 

I've seen such mixed result and information that just curious if anyone has first hand experience here.",analytics,2022-08-29 11:14:03,7
"With the director, ill assume it will be about your Softskills as how you are able to convey data to stakeholders. 

However I might be way off, depending on his background, I recommend you investigate his LinkedIn profile and his experience.",12,x0s6zx,"Hello, so like the title said I had 2 interviews so far with HR and the hiring manager, somehow got to round 3 even though in round 2 I stumbled on a lot of words... lucky I guess. 2nd round was all behavioral questions with the hiring manager and now my upcoming round 3 is with the director. 

I'm not exactly sure what he will ask me but in terms of tools, I'm only required to know Excel & Power BI as of now, and later I will learn SQL on the job. Do you think it will be more behavioral questions or do you think he'll hit me with a case study? What are some good ways to practice case studies, on excel and etc?   


Thanks anything will help",analytics,2022-08-29 08:52:46,15
You can ask the recruiter what to expect and if you should prepare for any technical assessments.,4,x10da9,"I applied for an entry level sales and marketing analyst position at a company and had my first interview last week. They liked me and set me up for another interview this week with the Sr. Manager Financial Planning & Analysis and the Sr. Manager of E-Commerce.

I just would like any advice on what to expect from the interview as well as any recommendations on what to prepare for in this interview. What type of questions do you think will be asked and will they ask me to do anything on the spot?

The job is very excel oriented and they are looking for a candidate who possess strong skills in excel. To be completely honest I am not much of an excel expert, I can do all the basic things with functions and I think I’m better than the average person but I am unsure that I know how to look at a set of data and draw conclusions from it.

I am a recent college graduate with a business finance and management degree, I am also very nervous about going into this second round interview so any kind words and advice would be much appreciated.

Thank you and I hope you have a good day.

&#x200B;

Some description of the job if you have the time to help.

* Manage excel-based bi-monthly budget reconciliation to compare actual spends to budget and keep expenses in line with plan. Includes invoice tracking/processing, internal budget reporting to sales & finance teams, and help with delivery on financial targets.
* Own and reconcile Shopper and E-Commerce couponing efforts from start to finish, from bar code creation to performance data and analysis
* Responsible for the execution of shopper marketing plans for the 3-5 specific managed retailers, including timeline, budget and in-market execution management.
* Deliver post program analyses and future recommendations based on measurable results.",analytics,2022-08-29 14:25:12,6
"""It depends"". Data Integration means different things to different people.

In large orgs with traditional systems it could be Extract Transform & Load (ETL).  In more modern settings you might be talking about application integration, direct database connections or API's.

For a less mature org/company it might mean something as mundane as exporting and importing data using .csv's, excel, powershell, powerbi or some other tools.

Turn the interview around, listen to what they are talking about and *ask them questions*. Demonstrate your ability to ask questions, listen and scope out a problem.  Ask them about data volumes and frequency.  How how and how often. Do you want to automate this? What tools sets do you use now?

Just be honest. Interviewers can smell bullshit a mile away. if you're faking it they will know.

Attitude & aptitude are more important. Show them that you're keen to learn, and excited about the opportunity. If you haven't done something before, that's ok. If relevant, point out that you done something similar and you're confident you can pick up the new tool/skills quickly.

Good luck.",1,x0yqj6,"I have an interview coming up with data integration as a primary requirement. I’ll appreciate it if anyone can point me to where I can get easy to understand info, what to know and how to prepare for this kind of question. 

Background: I use powerbi, excel and sql at my current job",analytics,2022-08-29 13:19:49,1
"For ealry career I’d recommend Prioritize learning opportunities and experience over both (within reason obviously). 

Getting the opportunity to learn a lot of new tools and skills era on the job is really invaluable and sets you up for a lot more career options later
On.",10,x0sj5j,"Title.  


I have some jobs I interview for that put me at $70k.  
I have others that put me at $60k.   


Of course, I'd love more money as a new grad but I want to make sure I can move on to other things in the future. Some mid-paying roles move towards regressions and AI a bit more.  


Some higher-paying roles are focused on Excel for the most part.   


Which route would you have taken if you had the chance to start again?",analytics,2022-08-29 09:06:19,2
"There***

If you don't step up your attention to detail, you won't find much success. People don't care how fancy your model is if they can't trust your reliability",19,x115hr,"I goofed the title. Is their* room in the industry!?

Some background. I majored in Business and Logistics in undergrad, but I am extremely tech savvy. Given a book I can hammer out how to do most anything on PC.

That said, I decided to go into education because I love the downtime. 

Right now, I have a ton of downtime at work because of the nature of my job and I was wanting to supplement my income using that downtime to fund a few small business projects in the next 6 months.

Like a lot of other people my math skills aren't the best. But my software skills are amazing. I could never interpret a p-value  for example, but I have no problem building out tools to do the job for me. 

My question is I suppose is there a niche or utility for just knowing something like SQL? or Power BI? Could I realistically make a few hundred extra a month?

My big thing is that I don't want my gigs to be a massive time commitment. I don't mind making $15-$20 an hour if I can learn to quickly complete a task.

I appreciate any insights offered. I like picking up new skills!",analytics,2022-08-29 14:57:57,15
I found it useful. It is 35 a month and gives a good starting point for total noobs.,22,x0cgz6,"Hello, as the title says. 

I am interested in pursuing the Google analytics after I am done with my current studies. I don’t have any prior data analytics experience and am wondering if this is a good option to build foundational knowledge. 

Thank you in advance!",analytics,2022-08-28 18:57:51,13
You have 4 years of experience and make $70k in San Francisco??? Apply for a new job! You should be able to land something making $100k or more. And then use *that* company’s tuition reimbursement if you think you need a masters degree to achieve your goals.,5,x0t4nc,"I graduated with a BS Statistics in 2018 and started working as a data reporting analyst at an insurance company right pretty soon after. My title and pay (under 70K) have stayed the same since. I have had a few ""growth"" projects on the side, and my success in these projects and regular duties lead me to believe I am underpaid, especially in San Francisco. I want more out of work, in both pay and duties. I'm looking for advice on some options I'm currently juggling.

My company offers to pay tuition for classes, so I am thinking about whether to stay at this job and pursue my masters (analytics, DS, or statistics related). Would have to stay employed through graduation of courses to receive the reimbursement, so IDK if I want to do this. I've also brought up with my manager that I want more, and it's possible that more interesting stuff is coming.

Should I ride it out at my current job and possibly take advantage of the tuition reimbursement, or pursue something else?

&#x200B;

Cheers!",analytics,2022-08-29 09:30:38,3
"Udemy and Coursera are great options. If your university sponsored the google analytics cert it might be worth asking to see if they might help sponsor others. If not, you can get financial aid to get Coursera classes for free",2,x0n9n4,"My university sponsored the Google data analytics cert so I am learning from it now. But I don’t know where should I learn next? Is courses from data camp or data quest good or after the course I should try to do some projects? Also what certicates should I get? 
Thanks everyone",analytics,2022-08-29 05:17:05,3
almost all those models are considered proprietary. About the only guides I remember were through a VISA seminar.,2,x0j1xq,,analytics,2022-08-29 01:02:47,2
I’d take a look at Fivetran or AirByte.,2,x0flpx," Hi folks,

At our startup, we are looking for a no-code ELT tool that is open source. We are in the early stages and the requirements for connectors and storage are not huge.

Please suggest the best tool according to your experience. Thanks!",analytics,2022-08-28 21:34:26,14
"Excel is the most flexible for data input.

You can brute force your way through it and then consume it from a BI tool pretty easily.

If you already have a list of the techniques, then maybe you can build a small interface with Python or use google forms to input data quickly.",7,wzx3ct,Basically the title I want to watch 100 fights and see what techniques consistently land the most and compile a list of all the fights I watched. I was gonna use excel because there's a lot of numbers but not sure if this would be the best program,analytics,2022-08-28 07:40:53,5
"I personally love the R project you have. It’s very well organized and detailed, shows a lot of different skills, and goes through the entire data analysis process. I also like your Python project but reading it at times was a little bit overwhelming. If you could find some space to add between some of the comments and sections it would make it less chaotic and more reader friendly imo. I also think the read.me for your Python project is great.

One other thing I might suggest is getting a sql project on your GitHub. There’s not much on there other than your two big projects, and I personally put all sorts of stuff on mine. Even little things where I’m working through a small problem because companies like seeing your learning process so I would say don’t be afraid to put anything and everything into repositories. I have a dedicated R repository that has super simple stuff all the way up to more intense things. A SQL project could really help round your projects out well because a TON of companies look for SQL experience now.

The last thing I would maybe suggest is getting some tableau experience potentially if you don’t already have it. My portfolio for the job I just got had 2 SQL projects, 1 Python, 1 R, and 3 Tableau dashboards. This not only would display a wider skill set for you but could also open up more job opportunities that you might qualify for. 

Just my 2 cents, what worked for me might not work for everyone but if you want to chat some more you can dm me.",2,wznjvi,"Hello all. I am trying to break into Data Analytics, and I am currently looking for a mentor. Right now, I am looking for someone who will look at my data analysis projects that I posted on Github and give me some constructive criticism. I cannot post my github link here since the post will get auto deleted. However, my github profile is thnaing1

If you can tell me what you like about my projects, what you don't like about them, and how to make them better that will be greatly appreciated. Thank you.

P.S.

I have a B.S. in Geology and have taken statistics, math, and programming courses. Right now, I am enrolled in a Masters program in Data Science. I am currently looking for internships and I am not sure what to do in order to land a data analyst position.",analytics,2022-08-27 22:30:15,5
Guessing 45-75k. I just got 60k base with a 3k bonus in an MCOL area.,10,wzfe3x,"For an entry level data analyst (i.e. with intermediate proficiency in Excel, SQL, Tableau and Power BI), with no prior experience, what is the expected salary range in Toronto, Canada?",analytics,2022-08-27 15:31:05,16
"They might insist on using SQL since you could be required to write stored procedures or views that others end up using. If others are expected to run and use these queries, it might not be practical for them to carry out these transformations that you're mentioning.

With that said, it's not very hard to memorize the main SQL statements you'll most likely be using. I use the mnemonics to help me out:

1. ""Some Friends Will Get Hung Over"":

&#8203;

    SELECT [column]
    FROM [table]
    WHERE [condition for non-aggregate functions]
    GROUP BY [fields with same values when aggregating]
    HAVING [conditions for aggregate functions]
    ORDER BY [fields to sort; DESC for descending order]

2. ""Drink From the Well"":

    DELETE FROM [table]
    WHERE [conditions]

3. ""Ice In the Vodka Water"":

    INSERT INTO [table] (field1, field2, field3, ...)
    VALUES (value1, value2, value3, ...)
    WHERE [conditions]

4. ""Uncle tried Sipping Whiskey"":

    UPDATE [table]
    SET [field1 = value1, field2 = value2, field3 = value3]
    WHERE [conditions]

Additionally, joins and sub queries are also very important. Depending on the circumstance, you can technically implement a join or sub query within all of the four statements listed above.

BTW, I hope the drinking theme doesn't offend you. I just find it funny which helps with the memorization. Feel free to swap words with others if this helps.

Best of luck to you in your interview!",89,wyycay,"I will have a SQL interview soon, I am kinda nervous because sometimes I don't know how to write that query in SQL or I don't know how to transform it so I use other tools such as Tableau and Excel if I know how to transform them in these tools. So I am wondering if I am allowed to use a combination of tools for my SQL interview? Because I am thinking, if I work for a company in BI and Analytics department then I should know how to use a variety of tools for data transformation and analysis , so I am wondering, despite this being a SQL interview, can I ask the interviewer for permission to use other tools to aid me in answer their questions? I am interviewed for a US-based insurance consulting company.",analytics,2022-08-27 02:16:11,28
"As you say that’s what they’re testing though - YOUR ability to come up with your own ideas about how to analyse the data. So you are basically cheating if you take someone else’s ideas.

Let’s face it if you can’t come up with at least some simple outputs from that dataset you prob don’t deserve the job.",12,wyydh2,"Hello guys

I'm not necessarily a data newbie, but I'm still trying to develop my skills.   


After much struggle, I managed to get a second stage interview, and was asked to prepare a task. I was given some mock e-commerce data.   
There wasn't much of a brief, no goals or objectives stipulated. I have a feeling they want to see what I can come up with without any external prompts. 

**This is the data I was given in a table (these are columns and the formats of the data in them):**  
Product SKU	- (numbers and letters)  
Product Category - (text)  
Product List Views - (whole numbers)	  
Product Detail Views - (whole numbers)  
Product Adds To Basket - (whole numbers)  
Product Removes From Basket - (whole numbers)  
Product Checkouts - (whole numbers)  
Unique Purchases - (whole numbers)  
Basket-to-Detail Rate - Ratio percentages  
Buy-to-Detail Rate - Ratio percentages  


I was wondering if anyone had any tips for cool things that can be done with this data?   
That would be extremely helpful. 

  
No need to tell me how to calculate stuff, just ideas! I'll do my own research!  :)   


Thaaaaaaanks",analytics,2022-08-27 02:18:07,14
All same. Different organizations classify them based on what is new flavor of the day.,6,wyl0di,"Hey reddit,

As I recently graduated as a master in business engineering, specialized in business information management, i have managed to get my self a starter job as a business IT consultant for a Tech consultancy firm located in Belgium.

When I had to pick my first client/project, I chose a functional analyst position. I’m starting the first of September and I wondered if anybody here on this subreddit could explain to me what the actual difference is between a functional analyst and a business analyst in the context of IT. If there even is a substantial difference anyways, I would love your knowledge on the matter as google isn’t that clear for me on the subject.

Thanks in advance!✌️😉",analytics,2022-08-26 14:38:22,2
"Congrats on getting a new job!

I think the more appropriate subreddit to ask this question is r/businessanalysis. Good luck!",4,wyl0ur,"Hey reddit,

As I recently graduated as a master in business engineering, specialized in business information management, i have managed to get my self a starter job as a business IT consultant for a Tech consultancy firm located in Belgium.

When I had to pick my first client/project, I chose a functional analyst position. I’m starting the first of September and I wondered if anybody here on this subreddit could explain to me what the actual difference is between a functional analyst and a business analyst in the context of IT. If there even is a substantial difference anyways, I would love your knowledge on the matter as google isn’t that clear for me on the subject.

Thanks in advance!✌️😉",analytics,2022-08-26 14:38:58,2
"Yes, GA4 is an incomplete product. 

It is currently worse but that doesn't mean it will always be.",21,wxmlyk,"I'm addressing the elephant in the room: GA4 is inferior product to UA (in my opinion).

My current hot topic with clients is data freshness.  
Apart from general work with tracking, I also create Data Studio dashboards. Clients are of course used to fairly up to date data including running day.  
Now any data which is presented as ""today's data"" is essentially useless, because during working hours it barely starts to populate any numbers in any metrics. Usually it takes full 2 days until the data is settled, especially conversion data. Which my clients correctly call being unacceptable, since previous version had no such issues.  
So there are cases where clients ask to migrate back to UA properties for time being.

There are plenty of other introduced headaches in my experience, including:  
\- Existence of ""Unassigned"" traffic  
\- No availability to hour and minute dimensions. Probably possible workaround is adding it as custom dimension to events, has anyone implemented it manually?  
\- Methods of conversion tracking in GTM, which worked flawlessly with UA, now for some unexplained reasons might not work. Probably unpolished consequences of events being batched?  
\- Probably a temporary bug, but for 3 days already conversions are not accumulating and there are other people who report that.

Has Google has addressed the ""refresh rate"" of data, rendering ""Today"" and to some degree ""Yesterday"" time frames useless and promised improvements, or is it what we all have to be accustomed from now on?

I just want to know what the general sentiment of people working with GA4 is and if I'm the only one who think the quality of work feels downgraded when migrating form UA to GA4.",analytics,2022-08-25 12:06:42,16
Is it possible to create a calculated field using the goal on GA? Can you do Total visitors - visitors who spent more than 1 minute?,1,wxspbx,"If memory serves me, you used to be able to set up a goal showing how many people spent b/w XX minutes or spent less than XX minutes on a page. 

But now GA is only allowing us to setup goals where they spend ""greater than"" xx minutes. Is there some other way around this?",analytics,2022-08-25 16:16:57,3
"My 2011 Macbook Air (with an upgraded i7 chip) crushed most modern laptops I tested until it died a couple of years ago. I ran R, MATLAB, Tableau etc on it and it was fine for anything a BA programme is likely to throw at you. More reliable as well; running big models on Windows often leads to programmes crashing; much rarer on MacOS.

A 2020 Macbook Air should do very nicely. Most of my students have Macbook Pros and honestly it’s probably overkill. Some other students have very old machines, and it’s only the really old laptops that weren’t that good in the first place that really cause issues.

Anyway, having a clunky laptop is a good thing if it forces you to write cleaner, more efficient code. More power is good, but more intelligent code/maths is better.",1,wxp9at,"Hi everyone,

I am about to start studying business analytics and the programs we’ll be using are Tableau and SAS Viya.

I’m thinking about getting either a new laptop or a new computer but I don’t know what to get. Currently I have the MacBook Air 2020, purchased in May 2020 so it doesn’t have the M1 chip. 

Is this ok? If not what should I get?
Are curved screens recommended?",analytics,2022-08-25 13:55:54,3
Bayes rules if you want to learn Bayesian stats. It's available online for free on the website of the same name. It uses R to teach. There are a lot of chapters and the material is pretty good and digestible for beginners.,7,wx87qj,"I am currently in my first year of undergrad and currently taking linear algebra but I got plenty of time after class so I started learning statistics. 

My goal is to learn machine learning once I’m done taking all of my calculus courses (which would be in a year). What books or video lecture would  you recommend to advance my knowledge in statistics to bring it to higher level? I am also planning to find an internship as early as next summer. Would love to read your recommendations, thanks!",analytics,2022-08-25 01:04:52,7
"Yes Filters in GA 4 are only available on an IP adress level, as it is recommended to controll the tag rather then filtering out data.

It is recommended to set such filters in the reporting itself or set the Tag trigger appropriately.",1,wxhf0x,"I'm in the process of moving a bunch of web properties from GA3 to GA4 and it's throwing up some difficulties that I am struggling to find answers to online.

1) Is there no way to use filters at a View level any more? One of our websites runs on 5 different languages and each of these languages had a separate view last time with a filter that basically just said (include only /de-de/) etc.

It now looks like the only filters are for filtering out internal traffic via IP.

Another one of the issues with filters is that one of the websites allows pages to be served using /en-gb/ in the URL as well as without. I am led to believe that this was necessary for email template purposes, I don't know the details of it completely but it wasn't a problem in Universal Analytics because I just had a filter that stripped out /en-gb/

2) I've setup a data studio dashboard to give a GA4 vs GA3 comparison and consistently across the board, pageviews are slightly more than doubled on every single page on GA4. It looks like they are being counted twice somehow but I don't see how that would be possible. Both of them are implemented using the same Analytics Tag.

Any help would be much appreciated, I'm pulling my hair out trying to recreate analytics setups that go back several years and precede my time here.",analytics,2022-08-25 08:36:50,1
"I'm in a similar situation if you'd like to connect! First job as a data analyst and I'm finishing a master's degree on in the meantime.
Look at Women in Analytics, Girls in tech, or Girls Who Code - all of them have mentorship programs (I did the one with GWC and it was cool). I'm sure there are more structured programs that match mentees with experienced professionals either for free or for a small membership fee",4,wx7ous,"I'm 24f looking for a mentor in data analysis.
I've been working for a year now as a data analyst (my first job also). I'm basically a first generation data analyst at an ecommerce company and have no seniors to learn from and ask for guidance.
I mostly use SQL, power BI and excel on daily basis but want to advance and get more into stats and Python. I also wish to learn more about the business side to apply it in my analysis.

I'm basically a very curious person and have no problem getting my hands dirty in whatever i can :)

Idk if this is the correct place to ask for mentorship but would appreciate any help.",analytics,2022-08-25 00:31:17,9
"Honestly, I don't think many firms care about online courses. Better start building a portfolio.

And practice another useful skill for analysts: search the group or the internet before asking a question 😉 there are so many people that already asked about that",10,wx95ff,Any recommendations or discords to join?,analytics,2022-08-25 02:04:23,13
"I had the same issue but it corrected itself after a few days.

Google Analytics 4 is still beta. Lots more headaches will happen over it.",2,wxfl4u,,analytics,2022-08-25 07:22:57,1
"SaaS data tools are greatly decreasing the barrier to entry to things like ML and AI. I think over time a lot of data science and data engineering work can/will be decentralized so that departments in big companies each have their own data teams with a stack of affordable SaaS products that effectively make data procurement and transformation easy. Sales teams, marketing teams, product teams, etc. will be less reliant on IT and the need for advanced programming skills (in analytics) will decline, replaced by data savviness and the ability to apply it to business problems that create better experiences for customers on digital properties and maximize dollars and time",4,wxf057,"Out of curiosity, what do you all see changing in the future of data analytics, BI, data science, etc? What tools will become legacy, which are up and coming, will expectations for the role and knowledge change? Which new roles will be created and which will cease to exist?",analytics,2022-08-25 06:59:23,6
"Hi, I can help as I have experienced in data driven marketing campaigns and marketing . DM me and we can discuss if is ok ? Cheers",1,wxdmar,"Hi everyone. I am doing test from one company i dont know how to start, what is important and what not etc.

I have: 

* dataset with 3 campaigns ( variants, datetime, customer ID)
* dataset with purchases (ID of campaign, customer ID, datetime)
* dataset with customers (customer ID and 10 other attributes for every customer)

I should make a campaign evaluation, but some customers has purchase with ID of campaign before campaign was sent (datetime of purchases is earlier than datetime of campaign) and i dont know how to make an analysis from it and what are relevant metrics

I plan to do A/B/C evaluation, control group vs targets, revenue per campaign/variant. Simple customer segmentation. 

Am i missing something? can you show me the right direction how to do it?",analytics,2022-08-25 05:59:28,2
levels.fyi,1,wx78d2,"For more context, I live in a VHCOL city in the west coast and just got offered a Senior Analyst role with a base of $100k (TC $116k if I reach 1 year mark) and I'm wondering if this employer is possibly low-balling the compensation package in your opinion (aka if I should wait it out and keep on applying till I get an even better offer as I'm not really in a rush to find a new job and have been getting a surprisingly amount of interviews).",analytics,2022-08-25 00:02:26,9
"I'd be concerned the 1% is too small or unrepresentative for some reason. Good to check observable differences both at a point in time and differentially over time (eg are the holdovers more or less likely to keep using the app, do they still represent 1%, do new users ever join this group, etc). If you know the timing of new features you can implement a stepped wedge design which ought to give your control group more juice at various times. 

Identifying the impact of individual components will be challenging if multiple changes are happening at once. Hopefully you can segment users into distinct cohorts.",1,wx4ynu,"Hey all, the company I'm working at has a mobile app and they have a 1% holdout control group (99% users are in treatment, 1% are in control) that hasn't received any new features for about 4 months. They're now wanting me to analyze the holdout to estimate the impact of all the features that the treatment group has access to but that the 1% holdout group does not have access to. As far as I know, the groups were picked randomly at the user level.

I am not sure if there is anything in particular I need to do to account for such an imbalance in the size of the treatment and control groups. I plan to check for pre-treatment bias since, even though the groups were picked randomly, the treatment group is so much larger that there's a chance a disproportionate amount of power users will be in the treatment group. I am also concerned about network effects because let's say that all the features in the treatment group are truly causing the treated users to spend more time/money/activity on the app. This would also likely cause the control users to become more active as well, but I'm not sure how to account for this. Would appreciate any advice!",analytics,2022-08-24 21:51:26,1
Do you have any appetite for coding? You could use Python and the pandas package for something like this. Just depends on what your end goal is I guess. Is there a specific reason you need it in a database or do you just want some stats and a few graphs?,2,wwulof,"I’m trying to start a project with IMDB datasets.  I have downloaded the data from IMDB directly and unzipped the files into .tsv files.
I want to import the data into Access (or do you suggest another database). 

Access doesn’t allow the import of .tsv files. The files are too big to convert them within Excel. 

Are there any free/easy ways to convert these files for Access or to import them into Access as is? 

Is there a better database I should use for this?

I am a beginner and want to be able to pair up actors with movies based on keys.",analytics,2022-08-24 13:59:03,8
"For the coding help I would try datacamp to start! 

For the rest of it I would suggest redoing your dissertation research in different languages. You should definitely learn SQL because at the bare minimum you are expected to understand how to pull your own data. Also R is fun and easy to convince a manager to let you use as it is open source,etc. 

I’m an accountant so my mind works a little differently but one of our analysts gave me this advice so I thought I would pass it on!",3,wwhuek,"hi! sorry if the title doesn't make sense. basically, i'm a business psych major with 0 experience wanting to potentially go into marketing or HR. i've been scouring the JDs out there and people seem to love people that have knowledge of analytics whether that's excel, SQL, tableau or like stuff like SEO and SEM. i'm definitely willing to learn these things but idk where to start with.

to be clear, i have practically zero experience with any of these tools and the most i know is SPSS which i've learnt for my dissertation. i have no knowledge of data besides this.

where do i begin? should i take the big picture approach and take business analytics courses or should i go more esoteric and do marketing analytics courses? what's the best place to start according to you?

bonus question: please point me towards some GOOD online courses on coursera (or wherever else) i could take for either. thanks a ton!",analytics,2022-08-24 05:07:33,7
"No, not if you don’t understand it.

I teach on an Analytics masters. Most academics use ML in their research, most students want to learn ML and AI when coming onto the programme. So we teach them ML and the coding and stats that is the foundation of it. And then they realise it’s very difficult, and that what many actually wanted to do was something like Tableau. 

Luckily on our programme we’ve been steering more towards Tableau-y type stuff, which is also useful as companies realise ML is not particularly that usefulfor most business decision-making and want analysts, not data scientists.

Before academia I was in industry and sat on many interview panels. A good quantitative degree where you learnt ML should get you some interviews. But once you’re in the interview the technical stuff is arguably less important (save making sure the person does actually know what they put on their CV), compared to your understanding of how to structure a problem, and how to translate any solution into useful, practical advice. Knowing why you need a ML model (or a Tableau dashboard) is just as important as knowing how to build one. This is the key difference between successful and unsuccessful interviewees. The good news is that you don’t need a degree to work that stuff out.",21,wwbdqo,"I’m aiming for a role where I get to use tableau, SQL, and R. Maybe Excel, too. 

My university taught us random forest, some naive bayes, and other things that I don’t know how to do. 

I don’t have tableau experience. Would talking to an employer about my random forest model make me look good in their eyes?",analytics,2022-08-23 22:53:43,9
"You could use `DuckDB` in combination with `dbplyr` to have the best of both worlds: keeping your existing Tidyverse syntax (no change needed) with the speed of an analytics-oriented database that doesn't require any setup.

```
library(duckdb)
library(DBI)
library(dplyr)
library(dbplyr)

con <- dbConnect(duckdb(), dbdir = "":memory:"")

duckdb_read_csv(
  con, 
  ""table_name"", 
  path_or_url_to_csv
)

tbl(con, ""table_name"") |> 
  {insert your tidyverse code here} |>
  collect()
```

It shouldn't take more than a few seconds to run basic queries on 50M rows that way.

You could also save the database locally instead of having it in-memory by editing the `dbdir` argument of `dbConnect`. Depends if this is a one-off operation on a CSV that changes frequently, or if it is content you will need to access again in the future.",10,wwb8xb,"Hello everyone!

I'm currently using R to clean my entire dataset.  When I had first created the script, I was only applying the transformations to 1-2 million rows and R could complete the transformations in under a minute.

However, the dataset that I need to process now has grown to over 50 million rows and it takes quite a while for R churn through everything.  My current workflow involves downloading large csv datasets which go through the R script to get cleaned.  

Everytime I looked into SQL to replace the script, I ran into the issue where I needed to replace the entire table in SQL with the updated data.  Is there something that I'm missing?

Due to the nature of the source of the data, I cannot query the main database.  So, is there a way for me to simply download the dataset to a singular location and have SQL updated itself everytime the script needs to run?",analytics,2022-08-23 22:45:54,7
"Yeah I've seen demos for Domo and it looked like ass.

Sure you might be paying more, but you get a robust community, support for their products, a growing product ecosystem (flagship product, prep, server, etc.), and potentially access to a wider talent pool by listing Tableau on job requirements.",2,wwpcqr,"Started at a new company a few months ago and they said they use ‘Domo’. I’d never heard of it and figured it’d be similar to the tools I’d used when I was in consulting. Used a lot of different ones as it was whatever the client used (tableau, power bi, looker, SSRS, qlik sense, micro strategy). Didn’t think much about this new tool, thought it’d be no big deal.

Wow was I wrong, this tool is hot garbage and there’s not much of an active online community either for troubleshooting issues. We pay for gold customer service but it takes on average months for them to get back to us on day 1 bugs.

The decision was made by the CIO before the analytics team was started (he’s friends with Domo’s CDO).

They chose Domo to get away from Tableau for cost. However we’re starting to hit the limits on the Domo cost allowance since some of the cost is based on data that is uploaded to their server. So we have to get permisssion to upload more data (aka do our job). We have a RDBMS (ms sql server) already so I really didn’t understand this decision.

We’ve finally been allowed to make a couple reports in power bi as even Domo themselves admitted they aren’t able to handle the capabilities the reports required.

Hoping for advice on building a  case to get off of Domo and maybe to power bi as cost is the biggest reason (supposedly) that they left tableau.

Thank you all in advance",analytics,2022-08-24 10:23:46,6
"£90k base, 30-50% annual bonus. 7 years experience.",2,wwl44q,"Hi,
I’m investing in a course to move into this field and was wondering what your average salary is with how many experience and which field.

Thank you!",analytics,2022-08-24 07:32:52,8
Root Cause Analysis? The problem is already defined. Collect data to confirm it. Identify causal factors. Determine cause and recommend/implement solutions.,7,wwgzf2,"(Logistics industry) (was asked during interview)

What would you do if someone tells you that there is spike in  numbers of parcel lost being lost ? 


What's your thought process ?",analytics,2022-08-24 04:25:38,9
"Depends on who's consuming the data, is it being consumed internally or is it being provided to a client. 

So, either really, depends on who you're working for. 

Most *are* client facing in my experience though, simply because so many businesses outsource this type of work to consultants such as myself.",16,wvrmgb,"I'm curious as to who the main stakeholders are for Data Analyst roles. I'm currently working in a role where my main reports are senior management, so I'm wondering what being on the client side is like.",analytics,2022-08-23 08:23:00,8
I love datacamp.  I recommend it to all of my entry level hires. Quite a bit in their skill tracks that you can delve into (powerbi / sql / python / tableau),9,wvdc03,"I don’t have a very technical background, but I do have a role in the industry and have some experience with SQL. Would appreciate any recommendations to any courses or certifications that don’t require a highly technical background.",analytics,2022-08-22 19:58:41,16
"It sounds like you’re in a good starting role. You could stay in that style role or pivot to a BI dev or Data engineer. SQL, Python, and excel are kind universal, so people bounce between fields periodically. As long as you’re learning, keep working there until a better role comes your way. In terms of compensation, you’ll probably find DE>BIE>DA.",7,wvmywq,"Currently I work as a data analyst but I do barely any analysis. I mostly use SQL and powerbi with a bit of python, r and power apps on the side. So I mainly focus on developing dashboards and ingesting data. Looking through job listings I mainly see BI Developers that use similar tools although they all require extensive experience. What would be a better role for my skillset as a junior? Would an ETL/SQL developer be a better role?",analytics,2022-08-23 05:01:33,5
"It depends on what side of the business you are. I’m in the startup world, in sales and marketing analytics, I tend to find ways to make more money, or spend less money for the same effect, through analytics. 

I’ve been through a couple of downturns and I always end up on top, albeit with more work on my plate than I can do.",16,wvcsgh,Thoughts on how cyclical the analytics industry is? Will it be greatly impacted by a recession (or economic boom)?,analytics,2022-08-22 19:34:04,8
No.  Taking notes in an interview is fine.,61,wuvi5r,Title.,analytics,2022-08-22 07:43:13,12
"Sounds cliche, but critical thinking & problem solving are your biggest ones.

Edit:  stats is important, but not super important for most analyst positions. 

I do absolutely no statistical analysis in my work. But having a basic understanding is important.  Just have at least the basics down.",3,wvglbm,"Eg - I was never that much into statistics
One thing I like is how data can bring such wonderful insights and trying to understand it. 

The reason I am asking is, I am completely new to analytics and don’t know how my interests and skillsets would align",analytics,2022-08-22 22:48:31,4
I dare you to apply for a job and tell them you have the tik tok certificate and it only took two months.,6,wv5mfp,"Hi, I learned about the existence of data analytics strangely through this data analyst on tik tok who makes videos about breaking into the field. She has turned it into a job of sorts, and has free and paid classes where she helps people get jobs in the field. It's all over her linked in page and she often boasts how she's gotten her students new jobs in data analytics in 2-3 months.

Is this legit or a scam. whats the market really like even if you have a portfollio",analytics,2022-08-22 14:23:10,6
"Hey so the projects you posted got removed so I can’t speak to that but don’t give up. I applied to anywhere from 50-100 jobs over the span of 6 months and only got two interviews. I ended up getting the second job that asked for 3+ years of experience when I only had ~1 year at the time. So, continue applying to those entry level/jr/associate positions but don’t be afraid to apply to some that ask for 2-3 years of experience. The VP of my department told me she hired me because I had a solid baseline of skills, but mainly because my soft skills and personality would match really well with her team. Don’t neglect the ability to interview well, and don’t give up. It’s a grind sometimes but I promise you it’s worth it.",32,wup35y," 

For  a few months now I have been on the journey watching Youtube videos and  course videos about SQL queries, excel and how to basically become a  data analyst. But lately I feel a little demotivated like most of you  might be in this sub reddit. I have completed multiple projects and  applied to multiple entry level data analysis jobs but I have not been  able to land a single interview so I thought to myself if I haven't been  able to get any interviews then it could be my projects. So I have  decided to paste two of them here: (In the comments)

What  do you guys think about these projects. Are they good enough? I have  other projects as well but I want to show them to people who can  actually critique them and not just anyone. I am also learning about  cloud data and how to implement data warehouses and databases on AWS (I  also have an unfinished project there) alongside the basic concept of  data analysis. It feels exhausting putting in 5hrs average a day to  learn and I am not even getting an interview back. I even got my resume  rewritten by a professional. It'd be really great if I could get any  feedback. I feel like I might be missing something. Also I live in the  USA if that helps.",analytics,2022-08-22 02:39:09,29
"I used to work in healthcare valuation and phys comp would come up frequently.

Lots of regulation surrounding it to make sure there are no violations to kickback / stark laws and HIPPA.

Phys comp is typically related to wRVUs and there are definitely reports and databases that show distribution of comp by type of physician. Having a general understand of wRVUs might be a good idea.

I’m not sure the particulars of the job, but the details matter a bunch in this arena (contracts, lawyers, etc.). Being able to confidently say what is reasonable and what is not (determine outliers, when things don’t tie out, what seems like bullshit - someone using average instead of median with no justification).",3,wuykfw,"Hello everyone. I have an interview at the hospital I work for as an internal physician compensation analyst. I've been working in insurance for 3 years at this hospital. I explained my daily tasks to the hiring manager, and he seemed to like me a lot,  and told me to apply for the position. My question is. I have an interview with the hiring manager I met, and his boss on wedensday. How do I prepare? What are some good resources I can look up while I wait? Are there spreadsheets I should practice one, and a data base I should explore? I want to ace this interview, as it would be a nice pay raise that I really need. I currently don't have any dregeee in engineering or analyticts, so Any help would be great!  Thanks!",analytics,2022-08-22 09:43:35,1
"If you’re having trouble finding stuff on Kaggle, Alex the Analyst has some solid projects on his YouTube page. Even if you didn’t want to do those projects exactly they’re some great ideas to work off of for your own projects should you choose to go a different direction. Those portfolio projects worked wonders in helping me land my first job.",27,wu8s0c,"I know everything I need to know to do a project in data analytics in terms of programming and math. However, I am not sure what a good project looks like. I am about to apply for an internship, so I would like to have two or three projects to show the recruiters. Can you send me links to some Kaggle pages that have high-quality content? I tried to find them myself, but I felt a bit overwhelmed. Thank you!",analytics,2022-08-21 13:02:25,18
"You don’t need ML/Advanced Stats, but it would help to understand how it works if you wish to pursue management level roles and not compete with those who have those skills. A good manager would at least take the effort to understand and recognize the capabilities of these tools, and help their employees to develop these skills further.

But if you wish to do IC a lot, then you’ll be replaceable by someone with similar domain expertise but better skills.",5,wuepxs,"Hi All,

I'm an accountant who is currently exploring a move to the analytics side and trying to get a better sense for how far I can go, if I change to analytics, without being a technical person. Frankly, the deepest I want to go into analytics is to use:

1. Alteryx (I'm in consulting so my firm pays for it) and/or Pandas/Numpy in Python to automate the ETL of large datasets quickly and in an auditable fashion. Everyone in accounting uses Excel and it drives me bananas when I know there's better tools out there.
2. Use PowerBI to visualize data and deep dive into some drivers to tell a story about what's happening in a business. Again, accounting and finance only rely on Excel and I hate it given even PowerBI is so much more powerful than Excel. Tableau/PowerBI are also obviously excellent for visualizations and story-telling with data, which is my main jam.
3. I'm good at math but I'm not a rock star. I aced stats/calculus (not the business versions either FWIW) in college, but I'm horrible at theoretical math that isn't applicable to business problems. I want to do more quantitative work than we do in accounting (i.e. all I'm doing is basic 4 functions in Excel - maybe some algebra on a good day) and use stats/probability to solve problems, but I don't want to do a PhD in Physics/Math or anything like that.

I'm wondering about the following:

1. Do companies still find it valuable to have folks that can really dive into historical and current data to generate insights, or are they still overindexing on ""the future"" and data science/machine learning to make predictions? I frankly think you can do a lot with historical data, if you're creative with your analysis, and make some useful, actionable insights just by understanding the current situation very well. That said, the money and ""sexy"" work seems to be in predicting the future and I'm wondering how far I can reasonably go just by being a ""historicals and present suggest that"" guy vs a ""we forecast in the future"" guy
2. In a similar vein, what job titles can I move up to or reasonably attain within analytics if I don't have a masters/PhD in a quantitative field? It seems like the big money (I consider big money upwards of $200K TC) is in ML/data science, not analytics and folks in analytics just top out at around $175Kish from what I've read online. Is that accurate or outdated?

I don't want to make it seem, btw, that I'm basing a move solely on money, but I'll be honest that it's an important consideration at this point. The reason being my total comp is already above $150K in accounting, and accounting has a clear path to controller, where I should comfortably make >$200K in comp within 7-8 years max. If I make CFO, then it's even higher.

The only problem is accounting is boring me at this point and I know even on my own job that I'm both talented with data and that I enjoy the data manipulation and solving quantitative problems piece of my job I sometimes get to do a lot. If I could that that full-time, it'd be great, but I only get to do that sometimes in accounting. A lot of the job is just compliance and bullshit documentation, which I hate. Money isn't everything, but I have to consider it in my decision and am wondering where I'd likely level off in terms of title and comp in analytics as part of my decision in making the switch.

Would appreciate any advice you all have greatly. Thanks in advance!",analytics,2022-08-21 17:19:17,8
"Ha. Sounds like it's a company who expects an analyst to be a magician. I also don't know if I'd stay with a company that explicitly says I need to ""prove my worth."" Like yeah, that's implied for any job. Saying out loud serves what purpose? To scare you? Lol.

Anyways. Start with the basics. What type of analytics are you doing? Really simple descriptive metrics in some charts through Excel would be where I begin. I'd also start to outline what you can't do/what's not possible because of the lack of infrastructure.",34,wu7g2w,"Hi!  


I have recently started working at a global company as an analyst, they have big expectations and want early deliverables/victories to prove my worth.. Not sure how to begin.

My main role/objective is to deliver insights/invent metrics, process improvements, and all the other data-related tasks. 

There was no onboarding provided to begin with, I spent my first month and a half completing required training material (company culture/values etc) and then scheduled 1:1 with all the relevant people to try to understand the product as well as the stakeholders.  

  
I realized the structure is a hot mess, with a very diverse portfolio, and an unnecessarily complex business model. My manager finally scheduled a 1:1 with me to check in on how am I doing? Gave a long speech about how important my role is, and I need to prove myself by the end of the quarter. 

There is so much to do, I am not sure where to begin with and I don't really have anyone to ask. I tried asking my manager, but he makes very vague comments and complains about the lack of data infrastructure we have..

&#x200B;

 I have a year and a half of experience from a small e-com company with a very structured role in the analytics team working as a reporting analyst with a team of analysts, mainly reporting ongoing performance and underlying reasons behind, never worked in such a vague role before, I have confidence in my capability but being told to deliver soon makes me feel extremely stressed. I feel like I might be out of my depth. Any advice on how can I make quick advancements and achieve small victories? Any input is appreciated!  


Thanks!",analytics,2022-08-21 12:06:10,13
"I would never trust that start-up. Unless they offer me partnership or stock then i will consider it. If I have spent years studying then ultimately it's for this. Time to cash in at the University job,it's logical. If you are talented and passionate about the startup job then keep money aside and start your own after 5 yrs.

(I'm a BA at known company in my city. Have a good service business plan which i plan to start parallel with this job and later drop it if i see I'm able to scale it)",6,wughcq,"
Hi all,
Good morning. I have recently started (1 month as of 18th of August) as a contract job for a university who is implementing Workday. I am the reporting guy due to 5 years of PhD experience in data generation, analytics, and visualisation (Python and all...) The pay is extremely good and having never used Workday at all, I have managed to learn a lot within a month just by googling and reading. I can create reports and basic calculated fields (a relatively difficult task), however, I miss coding and having complete control over the data.

Last week I cleared a technical round for a data analytics position in a startup. It focuses on tourism and creating predictive analytics tools to boost the same with clients around the world ( as per the interviewer). The company has less than 10 people and I have been invited for the next round of interview. 
My questions are, 
1. Should I trust a startup this small ?
2. Workday reporting means a career shift to a relatively closed source tool reporting and can people have a good career in the same?

Sorry for the length
TL;dr 
Working as a Workday Reporter or data analytics in a startup",analytics,2022-08-21 18:41:35,6
"A decent degree in a quantitative subject from a good uni - I will almost certainly interview depending on how many applicants.

Someone with a non-quant degree and only their side projects - not necessarily a dealbreaker but they had better be amazing, and very quick for me to identify whether they are good or not. I’m certainly not trawling through your github!

I’m always willing to consider someone, but generally a degree where someone has messed around with data (sciences, engineering, maths) means they have been taught the basics. It’s an easy (though not necessarily foolproof) way to identfy people who might be good at interview. When you have so many other things to do, it’s a necessary shortcut.",11,wtzs51,"Hi there,
   I wanted to ask the hiring managers and recruiters out there about data analyst positions... Do you care more about the project portfolio of a candidate rather than their educational background? Let's say I have a bunch of projects in Excel, SQL, Tableau, and Python... Would that be more important to you than seeing just a bachelors degree on a resume? Or do you care whether the candidate has a bachelor's degree?",analytics,2022-08-21 06:35:39,3
"Data scientists at faang are glorified data analyst though, and there are certainly mit folks there",7,wtwtaz,Does this degree makes one overqualified for data analyst? Are there many people with good degrees in this field? Or mb they choose some other branches of data jobs... Are this people paid significantly higher than guys without any STEM degree?,analytics,2022-08-21 03:56:59,9
Learn and do the analytics role then become a PM if it’s suits you.  Too many PMs who have never done analytics in their day to day but trying to manage projects just makes analysts miserable.,57,wswuoh,"I'm looking at project coordinator and similar positions since I feel like I really need to find a job soon. 

My bachelor's is in statistics and analytics and I have plenty of projects, including an analysis competition in which my team came second out of around 10 other groups.  


In the case that I get a project-management type role first, my plan was to stay for a year or so, get those soft skills/project mgt skills down, and then apply for analytics jobs. Ultimately, leaving the impression of a well-rounded person on my resume and have an easier time finding an analytics job.

If I get an analytics role from the get-go, I plan to get as good as possible so I can move into the industry I want after, again, a year and a half or so.  


Thoughts?",analytics,2022-08-19 20:17:17,17
"If it makes you feel better, you’re probably cutting them a discount charging 3x your current hourly. Outside consultants are crazy expensive.",14,wsyapr,"
My old company reached out to me and is considering hiring me to work on some projects. They are very small, and strained of resources. I’m on good terms with a lot of upper management (good friends with the one senior dev) and they’ve told me that they want me to complete some analytics projects that I was working on before I left.

Im young in my career (1.5 YOE in analytics), but played a pivotal role for them as Tableau/BI Developer and the only Data Analyst before leaving for a new Data Analyst role with higher salary.

I’m actually excited about this, as it would bring in more money and strengthen my skillset, but I have no idea what to charge. I see people saying to charge 3x your current hourly rate but as someone with only 1.5 YOE and them being a company already strained on resources, I feel like they would laugh in my face if I told them $100+ an hour. What’s a reasonable amount to charge for someone with my experience? How much leverage do I have?",analytics,2022-08-19 21:35:26,21
"Yes. You can write a script in Python that refreshes the excel file. After you write the code, you can schedule the .py file to run on your task scheduler (if you have a windows computer) at the desired interval.",9,wsx3qo,"Looking to automate some reports in Tableau, but I need the data to auto-refresh in excel without me having to go in and refresh data.

Is that possible? Maybe something in Tableau, Python maybe? 

Thank you!",analytics,2022-08-19 20:30:56,13
Sounds like you need to create another table with budget amounts and categories in it and have that in relation to your original dataset according to category.,2,wsx64l,"Basically, we have this huge dataset with individual line items for every transaction which includes prj info, budget, fiscal year etc. First, we need a formula to get the cumulative spent by budget by year. Second we need a formula that will subtract the actuals from given total budget amount again by budget and fiscal. Thing is the dataset doesn’t have budget amounts. It has what was allocated and what was spent where we need to show when the “bucket” will run out of money.",analytics,2022-08-19 20:34:33,2
"In my experience the best thing you can do as an analyst basically anytime, anywhere, is keep asking questions.

If you feel like you are able to help them on a technical level, and feel like you're able to learn quickly, but don't have enough info to go on, there's only one course of action really.  If you think a question may be dumb but want to ask anyway, ask it for sure. 

It's cold comfort but if their owner and CFO are both personally meeting with an entry level candidate and are not sufficiently impressed by good questions that demonstrate an underlying competence and voracious urge to learn as much about the data as possible, then it's a bad lead anyway",3,wskzpx,"The situation is this. I'm very new to the field and one of the first opportunities I have is to do some volunteer work with a local sports team to build experience. This was arranged for me through the organization that founded a bootcamp I recently completed (more later). It's a relatively new program; we were the first class, and there is a lot of support from the state to ensure we will be successful. So I would think I have some reason to suspect that the owner of this sports team understands that my experience level is low and entry level at best. 

The meeting is scheduled for next week and I was told that the owner is now also bringing his CFO along. I expect to be in the room alone with them. It's got me pretty nervous as I'm really not sure what benefits I can provide to their organization and I might be in over my head. I'm hoping they don't have outsized expectations.

So my request to this reddit is to help think up some questions and ideas that I can take into this meeting to help draw out the information I will need. Any input will be appreciated. I feel relatively confident that if I can get a good understanding of what they want me to achieve and perhaps even how...that I can then go out and learn what I may need to get started and provide some value. 

My background:
I recently completed a newly minted 24 week bootcamp on Data Analytics that conferred a certification from a state university. We covered a fair amount of techniques at what I would describe as relatively surface level. The course ended with a capstone group project that essentially boiled down to scraping data we could find, using SQL to join them with a provided public database into a larger relational database to meet our needs, some data cleaning (primarily exclusion in this case), and then the application of basic modeling techniques using Orange (a software that runs on Python) to draw out insights and make predictions or recommendations in a visual presentation. I can comment on the specifics we covered if that's helpful or answer any questions about the depth of my retained knowledge. I also have a B.A. in Marketing that was completed some years ago but never really employed professionally. I've been working a blue collar trade most of my adult life. I consider myself a relatively astute person and capable of learning pretty quickly. 

I really hope I can get some help here. I will also be turning to the organization that orchestrated the bootcamp as well as my instructor to see what I can do to prepare.

Thanks.",analytics,2022-08-19 11:19:32,7
"That's why masters degrees exsist so that you can also have a strong background.
But to make your life easier while in uni ,I suggest that you learn a lot on your own on YouTube or Coursera or udemy.
Basic coding skills  SQL and advanced Excel will come very handy. If you have. Some more time definitely pick up python/R.",21,ws4gyj,"I have no analytics background, a business (MBA) and commerce background and I am planning for a 1 year analytics Master’s. Would that one year be sufficient? Any advice? Cuz I know people come from strong backgrounds",analytics,2022-08-18 21:26:19,27
"It’s ok. This is a safe space. Anyone who has used pbi has come to this conclusion. Normally they come from excel, where you can be sloppy and get it to work through brute force. 

Pbi has a large learning curve. You now have to learn m for your etl and Dax for the modeling but nobody explained to you that you also need to know kimball data structures in order to get it to work. 

When you use pbi the first time, your model was soft as a wad of cookie dough. After a few weeks, it will be carved out of wood. 

Once you get it, you will get it and will tout its superiority and understand its shortcomings. 

What you are currently experiencing is but a small fraction of the frustration, the denial and anger stage, and I will welcome you with open arms when you finally reach acceptance. 

r/powerbi",86,wrnqrp,"I need to rant and start by saying I've used Tableau since 2011 and recently switched to Power BI.

I honestly hate everything about it. From DAX to Workspaces and Apps to having multiple accounts and different emails to use different workspaces. Transform data is It's just all clunky trash.  

Want to use a text box? Sorry, you have to type a paragraph in transform data and drag in the field. A text box isn't an ""Approved Visualization"".

I'm sure a majority of my hate is because I'm trying to force it to be Tableau mixed with user error and my company's environment. But damn, I do not like it. I do like the data modeling user presentation better in Power BI, so there's that...

Anyone else from Tableau move to Power BI and have similar frustrations or know of any resources that transfer Tableau experience and relate it to better understanding Power BI? 

endrant",analytics,2022-08-18 09:19:56,62
"I have 3 kids, 4, 2.5 and 1 so I know how it is to prioritize kids over work. 
Tableau and SQL opens up a lot of jobs honestly. 

Without knowing more I’d say your limitation is in looking at one field (higher education). I always think of a job as having two components (skill wise), there is the analytics portion and then the industry/specialty portion. I can take my analytics skill and learn another industry, it’s just learning what matters to them using the same analytics skills. That said if you are dead set on education look for education tech companies (education software etc) they will probably be more flexible with WFH and at least some of the stuff may seem familiar.

Edit: if you do venture out into a new industry study as much as you can and be honest in interviews. ‘I read a lot about this industry and I think it’s interesting because of xyz, I really want to take my analytics skills and apply them to a new industry’ etc",6,ws2uu0,"I am trying to come up with a plan B, but I'm not really sure what I can do. To put things in context, I've been in Institutional Research and Assessment Analyst roles since 2013 across 3 different institutions, and a Manager of IR/Assessment (Project Based) for the last 5 years at my current institution. In that time I've moved across three different departments with each reorganization and now work in the Office of Analytics. I am fortunate that my current responsibilities lean more towards a storyteller/analytics lead (usually described as a Director in higher education) but I am not being compensated for taking on these new challenges or given a new role, and it doesn't look like that opportunity is going to present itself at least for a year or two given budget constraints. Pair that with the challenge that I have to work remote so that I can be home to pick up my kid off the bus from his new school at 2pm everyday and there are no vacancies on the vendor-based after school program at his school since May. 

I know that my resume and experience speaks for itself if I could just find an organization that is close by or will allow remote work that will give me opportunities to help increase data fluency and maturity and investigate how to improve their KPIs. I say this because I had an in-person interview for a Director of Analytics role with an R1 institution that was 2hours away by train and it went so well that if I was local would have been offered the position. Unfortunately, they reminded me they are a person facing university with person-facing faculty and staff and that their hybrid policy was for those who were at max 35-50 miles away, not 150 or greater, so my offer of being on campus 1-2 times a month was not feasible for their needs and I wasn't offered the position. 

I've considered trying the entrepreneur route but I'm not sure what service I would provide or even how to get started. Institutions that are looking for a storyteller-like consultant usually hire the bigger name groups, if they even have a budget for that at all, and my main subject matter expertise is higher education. It doesn't help that the tools I have used for the past decade have mostly been Tableau and SQL, I don't have a lot of professional projects that have used R or Python, just some coursework here and there. 

I am trying to be positive about the situation. My salary is the most I have ever earned in my life for me to essentially work from home in my current role (105k with about 5% COLA/Merit per year from the state), but at the same time, I feel like I should be doing something more and getting more from it, especially in today's job market. 

Are there others out there that feel constrained in their job options due to family responsibilities? How do you adapt/cope? Any entrepreneurs that can give advice about how to try and present myself in what seems like a niche market? Other suggestions? I couldn't post this on LinkedIn without fear that someone at my institution would see it and report it to my VP but I just had try and get feedback on my situation. Thanks for your thoughts.",analytics,2022-08-18 20:06:18,7
"Doing a master’s degree is a solid option, especially if you can get a scholarship. You don’t need to go to a school in the LA area to get a job here, though it does help since you can potentially do a part-time internship with a local company to get your foot in the door.",3,ws8xi7,"I am a business intelligence and strategy analytics senior analyst with almost 5 years of experience in creating data driven solutions for clients in strategy, product, legal, sales, and even operations. I have worked at giants EY and S&P Global. I currently reside in Delhi, India and wish to move to the US specially california to further my career at the best possible location. 

I am adept at Python, R, SQL, Alteryx, Denodo, Tableau, PowerBI, AWS, Snowflake and Hadoop. 


Which companies recruit internationals and can help me in landing a role there? 

Any other tips? 

I am considering doing a masters there : I have a solid gmat score, mba style career and aspirations! 

I don’t intend to take a lot of debt as of now in order to study but what if I apply to mid tier schools like loyola or uc irvine or chapman or ucsd",analytics,2022-08-19 01:50:34,2
"Do you mean textbooks?? Because if so I have yet to find any that don’t treat Analytics as a very dense mathematical treatment. The only book I think is pretty good and reasonably accessible is David Spigelhalter’s The Art of Statistics.

However, there are loads of good books that are for the intelligent reader and really give a good grounding in why to do modelling and analysis (and its pitfalls). Off the top of my head anything by Naseem Taleb (but especially Fooled by Randomness), anything by Gerd Gigerenzer, Nate Silver’s Signal and the Noise, even stuff from General McChrystal give a nice insight into how data can be used effectively to make better decisions.

Just don’t expect a nice book to cuddle up to in bed if you’re trying to learn SQL… :-)",2,wsajjc,"Hi everyone,

Do you have any recommendation of less techincal books that don't require to constantly be taking notes, i.e, something to read while comutting or just to have a quick read before bed?  
I don't have anything specifically in mind as long as it is analytics related.

Please feel free to suggest.  
Thank you in advance.",analytics,2022-08-19 03:30:56,3
"I had a similar background, studied SQL and basic stats and interviewed until I found someone who would hire me, learning from the interviews that I failed.",12,wro0oa,I have a bachelors degree in Economics. I’ve been working in financial services for four years and I hate it. I know I need to gain some knowledge to enter the field. I’m considering certificate program. Just need help understanding what are some topics I need to learn. I’ve been looking at Microsoft’s program. Does anyone have experience with it or any other certificate program? Any advice on what employers would be looking for?,analytics,2022-08-18 09:30:54,15
r/datasets,2,wrxux5,Anyone here actually have real life data that I can use to improve my analytical thinking and data analysis skills? I have used datasets to build projects on Kaggle and other websites but I still feel they don't have as much value as if I had actual real world data. These projects just feel like I am playing a video game honestly with not much of a  real reward attached to it. I would appreciate it if someone could help me out because I feel I am lacking on the critical thinking portion of Data Analysis and also it could be a huge boost because I had have something tangible to talk about during interviews.,analytics,2022-08-18 16:11:57,14
Ask upstream or experienced hands who are familiar with your data source.,1,wrfqnz,"Hello! I have an E-Commerce client and I noticed that some of the conversions has come from the wrong localization. 
For example, 6 purchases in Germany (google analytics) and an specific city. But the truth is that  the purchases had come from USA and other locations. Can anyone explain me why that could happen ?",analytics,2022-08-18 03:14:42,8
You’ll likely need to use an API. [Here’s](https://developers.google.com/maps/documentation/geocoding/overview) the Google Maps documentation.,1,wrosd9,"I am trying to implement a vehicle routing problem or travelling salesman problem to find the maximum number of orders that can be delivered by a limited number of delivery agents over a certain period of time e.g 90 mins. In terms of data I have customer ID,  latitudes, longitudes and order ID. How do I calculate the distance between every customer ID based on the latitude and longitude?",analytics,2022-08-18 10:01:48,2
"Start applying to jobs and see what happens. 

Definitely learn SQL, that will open a lot more doors.",25,wqy3sl,"Hi Everyone,

I'm currently in a role at a startup where I am client facing but have been using Power BI and excel pretty frequently. I want to transition into more of a data analyst role but I'm not really sure how I'd go about doing so. I will try internally first, but if thats not an option would my current experience and a couple bullet points on a resume be enough to leverage into a new gig? I can definitely tune up other skills if need be but wanted to see what is possible and what I'd need to do.

Thank ya",analytics,2022-08-17 12:42:39,25
"You could export the data in CSV format, and then import the data into a database.",31,wqsi4w,"Hello All,

Is there a way to run SQL queries on an excel file? I started a new role where a lot of the analysis is done on excel and I don't have access to the original databases since I'm fairly new. Is there a way to use SQL queries on excel files? I",analytics,2022-08-17 08:53:37,50
"Anything where you can get your hands on some data or spreadsheets will help, especially if it is a cross-functional role.

I worked in a call center as my first job out of college where I took on some projects that were out of scope for the role like making excel dashboards to help my manager see and share rep’s metrics. We had a lot of separate systems for each metric and they aligned reps by their supervisor, but it was always out of date or had duplicate entries for people. I lead some trainings, did some presentations for corporate, and went out of my way to get some unsupported lines of business much needed updates. 

That lead me to getting a job as a business analyst and then a data analyst.",2,wr4x7s,"I’m a senior majoring in MIS and will be graduating soon. I recently completed a data analytics internship at a local company but I’m getting rejected from full time Data/Business analyst positions because lack of experience.

I’m just wondering what jobs I should apply to in order to get the required experience to transition to an analyst role?

I know Excel, SQL and Power BI as of now, haven’t worked with Python or R.",analytics,2022-08-17 17:31:21,4
"It is difficult to give useful advice, because I am not sure what you're measuring and what your objective is. So if I'm entirely on the wrong track, forgive me.

I'm not sure why you're doing a regression. I'm assuming that with meters 1, 2 and 3 you are measuring the same variable at three different locations.  Is your goal to see if the variable is significantly different across the three locations? If that is the case, then ANOVA might be the appropriate statistical test.  

However, if you're measuring different variables with each meter, then I am not sure why you would add them together.

If I were you,  I would start by outlining my Null and Alternate Hypotheses and then you can determine which statistical analyses are appropriate for your data type and research aims.",2,wr579n,"Hi!

I've been collecting data from meters and telling friends to send me data from their meters. Now they sent me one very interesintg and I have trouble dealing with the data and how to make an interpretation. The data below is ""space bar"" separated:

16019.33346	2730.813792	276.9844851	19027.13174	19083.26352	0.30%

16417.55886	2954.00554	271.4185775	19642.98298	19239.49155	-2.05%

17022.0989	2932.65623	286.250451	20241.00558	19702.34554	-2.66%

16425.92115	2731.654183	291.9672764	19449.54261	19072.1967	-1.94%

16425.92115	2906.306679	275.9512239	19608.17905	18911.79535	-3.55%

16584.36313	3013.738664	280.0397459	19878.14154	19427.75677	-2.27%

15823.28466	2911.85698	275.8005815	19010.94222	19163.7981	0.80%

15062.60373	2638.141648	282.6630302	17983.40841	18387.98474	2.25%

14884.58673	2664.862558	282.3356743	17831.78496	18146.20084	1.76%

15744.97186	2612.236895	286.819912	18644.02867	18668.0139	0.13%

15496.36202	2580.166907	296.1919671	18372.7209	18487.16704	0.62%

15444.36206	2787.331985	274.3617209	18506.05577	19038.24226	2.88%

15631.40427	2851.046991	276.2340573	18758.68531	18932.39167	0.93%

15631.40427	2961.983257	284.3361624	18877.72369	19062.66917	0.98%

15904.00724	2964.016059	280.5066366	19148.52993	19271.15468	0.64%

15904.00724	2939.834193	272.6283379	19116.46977	19206.36746	0.47%

16413.71847	2706.697042	296.7813791	19417.19689	19042.96	-1.93%

16039.19244	2706.909526	310.2347041	19056.33667	18609.48176	-2.34%

16426.75934	2894.507912	275.0256739	19596.29293	19199.95963	-2.02%

16717.94638	2861.701817	281.4843592	19861.13256	19442.10192	-2.11%

16717.94638	2955.357069	279.1364418	19952.4399	19617.74144	-1.68%

16478.30495	3037.572428	291.6055264	19807.48291	19526.61789	-1.42%

16876.17588	2846.278574	280.9932641	20003.44772	19699.34334	-1.52%

17072.80731	2867.524334	299.8732066	20240.20485	19852.06978	-1.92%

16071.15031	2660.120579	307.2629805	19038.53387	18835.54964	-1.07%

16071.15031	2668.34138	308.013286	19047.50498	18750.27577	-1.56%

16193.00775	2843.968906	289.0103325	19325.98698	18847.73469	-2.47%

&#x200B;

This is data from a set of meters:

Column A:

Meter 1 Data

Column B:

Meter 2 Data

Columb C:

Meter 3 Data

Column D:

Sum of Meter 1, 2 and 3, we will call it Meter 123

Column E:

Meter 4 Data

Column F:

Error, ie ( Meter 4 - Meter 123 ) / Meter123

The context: Meter 1 measures the bigger inlet, Meter 2 measures another inlet and meter 3 the last inlet. Meter 4 measures the OULET. So I suppose that Meter 123 and Meter 4 should be the same  ± some uncertainty.

Here are 3 interesting things:

1- It seems that if the reading of the Meter 123 is ""less"", the error tend to be positive, and if the reading of the Meter 123 is ""more"" then the error tend to be negative. This is quite a negative linear tendency with R2 of 0.643 (which is not much, but it's something)

2- If you do the same regression but instead of the data from meter 123 you take Meter 4, then the tendency simmilar but with a R2 of 0.193 (which is WAY lower)

3- If you do the same but just with Meter 1, you obtain almost the same result as with Meter 123. Actually, it will be almost the same tendency but with R2 of 0.7

If you do this with Meter 2 and Meter 3, R2 is less than 0.1.

I don't actually know how to interpret this, is Meter 1 causing the tendency (slope)? Is it meter 4?

Any input would be very much appreciated. Thanks!",analytics,2022-08-17 17:43:56,2
"Does medium + any metric work?

Do you have medium passed through in UTMs correctly?",1,wqkjhi,"Hi,

The title says it all. I'm trying to create an exploration report that uses the medium dimension and views as a metric, but that combination only returns zeros.  


Is this a new limitation in GA4 or am I doing something wrong?",analytics,2022-08-17 02:32:40,4
"The recruiters I've dealt with have worked on a commission basis (based on the employee's salary), so it's in their interest to get you a higher wage. It's a balancing act, trying to get you the highest wage within the employer's range, but not so high that you become a bad deal.

To answer your question, they want your salary to be as high as the employer is willing to pay for you.",24,wqhkhv,"Do y’all have incentive in finding a candidate who will take less pay?

I want to take this role doing just SQL. I’m a new grad. Eventually I’ll move out but it’s a perfect starting gig. 

Recruiter said pay is from $30-$35.
I don’t know whether to negotiate with a recruiter though or if that’s solely done with the hiring manager.",analytics,2022-08-16 23:28:08,26
"Well, I think the critical first step is to get an understanding of the business: their mission, vision, business outcomes, sources of revenue, clientele etc. You can't add value until you know what they need.",37,wpv28z,"Hey there,

I'm exploring how other data professionals strategize around understanding a companies data when they're hired to be the new data expert (read: senior analyst, director of analytics, etc.)

Excited to hear your strategies. Also posted this on r/dataanalysis

Thanks,

j/",analytics,2022-08-16 06:59:01,8
"I have been a pricing analyst for 3.5 years. Get very good with excel. And start researching the art of telling a story with data, a lot of the times the explanation on finding a trend or somthing is quite simple, start practicing spicing that simple explanation up, higher up managers and presidents love that stuff for some reason lol. (Personally, I am a VBA master but you’ll find most people you work with are complete bots so you can just use VBA or Python to automate a large part of your job while you work remotely)
I got into the role by doing an internship while in my senior year in college then transitioned to a. Shit tier company then a high tier after 6 months.

Average pay under two years exp is about 60-70k

Get your masters online and you’ll be getting paid on average 85k-115k",3,wq3r9q,"Hello,

I'm currently a rising senior in college majoring in marketing with a focus on business analytics (Python, SQL, Excel).

While my background is in web design and digital marketing consulting (WordPress, Google Ads, Google Analytics, SEO, social media marketing), I want to break into entry-level revenue management or pricing analyst roles after college.

I'm really interested in working in the hospitality, tourism, and services industries (airlines, hotels, theme parks, concerts).

What would be the best way to do transition into this field and what would be the best learning resources to learn more about the field?

Thank you!",analytics,2022-08-16 12:50:59,3
"8 months after graduation in this economy / pandemic wouldn't make me bat an eyelash, either for a gap year or someone trying to get a gig.",30,wpy8sh,"Hey yall, I want to ask a question. 

It’s been 8 months since I graduated with a stats major. 

I’ve started being asked what I’ve been doing after graduating.  

The real reason is bc I was in a manipulative relationship during my time in school and needed some time to recover. I tried going to therapy but I didn’t realize what was going on with myself bc.. manipulation. So it didn’t help too much. 

I did my absolute best during college. As a first-gen in a highly competitive environment, I though I didn’t know enough analytics to apply to internships so I only did a volunteer data analysis role and an AWS certification. 

It sucks that the economy is the way it is.

Do y’all have any general tips with regards to my situation that I’m not aware of besides applying to jobs through recruiters and the ATS?",analytics,2022-08-16 09:09:14,13
"Look deeply into the boot camp to decide if it is right for you. It might be sponsored by the college but is delivered by a company completely unaffiliated with the college, which is how a lot of those boot camps are. I took a boot camp at my state university and it was actually done by a company named trilogy. I am not sure if the company has changed, but I believe they are still the ones that do most of the college boot camps. If the instructor is good and you have little experience coding, it can be great. The boot camp I took changed my life, but it didn't for everyone in the class.",1,wq8uat,"Hello! I am interested in pursuing one of these boot camp programs, and I saw that Georgia Tech has a virtual program that’s 24 weeks and pretty well-rounded. Has anyone done the one at GT, or something similar? Has it helped with your career prospects in data/analytics? Thanks in advance!",analytics,2022-08-16 16:19:00,2
"This is not r/careeradvice, but it sounds like you could use some and I hope to help. I am very sorry for your loss, and know personally how the loss of parent can derail our best efforts. Depression sucks, but you have to fight for what you want out of life. 

'Life is what happens when you are busy making plans' 

Stop making them and start doing. More education is an option for very few and you may be better suited to the workforce at this point. You may find it better to join an organization that forces you to show up to work, as discipline and comradery will aid your outlook and performance over time. 

1. Apply everywhere - you never know who you will meet and have the chance to impress.
2. Take the available position, not the ideal position. Luck follows industry, you can transfer, get to know other hiring managers. I was promoted within three months, while I know my experience isn't typical - it is possible.
3. Freelance - do projects that allow you to build a portfolio to garner attention and justify your willingness to work hard to get what you want out of life.

Hiring managers want to see some 'fire in the belly' and you'll have to show that you have it. Be leery of shortcuts, do the work, force yourself to get out there, no zero days.",2,wq4x57,"Hi Everyone!

I'm a graduate in Economics from India (having an [M.Sc](https://M.Sc). in Economics from a Central University) with a GPA of 2.6 (CGPA - 6.52). I was an above average student at Senior/High School (75-85%) and had had a number of achievements like the President's Award in Girl Guiding. Due to family issues and the loss of a parent during my final year (as well as some University Politics), I've had my GPA drop from 3.2/3.4 to 2.6 gradually (a gradual decrease every year). 

I was supposed to graduate in 2016 but due to depression and physical health issues, could obtain the degree only in 2018. I held onto a few gigs (non-technical data research & analysis + technical writing) for over a year and a half or so between 2019-2020. Had taken a slip back with unemployment since COVID but have been doing some personal research (mostly with regards to further career) during this time. 

I had wished to pursue Data Sciences/Analytics or Business Analytics/Intelligence or some career with the likes of World Bank or UN but I feel like my lack of good grades & experience will be hindrance to a  good future career. (I also had MBA, Intl. Politics & Relations, Intl. Macroeconomics/Intl. or Specialised Law, Creative Designing/Design Theory & Research as other interests).

I've consistently had to be in and out of severe depression too, but have always wished to pursue higher studies abroad (U.K., France and the like. U.S. and Canada feel far-fetched for my academic standing either way). Would love to receive some solid advice on what I could do to further my career and have some exposure to well-paying jobs (possibly remote, work-from-anywhere because of my mental & physical health) and/or be able to shift my country of residence?).

Some difficulties I face now are:

Financial Standing - I'm on my own with no outside support for finances (can't apply for loans either for lack of collateral/nominee). Could look into Studentships/Scholarships but I'm not sure of securing good ones because of my grades. 

Letters of Recommendation - I don't have very good relationship with my Department's professors because of my family issues affecting my course. So academic references are really difficult to obtain (I can try from other departments where I know a few Professors though).

Possible PhD - I was hoping for a possible PhD ever since I joined my Masters course but now I'm not sure, given how many more years I would be losing, along with the lack of high RoI (in terms of time spent) and the hassle of academic life (with publications/grant seeking/research & assistantship, etc.). I'm not sure how far I'll be able to do this especially if the stress has any chances of affecting my depression. I was also looking into PhD as a way to obtain a PR from the country I take the degree from (like U.K. or something similar).

Seeking a career through low-cost means - Like choosing a University that has low fee or looking for online/certificate courses. This feels suitable if I want to choose Data Science/Analytics as a career but I've heard of lack of entry-level positions in the field/s and the requirement of a Masters/PhD in the said fields for a good career opening (along with experience/projects, I guess?)

Possible Colleges/Universities and Companies/Organizations to study at/find work at - What are my chances at either for studying and/or finding work? What tiers of Universities and Companies would be possible for me to pursue? 

Health - Depression has changed my physical appearance, personality, and social skills as well along with chronic ailments. Not sure if these would hindrance to pursue things either but that's possibly just an extra worry.

Would appreciate any valuable guidance regarding the above. Thanks in advance!",analytics,2022-08-16 13:37:50,2
"You have to start with a database. This is one thing that companies overlook. You can try and set one up for ur division and go through IT to get approval or have them build it for u, but it sounds like u might have accepted a shit storm.",39,wp8mt5,"Howdy, 

My title is data analyst but my duties make me an excel wizard. My company lives off excel macros that if God forbid the one dude that made them leaves, were all fucked. 

Anyways, our ""database"" is really just a third party so it's not our own. We use a website of this third party that stores all the data. 


My job revolves around tracking sales and sales volume data for all the different stores of our company. 

How can I start implementing power b.i, python, sql into the daily mix?",analytics,2022-08-15 12:34:33,29
"I didn’t like it. I found it to be a resource hog and not intuitive.

I have found using Bash, SQL, Node.js and Vue.js much easier, as I have granular control and all four technologies are well understood and commonplace.

I use Bash and SQL to extract data, Node.js to format and prepare it for view, and Vue.js to present the data.

This is just me though, I’m sort of old school which probably makes me biased.",1,wpuujw,"For people who are data analysts (i.e. you work directly with business clients to make dashboards, visualizations, answer questions about data) or support DA's, what are your thoughts on Kibana as a general purpose dashboarding tool?

We currently have Tableau as our own BI tool, which traditionally is fed by normalized tabular datasets our devs have ELT'd for us. We also have access to Jupyter notebooks if we want to explore datasets with more flexibility.

One of our largest datasets is a legacy reports store filled with semi-structured reports (semi-structured meaning like they have common text headers and some common metadata). There's work being done from the DS teams on NLP type stuff to get that accessible from a structured point of view, but thats a ways away.

Very often though we have clients who ask us to compare/enrich a new dataset with what's in that legacy repository, additionally there's been a few times we've been asked to visualize/dashboard ad-hoc JSON files too which became a nightmare to work with in Tableau once it was flattened and converted to a tabular format.

For these two reasons, I've been looking at an ELK stack as a possible interim solution. Now, I think I'm pretty set on using it myself, and right now I'm basically the only DA on my team actively working with business clients. That might change though, so I'm concerned moreso on the drawbacks with an ELK stack using it for BI work.

Learning about ELK coming from a traditional BI background has been tricky translating the terminology and difference in approaches. I understand ELT processes fairly abstractly, but index patterns have been a bit tricky to understand but having read through the docs I think I get it. You create index patterns which let you map out your data structure into Elasticsearch's schemaless structure. From the Kibana end, ""fields"" are intuitive enough and you can also run SQL on these fields.

My thinking here is that deploying an ELK stack maybe I could leverage both the legacy semi-structured reports repository we have along with our tabular data and make dashboards for them. I figure even if future users are not at all interested in index patterns and all that jazz, they could still easily connect to the same SQL data stores they can do in Tableau, and Kibana just has a few different visualizations and layouts and stuff that might just be good way to round out the toolkit.

What are your thoughts here? I'm worried that because I'm a DA that's interested in both the dev and data science side that while I'm perfectly keen to dive in and learn all the more technical things, I know that wont be the case for most DAs who might come on to the team, and I don't want to assume they can just ""figure it out"" because it does seem that this isn't traditionally used (that being said, one could also argue our org isn't not a traditional BI team either...). 

Similarly, I have very little insight into how much effort it is for the dev team to support an ELK stack. Our first MVP is just a sandbox for me to test ad-hoc stuff and use Kibana for my own viz and see its viability, and my hope here is to be largely self-sustaining when it comes to managing Logstash ingests and the like (there may be a future requirement for streaming event data too but not immediately).",analytics,2022-08-16 06:49:50,1
1st and 2nd year actuarial science books,2,wpc5wb,"Hi, pretty much the title. Looking fo material to read. Can be articles but I prefer something that's longer and gives reader info about how finance is changing through analytics.",analytics,2022-08-15 14:55:31,9
Doesn’t really matter,4,wpkysi,"I’m about to start learning SQL and see there are several database options:

• BigQuery

• MySQL

• Microsoft SQL Server

• PostgreSQL

• SQLite


Which is recommended for newbies? Is there one that is “industry standard” or preferred?

Edit: I’m on a Mac, if that means anything.",analytics,2022-08-15 21:40:24,11
"Look at your restock (purchasing) strategy and go nuts with that. How are you forecasting? Are you using an algorithm? If not, write one and optimize. If you have one, why do you still have stock outs? What are its weaknesses? Etc. Create or optimize your purchasing algorithm using data.

Another one you could do is to figure out which drinks have the highest profit margin and which have the lowest. Then discuss having specials on the highest and increasing the price on the lowest.",29,woxpj7,"Hello,

I have been bartending for two years (Graduated with a Bachelor’s in Economics in 2021) and I want to move into a career in Data Analytics. I have some pretty strong technical skills that I picked up at my University/on my own online and now I think the only thing that I can add to my resume would be projects that use SQL/Tableau/Data Analytics skills. I’m trying to brainstorm some projects I could do at the bar that I work at. Backstory is we only serve drinks, have toast POS system which tracks some great data, and have established profit margins on everything on the menu. Any ideas for projects that I could do?",analytics,2022-08-15 04:55:11,18
"Box and whisker is the standard, not sure if ppt has that.",1,wpmxcv,"Hello All, I just completed a project where I'm comparing the averages of two different groups. What I did was to simply perform descriptive statistics on the two groups and compare their means and distributions. I will be presenting this to the top guys at my job and wondering if there are free powerpoint templates out there that are good for analytics. 

Thanks.",analytics,2022-08-15 23:32:05,2
"I got a data analyst without finishing my bachelor's in business. I had 2/4 years completed when I got it.

But I'd definitely would of have it regardless of studies.

What set me apart was my experience in the company. I started as a customer service rep, then as an advisor for one of the domains of service. Then got the job as a analyst. Took me 4 years total.

In other words, extensive domain knowledge is what made the difference. You can know every function in SQL  and powerBI, but that means nothing if you don't know what you're looking for. See what I mean?",12,wp1uxf,"Hello everyone, wanted to hopefully hear from the community about any people who have found success in this field without any sort of degree.

I have been studying in my personal time and am beginning to get a good grasp of the skills, but I am curious what the trajectory has been like for others who have started this way.

Even if you eventually did get some degree I would be interested to hear what that path has looked like.",analytics,2022-08-15 08:01:15,7
"Staying with a company for 3 years isn’t job hopping in my opinion, that’s a normal tenure. Honestly I think staying in the same role more than 3-4 years without a promotion or change in responsibility/scope/focus is a red flag. It’s fine to stay with the same company, but if they aren’t helping you grow via promotions or an internal transfer or something like that - why not? 

Anyway, I’m a data scientist in tech and it’s not uncommon to see folks change jobs every 1.5-2 years. If you’ve been working 5 or more years but have yet to stay with any company more than 2 years, that’s job hopping. Otherwise it’s probably just growing your career.",6,wpavtk,"How does the Analytics community feel about job-hopping? What would you define as job hopping,  and how would that influence you feel about a candidate? Young professional within analytics field coming up on 3 YOE(1 year with one role and 2 at my current) in Business Intelligence in Insurance(SQL, Python, Power BI, Data warehouse). Full Comp including salary 64k, MCOL. Currently feel my compensation is lacking based off listed responsibilities and assumed /expected workload. Am I crazy for asking/expecting 75-78k+ for future roles and is that in line for this field with my experience?",analytics,2022-08-15 14:03:37,5
"I would avoid all negative aspects like the toxicity and burnout, and replace them with positive aspects: you saw an opportunity in the market, you wanted to grow in a different direction, etc and then realized it wasn't for you that 's why you're going back to your previous career path.

Sound upbeat, excited, thankful, and confident while you tell those stories.",14,wp09ne,"I'm a former developer looking to break into data analytics after a long, weird career break. Basically: 

* I used to work as a web app dev for a number of years, but I was in a toxic environment that was such a nightmare that it killed my interest in being a developer all together. I eventually quit and started an online store that was successful enough. 
* After a couple of years, I had a major health scare and I had to close my store and take a year off to get healthy.
* When I was healthy again, I was ready for a new challenge, and I decided to go back to school to finish my degree. I will graduate at the end of this semester. 

I will begin applying for jobs soon, but I don't know how to describe my career path. I know you aren't supposed to talk about how toxic previous jobs were in your interviews, but I'm going to have to say something to explain all this. What would you say?",analytics,2022-08-15 06:53:53,4
" Nice, yea I know 7 people with 3- 4 years out of college that just made the jump this year to Senior Data Analyst😂😂 with them being business analysts prior.

Great job market for people in our space I made the jump myself to one of the big 3 in IB aswell.",29,wof9sr,"Just posting here because most of my family and friends don’t really understand what I do or how big this step in my career is. 
Education Background: I have BS in Finance May 2020 and an MS in Analytics August 2021

Current Role: Associate Business Analyst at an education technology company. This role is intended for recent college graduates and in it I was responsible for operational reporting to divisional leadership. I used very little of my coding or analytics skills in this role although my leadership loves to throw buzz words around and held me up as a great example of success for a graduate. This was highly depressing and I had little room for improvement or growth and could feel my window to be a real analytics professional closing as I was getting continually further from any real technical work.

New Role: Senior Analyst, Advanced Analytics Division at a financial services company. I will be the senior on a project developing how we track customer experience with the aim to leverage this data to enhance and optimize customers interactions with this business. I am getting a 84% bump in total compensation and 69% bump in base pay. I really feel like I have finally started my career and hope that this position will allow me to grow more technical and eventually into a lead role. 

Thanks for reading I just needed to share with someone who would understand things beyond the title sounds cool and the salary is obscenely large.",analytics,2022-08-14 13:01:29,20
"This is fantastic! I’m in the middle of the Google Data Analytics course on Coursera now and it’s been great so far. I have some low level experience in my current role which caught my interest and helped me decide to work towards making a career switch. Success stories always help me stay on track and stick to my goal, so thank you",2,wp0gz5,"Hey all! This is my first post in this sub, but I have been on here for a good while trying to gain as much information as I possibly can for my own data analysis journey. I wanted to share how I went from being a health coach with absolutely no data analytics experience to becoming a data analyst in under a year. I would like to first start by saying that this by no means is an end all be all track for getting you a data analyst position. Everyone's story is different and it takes a lot of hard work and a lot of hours outside of my previous full time job to be able to get to where I am today. 

&#x200B;

Background:  


I graduated from Purdue University with a degree in Kinesiology in 2019. I began working as a health coach right out of college making $37,500 per year. With this being the only income in my household for a year or so it was definitely tough to save money, but it was a job and I expected there to be plenty of opportunities for advancement considering I was good at my job and I like to push for advancement as soon as I feel comfortable. Unfortunately, this company provided virtually no opportunities to advance as promised so I was stuck in an endless cycle of getting a 2-3% raise every 6 months and making virtually no progress in my career. Needless to say I realized that health coaching wasn't going to be a sustainable career if I was going to be able to provide not only for myself, but also for the family I was hoping to raise long term. I began looking into other avenues I could explore such as PT, OT, Sports management, data analytics, etc. and decided on data analytics as something I felt would compliment my strengths well. 

&#x200B;

Steps Taken:

Since I had no experience I decided that grad school would be a good way to not only get some experience under my belt, but also to get something tangible on my resume. The problem here was that I severely underperformed in my undergrad to the tune of a 2.4 GPA. This wasn't going to get me into any reputable analytics program so I had to go back to Purdue, retake 2 full semesters worth of classes, work my ass off, and get my GPA up to a 3.2. I did this in the fall of 2020 and spring of 2021. After taking that time to reset my academic standing, I started applying to grad school. I ended up getting into the University of Mississippi in their Data Analytics master's program which I started in January of 2022.

&#x200B;

Since I knew I was going to have about 5 months before my grad program was going to start, I decided to take some Udemy/Coursera courses on my own to get a solid baseline knowledge of coding under my belt. I took a course on SQL ([https://www.udemy.com/course/the-ultimate-mysql-bootcamp-go-from-sql-beginner-to-expert/](https://www.udemy.com/course/the-ultimate-mysql-bootcamp-go-from-sql-beginner-to-expert/)) and the Google Data Analytics Coursera course (teaches SQL, Tableau, and Excel basic/intermediate skills) which I felt gave me a super strong foundation going into grad school. In my first semester in grad school I also learned R programming, Machine Learning Basics, Statistical Modeling, and Predictive analytics which only served to compliment what I had already learned before starting grad school. After my first semester was over I decided it might be worth it to start applying for jobs. I was able to land an interview with a company in California that was starting a data analyst training program and wanted to higher people at an entry level to learn the ins and outs of working as a data analyst for their company. I didn't end up getting the job, but it was invaluable experience finding out what data analyst interviews look like, and what coding challenges can include. After not getting this job I realized that I needed to have some more projects on my resume and that it might be worth it to create my own website. I started following this guy's Youtube channel because I felt like he had a lot of great advice and he even provided some code-along projects that he said would look great on a resume/portfolio ( [https://www.youtube.com/c/AlexTheAnalyst](https://www.youtube.com/c/AlexTheAnalyst)). Not only did he provide portfolio projects, but he also went more in depth on SQL, Tableau, and Excel which taught me more than I already knew about the three tools. He also gives you a great resource and teaches you how you can set up your own website using [html5up.net](https://html5up.net) and github pages to display your own work. Here's his if you would like to see an example: [https://alextheanalyst.github.io/](https://alextheanalyst.github.io/). 

&#x200B;

Results:

Within a month of having this website up and applying to numerous jobs I received a call about an interview for a data analyst position I didn't even remember applying for because I had probably submitted around 50-100 applications in the months of May-July. They had me do a virtual interview and the very next day offered me my first job as a data analyst where I would be getting paid $75,000 as my starting salary. This all happened before I even finished my grad degree which I will obtain in May of 2023. 

&#x200B;

My 2 Cents:

Like I said earlier, this might not be a process that works 100% of the time for everyone. I just wanted to share my story because I know how encouraging it was for me to read other's similar to mine when I was going through the process of submitting application after application over the summer. It is a grind and it can feel like you're going the wrong direction at times (and who knows maybe data analytics won't end up being for you), but keep pushing and don't sell yourself short. I was an inch away from simply accepting a contract to hire job because I was feeling desperate about getting my first job. They were going to relocate me to a location not of my choosing and couldn't guarantee I would get a job but boasted such a strong success rate I thought maybe that was going to be my route to take. 

Another thing I want to mention is that the job I ended up getting said that they wanted 3+ years of experience. I know that most every entry level job posting for data analytics says virtually the same thing which is ridiculous, but I was able to get the job I have now with less than 1 year of experience under my belt. Part of the reason for that was because I was super confident in my soft skills. If you have a semi-decent personality, can sell yourself well in an interview, and can get comfortable having conversations with hiring managers that will take you such a long way. The VP of the department I'm in now who was in on my interview said that they hired me because they knew I had a solid foundation, but mostly because they could tell that I was going to fit in well with the team. She said to me verbatim: ""We know you might not know everything to succeed in this job right away and that's okay. We hired you because we could tell you were a good person, would fit in well with the team, and had a strong desire to learn. We can teach you the hard skills, but the soft skills are invaluable."" 

I know not every organization is going to have that mindset, but any company with good upper leadership will so if you can market yourself well even with a little bit of experience I think you will see yourself take that next step in your data analytics journey sooner than you think.

&#x200B;

I really hope this serves someone well, and that it's an encouragement to so many of you who are looking for a job right now because it can be an absolute grind to find that first job. If you guys want to reach out to me, dm me, whatever you're more than welcome to and I can offer any advice that I do have or point you in some directions that I went to learn what I did to get this first job.

&#x200B;

Best of luck!!",analytics,2022-08-15 07:02:19,6
"University drop-out > Retail salesperson > B2B software sales > Junior Business Analyst for a marketing agency working for a large tech company > Sales Operations analyst for SaaS > Insights Analyst 

You will be surprised how much working in retail & sales got me started into analytics.

Communication and 'soft' skills will get you further in your career than straight technical if you don't have a degree",7,wozyr6,"I want to know what are your job titles and responsibilities before the craze in data science and/or analytics. What industries did you worked for? What did your day-to-day looked liked? An how did you transition to your current career?

Thing is,  I sucked at college. It is only after I did a couple of jobs and transferred to a local college where I kinda got my shit  together. I did enjoy learning new things and putting the effort to study. Finally obtained a bachelor's in Industrial Engineering but then sidelined by covid and a family matter. 

Now I am looking for an entry level position in analytics, and it seems like I wouldn't land one with my current knowledge. I know it'll be more than excel, visualisations, and presentations. I need to brush up on my statistics and have a good programming skills to land a junior role. 

So my question is, while I'm building my skills, what industries/job positions would be worth my while so I'll have a good domain knowledge and a place where I can practice topics that I'll be studying. 

Do you think working as a warehouse/logistics/supply chain assistant will be a good fit? I can't seem to think other job positions for me. 

I am hoping for your guidance guys coz I think being a mediocre student during college came back and bit me in the ass.",analytics,2022-08-15 06:40:20,6
"I'd say it's just not enough time lapsed to make any judgements - unless something is wrong in your targetting.

If it's random 50% then just let it run for a couple weeks.

It takes time for things to settle in an experiment.",1,wouxtz,"Hi, I'm fairly new to using Google Optimize so apologies if this is a known issue, but I couldn't find anything about it. 

I'm running an Optimize experience on my HP to track conversion rates. 50% of traffic is viewing the experience, of which 50% sees the variant and 50% sees the original layout. After 5 days and 11k sessions, both my variant and control are seeing \~10% conversion rate, whereas traffic NOT from the experiment is seeing \~4%. Why would my control group have such a variance against the non-experiment traffic, if they are ultimately having the same experience?  


Thanks in advance.",analytics,2022-08-15 02:14:50,1
"We might have something you could tinker with. We have an internal none urgent analytics task in the pipeline, you could take a run at it, we'd give you a bit of support. If you mess it up, meh, one of our staff were going to do it anyway. So, no time pressure, no success pressure. Win or loss it'll look good on your resume.

Let me know if you want to talk further.",24,wnuiow,"I know there are sites like Upwork or Fiverr etc where people can do freelance work.

I'm wondering if there's a way to maybe get access to past projects or ""asks"" from people who need analysis done?

I'm not confident enough to take on actual tasks from people and end up wasting their time by not actually being able to answer their questions or taking too long.

I get that I can go on Kaggle, data.gov and get datasets & maybe analyze them that way, but I want to actually feel like I'm doing something for someone, not just myself. 

tl;dr 
Where can I get real tasks/asks from people who need analysis, visualizations, etc from an free lance analyst for practice? (I don't want to take on an actual project out of fear of wasting the asker's time.)


Thank you!",analytics,2022-08-13 18:52:52,15
"Lots of applications. I've used GiS network processes to establish better routing and targeted marketing for retail services that use delivery services. We analyzed patterns in traffic, client locations, and survey results creating intelligent route delivery, expanding their services without incurring additional operation costs, and targeted new neighborhoods for targeted marketing.
In the organization I am with now we are in the media and marketing space, we do network analysis between client/customer relations to create a better view on target groups thenuse surveys and profile patterns to match clients to customers through direct and indirect targeting. Essentially fancy social network analysis.

Really network analysis is used in a bunch of things.",4,wn9uwv,"Basically, my university has a course on network analysis (within the data science major) and I am interested in taking it although I do have some questions. I wanted to know what some real life applications of network analysis have been (that you have worked on or heard of)? Also what sort of data is needed and is network analysis feasible in the corporate world (I’d imagine there would be a lot of use cases in marketing and logistics)? Or is it largely used for government projects like city and public transport planning?

TLDR: where is network analysis used, what are some examples of it (if you know any) and what sort of data is required to perform network analysis?",analytics,2022-08-13 01:57:28,6
Is it silly to be happy?,58,wmqb2w,,analytics,2022-08-12 10:00:35,38
I make that amount and I don't have your experience or masters degree. I'm in Wisconsin. You deserve more.,43,wmdw2e,"Can someone tell me if I can potentially make more?

Educational qualification : bachelor in mechanical engineering and master in industrial and systems engineering.

LCOL : 76K

7 years of total experience.

2.5 as a construction equipment engineer

4.5 as a Business analyst (sql, power bi). Fairly advanced sql and power bi skills. I do know python and r but do not get to use it at job 

I feel i should be making more but just want to know what everyone thinks here.",analytics,2022-08-11 23:18:52,61
"I don't want this to come off as a ""just Google it"" response, but you will save yourself a ton of time if you search for current year white papers / state of the industry articles for this. Those are great places to start for any vendor comparison work.",1,wmorxi,"I'm trying to assess all the options for server-side tag management, GTM and Adobe Launch Serverside are the main ones that come up with any type of searching.

What other vendors or platforms are you all aware?",analytics,2022-08-12 08:56:37,5
Yes for CRM Analytics. No for Tableau.,5,wm9nom,"I received an e-mail from Tableau about the Dreamforce conference. I'd be interested in learning more about Tableau and networking with other professionals who use it.

However, after looking around at the Dreamforce promo material, it looks like it's heavily skewed towards Salesforce content. Has anyone attended since Salesforce acquired Tableau? Would it be worth going to if I have no interest in the CRM side of things?",analytics,2022-08-11 19:36:28,3
"Nothing else is remotely close at being both useful and usable by unsophisticated users while also delivering power and flexibility to sophisticated users; Excel is the language of business.  In most businesses, if what the average user sees and touches isn't in Excel, then it's not going to get traction or be successful.   

There are a ton of tools better for specific tasks IF someone is trained in them, but short of C-level intervention, such training on a widespread basis is unlikely in most circumstances.",8,wlvn9e,"Not sure where to ask this but I'm in a new Business/Data Analyst role for a financial firm, kinda like the data transformation team providing support for dashboard tools (Tableau/PowerBI) and excel.

&#x200B;

Because of Oracle BI and Excel plugins, theres such a HIGH Reliance on Excel and its honestly feeling like a shitshow with Excel docs linked to one another. Just curious for anyone else in this role, is there any way to improve this kind of work or are we forever BOUND to Excel??

&#x200B;

I wanna start pushing some initiatives in Python but I cannot for the life of me see alot of business users try to use Python for their repetitive work, excel seems to be the only way to make alterations to their tables but when an Excel doc corrupts, ALL HELL BREAKS LOOSE!!",analytics,2022-08-11 09:28:13,9
"Could your workplace finance some certification or a master's degree? I work for a huge company and we have tuition reimbursement to some extent.

I'd honestly keep the job but start applying to other data analyst/ data science positions in the meantime. It doesn't hurt to try and possibly you'll get a better offer of at least the possibility to work remotely for another firm.",10,wlun9t,"Hi there,

I'm currently two years into a data analyst position but things are getting dry at work - I've had three managers this year (two retired, one on mat leave) and am basically the back-bone for my business unit's analytics, but not paid or respected in that way. Things are going well, but I just got married and am needing to plan for kids and buying a house.

Some background: I have a bachelors in pure mathematics from a Canadian university. I worked on multiple paid research projects between physics and mathematics during undergrad, had a strong GPA until my fourth year due to mental health issues where I then subsequently withdrew and graduated with only a 3-year degree. I want to get into data science, but the 3-year degree makes it almost impossible to get accepted into a masters or PhD program without going back to do undergrad courses. My mental health issues are well taken care of now and in the past, but their effects linger affecting my opportunities for growth.

While working, I've been completing a data science certificate for professional development, scoring almost perfect in all courses throughout the certificate so far (99% and 100%). The certificate is a joint program between two well established Canadian universities and prepares the students to take INFORMS' CAP exam - I thought it would be more challenging, but my biggest take-away is just extra practice with python, the stats taught is rather elementary for my background sadly. However, to take the exam I need 5 years work experience with only a 3-year bachelors, so I'm still a few years short of getting that qualification.

I'm paid decently well, about $90k CAD after bonuses this year. My performance reviews have been exceeds expectation, but I'm finding work stale and the revolving door of managers has me feeling like I'm constantly building report and social capital with people who leave quickly, making me start all over. My company is also calling us back into offices soon and it's 2hrs door-to-door so I'm starting to feel an itch to find something else. While I've not wanted to be in management previously, my work experiences over the last few years has me thinking I'd do well in management, being able to pick out and mentor new talent to stand out for leadership. A lot of my time right now is building models and presenting them, only to have me be seen as a grunt whose advice and knowledge help the business, but is often ignored due to MBAs and senior leadership thinking ""complicated"" math is too heterodox compared to what they learned in business school. 

So my options are few:

1) Keep at it in my current position with loyalty, maybe get 2% raises each year while commuting 4 hours a day. Continue to get my CAP and maybe pivot from there.

2) Start a new job search, seeing if I can leverage experience in the face of lacking a masters/PhD.

3) Leave the workforce, upgrade to an Honours degree and then apply for 1-2 year masters programs like MMath in Optimiziation or MDSAI or MEng in Analytics, etc., for the hope it'll have a decent ROI after losing 3-4 years of salary.

4) Stay at current position, but do a paid Masters of Management in Analytics program at one of the various reputable business schools in my surrounding area (Ivey, Schulich, Rotman, etc.). Program doesn't require honors 4-year bachelor and recruiters that have seen my profile are satisfied thinking I'll get through admissions. Program is 12 months while working full time, but costs about $50k in tuition (I have the cash saved up, but can also finance tuition if necessary). I understand that these programs are more about the prestige of the name of the school and networking than content of the classes.

I guess I'm asking if anyone has experience in a similar situation? Or if anyone has done an MMA or equivalent paid masters program and the ROI was decent? If it's worth it to drop out of the rat race and stable income to upgrade my bachelors and get an academic masters?

Old me would want to hop back into academia for the love of chasing knowledge, but at this point in my life I'm much more oriented toward securing the future of my family at the expense of my academic curiosity.

Anyway, thank you for any advice and comments!",analytics,2022-08-11 08:47:05,6
Are you running any bot detection software on your website to determine if the visits are fake?,3,wm2x9e,"For the last several days, there are bunch of suspicious visits coming from Seychelles to my website.

All of them bounce, and all of them come from Google. I haven't had any visitors from Seychelles before, haven't had targeted this country or any other African countries, basically did nothing that might cause visitors from Seychelles.

&#x200B;

What do you think? If it's a scam, shouldn't it have a referrer source?

Also, please comment if you've experienced the same thing.",analytics,2022-08-11 14:33:12,10
"This is not data management, this is data engineering. You will need to speak to the Data architect.",1,wlqdnu,,analytics,2022-08-11 05:38:17,1
Not difficult to use. The rules are just different from PowerBI. You will miss powerquery,2,wluqsf,"Looking to get familiar with Tableau based off of discussion in my last few working groups. Leadership currently have me using PowerBI, but they mentioned Tableau now as well. How hard is it to learn/use, and is there any free WBT for it?

Thanks in advance!",analytics,2022-08-11 08:51:10,1
Data Engineering is largely around writing code to move data around (and setting up efficient structures for doing so).,26,wldi6u,"I know R and SQL really well. At least I think I do.

Most of my classes in university were in R so that's what I have fresh in my mind.

I found a position a while back where I would make use of R in banking to transfer data, basically, but I didn't present myself well enough.

How can I find more positions like this?",analytics,2022-08-10 17:33:08,25
"I think you're approaching this a bit backwards. Your job as an analyst is to solve problems so the first thing I would do is to figure out what problems currently exist in your company. This could be anything. 

1) Tons of reports that no one really reads even though it has tons of good information? Include a summary in each report and a monthly check in with management.

2) Too much call center resourcing for the work being done? Create a predictive analytics model with company data and seasonal trends to determine expected capacity so those resources can be used elsewhere.

You probably don't have enough visibility to figure out what the problems are, so work with your manager to figure out what are critical problems that need solutions right now. Make an entire list and then see what can be solved with solutions you want to work on. Solving a person's problem is the easiest way to make them advocate for you and your work. Once you can deliver this value, new projects will come quickly.",7,wll2lx,"Hi,

For last two months I don't have a good project in-hand at work. Most of my other team members have been assigned some BI report development.

My manager had told that he would bring in a good machine learning modeling project to suit my skills but there has been nothing. 

My mind is now just trying to create work that probably isn't required -

1. I think about interviewing my report users for what can be improved in them.

2. I think of adding new views, but don't know if anyone even needs them.

3. I think of trying to solve random business problem using complex ML models. Don't even know if the problem exists.

My manager has told me to create something out of thin air essentially. Like do a demo project so that he can pitch to business teams that our team has capability to do something like this and that way we'll get more projects. I find this pretty difficult actually. 

I actually even created a simulation model for call center operations (capacity planning). But doesn't feel like a viable thing. And, our call centers are already outsourced so I have nothing else to do but assume the external organization uses better models for the same already.

What should I do? Suggestions please...",analytics,2022-08-11 00:14:07,2
You’ll never know enough. Once you get into work you’ll have problems that haven’t been solved before with data that’s never been looked at yet. Gotta learn as you go,21,wl0l4n,"I graduated with a statistics degree from a good school. 

I know R really well for modeling and visualization. I’m also a first gen so I have no idea just HOW well I do these things. 

I did my best during my classes but I’m just not sure bc I have nothing to base my skills off of.",analytics,2022-08-10 08:38:20,23
Price alone suggests no,1,wlg5o4,"Hello folks. Just hoping for a quick sanity check here.

In the context of digital marketing, am I understanding correctly that Segment CDP basically allows businesses to recreate the full functionality of all-in-one platforms like HubSpot by using Segment to synchronize customer data across a collection of more specialized tools?

Costs aside, I can certainly see great value from the standpoint of being able to swap tools at will. HubSpot for instance has loads of feature deficits that are available in other, more specialized platforms. Plus, from what I understand, Segment stores ingested data for a meaningful period of time, allowing users to load past data into new tools (unless I misunderstood of course).

Anyway, just hoping someone here can chime in to let me know if I'm on the right track towards making sense of Segment in the digital marketing context.

I appreciate any guidance that you can provide!",analytics,2022-08-10 19:39:40,4
Looks good 👍,2,wli24n,"Hello everyone, 

Hope you are doing well.

I’m trying to help a friend out in finding a job in analytics field in Dubai. Can you please advice on his resume? I will share it with you. 

Thanks a lot!",analytics,2022-08-10 21:15:00,1
"Is there a standardized campaign code system in place, and do the various systems that need to capture and associate codes to other entities have the required rules implemented? 

I'd expect this to be managed by marketing ops (which I assume is a dept or at least is a formal responsibility). If there isn't a standard way to generate codes and populate/capture campaign metadata, that makes things quite difficult when it comes time to centralize reporting while preserving the ability to separate along different dimensions.

Also, can you clarify what you mean by product and account? Is product the marketed product? And is account the target account for upsell/cross-sell? Or do you mean the provider for the respective channel?",2,wlbv7n,"So I recently started working with a digital marketing group who uses LI/FB/G/Bing Ads and we are relying on account names to distinguish which line of business (B2B/B2C/Etc...) the campaign belongs to..

The marketing team has a consultancy suggesting we should consolidate all of our accounts and it seems that we will lose the ability to properly report on each business line and respective ""product"" for each. 

Where can I go to learn more about the best standards with data modeling and analytics for digital marketing, specifically? I've googled around and can't find anything beyond the basic, which this is a bit beyond those articles.",analytics,2022-08-10 16:15:25,6
"To be a great analyst, you need to learn domain knowledge of the area of business you are in. And you need to be excellent at presenting and communicating business opportunities you find in your data analytics work. 

Most people you present to are not technical, so you need to be able to effectively communicate in non-technical terms.",70,wkes6t,"I currently work in powerBi, mySQL, and excel doing reports. I want to learn more and stand out. I started automating some reports instead of using powerbi every time. How to be the best of the best?",analytics,2022-08-09 14:09:59,25
What do you want to do. I don’t see value in getting your MBA to go into analytics or data science.,8,wklnmk,"I currently work at a University and said University will give me tuition for free. I’m looking to change fields - they have a wide variety of different MBAs and concentrations. I’m looking at the dual program for MSBA/MBA with a concentration in Business Essentials. I’m in the Student Affairs field now; managing people and dealing with issues that arise and solving them. Much more customer service based.

Is getting this degree worth it? Should I just stick to an MBA for now or just an MSBA? My undergrad degree is in Business Admin & I have a Masters of Education in Student Affairs.

I could get the dual degree in 2 years or just 1 in 1 year.",analytics,2022-08-09 19:24:23,5
Live exercises are ridiculous and not reflective of the real environment (where you will be using Google all the time). However I guess it cuts down on potential cheating.,52,wjoblz,"Why do companies have so many interviews for analyst positions??

I've been job hunting for a couple of months (trying to transition from manufacturing) and EVERY company has 2 interviews minimum with some companies doing 4-5 total. And don't even get me started on Amazon having 2 phone interviews plus 6 hours of in person interviews - total bullshit. 

And why the hard as fuck live coding challenges?? I am applying for an entry level analyst position. I shouldn't have to be a SQL god and be able to do every LeetCode SQL hard question to be able to get a job. It's also BS having to live code since I can't query an actual database and test my code  like I would at my current job. 

Jfc can I just have a job.",analytics,2022-08-08 17:00:25,52
Check your GA4 setup and make sure the tag isn’t over firing,5,wk3d9h,"Hi,

I'm migrating to GA4 but can't seem to figure out how sessions have changed.

In Universal Analytics one source had about 30K sessions but now in GA4 it says it has 800K in the same time period. That a gigantic difference and I don't understand what causes it, how are session sources marked now to make this happen?",analytics,2022-08-09 06:30:06,5
Best place is to practice on the database at your job,18,wjo91u,"Basically title.

I did a lot of data camp exercises but i feel i am still lacking too much in my job",analytics,2022-08-08 16:57:24,12
"This is due to batching method used by ga4. Cannot over-ride this but end result in event counts should be roughly the same.

If you mark the event as a conversion in interface it will bypass batching.",2,wjyl1i,"Hi,

I'm have noticed a problem with my GA4 set-up where the time needed for a page\_view event or hit to fire is about 4-6 seconds, where Universal Analytics takes only about 1 second or less.

Does anybody have any advice on how to tackle this problem?",analytics,2022-08-09 02:13:05,2
"You could try some of the following:
1) use ga debugger extension to check if displayfeatures is enabled in console 
2) check if Google signals in admin interface is turned off 

If using gtm:
3)  check if advertising features are manually disabled on gas variable
4) if your property receives hits from a feature like custom task, it won't inherit advertising feature settings
5) check if enable advertising features tick box is enabled",2,wjx1y8,"Did anyone else encounter this problem? I found some posts on other sites about people getting this problem but only persisting for 1-2 months, our site has received no demographics data since 4th of February. The Demographics and Interest Reports feature has been enabled all this time.

Does anyone know what could be the cause and what we could do to fix this?",analytics,2022-08-09 00:35:04,2
"Kind of the same as small business suggestion, but I got a good bit of experience doing local non-profit analysis. They likely could really use the help and it was always a nice talking point in interviews.",1,wjjonp,"I'm currently a rising Junior in college studying Business Analytics, what kind of projects should I work on to put on my resume? I've been told that reaching out to small businesses on my own and asking them for data in order to generate insights for them is an option, but are there any other options I should explore?  
I have experience in using Excel, SQL, and Tableau, and have created dashboards and presentations of insights generated from them.",analytics,2022-08-08 13:45:44,1
"Get a resume writer to fix up your resume and apply again. Cast a wider net, but definitely hire a resume writer to help you get interviews with your resume. If you applied to 80 places and haven’t gotten a response, then your messaging in your resume may not have been clear enough as to what you’re capable of and what you do.",3,wjigve,"Anyone on here have worked for Skillstorm? Looking for any input. For what I read,  they will train you for a short period (you have to have a little bit of role background knowledge though) and you can get paid for the training period but the catch is that you have to complete a 2 year contract.

A little about my situation.... I am about to graduate with a professional 2 year degree in Artificial Intelligence this month (switching careers from unrelated engineering). I had an interview with Google am even made it to the onsite but was told last week that they are pausing hiring and can no longer continue  with my application despite a positive first interview. I haven't got any other interviews (applied to around 80 jobs on LinkedIn) and I am thinking it is in part to the slow down of hiring in tech. I am in the Seattle area and a large number of companies with data science roles are tech. I will be without income at the end of the month and although my wife makes enough to just get us by, I am also going have my first kid towards the end of the year and feel the pressure of just landing any job right now to have better financial security just in case something comes up.",analytics,2022-08-08 12:56:46,2
Using unrelated data for take home tests is proof you're not working for free for them.,13,wjb9vh,"Just wondered about this while making food. Applying for a new job, and am on step 2 for a company in Oakland. It's a task to analyze water usage based on county. Data gives me states which county had what reserves and how much was being used. 

Has nothing to do with the final material, which is insure-tech and product development for a few insurance related web and app features. 

Just wondering if this is already an indication for a not so interesting role and what challenges may have come your way that you thought of like, damn that's a good challenge to analyze and figure out. Of course, if it had relevance to the final job requirements to see if the challenge was applicable. 

Just curious about people's experience and if this is already a good guiding factor in job search for an analyst. 

Thanks for your feedback. 

SD",analytics,2022-08-08 08:10:24,5
"This is a very interesting question, I’ve held quant roles for >10yrs now, and the funny thing is there is a wide range of positions quants occupy.  You have risk quants, quant developers, quant traders, quant analysts, etc.  All tend to have blended backgrounds and varying competency in Mathematics, statistics, programming and finance.  Some are physics PHDs that wanted to be in finance, other are computer science, others are financial engineers/mathematics guys with applied emphasis on derivatives and those models.

Quants run a wide range of positions is the point I’m trying to make.  If you have interest in a specific area, I’d look at roles and requirements to get those types of jobs.  Typically, business analytics programs lack the typical rigor that a masters or PHD in an applied science provides. It’s why most firms want engineers, math, or physics people instead of a finance degree for quant roles.",13,winww0,"Hey, I currently work for an analytics provider for investment firms. I got the gig mostly based on my data analysis experience in other career fields and coding experience.

I interviewed recently for a trading analytics role and didn’t get it. I’m not mad because I’m fine where I’m at, but that raise would’ve been sweet. The feedback was I wasn’t “quanty” enough. Any thoughts on how I can get there?",analytics,2022-08-07 12:31:02,11
"The article describes everything in sufficient detail, thank you. This is the work that has been performed by my company's sales and marketing department. I never managed to get free publications. Perhaps, there was something wrong with the letters we sent out. I would also like a list of the influencers who reviewed my product. Will this be possible to do?",1,wj0j6r,"Every company owner wants their business to be as successful as possible. To achieve this, they rely on various tools. Data-driven marketing is an activity aimed at promoting a company and its products with strategies based on data collection and analysis.

Data-driven marketing involves collecting and analyzing various data for studying the target audience, building a promotion strategy, making decisions regarding the advertising budget, etc. The necessary information is extracted from a variety of available sources: social media, search engines, websites, email newsletters, and chatbots.

If there is not enough data for analysis, companies conduct market research. All collected information must be structured and processed with the goal of filtering out the necessary data.

The data-driven marketing approach allows you to make decisions, build forecasts, draw conclusions, and study the market. It helps to optimize the budget, think over PR campaigns, choose the right communication channels, and interact with potential buyers or clients. Data-driven marketing is the key to building an effective company development and promotion strategy. Read on to learn more about the benefits of this approach.

Data analysis enables you to base your decisions on facts and not assumptions. These can be KPIs such as LTV, average check, CAC, customer churn rate, A/B tests and problem interviews, Google Analytics statistics, Yandex.Metrics, and many others. The data-driven marketing approach directly affects the increase in ROI. Check out the list of its main benefits:

* it allows making much more accurate critical marketing decisions;
* lets you personalize content and offers based on target audience data;
* improves potential customer segmentation;
* helps optimize marketing budget;
* heavily contributes towards building an omnichannel communication strategy;
* improves the quality of service based on customer behavioral factors and satisfaction data;
* facilitates the selection of the most effective advertising channels and creatives.

There are five main steps for building a data-driven marketing strategy. You can follow this step-by-step guide:

1. Set goals. Conduct research and analyze where your company is currently at, what direction it is moving in, its main competitors, as well as your strengths and weaknesses. Set goals and prioritize them based on all of this information.
2. Make a list of the required data. Analyze the set goals and write down KPIs to track results. Look at what information is available and which data is missing, then think of the ways to learn what you need.
3. Define data sources. These can be interviews, Google Analytics and Yandex.Metrics reports, CRM system statistics, emailing services, etc.
4. Gather and structure the required information. Store it in one place to make analyzing data more convenient. Use spreadsheets, charts, graphs, heat maps, and other visualization formats.
5. Analyze the gathered data. Once everything is collected and structured, proceed with studying it. Analyze metrics, draw conclusions, and implement the necessary changes. Continue to monitor key statistics regularly and compare results.

Data analysis allows you to keep track of critical business metrics and, based on their alterations, improve your marketing effectiveness. Start implementing data-driven marketing today to achieve your goals!

## How can you do it yourself? 

Our company works in Data Driven Marketing, let us take a closer look at this process. You can then use this information as a basis for your own research or to choose the right specialists for the job. Services like this primarily depend on formulating the concept and putting the concept plan together. Subsequent project promotion includes influencer management, i.e. purchasing ads and promoting a project via influencers and social media publications. We specialize in promoting via social media, such as Bitcointalk and Reddit, but there are also promotion options for other social platforms. These networks allow you to post extensive publications to talk about your project, such as via Medium and others. One can also work with magazines and other publications so that the information about the project catches as many eyes as possible.

In addition to Bitcointalk, Reddit, and Medium, you can secure free posts from influencers and the media, such as articles about your project. We contacted journalists for free placements on one of the latest NFT projects. This is where the concept plays a significant role. One must develop and showcase the idea at the core of the project so that the media people would want to write about it on their own, which requires a well-presented concept plan to maximize project attractiveness. In addition, we prepared personalized messages for every journalist we were planning to contact by learning as much as possible about them in advance.

The messages consisted of 2 parts:

1. First, we demonstrated that we held the journalist's views in high regard and expressed interest in their opinion;
2. Then we discussed the project from an angle that would pique this particular journalist's interest.

We were looking for an intersection of interests between a particular journalist and the parties involved with the project. Once we identified the correct angle, it greatly raised the probability of securing a free placement. Since the journalists we hand-picked were already interested in similar topics, they were willing to discuss the project free of charge.

Implementing such a process requires a pre-made database. Data collection in general is a critical component for Data-Driven Marketing. In this case, we collected data about journalists and their articles so that we could turn to specific authors who had previously written about similar projects. You can assemble such a database yourself, but this may require a lot of time and effort. The Internet is vast and full of possibilities, so you should always look around and assess your options, weigh down their pros and cons, then make a decision that's best for your business.",analytics,2022-08-07 22:27:44,4
The hardest part about this will be collecting the data. Imo the effort won’t be worth it.,22,wiijbj,"Hi. My parents are very picky and looking for a home. Any pointers on how I might acquire started to look at big datasets to find possible locations that match all the points below, and others? Where would I find relevant data? How do I query it and join it together? Not looking for step by step instructions -- just some information to get started. They are looking for a place where all the following is true: \- X distance from a Costco \- houses that have 5+ acres \- must have 3-4 supermarkets within 20 minutes (10 miles?) \- X distance from Amtrak or other train station \- houses built ⠀after 19YY \- show the income tax / show the property tax \- some kind of measure of traffic \- and more....",analytics,2022-08-07 08:36:53,14
"Try to do that, you will realize how everyone in the company hates self serve dashboard so much",23,wid36q,"Has anyone created a portal where users can see all tables(facts &dims). Drag and drop them to generate a query(auto join based on keys ) and download data in excel etc ?
If yea, what is the tech stack used ? Can this be possible to embed a BI tool ?",analytics,2022-08-07 04:00:02,30
"Should be a very important discussion, especially after the CNIL decision that using GA is illegal. 

But then again it's still shuch a grey area. Europe doesn't want their data within US territory and US doesn't want to give their right on spying on everything they can put their hands on. It's hard to see how they'll find a middle ground and make an agreement.

Which is a pity, since no other GA alternative (the ones that comply with GDPR) come even close to what GA offers!",3,wihr57,"Users being required to give consent to a company for collecting their user data is now a requirement in the EU with the GDPR policy.

It's an big shift in the industry. Previously we could track everyone's user data and no one would bat an eye.

But Google Ads and Analytics has recently reinforced this law, and they now require you to have this in place in order to use their services.

Having this cookie consent banner or pop up requires  users to approve or reject.

If they choose to reject it (which most do) you will have much less data user data to analyze. 

Data clearly shows that most people reject this consent, so therefore I beg this question:

How does digital marketers nowadays adapt to this shift in the digital advertising and analysis space?

Are there perhaps good alternatives or supplements to GA4 in the cookie-less future?

It's an important discussion in my opinion, grateful for any input.",analytics,2022-08-07 08:02:43,3
"Hey! Another mobile game analyst here (EA mobile now, was T2)!

I’ve worked on a number of games, some I have liked more then others. You don’t have to love mobile games. But at least play the game you work on. For work! It’s super important to know the product, really helps with analysing a feature/test. 

You can even do it on work time, as it is work! Playing competitors games can also help. 

The team I work with it great and we all have one thing in common; we are all passionate about making the game better.",10,wi8he5,"I'm currently working as an analyst for mobile game company, however, I've never been a mobile gamer. I enjoy working with my co-workers and I feel appreciated, but I think its important for me to be passionate about the product that my company makes. I feel like I would be much more passionate if I work for bigger game studio company such as Take-Two or Bethesda, since I would support something I like.  


For gamers who work as analyst in game industry, what is it like?",analytics,2022-08-06 22:58:13,24
"Get the data entry job first, do it well, then figure out how to do the rest. Don’t try to run before you can walk.",5,wieilq,I have an interview tomorrow for data entry but I really would like to morph into Data Analytics experience as well. Does anyone have any tips on how I could approach the boss a few months down the road to offer those types of services and potentially get a raise?,analytics,2022-08-07 05:23:34,5
"You are not entirely wrong, you can easily be a data analyst without ever touching R or Python. The true value, in my opinion, comes from the flexibility you get from your coding. Secondly, the other methods are limited in their capabilities so if you get beyond basic analysis you’ll want R/Python. It’s also just fun!",44,whonvm,"I don't mean to phrase this question in a bad way at all. I'm VERY optimistic about learning both R & Python. 

I see a lot of people saying Python is ALL they need for data analysis, for example. 

I just want to know what makes them so good as analytical languages? 

The work that can be done with them can be done in Excel, SQL, Tableau, PowerBI, without learning a programming language, right?  What can they do that other, I guess, ""easy-to-use UIs"" can't do? 

Statistical analysis to my knowledge can be done in Excel & even Tableau.

Is it automation? Something else? 

I think I'm missing something. 

Again, I'm not being negative, I just actually have NO IDEA of the benefits.",analytics,2022-08-06 06:44:10,49
Quick note. That is going to be captured as PII in GA and you might get flagged by Google and have your account locked.,11,whbus9,"Hi all,

I am setting up UTM tracking to identify who specifically clicked my email that includes UTM links. I was thinking of using \[\[email\]\] where (in the hopes of) a person who received the email will click on this UTM link and the medium populates the persons email as the medium rather than a fixed medium. Not sure if this can work but wanted to see everyones thoughts on whether this would work or is there a better easier way of doing this?

?utm\_source=google&utm\_medium=\[\[email\]\] - sample UTM tag i wanted to incorporate - would \[\[email\]\], {{email}}, $email be the parameters or am I dreaming here?

TIA!",analytics,2022-08-05 18:19:40,11
There is not as much overlap as you think.,21,wgqibr,"I’m interested in these 4 fields.

Would you say it’s like a Venn diagram where the are all kinda linked by overlaps?

What would I do if I want to get experience in at least 3 of these? Would you say there’s a career that implements 3 or 4 of these choices?",analytics,2022-08-05 02:03:43,38
"1. Learn SQL
2. Learn a reporting tool such as Power BI / Tableau. 
3. Learn SQL
4. Get a beginner understanding of Stats (Quartiles, knowing groups can't take an average of an average, etc)
5. Learn  SQL

Bonus:
1. Create a LinkedIn account and start following some experts in the community.
2. Become an expert at SQL.
3. Volunteer your skills for free at a DataCamp or other local orgs. You need some examples to show on tour resume.


If I haven't mentioned it, you really must learn SQL.",34,wh5cm8,"What if I don't have a data analytics degree?

What if I only have an IT degree?

What if I only had a marketing degree?

What if I didn't have a degree at all but I got IT certifications and google analytics certifications?

How can I become a data analyst? I only have customer service experience and a useless associates in arts. What can I do right now that can get me a data analytics job? Do I really have to go back and do college for another 4 years? Is there a way I can break into the industry WITHOUT a degree? Or if I really don't want to go to college, am I best off trying to find a different career path?",analytics,2022-08-05 13:22:07,25
What kind of question is this? All bots need training. Check out openai.com if you wanna see the latest and greatest.,2,wguodl,"How far is the advancement of chatbot? I went to purchase a laptop online and was (not) surprised that a bot started messaging‏‏‎‏‏‎‏‏‎‏‏‎­me asking for help - giving the benefit of the doubt that maybe it's just bad UI, and not really a bot, I started asking some questions on what I ""feel"" I want to search - well what do you know, the bot started giving me some arbitrary answers like that Word Clip from Microsoft Word. But seriously, has it advanced really that far with the advancement of data science and analytics?",analytics,2022-08-05 05:50:28,1
"Analytics as a career in which industry? The closest given your experience is “experimentation” analytics - and I’d recommend banking on R or Python. DataCamp is a good place to start on how to measure experiments and conducting hypothesis testing using those scripts and their respective libraries. You should also include SQL in your skill set. In summary:
* R + SQL
* Python + SQL",5,wgkzhl,"I’m graduating with a MSc in experimental psychology, and I have grad level training in research design, univariate, and multivariate statistics using SPSS. I would like to get into analytics as a career- I realize that I need to learn SAS, R, etc. where should I start, what courses should I take online, and do I have a solid enough background to get into this field?

Thanks so much in advance!",analytics,2022-08-04 20:43:40,6
What’s the rush? Just do the Google courses first and enjoy it. Gradually take on more new learning and pace yourself. Cramming will not help,54,wfy5h8,"I'm doing the google data analytics course and also learning SQL

i'm trying to also learn Excel skills but everything is so overwhelming i feel so dumb and stupid

I;'m trying to become a data analysts

should I give up?",analytics,2022-08-04 03:53:32,65
"When I was applying for my first analyst role the project I had was just on a simple store reviews dataset. I remember I created it with tableau public using a single csv and it consisted of a dashboard and then a medium article I had written about what I found in there past ""store 1 did 50k in sales. Store 2 did 35k in sales etc."" such as how it could be used. It had the gender of the person submitted their review as well as a department so I compared gender reviews side by side per department to see if one or the other was putting in substantially more negative or low reviews in a particular department.

Basically they want to know if you are actually analyzing things and not just slapping a bunch of aggregates onto a dashboard. So go into your thoughts on something and thats where the famous ""pick something you're interested in"" comes from. If you know about basketball, make a dashboard showing why x player is as good as people say or maybe they aren't and its hype alone, make something that displays that.",3,wg9e2h,"I am going into my 3rd year at university and starting to look for an internship for next summer. Recently I have been polishing my resume and have not got an interview. I have 3 projects for now, 2 for python and 1 for SQL(with Tableau Dashboard), and I am including all these 3 on my resume. Considering that I have only one non-data work experience, I am thinking about doing more projects. May I ask how many projects are considered enough for applying for a da internship? Also, do I need to do an Excel project, or are SQL and Python are ok to showcase my skills? 

Btw, I also have created a website for my portfolio, but it seems I do not have enough projects to post.

Feel free to share your ideas!",analytics,2022-08-04 12:05:17,4
"The general advice is to create your own datasets for projects, because Kaggle datasets already come all cleaned and ready for you. And because they aren’t unique—and being unique/standing out is one of the primary goals of your resume.",13,wftetp,"The whole summer I practice my analytics skills by downloading a few datasets on Kaggle and Forage (free Virtual Work Experience Program site), cleaning and visualizing those datasets on Tableau (I visualized like 10 of them), and building 1 or 2 predictive models. I am wondering if these extracurricular activities would look good on my resume?",analytics,2022-08-03 23:01:35,6
"Pay?

Edit: I’m not interested but everyone I going to want to know",25,wfc49h,"Hiring! Healthcare analysts interested in the hard-to-find bridge position (analyst to DS).  Or you’re welcome to remain a senior analyst as long as you like. 

Who am I: 

I’ve been involved with the r/datascience for 7 (8?) years and been a moderator for 4(?).  I’ve been in a official DS role since graduating with my MS in 2013.  Most recently, as principal data scientist, I helped the last start up I worked for to make a big exit and there’s a great chance we’ll do it again with this company. 

Where do I work:

I’m not going to give out a ton of details publicly, but we’re a healthcare analytics company.  Our clients historically have been providers but we’re now moving into additional verticals.  At a time where major tech is starting hiring freezes, we’re growing.  Honestly, the culture here is better than anywhere I’ve worked in 20 years. 

What’s the job:

I’m looking for a few analysts to work in my R&D department.  Our clients will primarily be the CEO and department heads for new and existing divisions.  The analytics work will require us to support investigatory efforts around product ideation and market fit.  Our data science work will revolve around turning potentially interesting business problems into proof-of-concept models. 

Who I’m looking for:

** I don’t really give a shit what your education level is**

People that ask good questions. 

People who are curious. 

People with healthcare claims experience (huge plus)

People with formal analytics experience. (A must)

Local to Nashville is nice but I don’t live there so remote is on the table. 

As I said in the title - I view this as a bridge position.  How do I transfer from analytics to data science?  With a job like this.  Your primary duty will be writing SQL and meeting with stakeholders to understand what questions they want to answer.  

You will not be on an island - I actually enjoy developing employees (it’s why I’ve been in the sub for 8 years).  My plan is to increasingly involve you in the DS work we’ll be doing.  **If you just want to do the analyst work that’s cool too**.  The idea is that at some point down the road you could transition to our DS team that’s focused on engineering or stay with me working within R&D. 

We absolutely will be working on novel, cutting edge solutions unlike anything in the industry.  I’ve been in the field 9 years and this is the most excited I’ve been to crank out new stuff. 

Comment below and/or send me a DM and maybe we’ll end up working together.",analytics,2022-08-03 10:09:52,56
"This may be useful 

https://www.reddit.com/r/analytics/comments/q38tus/what_does_a_pricing_scenario_excel_assessment/?utm_source=share&utm_medium=ios_app&utm_name=iossmf",6,wflfpd,"Hello Everyone, 

I have an Excel test coming up for a Pricing Analyst role i''m interviewing for. Just wanted to ask, is there any type of questions i should be prepared for ? Additionally, if any one is aware of the format that these type of test usually come in, specifically for Pricing Analyst or any role that relates to it that help me prepare for the test. 

Thank you in advance !",analytics,2022-08-03 16:26:29,23
"Mine, for entry level at least, have been leetcode easy for python and SQL easy to medium. Need to know how to use all basic data structures Python, as well as CTEs and most join types in SQL. Also, exams have been timed, with and without an ide.",5,wfb22t,"Could anyone enlighten me on the content of the Hackerrank BI Analyst test? I have solid SQL basics but my Python is… meh. Most of what I see online about hackerrank are really SWE type questions, nothing at all to do with what a BI analyst would ever do in real life or even in his dreams.

Asking to know on what to study up !",analytics,2022-08-03 09:28:48,3
"Yeah, I'm a data science enthusiast and just do it for fun and something to blog about. It helped me land a job as an analyst on a data science team too. Blogging about it has landed me a few side gigs too for a few extra bucks.",5,wfn537,"Hi, do any of you guys ever just do data science as a hobby, or started out as such and transformed that into a professional career? If so, what were your experiences like with it being more of a hobby at the time?",analytics,2022-08-03 17:43:53,10
"df[‘col’] = df[‘col’].str[10:]

Edit:

If it’s a date time column, you can either convert to a string and then perform the above, or parse out the time. Top of my head, something like df[‘col’].dt.time, but there’s good documentation out there.",4,wfnajs,"**Column A = 2022-05-17 13:12:30**

&#x200B;

How can I delete the first 10 characters in column A above please? Either in SQL or Python",analytics,2022-08-03 17:50:49,4
"Yep! Number 1 reason I use Bigquery is because you can use Google Sheet tabs as tables. Just create a table in Bigquery, select Google drive as the source, Google sheet as the file type.",1,wfh5ic,"I am taking inputs from my org in multiple google sheet tabs (50+) and I want to aggregate this data in bigquery.

Is this possible?

The reason why I want to do it this way is because I don't want 15, 20 people editing one tab",analytics,2022-08-03 13:29:37,3
"Can you edit the post and in the bottom just list the variables and their respective values and then show some example calculations. I know you mentioned a lot of them in the text but also just write them again below the paragraph and it would make it easier to digest

With all that being said, it sounds like it’s the right approach  but I would like to get the take a few more redditors",4,wf5gq5,"Hi everyoneWould need you for a sec!I am working on a Deal Funnel Analysis (CRM=Hubspot) to calculate the Salescycle and Conversion Rates for our Business Development Team - I am not 100% sure if my reasoning is right though!

**Context:**I am extracting all the data points from our CRM Hubspot to get the timestamps when a deal moved through the funnel - from one stage to the other (Column1=Deal Stage Entered; Column2=Deal Stage Exited). In the next step, I have to separate the data points, as we have deals that entered and exited the stages + deals that entered but did not exit the stage, which is then considered their final stage of the snapshot (Hubspot sets a default date 31/12/9999 for those deals).

**Calculation:**After separating the data, I count the deals that moved and the deals that are “stuck” in their final stage. E.g.: Moved deals = 200 and Final Stage Deals = 500. Therefore, 700 Deals have been in the respective stage, and 200 of them moved to the next one (next step conversion rate = 40%).I also need to calculate the conversion rate from the first deal stage to the stage, where the potential client becomes a customer. Therefore I take the number of deals “stuck” in the last stage (Customer) as they entered this stage and did not exit the stage (e.g., to the stage “churned”) and divide it by the total number of deals (moved deals + stuck deals) in the respective stages. Questions I have:

1. Does the separation of the data make sense?
2. Is my reasoning right when calculating the conversion rates?

I hope it’s clear and you can help me! Looking forward to your feedback!Thanks a lot  


Edit: Adding the following example to make this post clearer, thanks for the feedback already!  


1. 1st Stage = 200 deals exited the stage
2. 1st Stage = 200 deals entered the stage but did not exit the stage (considered as “stuck”)
3. Total of deals that are or have been in the 1st Stage = 400
4. Next step conversion to 2nd Stage = 50%
5. 5th Stage (the “Live stage” = Customer) = 7 Deals that entered, but did not exit
6. Conversion rate of 1st Stage to 5th Stage = 7/400 = 1.75%",analytics,2022-08-03 05:34:38,6
"Consider applying in other countries like UK, GERMANY, Australia ETC US is not the only country. Trust me !",4,wfakrl,"So, my F1 visa for the states is rejected today. To be honest, I applied to the business analytics program to learn skills. Now the thought came to me that I can complete that degree online as well. But I am not sure how it will be perceived in the market. I hope someone here can guide me in this regard.",analytics,2022-08-03 09:09:57,10
"Yes, you can send events to GA4 using the global site tag: https://support.google.com/analytics/answer/11147304?hl=en#zippy=%2Cglobal-site-tag-websites",1,wfa0xv,"I have multiple websites using tel: for click to call.  I've seen tutorials on using GTM to set up specific link tracking, but this would be an extensive way to scale. 

Currently I'm using MonsterINsights free plugin which adds a ""tel:"" event in GA4, but I can't figure out how to create this on my own without using MonsterInsights. Is there a way to set this up in GA4 without using GTM or MonsterInsights?",analytics,2022-08-03 08:48:27,3
"Sorry to ask the obvious, but is your GA4 property connected with your Ads account?

Also, do you have autotagging enabled on your Ads campaigns? If not, are you adding UTM parameters to all of your links?

If the GA4 property isn't connected, your Ads data won't show up in the GA4 Advertising reports ([more info here](https://support.google.com/analytics/answer/10607798#zippy=%2Cin-this-article)). But if you're using autotagging or at least adding UTM parameters to your links, your Ads traffic should at least be showing up within the ""Paid Search"" and ""Display"" channel groupings in the regular reports.",1,wf8js6,Is this a bug or a convoluted way to make it work? Do you have good tutorials?,analytics,2022-08-03 07:48:57,4
"I use LinkedIn and try to focus primarily on Easy Apply. Unless it's an absolute perfect role, don't waste time on systems that require you to re-type your entire resume (or put ""see resume"" for all of the fields).",22,wejo4a,"I've mostly been using indeed to search for job postings for entry level analytics roles; but I am unsure if there are better platforms to use. I have a linkedin but haven't used their jobs section much, I also don't know if trying to connect to a recruiter is something that is done in this field. I am in the US, but am open to jobs elsewhere, or fully remote so not sure if there are things that work specifically well in those cases. Lastly I am also open to looking into things such as internships or freelance work. Overall just trying to do anything I can to get a foot in and unsure where best to focus that effort and time, and which are a waste.",analytics,2022-08-02 11:47:19,21
"IT manager to DA/BI would be a step down, and 3 steps down in pay.  Just my 2 cents.",3,wes7ml,"Right.  I am 50.  I am an IT Manger making descent money, in the midwest.  But I am not 100% that my current company is my last stop.  I think I would like to move up, to a higher role, different organization. 

Regardless as to whether I continue here or there, I do want to start doing something along the Business Intelligence route.  I recently tripped over PowerBI, and I am fascinated.  I am enjoying myself in a genuine way.  The old feeling of achievement, after doing something you have never done before.  Bu t I also know this is the tip of the iceberg.

I am 22 years in IT.  4.5  years in my current roll.  I have a background in Support to Systems to Management.  Kind of a jack of all trades, master of none.  I have a BS in CIS, and have done some training in networking, have done some programming.  Not deep in Databases.

Any thoughts on how to proceed?  Masters from WGU?  Or a cert program?  Where to focus?  Even what kind of questions to ask?  

&#x200B;

Thoughts?  Thanks!",analytics,2022-08-02 17:43:31,4
"My current work is mostly based on Excel, but I struggle to find similar roles in the market.
I generally find much more job postings for SQL and Python data analysts.",16,weblok,"I recently interviewed for two different analyst roles and managed to get an offer for: 1) Risk MI Analyst and 2) People data and insights analyst.

The Risk analyst position pays better with the added bonus of 1 day in the office a month and good benefits. However, it is a newly formed team and it will be all based on Excel. There is use of qliksense and sharepoint

The other role is also a new team and pays less with more travel (twice a week) but uses power bi, excel, tableau, as well as other HR tools

My question is:

If i took the risk analyst role would i have trouble in future to find another data analyst role with just excel?

If I took the people data and insights analyst role, how hard would it be, say for example; to get a different kind of data analyst role not related to HR? 

Note - I am new to analysis having moved from a non data role. I have used power bi and excel for reports in my previous role and took a university course on python. But no hands on experience with SQL or tableau.

Sorry if it's an obvious choice. I just don't want to be tempted with more money and less travel and then find out later that I am screwed if I try to leave later down the line.",analytics,2022-08-02 06:23:43,32
"Of Excel, SQL and Python, in a data analyst role SQL would be the one to focus most of your effort for now. With Excel as long as you can filter data and do pivot charts, pivot tables and regular charts you’ll be fine.

Python comes in handy for data science.

Perhaps add cloud skills, like Azure data bricks.

P.S. I have a bachelor degree in mathematics and master degree in finance. You should be fine.",4,weydxx,"Hey, everyone!  


Planning to switch to data analysis, however, I don't have any special degree for that. Is top math masters degree is fine for entry positions (internships and so on)? I don't have any experience in this field, but I am a fast learner, can solve complicated problems and think analytically (that's what my good math degree shows, I guess?). Also I have some knowledge of excel, SQL an python.  


If it's now enouph, what should I do? Should I get any certificates?",analytics,2022-08-02 22:50:46,2
"Hi, for Logistic Regression explainer I suggest the SHAP library. 

It is really helpful to explain how the values contribute to the label. If you use an Auto ML library such as pycaret it is even easier, because the library is built-in. Happy to help more if you need further details. Cheers",1,weuj62,I just built a logistic regression model in Python and would like to show it in the ExplainerDashboard. I imported the RandomForest Regressor but not sure if this is right for a logistic regression model. Does anyone know the right package to use?,analytics,2022-08-02 19:32:56,1
"/r/datasets

but why not build the whole model from the single csv?

A fact table, then you take the descriptive dimensions of it and turn them into lookup tables using the unique values and an ID.

Being able to do that is more real life oriented than finding a multi-table dataset in the wild.",7,wdxqhv,"I have an assessment to do for a data analyst role and it involves making a dashboard to present like im presenting it to a client. The main problem im having is that in the requirements it has 3 tables minimum for the underlying datasource and all I have been finding are singular csv files or ones that would be a union instead of a join like separate months etc. in kaggle and data.gov.

Is there somewhere specifically made for datasets that you have/can join together?",analytics,2022-08-01 17:54:55,11
What app are you using?,1,wdt3k3,"The site I'm reporting on has several pages displaying rental properties, with URLs :   
domain.com/for-rent  
domain.com/for-rent?/page=1  
domain.com/for-rent?/page=2  
domain.com/for-rent?/page=3

The report I'm running is destination pages and users. The output lists the pages separately as shown in the list above. How can I combine all of the for-rent pages into one line of reporting?  


Thanks for any help and advice.",analytics,2022-08-01 14:30:29,4
"Bruh with numbers like that, every program in the country is within your reach",9,wdv689,"Hey guys a little background about me:

\-About to be a senior this fall doing my B.S in Psychology from a Texas state school, was pre-med but want to change careers

\- 3.92 GPA, 334 GRE (164V,170Q) (Should be equal to 770 GMAT), no experience in DS/analytics, have taken a lot of science classes like physics, bio, chem, etc. Have taken a couple calc + stats classes, and done well in all of them. Intermediate knowledge in Python.

My goal is to eventually work in a big tech company like FAANG+ in DS/Data analytics. 

Can anyone recommend me DS Masters/Masters in Business Analytics programs that are within reach for me as someone who is not coming from a super technical undergrad? For example am I completely out of the running for MIT MBAn etc because I am lacking in internships/technical experience in this field? Ideally I need a program that allows people that are changing their careers like me and has good on campus recruiting so I can get an internship at a tech company to start my career.

Thank you!",analytics,2022-08-01 15:58:02,6
"Answer: Likely due to session expiry, this is also a 'thing' on Google's very own implementation for the Google Merchandise store demo account so we're stuck with it.

See comments on my duplicated post on r/GoogleAnalytics",1,wdj9ue," Hi guys,

I've had a GA4 tag running for about a month now, client's site gets a lot of traffic > 50K sessions/month.

Doing basic QA against the UA tag. I'm noticing quite a few ""*landing page = (not set)* in GA4"". At least 7% of the landing pages are (not set) and that in turns takes position #1 on a basic landing page exploration report.

I dug a bit deeper and I'm noticing that the associated *new* users are basically 0. Checked the associated events as well, 95% of the events are either **session\_start** or **user\_engagement.**

Is anyone else seeing this in their reports? Seems like a session expiry issue...

repost from /r/GoogleAnalytics",analytics,2022-08-01 07:46:34,1
"I measure it based on how quickly and effectively I can get business value out of any particular tool. 

If I can do it so quickly it’s like the tool wasn’t used, I’m expert. 

If I’m quick but it takes some time to look up how to use the tool, I’m intermediate. 

If I spend more time dealing with the tool than the problem, I’m beginner.",34,wd3ad5,"I understand different people may have different opinions on their 'levels' in these tools, but I just wanted to get a general consensus on what people use to define their levels of proficiency. I feel like when I'm trying to determine my level, I'm just throwing a random number/label out there.

If you want, you can add in other useful tools,

Example (Please edit the excel one if you think I may have misplaced something.)

* **Excel:**

*Beginner* \- Knows how to link cells, do basic cell aggregation

*Intermediate* \- IF, v,h,xlookup, Index+Match, etc

*Advanced* \- VBA, PowerPivot, PowerQuery

* **SQL:**

*Beginner* \-

*Intermediate* \-

*Advanced* \-

* **Python:**

*Beginner* \-

*Intermediate* \-

*Advanced* \-

* ***Tableau:***

*Beginner* \-

*Intermediate* \-

*Advanced* \-

* **R** 

*Beginner* \-

*Intermediate* \-

*Advanced* \-",analytics,2022-07-31 17:22:54,16
"Don’t underestimate the power of simply understanding the drivers of business and the needs and challenges of your stakeholders.  Not every organization and certainly not every solution requires advanced analytics / ML / heavy stats.  

That stuff would be good to learn/know about fundamentally so you know which cases it might be useful in (and then collaborate with data scientists), but most of what businesses need is simple.  

But your skill set might make you a great product owner or  analytics / business / finance manager in that you know enough about the technical aspects but can also communicate both to those types as well as to senior leadership and non-technical stakeholders.",8,wd6owi,"Hi Everyone,

I recently switched to an analytics consulting team within my firm after working as a CPA and absolutely love it. I'm super happy to finally have found something I'm passionate about and definitely want to continue down this track.

I am a bit confused so far, however, about what to do next or how to progress from here. Now that I have a year under my belt in analytics vs finance, I have some ideas on what I like/dislike and mostly wondering if more experienced folks could help me figure out what path I should be taking:

My favorite part about this job is organizing data, collaborating with stakeholders/business function leaders to understand it, and then creating dashboards, visuals, or reports to communicate what it means. I actually don't completely hate ETLing like most and think it's kind of an intellectual puzzle to create a script or steps in PBI to load things more effectively. That said, by far my favorite part is when we get past that and are doing the analysis piece (obviously).

The thing I'm concerned about in this industry is that I don't have a PhD in Stats from Stanford/Berkeley and am basically just a lowly state school accounting grad. I got transferred in as a Manager, even though my staff is full of STEM grads, because I have a lot of business sense and immediately can understand and dig into key drivers we should be analyzing. That, coupled with prior experience working with clients as a CPA, and knowing how/what to communicate with why they took a chance on me.

Even though I've done well, I'll be candid that staff on my team would kick my butt when it comes to writing DAX and certainly when it comes to building predictive forecasting/basic machine learning algorithms. I can code in Python and know generally what these algorithms do/how they operate, but they're way better at knowing how to de-bug data preprocessing and doing the actual work. Further, I can direct my staff on what we need to put in a dashboard and can help collaborate with them on the DAX needed to create more advanced visuals, but they're better than me at these technical details.

My question is:

1. What would be the usual career trajectory/path for someone like me in analytics who only knows SQL, very basic Python (i.e. just numpy, pandas, seaborn/matplotlib and Excel scripting libraries), and PowerBI? I am not dumb when it comes to math and am willing to do the self-study needed to understand what's happening, but I just can't compete in intellectual horsepower with guys that are EE/CS grads or PhDs/masters in stats.

2. As I mentioned, being a business guy, I love communicating results and collaborating with teams to drive results at a company. As such, I know data engineering is not where I want to be. Data science seems ideal, but, building on point 1, I might not be quantitative enough for those roles. If that's the case, can I climb to a jr exec role as just a data analyst/business analyst who's really good with soft-skills,  communicating past/present results, and knowing just enough about more advanced stats/CS techniques to communicate with data science folks?

3. This is a ""my firm/team"" problem, but if I stay in consulting for  a few more years, are there firms that work on longer-term projects? Right now, I'm working on a ton of M&A deals and supporting that group (the problem with transferring internally is they stuck me there because of my CPA background) and every project is a high-burn 2-3 week gig. It's super exhausting and I barely have time to think/do in-depth analysis with these deadlines. I don't hate consulting, but I want to work on projects that are 3-6 months long so I can actually do a thorough job. Are there firms that do these types of projects in analytics or is everything pretty much just a 3-week sprint in this world?

Thanks for all the help in advance!",analytics,2022-07-31 20:08:57,2
"From my understanding from stats class, when modeling repeated measurements, fixed effects are what does not change for each measurement, such as the subject being measured, and random effects are what does change, such as the measurement number (i.e. measurement 1, 2, 3, 4).",1,wdaj6z,Can someone explain to me the difference between random effects and fixed effects from panel data regression ?,analytics,2022-07-31 23:45:52,5
"I think it depends on what you’re trying to get out of the tool. You can see a lot of basic activity with just page views, but you might already have this with another tool. My recommendation would be to focus on one area of the platform, like one type of workflow the user goes on. Depending on what your product does and what your questions are, you might be able to get away with a few milestone events, like a button that begins or completes a workflow. Once you go through this process once or twice, I think it will be easier to understand what you want to track across the product.",3,wd5pwp,"Hi All,

Currently trying to implement Mixpanel for our B2C product at my company and don't have too much experience in this domain. 

In your experience, have you found it more beneficial to track every event (page view, button clicks, file download/uploads etc...) or only specific events leading to goals?

Thanks!",analytics,2022-07-31 19:19:36,8
"Code Academy or Udemy have better courses and cheaper that will help you get an intro.

Those college ""courses"" are meant to be a cash grab to supplement falling MBA and MS enrollments over the last few years",3,wd713v,"Does anyone have any experience with places like Noble Desktop and NYC Data Science Academy? These places are offering week-long 9-5 classes for a total of 40 hours of instruction. They also offer weekday classes after business hours. **I'm not looking to become a SWE or anything, I just want an introduction to programming.**

I'm the kind of person to need a structured environment to properly learn and I want to make sure that these places will actually give me a basic foundation about programming.",analytics,2022-07-31 20:26:18,6
"Once hired, develop your relationship with your boss and work with him/her on what you need to do, etc",3,wbz1xl,"Hey guys I just got hired as an analyst in the health sector, I'm wondering what kind of advice I need in order to get a somewhat impressive start within the company.

We'll be using python, R, SQL and Excel, and it is pretty much what you'd expect. I'm just a bit nervous since it'f my first jobs outside of university",analytics,2022-07-30 07:41:44,14
From this post and your 1 reply so far it sounds like this is going to be a data entry/cleaning/quality assurance type role.,8,wbzx9f,"I interviewed for an entry level data analytics role yesterday and while they seem to be interested in me, it seems like they want me to join the data hygiene team instead. Does anyone have any idea what that could mean? Is anyone else out there on a data hygiene team? They mentioned Salesforce as well. Will the skills I learn there be applicable to a data analytics role?

some background: I’m currently making a career change from a very different industry- I did take a few courses (google data analytics certificate) but I understand my knowledge/skills are limited compared to a fresh computer science/STEM college grad.",analytics,2022-07-30 08:22:53,7
"If you're starting your journey with Procurement, I'd suggest keeping things simple. Find out what data is available. Make sure you understand your inputs before you start analyzing them. You presented a broad range of tasks so you will need to prioritize.",5,wbsntt,"I just got an internship in which I am going to work with data related to Procurement. My goal is providing insights, improving process, reducing costs, changing suppliers, building supplier portifolio in other countries, and so on. It is a new role, só I need to create things from zero. My excel level is intermediate. I would like to know, what should I start studying? Excel, python, R...? And most important, what kind of analysis will I be doing? Like statistics involving regression, hypothesis testing...? Or what? Could someone help me, please? Thanks in advance!",analytics,2022-07-30 01:29:38,1
"As a head of analytics, no it is not saturated by any means.

What is saturated is people talking about building reports, pipelines, tech.

What isn’t saturated is making true incremental impact, knowing how to do structured thinking and story telling, producing sound evidence based recommendations.

Do the bottom well and represent it on your resume properly and you will never find the field saturated.",40,wbh7fd,"Do you think that more people are going into the field than there are jobs available? I'd like to look into a career change, but am worried.",analytics,2022-07-29 15:12:19,13
"Interviewquery.com

Also I re do some data camp sql courses.",7,wbdxwi,I just had an absolutely terrible one. It was so bad I'd bet the interviewer thinks I was lying on my resume. I went blank a lot on everything I studied in sql. Also blanked out on excel since it's been a while since I've done anything complex. I really need tips to improve here. Thanks in advance.,analytics,2022-07-29 12:47:26,8
What is the Fai-P/cancel and fai-R/churn metrics?,1,wbm20l,"Hello All,

I work for a solar company and trying to see if there is a statistical difference in the relationship between services at different regions.  Before  I approach, I would like to define some metrics and the calculations that went into them.

&#x200B;

**First-Available Interval**: The is metric measures the time from qualified work order (WO) creation to booking system's first-available time slot divided by the number of qualified work WOs.

Please note the following also:

WOs are calculated in whole days and the metric is shown in fraction of those days

**Qualified WOs are:**

1. New install, former install or service
2. Created between the start and end of month prior to reporting month
3. Residential WOs only
4. Only WOs where the customer selected the first available timeslot are considered, WOs where the customer asked for a different date are omitted to avoid skewing
5. WOs with no first available timeslot are omitted
6. WOs with no created date are omitted

&#x200B;

**Here is the problem**: The FAI targets are set at the organizational level. However, there are distinct challenges in meeting personnel targets between the rural and urban environments and there may be distinct expectations of those customers - that is, rural customers may have a different expectation of service intervals than urban.

**Does it make sense to set FAI at a more finite level i.e., rural or suburban or urban or even at  regions level?**

**Are service regions drawn appropriately today? Maybe a need for Remap?**

&#x200B;

**Task:** I have been tasked to clarify that there is a statistical difference in the relationship between (or not) between FAI-P/cancels and FAI-R/Churn when taking into geography account.

Churn: It is calculated by dividing the number of customers who discontinue a service during a specified time period by the average total number of customers over that same time period. **Key metric:** **Churn Rate**

Cancel:  When a customer cancels their account.  **Key metric: Cancellation Rate:** To calculate a cancellation rate is to identify the number of customers at the end of a certain amount of time minus the number of new customers acquired during this same amount of time. Once calculated, divide that number by the number of customers at the start of the same time frame 

How do I approach this task please?  I am new to analytics and I am learning everyday and also looking forward to learning from you all. Thanks",analytics,2022-07-29 19:06:47,3
"I kind of lucked up personally. The company I worked for learned I was in school in a MSBA program and from there, I was offered a role as an analyst while I continued work on my degree. I didn't have any formal analysis experience but I had exposure to the concepts.",1,wbdvk3,"I know there have been a lot of questions on MSBA program here but most of them did a related undergrad program or a related work experience. I am a food science major, currently working as a food scientist and I want to switch my career to analytics.

I’ve always liked working with data and math and recently I’ve been taking some SQL and R classes on data camp on my free time. I’m thinking of applying to MSBA program that has an internship program so I can at least get an experience before trying to find an actual analytics job. I’m looking to go back to school because I don’t think I can get experience or transition to data analysis job in my current job and I am also on a working visa, which means that I can only work food science related job because my undergrad was in food science.

My questions are:

1. Will getting into MSBA program be hard especially without any official experience?

2. How does post-grad life look like for someone who doesn’t have prior analytics education/work experience? 

3. Are there any college courses that I can audit/take partly to prepare me for MSBA program?

Any kind of advice/suggestion would be highly appreciated! Thank you!",analytics,2022-07-29 12:44:39,4
"Do a weekend Udemy course project. Anything beginner Python dashboarding or data collecting will do. Finish it and show it off. If they aren’t impressed, rinse and repeat until you can execute on enough things to be helpful. By then you won’t need the prof.",2,wb76u7,"Hey guys, long story short I’m a third year accounting student trying into break into data analytics after I graduate. I want to start learning technical skills required for this role. 

I just started learning the technical skills and would most likely be learning while in my final year of college.

I was wondering that if I could work for a math/stats/cs/ informatics professor (probably in research) to build some of my technical skills (excel, tableau, sql, python). Is that even possible?Would a professor in that department even hire a business student?  

If so, how do I approach a professor about this?

I don’t have a specific topic in mind. Just want to beef up my technical skills.",analytics,2022-07-29 08:06:52,5
"I got a job after that course…but I also made my resume and LinkedIn super fancy. I did a lot with Excel, too. Not enough apparently because this new job is still a super learning curve.",44,waqk78,I have 0 experience with data analytics and I feel this certification won’t be enough to land a job. What do you recommend me to do next?,analytics,2022-07-28 17:49:46,14
"If you are a freshman, start with education, skills and spoke languages section, projects (problem statement, what value was created, tools used to complete project) , interests.

Don’t touch a cover letter, make a LinkedIn profile. I won’t advertise my services, but you have enough free services online to start you up.

Practice on leetcode or hackerank. Focus on data structures, algorithms and sql more than anything else. 

Remember to always communicate what value you’d bring and not assume people know the outcome of your work.

Hope this helps",3,was9ya,"Hi!! I am a rising freshman (undergrad) and am majoring in Data Science & Statistics, I was wondering what you all would recommend doing to build a solid resume for the future. Thank you!!",analytics,2022-07-28 19:08:24,11
"Not provided literally means GA doesn’t know what that user’s organic search keywords were. 

GA has been useless for keywords for several years now. Best you can do is anonymous data from Google search console and judge your organic traffic by landing page.",2,waupor,"For example let's say cat is the top keyword filtered by sessions.

Can the kw cat then appear within ""not provided""?",analytics,2022-07-28 21:08:04,2
I'm about to start my last semester of OMSA in August. Can recommend it,2,waj7s1,,analytics,2022-07-28 12:42:10,32
There only ever needed to be like 2 of the 5 urchin parameters. Those wild idiots had no idea what havoc they were causing.,7,waifow,,analytics,2022-07-28 12:10:43,4
"You can go full open-source with PostgreSQL as your Data Warehouse, Apache Airflow for pipeline orchestration, and dbt or even plain Python for your transformations",13,wa6jur,"Hi folks,

We are currently using a managed data warehouse that uses Redshift and provides an in-built ETL tool. The prices have gone through the roof so we are planning to look into cheaper alternatives.

>***Costs:***  
Current spending: $2200 per month  
Targetted spending: as low as possible

I have been looking into DW alternatives like BigQuery, and Snowflake, & keeping the Redshift instance. I wanted to know which DW seems good and cheapest for our requirements in the long term. I read that BigQuery would be the cheapest and best(managed) but wanted to know if there are any downsides/disadvantages.

For ELT, I am looking into open source options like Airbyte, Meltano, and Singer. Any recommendations from people who are using these would be welcome.

>***Requirements:***  
Storage: 100-150GB storage  
Compute: 50-100 million rows per month  
3/4 users (1 main user, rest view access)  
Startup 15 people",analytics,2022-07-28 03:44:04,10
Try the Google Data Analytics course. It will give you an idea on what skills you need and it really defines the phases and processes of analyzing data in the beginning of the course. I enjoyed it and recommend it for a beginner interested in wanting to become a DA.,26,w9rcpo,"Hello all 26 M,

I'm interested in becoming a data analyst but I'm kind of lost in what to learn first. 

Is the Google Data Analytics course a good start?

I know I need a quick refresher in statistics as well. 

My current job has me working with utility data(electric, gas, water) so far I only use excel to deliver basic reports to whoever requests it but I know I can do more with it. 

Since my current job has me working with all this utility data from 500+ buildings  I think taking a course and having real data to work with will go a long way.


*Edit*
Thanks everyone for the advice!
I have started the Google Analytics course.",analytics,2022-07-27 15:00:58,29
"Analysts have the attitude of constantly investigating the story behind the data. Lots of Sherlock Holmes type of activity where you don’t know what’s going on at first but then identify each potential factor one by one through use of data.

Most data analytics roles are postmortem (work that comes after a project is completed). For example, after building a phone and launching it to market, exploring how customers reacted. There’s pre-work but usually it’s managed by Business Strategists or Business Intelligence professionals who do some data analysis but primarily leverage business theories as they’re dealing with net new products or services.

The “creation” part with data comes from Machine Learning Engineers and Data Scientists. Data Science is the art of creating robots that can interpret and take action on data - eg Siri or detecting spam in emails. Machine Learning Engineers help scale the work of Data Scientists.

All in all, work life balance depends on company and project. Though, pay wise Software Engineers makes a lot more than any of the above.",7,wac9hs,"Hello everyone,

 I am currently a full time Embedded Software Developer with almost 5 years experience, lately I have been thinking to make a switch from development to something more different and maybe a bit less stressful?! 

My question is, what kind of person you need to be to succeed as Data Analytic ?",analytics,2022-07-28 08:03:15,5
"You are using longitudinal, panel data. I think that what you are doing is ""event history"" analysis (it could be cohort effects, piecewise effects etc). Check the STATA manual with such terms.",7,wabesy,"Hi all,

I want to do an analysis where I take our current candidates, and track their progression over time.

I am hoping to see like ""In the first 2 weeks, candidates have a low productivity. They hit their peak productivity in week 5 after which it drops"".

To pull this, I have the candidates ID numbers, the date they started and the various productivity rates they hit at different dates.

So essentially, I would be taking one candidate and graphing their information (with productivity on the y axis and tenured days on the x axis), then I would do the same for all the candidates.

I assume this is easiest in Python as I can just put in the start dates and have python calculate all my data points, but I'm not actually sure what this type of analysis is called, to Google what I need to look for to do it.

Does someone know the exact method I'm hoping to do, so I can Google it?

Any helped would be much appreciated?",analytics,2022-07-28 07:27:57,5
"As a former director of analytics, No. no matter what you’d learn you’d be re-taught on the job at an entry level position, and nobody cares about minors. You’re just taking on more debt and delaying the beginning of your career in a hot job market that may cool down. There is almost nothing they could teach you in that semester to make that 10k ROI worth it and any skills you think you’d need you can learn for free: SQL, Tableau, Power BI, Python (never worked somewhere that uses python even)",7,wafr98,"I am pursuing a bachelor's in MIS and figure an analytics minor would be very useful to complement it. Only reason I'd hesitate is I took a while to decide my major and will need an extra semester to graduate, but just for one capstone course at least.

If I take the minor, I'll just need a full 5th year.

I think a Business Analytics minor would make me more hire-able, though it would cost an extra $10k+, so would it be worth it?",analytics,2022-07-28 10:24:16,7
Im guessing no. Data is gold. Most companies know that and don’t give it away for free.,2,w9kadq,In measurements like unique users or search volume.,analytics,2022-07-27 10:10:55,13
"There’s career business analysts. Next step would probably be manager.  Data analyst does not naturally become business analyst or similar, the two roles are intersecting but not the same. Normal patterns for data analytics are data analyst->data scientist/data engineer, with management opportunities sprinkled into either.  Normal patterns for business analysts are: business analyst-> senior business analyst, with management and higher IT roles above it.  Business analysts focus more on the business requirements, specs, program/process flows, etc. Typically use more excel, sql, some dive deep into the pseudocode or are partly developers themselves . Data analysts focus more on data visualization, cleaning, and analysis. Typically have a wider range of items they use, like Python, tableau, but also use excel/SQL normally.

Probably an MBA would suit your position better. Doesn’t appear like you’re really investigating into the data/Mathy side of it, more the CS/ developer support side.",8,w93qy8,"I am currently 24 year old.


I finished my bachelors degree a little late since I had originally dropped out of college for a year to figure out what I wanted to do. I got my bachelors in Data Analytics with minors in CS/Math/Statistics/Business Administration.
 

I wrapped up school with a lot of experience in python / sql / r and excel 


Learned tableau on the job and power BI


At the time I was working for a non profit on the operations and logistics end doing Data Analytics and as soon as I wrapped up my degree in may 2021 I was promoted to a junior business analyst position. I was the only business analyst for about 6 months? And during that time I did many things like working with completely trash data and cleaning it up for usage across the business. Streamlining a lot of the work that was being done manually with scripts. Helping with business requirements for the new data store we were building and helping with building pipelines and tons of functional testing.

I also helped build out the team by being involved with hiring a manager and filling out the team with a pair of senior analysts and a junior analyst. Right before my director who hired me on left I was promoted to a senior business analyst position (for about 2 months now). 

I’ve gotten to a point where I’m not sure what my next direction is. I’m lost whether to shoot for a masters degree for either an MBA or OMSA from Georgia tech. Work had exposed me to both business analytics as well as data engineering. I have enjoyed the data engineering part of my work but honestly I don’t see myself doing that long term and I’ve started to enjoy the business focused part. 

Any advice for me? I have access to LinkedIn learning so I do take courses weekly to learn new things on python/R/sql/dashboard applications.",analytics,2022-07-26 20:24:55,7
"Apply to healthcare, they badly need data analysts. So much so that they are offering remote full time positions in other states to cast a wider net.",60,w8ynwg,"I just recently graduated in a field where I can do data analytics/consulting. I unfortunately started my job hunt pretty late and am now having a hard time finding a job. I don't have a great gpa (3.0) either and I think it is actually hurting me.

I'm trying to apply for positions at big companies because that is what I prefer currently (I want to travel, meet more younger people ect.), but I'm starting to think I won't be able to due to my gpa. I was wondering what big companies similar to the big 4 exist that I can research and try and get into, especially with a lower gpa.",analytics,2022-07-26 16:33:17,33
"For most of the measures provided, I would run some basic statistics like mean and std d. This will set a baseline and let you k ow quartiles. From there you can standardize what “good, better, best” is. 

You need to think about regionality and seasonality in your trends else you can introduce outlier standards.",9,w91vlm,"I am working on a project where I evaluate the  Key Performance Index (KPI) for a solar company. The aim to see how the company is performing generally and see if it is efficient. Some of the metrics that I look at are:

&#x200B;

1. How many solar installation appointments were made weekly?

2. How may dispatch were made based on the appointments?

3. How many appointments were carried over in the week?

4. What is the completion rate for the solar installation?

5. What is the same day rate completion for the solar installation?

&#x200B;

For each of these metrics, an arbitrary target rate was set to measure performance. For example, a target rate of 80% was set for item #1, 90% for item #2, 60% for item #3 etc...Anything below these percentages means poor performance for each of the metrics. Please not that these target rates were not based on data or science. I have now been tasked to come up with a target rate that is backed by data and statistics and not one from a manager just throwing out some numbers.

&#x200B;

How best can I approach this please? How can I provide a better framework on targets setting? What is the cost benefits of setting these targets?

&#x200B;

Thanks.",analytics,2022-07-26 18:58:40,18
"My company uses bit.Ly. However we do some other stuff to track attribution 

Alternative solutions off the top of my head:

1)Work with your influencers to Integrate tag management solutions to track click events. Connect your GA account with tag management from the influencers site. 

2)Adding custom Query string parameters to your link. Make sure the query string parameters has meta data on which influencer sent the conversion through. On your e-commerce site you can use JS to parse through the query string parameter then trigger events/page views through your tag management system in order for the data to flow to GA.",4,w8v34m,"Hey guys,

My company does not have an e-commerce website. However, our products are sold on other e-commerce websites.

We work with influencers and make them share e-commerce website links so people can buy our products from websites that we don't own.

We want to know how many people click those links. So I came up with the idea that if I share them a link from our website which redirects to the e-commerce website, I can track the amount of clicks to my page.

When I check the internet, I see that anGoogle Analytics do not track redirecting pages. So, what can I do to solve this issue? Please help.",analytics,2022-07-26 14:05:11,8
"I suggest learning Python basics first before attempting data analytics with it if you haven’t. While not free the Tim Buchalka Python course on Udemy is great for learning the basics. 

Without learning how python handles data types and data structures it will be hard to do much. But once you do it becomes very easy. Pandas is used the most for pulling data and data manipulation.",2,w8spzm,Thanks in advance!,analytics,2022-07-26 12:30:18,5
"Pretty sure you have to build out an exploration to set this up. 

But truly I have no idea. Fuck ga4.",2,w8ooyv,"I want to sort my Traffic Acquisition with the following dimensions:

1st dimension: Campaigns  
2nd dimension: Source / Medium

I'm able to add the source medium to the dimensions and sort it with that being the primary one, but doing so makes most of my traffic just disappear.

I'm unable to add it as a 2nd dimension. I was watching a youtube video and this guy there had entirely different groups / sorting alternatives than me, how do I add those?

I want to track my UTMs.

Anyone with experience here?",analytics,2022-07-26 09:49:09,3
"In the BI space the lower the amount of Dashboards you have the better, stakeholders dont want to be clicking around to get the info they need. 

The performance hit is minuscule if your ETL is set up properly.

Also you can grant permissions to certain people to view certain things creating several tabs.",2,w8na48,"Dear Pbi  users, good afternoon.

I received a request from the company to consolidate all of our Kpis into one single dashboard. Right now we have 4 different dashboards each with different data (Sales, customer erosion, P&L, Country tesult).

In my opinion this may be detrimental because:

1. The dahsboard performance will slow down, as the amount of metrics and data will be higher.
2. Security, there is data that Sales should access that operations for example no and vice versa, placing eveything on one dash may create security issies.

Could anyone share some insight on this típico and their experience?",analytics,2022-07-26 08:52:51,3
I'm having a hard time thinking of a situation where they would be the same. What are you trying to accomplish?,1,w8ht2l,This would be under the acquisition tab,analytics,2022-07-26 04:54:14,2
"Google stripped referrer organic keyword data years ago, so there isn't anything you can do with ""(not provided)."" They did it in the name of ""privacy,"" yet you can still get PAID keyword data - from the same search results - as long as you're paying for Google Ads, of course. But that's a rant for another day.

What you CAN do is look at landing pages from organic search instead of keywords (go to Acquisition > All Traffic > Channels > Organic Search, then change the primary dimension to Landing Page). Then take that list and go over to Search Console and go to the Performance report. Add a filter for the page you want to see Query data for.

Lots of limitations here, obviously. You can only look at query data for one page at a time and of course you don't know which queries actually led to revenue, but you know that these are the pages driving revenue from organic search and you can at least see which queries are driving the most clicks and impressions to that page, so you're most of the way there. It's also nice to see the average position of those queries/keywords which can uncover some opportunities for those that are within striking distance of top rankings.",3,w8kcah,,analytics,2022-07-26 06:51:37,4
"The ideal recipe is combination of Business Tools (SQL, PowerBI/Tableau) and a coding language (R or Python). You can develop ML models using R and Python, and don’t need research level knowledge for application of ML capabilities. The research part comes in if you are asked to further optimize the ML algorithms, where it’s less about statistics and more Calculus/Computer Science (lots of partial derivatives and linear algebra).

I’m sure as applied mathematician you did some statistics - like hypothesis testing or be introduced to different statistical tests?

Also, pursuing internship in Data Analytics will be very helpful for job hunting upon graduation.

As a starting point, a degree in STEM is more than sufficient, but entering the job market without immediately possessing the basic skills for the job won’t help, so I’d recommend focusing significantly on acquiring the tools and familiarizing yourself with the coding languages.",2,w8dxmj,"In a year I will get a top UK degree in applied mathematics (in fact I will study theoretical physics there at it's really important). However, I am planning to get a job in analytics/data analisys/ML. On this masters programme I can take statistical courses, but it seems that I won't be able to get a PhD position in statistics. Also I guess that my marks will be lower that if I took theoretical physics courses (I took some courses in probability and statistics in undergrad school, but I am not sure that I have enouph experience in this field). Staying in physics gives me some chances to get a PhD pos.  


Should I switch to statistics? Or it doesn't matter that much (just having a good STEM degree is important)?",analytics,2022-07-26 01:07:37,7
"Scratch your list. Focus on understanding the business and key drivers and why you’re doing what with the data. Tech skills are good, business knowledge is superior.",11,w7us7x,"Hello data geeks,

I'm having a great time as my first 2 months as a DA intern. I study CS and stats in Uni and am currently a senior. I'm looking to find what are some good skills to look for during my final month here.

Currently I would consider myself proficient in:
* SQL (mySql/Oracle)
* PowerBI (Tableau to a lesser degree)
* Excel / Access
* Pandas + Tidyverse libs for Python and R

My greatest weakness is, 

Shyness... I struggle with conversations and making friends, my previous internships I was working alone and didn't need to reach out to professionals of other fields.

Financial, marketing, and accounting terms are also new to me as I have never studied or been acquainted with them.

The thing is, I'm starting to feel more inclined towards data engineering or a field a bit more technical. Sometimes I feel disappointed that I almost never need to use R or Python despite learning it for years.

Any way, any tips are welcome. Thank you for listening to my rant.~",analytics,2022-07-25 10:25:42,5
"Assuming it's Sql and entry level they will probably give you a few tables and ask you to write some queries to answer some business questions.you could check out some examples at hackerrank (my favorite, or leetcode or questions at the end of each chapter of T-SQL Fundamentals by Ben-Gan Itzik (specific to MSSQL).

I hear most hiring managers expect you to be familiar with the basics: select statements, joins, aggregates, filtering with WHERE and HAVING clauses.",4,w89buf,Hi community! I have a written exam coming up for an analyst role - the focus of the exam is to test experience with programming language and developing data custom reporting/ visualization. Anyone know what that could potentially look like?!,analytics,2022-07-25 20:48:49,5
"Feel like a lot of logistics will be having to deal with some bad data (inputs form users being correctly given at a factory level) and how you’ve thought of clever ways to outline where things are off. (Don’t for sure go in thinking they have bad data collection going on, but maybe they’ll mention something? I had to do a lot of work with commercial sourcing teams for a large cpg company in the past and this was our main blocker a lot of the time when looking to do advanced analytics) 


You can also maybe look into route optimization problems. For example, how many distribution centers are really needed to adequately cover an area, sometimes big orgs are over invested and can make due with 10 centers in a country instead of 20. 

That’s what comes to mind, just prepare as normal. I’d you come with an idea of what kind of optimization problems an organization like them would try to solve it couldn’t hurt",3,w7tlvd,"What should I expect ?

&#x200B;

I do know that there will be case studies, hence, are there any resources regarding analytics in logistics I can study in prior to the interview?

&#x200B;

Not sure if it helps but, it is a fast-mile logistics start-up company",analytics,2022-07-25 09:39:10,5
"Sharing few ideas for projects that I had done -

Market basket analysis (on grocery retail data, created a tool that tells what products customer may buy next based on products in cart. Also designed a grocery store layout based on relationship between various products).

Loan feature study (surveyed students on loan features - interest rate, brand, app/tech, repayment period, flexibility etc. Performed a feature conjoint analysis to recommend a new market entrant what interest rate to charge)

Pair trading strategy (found pairs of stocks for which profitable pair trading strategy could be deployed. Also forecasted estimated time to close a pair trade)

Did you see one thing? I hardly mentioned the techniques I used. My main focus was on business outcome. So do this - find business problems, solve them by using analytics techniques, when you present it, focus on business value as an outcome.",3,w7lnh6,"  

Background : 

24M , Bachelor in engineering (2020), India

Internship : EV manufacturing at a MSME as Sales Intern, 6Months, India 2021

Master in Management, Neoma Business School, France 2021-22

Master of Science in Business Analytics, Neoma Business School, France 2022-23

No special certifications

No professional work experience

Preferred field- Technology(e-commerce, web3.0 etc.), Technology/ Strategy Consulting, Tourism. 

Preferred Location- Europe, US, Japan.

Languages spoken- English

&#x200B;

Situation:

Hello, I'm enrolled in MSc in business Analytics in France, and I will be graduating next year. So, during these vacations, I've been trying to upskill myself so I could start looking for opportunities asap. I've been looking over the internet, but most of the content is about processes, charts, etc that are used. I totally understand that these are important when you work, but as a newbie, who doesn't have a solid Business Analysis background, I don't think I could showcase these in my resume, which would be the only thing that'll get me my interview. Now, in France, language is a barrier for me as I don't speak French very well. I am still learning but I'm sure that it wouldn't be that good so soon. So I have been learning SQL, python, excel, and tableau so at least I could mention them in my resume.

Question:

What skills, tools, and things I should definitely work on, which I could showcase in my resume, or mention in the interview to land that opportunity. Also what are the projects I could do and mention them? I am also looking for the requirements of top companies like Amazon, they only demand some SQL knowledge and French and English skills. But I think it'll be a problem for me with the language, as well as my experience. Please suggest me things to look forward to, work upon, and companies I should target for better work-life balance with reasonable pay for an entry-level applicant. Any resume tips and job/internship hunting tips are welcome as well.

Also, is anybody working in any field of web 3.0 as a business analyst? I would just love to know which companies have such opportunities and I would love to work on web3 technologies. Also, talk about the relevant skill set and knowledge required for it.

Thank you in advance.",analytics,2022-07-25 03:43:36,2
"Got to r/OMSA and look at the discussion with the fall 23 acceptance spreadsheet. It has applicants and application statistics. 

Draw your own insights from it.",7,w79qm3,"I have a BBA with specialization in analytics with 3.95 CGPA. It has some business math and stats courses but wasn't as analytics focused as a BSBA would be. I have about 2 years of exp in analytics as a analyst and team lead. I have a great portfolio and achieved much at work. I know tableau, SQL and programming well. I don't want to give any GRE or GMAT.",analytics,2022-07-24 16:55:30,12
Ask your business analysts or domain experts.,6,w707oe,"Hey y'all,

I recently started a new job and would love some advice. Where I can go for resources on Business/marketing metrics/KPI's or understand how to approach thinking and creating  KPI's and/or metrics that are relevant to specific business contexts.

Thanks for the help!",analytics,2022-07-24 09:58:00,3
Thanks man,2,w6osp4,,analytics,2022-07-23 23:13:28,1
"I’m an individual contributor, pretty senior data scientist and it currently looks like below. I’ve been a manager, department head and jr/sr data scientist / analyst over the past decade. The more senior I’ve been the more reviews, influencing stakeholders, and meetings I have been in, but it also comes with much easier path to making the analysis I do impactful. To impact the business, generate value and advance in your career, regardless of level, you will need to be able to influence people, drive the org strategy, and get stakeholder buy-in.

- 10%-15%- meetings either 1:1’s with PM/ENG/Research or team/execution-syncs and reviews. Sometimes this is 25% and those weeks suck.
- 5% mentoring meetings
- 5% interviewing
- 10% setting goals (team/org), product roadmaps, aligning with cross functional partner teams, individual roadmap, personal/career/performance-review oriented stuff
- 25% SQL work either getting data for analysis/research, building data pipelines for measurement, analysis, or visualization, or just trying to understand what data exists where and who the experts are.
- 5%-10% is actual statistical work in R
- 25% communication/evangelizing of analysis/research. Writing it, posting/sharing it, presenting it, having discussions with the relevant people to make sure it has impact and people have been influenced.
- 10% or whatever is left I mostly just mess around with social stuff for the team or totally ignore work while I read or watch the latest NBA and cycling stuff.",42,w6ill2,"Hi all, I'm considering a bit of a career change into a data analytics role, but I'm wondering how much you actually analyse data compared to managing people, working with stakeholders etc., particularly as you progress in your career. I like working with data but don't like the latter, however I'm worried that when I move past an entry level role it will devolve into this like most other things seem to. So, how much of this is there and do you still enjoy your role? Thanks",analytics,2022-07-23 17:33:50,25
"Implementations of GA are different, but I’m assuming it has to do with one or two things: data estimation, or 
missing data. GA estimates their data if you let it(check the green or yellow shield), especially prevalent if you’re missing data, which can happen when UTM parameters are not correctly captured.  Missing data can cause you to not only estimate data, but can also cause Google to complete omit the record collected if it does not have a field that corresponds to the missing data column.(i.e. if a record has only source and you tell it to show source/medium the record may just be omitted). All this is even more likely if you further filter a set of data.",1,w6pc91,"This is a data studio dashboard connected to google analytics

Originally I've been using Dimension ""Source"" set with ""System segment"" ""Organic""(medium: organic) as available on default on Google analytics. 

When i changed the dimension to ""Source / Medium"" however with filter ""Source / Medium contains organic"" **im seeing different YoY information.**

Could anyone explain why there is a discrepancy? 

Shouldn't **Source** & **Source / Medium** be the same if the same organic filter is applied?",analytics,2022-07-23 23:48:48,6
"In my last interview, I just said ""I'm actually pretty satisfied with where I am right now but decided to scan the market to stay up to date. I felt this job was a good fit and decided to apply, but I don't have any issues with my current position""

I got the job, so it might have been a good one.",57,w6c2t9,"I've been asked this around 2-3 times now. 

I'm assuming they're screening for genuine interest in the company though I'm not sure.

What do you guys think?",analytics,2022-07-23 12:30:12,42
"Tell them.

Just don't tell them you did it because of them, but rather for yourself and the things you care about.",12,w6eg4z,"I have a blog that has around 5-7 posts. All related to health/wellness/mental health. I have an upcoming interview with a healthcare company. I would be so, so, so happy if I got the job since I would get to use my statistics degree just slightly and it'd be for a cause that I genuinely care about.

I want to show off my enthusiasm for the role but don't want to scare them away somehow. Would it be a good idea to say that I started a blog to showcase my enthusiasm?",analytics,2022-07-23 14:19:35,6
"Ireland might be your best bet, just don't live in Dublin, or you'll work to pay rent.

I moved from Argentina to Germany, and the market is smaller than I expected. Language is also something to consider, as a lot of businesses don't use english primarily.",2,w5sctm,"Hey folks! Hope you're all well.

I've been working in web analytics for a couple of years now. I'm a citizen of a country in the EU, but I've never lived anywhere in the EU -- I've been in South America all my life.

I work remotely right now, and I can pretty much work from anywhere. So I've been thinking of moving to the EU and being there for a while. And while I'm there, I could also take a look at the job market there.

But I'd probably need to pick a country to go through the paperwork for. And I think I'd like to make that choice based on the job market in Europe. I know I could live in pretty much any of the member states, but I'd rather make a safe bet at first.

So... do any of you know what the job market for web/data analytics looks like in the EU? Which country has the most job opportunities, or the hottest data/analytics space? Where would I be the most likely to land a good job, and make good contacts in the industry? Any insight is welcome.

Thanks!",analytics,2022-07-22 19:24:59,19
"Go on LinkedIn and look for entry level BI roles. Search for people at those same companies with BI or analytics in their job title, and message them politely asking if they'd be open to a conversation about how they got into the role, their career progression, etc. 

Don’t mention the open role. If the conversation goes well it will come up naturally, and they may tell you to apply. This gives you an in. If the convo doesn't go well or they're friendly but don't mention the opening, you've at least done some networking and will have more information about the landscape.

This is not quick or easy. A lot of people simply won't reply. But the few hits you do get can end up being pretty valuable. Good luck.",4,w5k5f8,"My work is generic market research articles on different sectors and macroeconomic developments, but they are not quantitatively or methodologically rigorous.

The organization I work for is an international business council. However, I’ve done no real business intelligence work. I used Excel (Some power query for data analysis) pivot tables, filters. 

I also did a statistics minor during my Bachelors, but that was three years ago and my programming skills are weak.

What would it take to get an entry-level business intelligence role from where I am now?",analytics,2022-07-22 13:12:57,4
Data Visualization concepts are super important. It’s basically communication skills- check out “Storytelling with Data”,25,w5anfd,"Hey everyone, I’m starting a new job next week where I am going to be using power bi for the first time.  I already know how to make basic graphs/charts (line, bar, donut, pie), add filters and slicers, and add calculated columns.  Is that what is typically done in Power Bi?  Or is there more to learn?  Any answers are super appreciated!  Thanks and happy analyzing!",analytics,2022-07-22 06:28:16,21
"I deal with this all the time & I solve locally with Python. There is zero good way to do this and took me months of trial and error to get here. 

For files in share point I ‘sync’ the share point directory to my local directory and pick up from there. 

For SharePoint ‘Lists’, as there are no true SharePoint database or good API, download the excel, save as xlsx and make sure to keep the data connection open. I think I use the win32 Python module to open the xlsx, refresh the data, close the excel, load into Python with pandas, then insert with sqlalchemy into my Sql Server. 

Ultimately anyone using SharePoint generating more than 200 record & 10 users a day for an important business use I tell them to hire a dev team and build an application with a database back end. 

But why would they do that when they have SharePoint?",6,w5eluu," I am working on a project where I am reading in data from a database in SharePoint. It is a Microsoft Infopath application file and is a site to keep track of projects. Right now, we use the ""Export to Excel"" feature every time someone updates the tracker site, but I want to automate the process where every time someone updates on SharePoint (ie., uploads a new project), one shared Excel file will automatically update as well, instead of requiring us to download a new file locally every time. Is there a way to do this through Power Automate, or perhaps creating a VBA macro? I would appreciate any insights!",analytics,2022-07-22 09:19:13,11
"Do they engage in projects? Maybe “inter-squad” projects to get them talking to each other etc would be a soft introduction. It could be a whole squad working with another whole squad, or maybe you can get 2 people each from 2 squads on a complex project.

Or is it more BAU / RUN work",1,w5jdmi,"I'm leading 5 reporting teams that are all siloed right now by the areas of the business they support, e.g., call center, implementations, product A usage, product B usage, etc. Each team is basically just data analysts who are mostly SQL / PowerBI people but some mix of Salesforce or the reporting interface for a specific product.

Ultimately the data all ties together: our Sales data connects to Orders connects to Implementations which results in Support Cases and Escalations and there's Product Usage data, etc.

Now that these teams have come together in my (new) organization, I'm thinking that diving our organization based on which area of the data/business they happen to have familiarity with is not ideal. It would not be that hard to get someone up to speed on another area of our business and most of what we're working with sits in the same data warehouse anyway.

My dilemma is how to actually organize this in a way that is 1) engaging for the individuals and doesn't create chaos and 2) provides for good professional development opportunities. Part of me wants to take my rockstar PowerBI developers and train them to be managers and then have them scale their rockstarness to teams of analysts under them... then a separate team that focuses on business analysis (to help inform reporting requirements) and insights delivery (to deliver insights back to our business units). Then maybe...not sure what else. Maybe those are my two functional areas.

Very curious to hear how other orgs are laid out and what the merits or challenges have been with your approaches.",analytics,2022-07-22 12:40:15,4
"You would need to have more context.

Job titles can be fuzzy. 

The data analyst position might mostly be an excel-based job and product analyst may actually be an impactful analysis position using topline tools.

Or the product analyst may really be a strategist role and that data analyst role might be a key position.

In some sense it may not matter as roles and careers evolve anyway.",3,w5it7b,"I may have a choice between a data analyst role and a product analyst role?

I want to be able to rely on anything so I never get left without a job. 

Im a recent grad so I’m not sure if I should take a product analyst role and leave within a year to get a data analyst role or go the other way around?",analytics,2022-07-22 12:16:02,7
There is someone in my work who works on the phones taking complaints. On their LinkedIn they call themselves a complaints analyst!,84,w4ltkv,"I interviewed for a ""retail analyst"" position that was just a database manager position. No cleaning, no interpreting, no analyzing, just keeping a database up to date. A ""Marketing support analyst"" position that was just a website manager position that had some copy writing in it. Like For the marketing support position the manager stated that there is a marketing analytics team but that this position was specifically outside of that department. I'm getting sick and tired of companies putting analyst on jobs that don't require heavy analytical work. 

A cashier could be a ""retail analyst"" because they analyze the money that is coming in and making sure it's not fake. Using computer assistance to calculate the total, that would assist with the overall financial aspect of the company. A ""mechanical analyst"" that looks over a car and interprets the issues using prior analysis and verifiable actions, they also repair the vehicle which is the majority of the workload. My wedding needs a ""photography analyst"", a photographer that uses reports and data to know exactly the best position to take a picture, they also have to take at least 100 photos and make sure they document the entire day.",analytics,2022-07-21 10:01:50,30
"You have enough skill for an entry position in the right environment. Start applying and see how your interviews go.

Hackerrank.com has some practice problems for sql",29,w4r6wu,"Hi everyone, 

just like wat the topic asked, like what exactly do u need to know for an entry level data analyst? like is there some website like leetcode or other thing that can somewat determine i have enough technical skills for the job or which part i need to work on.  Thanks in advance.",analytics,2022-07-21 13:42:00,14
"High chance of failure if u r not comfortable with coding, stats, high level of maths if ur program involves that. U better prepare for that first.",2,w4vg5e,"My first career was Dental Hygiene, then dental lab, and for the past 20 years I’ve been an RN.  Over the years I’ve become the ‘IT guru’ for my coworkers who call me first before the help desk.  I know most Windows programs very well and some Visual Basic stuff in Excel.  I think data is crazy sexy and was originally going to go for just a Masters in Analytics but when I saw the IT Analytics option I signed up.  I am so excited, but a bit scared.

Don’t get me wrong, I’m decently smart, graduated Summa Cum Laude, but that’s probably mostly because I’m just a really good test taker and paper writer.  My worry is that the university’s program will assume anyone going into this program will have more foundational knowledge than I have.  Does anyone have any idea if my worries are valid?

Thanks in advance! (I’ll be going to Capella if that means anything)",analytics,2022-07-21 16:43:19,8
"You’re looking at it wrong, in my opinion. Tackle tasks and projects, and learn what you need along the way. If you tackle projects you have to learn all the bits (data, problem-solving, business etc). 

You’re right in that there is just too much to learn, to the point where you can’t possibly be expected to know everything. Which is why having an overall strategy for dealing with analytical tasks is a much stronger foundation that any set of quantitative methods or domain experience. Perhaps looking up problem-solving or analytical processes.",6,w4l327,"Learning - data, business, industry and domain. What should be the priority?

So you started career in BI. Where do you focus your energy? About learning what?

Data - stats, visualization, machine learning, deep learning, nlp etc

Business - strategy, marketing, operations, finance, human resources, economics

Industry - top players/competitors, regulations, industry structure, company capabilities, future trends

Domain - programs/projects specific to the company (and the role), strategic direction, priorities, people

There's so much to learn and I'm feeling paralyzed by just looking at sheer amount of things to do. Where should I focus myself on? How do you analyse/prioritise on something like this? And yes, I'm not sure where I see myself 5 years from now. I just want to be recognised as doing great at what I do.",analytics,2022-07-21 09:32:00,5
"1. Yes network

2. Interns have broad shallow tech skills and little to no understanding of business. Hopefully you'll end up in a org that has structured work packets, or a non critical project that you can see through. But ypu may end up rewriting old reports or analysis in the latest tool the business uses. 

I've no doubt that you could probably figure out a market basket  analysis, but without business context you might not know that your highly correlated items (itemNo: qwtyu & 12348%gh) are a left and a right shoe. Network but also ask what kind of projects the business is targeting for next year to get a feel for the organisation",1,w4pngz,"Hi, I'm a rising sophomore at UCLA. I have some questions regarding finding summer internship next year for data analyst positions and I would love to receive some advice.

1. I just completed the google data analytics recently. So, I think I have learned most of the basic skillsets. I also created some projects and built a somewhat decent portfolio (SQL, Excel, Tableau). Am I ready to apply for summer internship next year? How do I start? Do I just start networking to get referrals  and applying everywhere (Linkedln, Handshake, google, etc) like that?
2. I think I'm decent at SQL, Tableau, and Spreadsheet. I know a little of R and I'm also learning it on codecademy to be more proficient in the language. But I'm definitely still a beginner. I don't know anything about Python as of right now ( but I will take a course at my university in the future). Will this be a disadvantage for me in my job searching? Are my skillsets good enough to land an internship for next summer?

Thank you for reading my long post. I really hope to land an internship next summer so any advice/ input is much appreciated. It's my first one so I'm quite nervous and I don't know how much preparation is enough.",analytics,2022-07-21 12:38:33,1
"There are a lot of unspoken and learned experiences that build your business acumen.

You just need experience.

Here are a few things that's helped me along in my career, in no particular order:

Your self worth shouldn't be tied to your income, work/career, or what people think of you
You are a valuable and important person just by being 

Learn to tell a story with the data 

Be helpful with the data 
It should lead to an outcome, answer a question, or present a solution 
Don't just be the guy to point out problems and not offer solutions 

Ask for feedback and coaching, ask for help in general 
Not knowing something and being curious isn't a sign of incompetence
Asking for help, etc shows maturity and confidence 

Try to learn the business you're working with and ask questions/think about how your data/insights can help achieve business goals
Try to anticipate ways to add value to your colleagues, other departments, the business in general 

Making other people look good and being a vital player in the success of the team is the best way to job security 

Find a mentor",125,w3ra4t,"I need some advice, I was a Marketing Data Analyst, the only one in my team with a non-tech boss with no experience at all at the time, only passion and enthusiasm. 

I worked there for 5 months before they decide I wasn’t worth the investment and it’s partially on me, “you were good at crunching numbers, but you weren’t innovative enough to make a difference”. 

It was a harsh statement but it carries some meaning. I love data and I love working on it but I just can’t figure out their expectations from me, they weren’t directing me or training me, but they expect me to be a professional by pushing myself by myself while they are waiting for results. 

I presented some suggestions like trying to do RFM analysis, clustering, some ways to reduce wasted resources but to no avail, they made up their mind already. 

So, it was an experience, certainly gained some knowledge and I want to address my problems to better myself. 

How do you guys keep your heads in the game? 

How do you gain that skill where you can spot there is something wrong in your business processes?

How do I make myself a better analyst rather than reports monkey?

How do you develop your soft skills and sharpen it?

Thank you brothers in data.",analytics,2022-07-20 09:52:08,56
"Some clarifying questions.  


The pages through the questionaire do trigger a page refresh, instead of being a single page application (SPA)?  


You are using the behavior > site content > all pages report?",1,w4cbiw,"Have set up universal GA on a website, goal is to get patients to fill out a questionnaire. If GA is to believed users are not progressing through said questionnaire, however having watched some hotjar recordings I can see users are. 

I have checked in GTM and the tag is firing on the pages in question. Any idea what could cause this?",analytics,2022-07-21 02:33:27,2
"Hey, it’s pretty hard to think about a workflow or process to tackle problems like this when you’re first starting out. I wouldn’t worry too much about it.

Here’s how I would go about it(not saying it’s the right way):

1. I’d explore the data find things that are unique about it. Like are there any null/empty values. So, basically put on a filter then sort by empty values. Then, I’d try to find any unique values. Like what’s the call error, max call error, average call error. Just some basic statistics.

2. I’d organize the data by creating a pivot table to see it on a more aggregated high level view. This way I can see things that aren’t blatantly obvious like for example if you collate all of the data on call errors for a specific division you might find that there’s a specific type of call that incurs more errors than another.

3. After identifying what some problem areas were I’d sort the data and apply conditional formatting to highlight any values that could be of concern. Like, if we set a threshold for if errors were higher than 40% then a cell would get highlighted. Then you could sort by just the highlighted values and see what divisions had what issues with what types of calls but more broken out on a table.

4. Maybe they want to see a smaller table with more specific information. You could vlookup the information you want into another tab and clean up and create columns with your own formulas like average, sumifs, etc. again some basic statistics.

5. Macros are kind of a situational thing. Typically if a workflow is redundant and easy to automate like pulling data and creating a list out of it then you can use a macro to basically do it all at once for you. You literally train the macro by recording your clicks doing what you want it to do.

This is basically how I’d go about it without seeing any source data.

Hope it helps !",21,w3pv5e,"I am currently trying to transition from a non analyst role to an analyst role. Experience wise, I don't have much but I have some project work and have enrolled in a university course to improve.

Today I had the competency interview and after that, I was required to do a 1 hour excel test which asked to:


Produce / use conditional formatting

Produce / use V Look Ups

Produce / use a range of formula's

Produce / use a pivot table

Produce / use some macro's



In another tab was a large data set which showed figures of a call centre such as calls offered, call error %, line of division, sub division, product, enquiry type etc (I can go into more if needed).

What startled me was that, there were no clear questions or instructions but rather than to get some insight from the data using those tools. And I was had no idea where to start. (I am used to having a goal in mind such as find out how many claims we are getting in a certain area and then from there I can apply my knowledge of functions etc to find that) 

As the title said, i pretty much fluffed it and Im 90% sure that will cost me the job but I would be grateful if i could get some opinions on how you would have gone about it?",analytics,2022-07-20 08:55:44,14
"SQL, basic statistics like mean, median, stddev/variance, basic distributions and how it relates to product, A/B tests, MDE/sample size. 

Most product analytics interview failures are product interpretation issues so try to keep in
Mind solving the business problem and using data to inform product.",7,w3thlt,Hi folks! I have a product analyst interview coming up with one of the FAANG companies. Could you please give me some pointers for a technical screening interview?,analytics,2022-07-20 11:20:48,13
Depends which direction you want to take your career,2,w43r1m,I am about to graduate with an accounting degree and looking for a masters degree choice so i have enough credit hours for CPA. I was wondering which is better for career growth. I went to a normal state school with a 3.75 GPA and continuing at the same university. I like both topics so it it hard to choose which one.,analytics,2022-07-20 18:39:15,1
"I applied to and got into the OMSA program for spring 2023. so I haven't started yet. however, I do know  that it's in the top 10 ranking in the US and known for its quality and rigour.

It has a very active network of people /students ready to help on slack and on reddit/r/omsa

Also, it's cost competitive (<10000) similar to UofT Austin and few others.

I have only heard of good things about this program.

Also, the deadline to apply is fast approaching. I think it's in August.",5,w3njvg,I have an undergraduate degree in MIS and I want to become a data analyst. How good is Georgia Tech's master's program in analytics? How well does it prepare you to become a data analyst? Has anyone taken the program and what did you think of it?,analytics,2022-07-20 07:20:39,22
"WGU has an MSDA program. Costs about $4k for a 6 month term. Fully self paced and possible to do in one term, but two is definitely more feasible if you work full time. If you did 2 terms it’d be about $8k",3,w3xfqs,"The only one I know of is the MSBA at Georgia Tech. It's online and for 10,000$. Any other ones?",analytics,2022-07-20 14:02:43,7
"You could use Seaborn and Matplotlib, but no one's gonna be impressed because the colours are pretty or anything.

Think about what story you're telling with your visualisation. People need to understand what they're looking at at first glance.

Random suggestion because I know nothing about your data set: sentiment analysis lends itself very well for a heat map. It's easy on the eyes and as long as you put some sensible variables on the axis will give you something interesting to show.",4,w3ohft,"Hi, I'm a college student trying to impress a bunch of people in a company that I'm interning for.

What is the the best-looking Python or R data visualization package/method for a non-CS major student to make in less than an hour? My level of Python is enough to do web scraping from Google news through Selenium and automate Windows tasks in office jobs (Excel and ERP stuff).

I have csv files of data that show sentiment analysis results in numbers (from -1 being worst and +1 being most positive). I am new to data visualization except for WordCloud. What's a good, pretty-looking package that is easy to use with dataframes?",analytics,2022-07-20 07:59:29,1
"The referrer value (document.referrer) will give you what you want.

You'll also see this value in your server's apache/nginx logs.",3,w3o47i,"I am looking for the best way to track where the visitors come from. I am currently using Google Analytics and Hubspot and those are both great but the source in which they say the visitor came from is very vague and was wondering if there is a way to track the exact page they came from. Like if someone came from a particular facebook post, it would show the exact link/source instead of showing something like ""refer/facebook""

I love how hubspot assigns each user a cookie to that it can track if they come back to the site and which pages they view, all in one dashboard. The only missing piece is the specific source from which they came from. Hubspot is very vague as well just showing ""Source - Linkedin""

Any help or advice is appreciated.

Thanks,

Cam",analytics,2022-07-20 07:44:28,4
"If they don’t want to listen, look for another job. If they don’t want to use a database, you can try suggesting storing in partitioned parquet files instead of excel. The file size and query speed would be significantly faster. You would just need to ingest any new excel reports. 

It is hard getting people who are not data savvy to not use Excel as a database though. Good luck",14,w35wpt,"I don’t usually do this, but I’m pretty stuck. 

My situation is this: I accepted a promotion to a new department in my company about a year ago, and during the interview, a point of emphasis was streamlining and automating analytics & reporting. I accepted the offer based on this. 

In the last year; despite my best attempts, we continue to use 50+MB excel spreadsheets as our data sources, we have about 9 different reports of the same size. There is no data integrity whatsoever. The same persons name may have different iterations across all the reports, and many other examples. Most of the people working on these reports spend their entire day playing around doing manual work in these various spreadsheets.

They constantly act like they want optimization and automation, but don’t want to change any sources, they want their reports to look exactly like they do in excel, despite forcing me to visualize them in power bi. I did what I could but largely couldn’t do much.

This all came to a head when a supervisor had to cover for someone on vacation who works on these reports, and it took 3 hours to complete. They came back to me because this report had to be automated because it took too long to complete. So I did exactly that. They came back and said that it needs to look exactly like it does in excel (tables, which someone is manually working with data) and the visuals need to look like they do in the excel report. 

TL:DR: how do I politely express to my employer that data cannot simultaneously be exactly like the manual reporting they do in excel and automated in platforms like power bi. 

As an example there is a table where someone goes in and manually types in a 4 week rolling average, a current YTD average, a previous year TD average, and a previous year total average.",analytics,2022-07-19 15:46:27,24
The most important thing is trying to truly understand what problem is trying to be solved. Who will be using the dashboards? Why don’t current tools work for them? What capabilities are they trying to get aside from pretty numbers and visualization… is this going to allow them to spot problems more quickly? Are there different participants that will need to use it in different ways (execs vs management vs worker bees). Look at it from the user’s POV. Only then should you worry about the data sources and how it needs to be displayed.,12,w3ek0r,"This is a common question I am getting asked during my interviews and I'm having just a slightly hard time answering it as I haven't made dashboards within the industry and visualization ideas made within university all seemed to have been ad-hoc or out-of-the-blue rather than having structure to how I went about them.

Things I have in mind to say when answering this question (in no particular order):

\-- Get a sense of the numerical values using pivot tables.

\-- Ensure accuracy of data with proper data cleaning techniques (no NA values, consistent date formats, eliminating duplicates, and for inferential statistic purposes, filling NA values in with the mean of the column).

\-- Confirm my audience and determine whether I'll be presenting findings to stakeholders or fellow analysts before beginning to work with BI Tools.

\-- If importing data from SQL into Tableau/PowerBI, ensure that I have adequate fields such as geography and dates to streamline visualization.

\-- Determine the best kind of graph for visualization (scatter for correlation, line for trends over time, etc.)

\-- Being conscious of colors and visualization order to tell a story.

&#x200B;

I feel like what I have listed are primarily technical steps. What questions could I say that I thought through as I visualize, that could help explain what I'm doing to present data in the best way possible?",analytics,2022-07-19 22:50:03,6
"For data analysts, I can't recommend Alteryx high enough - I have been a user for many years and now sell the product.

For data visualisation, it depends if you want user interraction to gain real insight or just simple reporting.  For the former, I honestly believe that Qlik's SaaS product is the best and most fully featured (no hate from Tableau fans please!) and, for the latter, Power BI is cheap and cheerful but requires solid data prep.",2,w3hz8w,"My current company just went through a pretty big general firing of people and the ones left behind are being ask to create ""ground breaking new data products"". We have a brainstorming meeting next week and I would like to know how other analytics teams showcase data to their users.

For context my department is a ""business monitoring"" department, meaning we have all the business data. At the moment we have a very big report we do twice per week, then we have a weekly email every Monday and daily emails with general status of the different products / countries. These are generally Looker dashboard snapshots.

We use snowflake and Looker mainly, but also quite a bit of python.

I was thinking of doing something in an embedded site, but the things that our director is asking from this brainstorming really seems to me that he just wants to create a data analyst from code and have the code automatically come up with ""interesting insights from the business"" like what happened with x product during week 28, or things like that.

Our main objective is to have people switch from excel (and a pivot table that someone from finance did) to Looker / our reports, but we have had little success with this. 

I will keep on investigating, any input, information or suggestion is greatly appreciated, let's talk about reporting!

Cheers and keep safe from the heat (apparently Looker servers are down since a weat-eu Google server is down because of a malfunction from a cooling system lol)",analytics,2022-07-20 02:24:39,5
"If you wish to pursue Data Science (ML/AI or advanced analytics using modeling) then I’d recommend a more technically rigorous program.

I looked at University of Toronto’s curriculum and it’s decent and covers most subjects that touch data science. It lacks in Data Engineering or Analytics Engineering skill set - which you may need to help you in this market in terms of competition against other candidate. But the curriculum is wide enough that it’ll put you in the path of Data Science.
Though, I’d recommend supplementing it with more engineering (computer science centric or statistics centric) courses if you can.",1,w39fl0,"Hi folks, curious to hear your take on the MMA programs offered in Canada and if it’s worth it for non-technical individuals to pave their path towards becoming a Data Analyst to Data Scientist?",analytics,2022-07-19 18:28:43,1
"Analytics engineering is the closest, but even then you are expected to know how to model data (in a relational sense, not an ML sense). Cleaning is a part of the job and largely a means to an end.

I haven't heard of a role that is dedicated to just cleaning data. There's no direct value in the act of cleaning -- it's what you expect to be able to do with the results that matter. And, while I have worked with some pretty gnarly data, it's uncommon that a data source needs THAT much work, constantly -- and if it does, I will script it out so I can move on to the higher-value (and frankly more interesting) activities.",3,w35ed6,"I realize that there are jobs out there that deal solely with cleaning data and setting it up correctly for people who will do auditing and modeling.

Which jobs titles exist for these? I assume they don't advertise it like that but I wouldn't mind at this point.",analytics,2022-07-19 15:24:12,5
"From my limited experience BAs don’t do much data analysis or storytelling. They mostly do requirements solicitation and technical project management/support. So I guess I wouldn’t personally worry about the transition you are referencing.

I might be wrong, that’s just my experience. BA is a job that can be totally different between orgs.",12,w2s3ww,"I graduated with an electrical engineering degree 5 years ago and spent the first 4 years of my career working in more classical engineering roles; fiber network design, radio network technology, etc. 

I recently made a transition to BA on an entirely different side of the same company (AT&T). I absolutely love the culture and the team, and even the way the job functions; I love the *idea* of diving deeper into business partners needs and leveraging our data to generate an insight.

But I’m finding it really hard to make that mindset shift from prescriptive classical engineering design work, to the analytical, curious BA process that this job requires. I know great work starts with asking great questions and really getting to the heart of what my partners are really asking. I suppose I’m having issues knowing how to ask those questions. 

I’ve been intentional about touching base with other BAs in the business about their storytelling practices, because that’s another big component of the job role. Developing insights and a decision point for the business to make based off of the analysis I complete.

Has anyone had issues with this transition before? Specifically when it comes down to asking the right questions of your business partners and refining that storytelling technique?",analytics,2022-07-19 05:57:19,6
"1) Dimensions. Metrics cut by different dimensions. This includes time period but I personally treat the date/time as it's own thing as it's always relevant.

2) A single spreadsheet works. I'd organise by function and what insight they're interested in rather than by the function that 'owns' the data the metrics are built on. I.e. you might see the same metric loads but if these are true KPIs, stakeholders should understand how they influence them anyway so a lot of people will be interested in them.

As per the other commentor, what you capture now might not be relevant in the future and things you aren't looking at now will be. I would approach this in two separate tranches: a view of KPIs that align to your business' strategy and objectives in the first instance, and then the KPIs of each stakeholder group, function, however you're doing it and tackle them in a priority order. Have a backlog of who you'll speak to, but don't necessary do all the reqs capture in oen big go, do it as need - this is called 'just in time' requirements gathering and is considered best practice for business change.",5,w2yxkf,"We're working across the business to collect requirements for a data transformation / EDW implementation. The way we're approaching this is 1) collect the list of KPIs/metrics and 2) identify the ways that data needs to be cut by (e.g., Annual Recurring Revenue, cut by: Month, Product, Sales Rep). The next step would be a gap analysis between these requirements and our source systems.

This way we can separate the two and apply the 'cuts' to multiple metrics (e.g., how are we defining product, and apply that cut/definition to other metrics (like leads or opportunities). 

2 questions: 1) what is the right terminology for the 'cuts'? So we want to talk to you about your metrics and...?. and 2) any suggestions on best practices for collecting this information, or is a series of spreadsheets our best bet?",analytics,2022-07-19 10:54:38,3
"I’m an ex Nielsen based in Mumbai and in my experience there isn’t much growth in market research both as career and monetary compared to modern data analysts. You’ll be drowned in mediocre excel and PowerPoint presentations, researching mostly soap and shampoo. Both market research and Nielsen are slowly diminishing its charm. Maybe start with Nielsen only for learning purpose and join somewhere it requires most but less competition for you.",3,w2u9yh,"Career growth, work life, how does it compare with other industries like tech etc. ?

&#x200B;

e.g. in companies like NielsenIQ / Nielsen",analytics,2022-07-19 07:36:16,4
"Engage with the business, learn how they work, and how they think. 

Come up with new metrics to answer their questions or tell a new story uncovering new opportunities to move the business forward. 

Identify data analysis operations they do today that you can help automate and take tasks off their plate so they can focus on making decisions.",18,w24prv,How do I make the most of my business skills while working in analytics/BI,analytics,2022-07-18 10:14:33,10
I mean even google wouldn’t be able to answer such questions with barely any details,33,w25wmw,I mean Source domain section in google analytic. Not search query. It only happen with backlinks.,analytics,2022-07-18 11:04:37,13
"1. Get a non-data job using a computer
2. Do data analysis
3. Use experience to apply for data analyst jobs",114,w0vneo,"Hi everyone,

I don't know if it is just me or i searched on the wrong job board but whenever i am trying to find like entry level position or internship in the data analytic field there isn't rly ""entry"" level job. most of them required like 2+ years of experience in analytic field. 

so i was wondering how did u guys find the entry level data analyst position? like website...etc. Thanks in advance.",analytics,2022-07-16 18:27:17,34
"The job that gives you better experience. Company name doesn’t market as what matters is what you did and your skills makes you employable. 

Ask during your interview move questions regarding day to day role. Figure out where you’ll get a better exposure. How’s your immediate boss and colleagues like. 

If nothing else then go for better pay",16,w1401v,"I’m an international student and have just graduated from an American University. I have future plans to attend grad school and work in America. Due to military obligations I need to go back to my home country and can’t pursue OPT and work in America. After my service I plan on either working in my home country(Asia) or trying to recruit in Canada as I can potentially get a working visa. I know pretty much by now that I can get a job at a Big 4 firm as a Data Analyst but the pay isn’t as much as I’d get if I were to work in an equivalent job title in Canada. I know that if I were to get a job in Canada I’d most likely not get into any major company but the role alone would pay more even at a smaller less well known company. I’m trying to make a decision on which choice is better since I don’t want to waste too much time on recruiting. I’m specifically looking into data related jobs(analyst) have a few questions as follows:

Which one would look better for my resume for grad school in America? A lesser known firm in Canada or a Big 4 firm in Asia? 

Does the industry of the job itself matter(like if I get into a Data analyst job in tech in Canada vs Big4 in Asia )?

Does working experience in Canada make more sense if I were to want to work in America later on? Or does the company itself matter more ? 


Sorry for the convoluted questions and I appreciate any and all help!

Also TLDR; Big 4 Data analyst in Asia and less pay or Small tech/fintech potentially no name company Data Analyst in Canada with more pay?",analytics,2022-07-17 03:00:37,12
We are seeing some discrepancies between the two as well. In our case we are seeing higher numbers on the GA4 side. We are also seeing a big difference in our channel groupings. I have seen support articles that speak to the differences between the two [https://support.google.com/analytics/answer/11986666?hl=en#zippy=%2Cin-this-article](https://support.google.com/analytics/answer/11986666?hl=en#zippy=%2Cin-this-article).,2,w0vu64,"On July 14 UA says I had 201 pageviews, in GA4 only 119. 

On Jul 15 it is 164 on UA and 100 on GA4.

 I see data in real time in GA4  so its working.

Is this discrepancy normal? The difference I read is 10% but this is around 60%+.

I installed ga4 in my wordpress site a few days ago. I copied the tag into Insert Headers and Footers after connecting my site to ga4.",analytics,2022-07-16 18:37:12,1
Metabase is great,13,w0ij6a,Any dashboard tool that can be at par with powerBi but free ?,analytics,2022-07-16 07:58:04,14
Everybody starts at zero. I would start writing down where you feel your weaknesses are and see how you can improve them. Get feedback from your boss or even your peers.,17,vzrm2t,"For context, I came from non-technical background. I was in business-related role for over 9 years before making the switch to be a data analyst since 2 years ago. I've been employed under my current company for 5 months to date. Even though my colleagues are younger than me they have more experiences in the field than me.

 I do enjoy the scope of my work 
 but I also feel overwhelmed by the amount of work and the intensity of the problems faced everyday. Everyday I struggle with this heavy weight of imposter syndrome. I can't help but worry constantly that my work is not clean enough, or that I'm not working fast enough. I guess one of my biggest fear is that my colleagues or boss think I'm incompetent at work.

Logically and rationally I don't think my fear is 100% valid since my boss and colleagues have not said anything that suggest so but I still can't shake off that feeling. My anxiety is all-time high and I also find it difficult to detach my brain from work even after I'm off work.

I'm usually very efficient at work but I don't feel this way in current company. I don't think I'm doing terribly at my job but I feel that I have to put in extra efforts and hours than my peers in order to catch up with where they are. I do feel like I'm barely able to keep my head above the water most days. Honestly, I feel a little burned out from the job and I want to try to manage this before it gets worse.

Does anyone else experience similar struggle in this field? Would appreciate some advice on how to manage this. Thanks!",analytics,2022-07-15 08:35:30,14
Maybe one is doing a distinct count and other isn’t . We need more context to help,5,w067y1,,analytics,2022-07-15 19:50:09,6
"Can’t speak to the government side, but I’m at a marketing agency doing reporting and analysis for various clients (a/b test interpretation, recurring reporting, etc.). I spend the majority of my time in GA4 and SQL, and the remainder in Excel and PowerPoint. 

If you don’t have formal experience in data analysis, I would recommend taking public data sets (Kaggle, etc.) and putting together some kind of a portfolio to demonstrate to future employers that you know how to interpret and analyze data.",14,vzfjao," I will be graduating in December with a degree in public relations and political science. Despite my degrees, I have noticed my focus has gone more into marketing. I either wanted to do content marketing or advertising with a focus on government or political work(willing to work outside this).

I have an interest in using some sort of data analysis in my career. I love marketing and following a user's journey, and I enjoy writing and designing, but I do not want to get stuck just cranking out content. If I wanted to do sales, I wouldn't have gone back to school. I want to be able to analyze and make a decision on what to do next with the result of a campaign as well as help create it. I also feel like the industry is moving to a  place where data is the only way you can prove your value. I want to know if I am being realistic in what I am looking for in my career because it seems like marketing analytics and data analytics blur.

I  have been taking some time every day to dive into GA4, Excel,  WordPress, and Google Ads. I have several basic certifications to make up for the lack of analytics courses I've taken in college, and I am still learning. I assumed if you can track campaigns, whether it's conversions, website visits, and more, using the known tools and the n  make sense of it you can do marketing analytics. Where I am confused is if there are different types of marketing analytics roles. Maybe I am blurring the lines between data analytics and marketing analytics. What I  am seeing from discussions online is that marketing analysts at all levels deal with huge amounts of data and need to learn SQL, accounting,  and other math, which seems to dive into data analytics (I have not learned any of these in-depth).

Do  I  need to learn these detailed data analysis skills for marketing analytics, I don't want to be to crunching out numbers and doing reports all day either? Are there roles that have you in the middle of analytics and the marketing/business side? I would like to be connected to the marketing and business side, where I am involved in the campaign as well as assisting in tracking results. I do not want to crunch numbers all day.

This could be at a  smaller company, but is this realistic? If what I am asking for usually happens at a management level what is my best track to get there?  Sometimes I feel like a lot of advice comes from people who want you to shoot for the stars, so let me add, I don't care to work for a big company or make big money. I am not trying to be the best or some rockstar, just happy and live a normal life and start a family. I went decision back to school late, and I am 30ish. This is not me saying I have a low work ethic, I have done over four internships and have been involved with more than three organizations on campus in relation to marketing and PR. I grew up poor all of my life I just want a normal life and to enjoy what I do and I am so happy I  have made it this far. Thank you for taking the time to read",analytics,2022-07-14 21:22:27,15
"The real transferable skills are not statistics or software, but problem-solving and communicating. Maybe you like those bits and not the data stuff? If so, plenty of qualitative analyst posts are available, you don’t have to be a data monkey.

Also there’s WAY more interesting things to do as an analyst than monthly reports and data processing. I’ve been an analyst for twenty years and never once had to produce a report in my life (and would quit any job where that was necessary). Perhaps it’s the type of quantitative analysis you’re doing? Maybe a modeller job is of more interest.",7,vznp81,"I hope this doesn’t come off as a rant, but I’m burnt out as an analyst. I’ve been in an analyst role for 10+ years and have never been promoted. I just don’t think it’s the right role for me. I’ve been through 2 layoffs with different companies just to slide into another analyst role somewhere else. I don’t enjoy the work at all. Others have been promoted while I sit stagnant. I have the same skill sets and know the same software (SQL, SAS, Excel, PowerBI, Tableau) yet no promotion. I just don’t think it’s the right kind of job for me. I have no interest in data, I don’t like the tech side of it, I can’t stand being a report monkey, and I just don’t think I’m good at it.

Has anyone successfully transitioned out of an analyst role into some type of other job? What skills can you use from being an analyst that are transferrable to other careers?",analytics,2022-07-15 05:38:14,7
"1000+ properties o.O.
It's will be lot of work, yes.

With GA3, you can use Google AppScript and the Management API

You can get the CDs, write some new ones or just copy some to another properties. 

Unfortunately, for know, the management seems to not be available for GA4. Only the Admin API is open.

So you need to wait. Or someone else have another solution.",1,vyzha4,"I have a lot of parameters that I collect through GTM events, and I must set custom dimensions in GA4 for each of those parameters. My issue is that I have nearly 30 custom dimensions that I must create for a great amount of properties (1000+) now that we migrated to GA4.

Is there another way to create those custom dimensions other than manually? This sounds like a lot of work.  

I've looked into data import for GA4, but it sounds like Custom Dimensions are not supported.",analytics,2022-07-14 09:06:04,3
"If you don’t have a use case, then don’t use it. It’s really important not to use technology just for the sake of it.

One of the challenges to using Python/R in analytics is orchestrating it. A lot of analysts don’t have access to server-side environments to run their scripts on, so are limited to personal workflows they run on their desktop - not really good practice.

If your analytics environment is based around DBT and enterprise visualisation tools, you should stick with those",22,vyk3ru,"So I've been an analyst for the best part of a decade, self-learning my way from Excel to SQL to data warehouses to DBT, etc. and today I still find myself a novice with Python.

I try, over and over, to get the basics down, but the company I work with has no major use-cases for Python work, so I find it hard to actually apply the knowledge I acquire in my work. I learn fast, but I learn best practically by applying my learning to problems.

I'm looking for recommendations for switching up my workflow to integrate Python into my day to day. On the daily, my toolbox consists of SQL, BI Viz Applications (and their modelling layers), DBT, GIT, Google Sheets, the CMD line, Text editor, and various other tools and services.

How might you best inject Python into this collection of tools?",analytics,2022-07-13 18:53:28,7
"Use a third party tool to split randomly. Let it run for a few weeks.
Measure your metrics like orders and app openers:
Use an online calculator to measure the percentage change and statistical significance 

Googling such questions goes a long way",3,vyyeci,"I'm working in an e-commerce company where we are planning to run an A/B test to optimize for O/U (Orders per App Opener).

I'm new to this stuff. How do we ensure the two user groups are homogeneous?


One way I find is to check the delta O/U for both the groups, and few other check metrics if required. One of mt colleague suggested we do a T-test.

Please help me understand how we use T-test for this. How do we interpret the outcome?",analytics,2022-07-14 08:20:07,4
"Current employer has recently started using Segment. And I’ve gotten some exposure to the tool. From my understanding, it’s billed as a customer data platform (CDP) that functions as a central hub for your company’s data. Our current use case is fairly limited (just ingesting events from corporate site like signups, user interactions, etc), but you can then take that data, and pipe it to any number of other systems. For example, you could feed groups of users to an email platform for automated new customer onboarding, send prospects to your CRM for sales to follow up, etc.",3,vyq9qs,"Hello friends,

I am not familiar with Segment, but my current employer has it set up. I am thinking about moving towards Google Tag Manager to measure website + app. I suspect GTM is a lot easier to use for not so techy people. Would you say this is true? Are there specific benefits to Segment that GTM does not offer? Maybe I need to work harder to understand Segment?

Would love to read some expert opinions from you guys!",analytics,2022-07-14 00:47:56,4
"I did it, I thought it was a great introduction for someone currently working, has experience with Excel and some stats background and wanting to learn about other tools available for BI.

It introduces SQL, Python, and Tableau all of which I use currently and frequently in my BI role.",12,vy9x50,"If so, was the course worth it? Did it help you in your transition to Analytics?",analytics,2022-07-13 11:15:06,21
"In GA4 they are called sub-properties and will allow you filter by a number of things, such a hostname",2,vymyix,"On Universal Analytics, I can create a View with filters to view the pages' domain names since I track multiple subdomains. How do I do the same on GA4? Currently only able to see the page title. I need it to be able to filter page views by domains/subdomains",analytics,2022-07-13 21:22:40,3
"Here's advice from someone 10 years older - don't regret anything in life! Every decision is an opportunity. The way I see it, you've realized at an early age what you like & what you don't like. That's actually a fantastic place to be!

For your current situation - I don't think you're in as bad as a place as you think you are. The majority of your job still seems to be doing the things you like. There is obviously a portion that you do not enjoy but that isn't necessarily a bad thing. All jobs have tasks like this.

What I would recommend is figuring out why you don't like doing the analysis & presentations portion of your job - it could be that you do enjoy it, but you are much worse at it compared to data engineering, so it takes longer & you don't enjoy it as much. 

We are naturally inclined to do what we are good at, but that can be pretty limiting as well. Just something to be aware of before you make a big decision. Analytics & presentations will be a part of most jobs, so these are not skills that you are wasting your time developing.

If you find that you truly enjoy data engineering more than analytics & presentations, you're still not in a bad spot! Here is what I would do:

1. Figure out the exact kind of role you like i.e. is it pure Data Analytics, pure Data Engineering what aspects of these roles do you truly enjoy.
2. Start finding roles right now. You don't have to wait. You've managed to get two jobs in the span of a year, it shouldn't be that difficult to find another role you enjoy better. This will also help you figure out what's currently on the market. If it comes up in an interview, you actually seem to have a pretty cool story to tell your interviewer! 
3. As you are starting your job search you can if you want, talk to your manager. You may be valuable enough to him to keep you despite not having you do presentations & analysis. For example, he may be looking to expand his team, in which case having one person focussed on the data engineering side & 1 person on the analytical side might actually be preferable. There may also be other roles within the company that you can move to that would have a data engineering role. Especially if this is a big company, they'd much rather just move you between teams than have you leave (assuming you want to stay). It costs a lot more to hire someone new.

Hopefully this gives you some direction & makes you feel a bit better. Best of Luck!",16,vxkam3,"So I started a new, fully remote role as a Marketing Data Analyst a little less than 3 months ago at a large hospitality company. 

I finished my undergrad (BBA in Economics) 1 year ago and this is now my second job - first was a Marketing Analyst at a very small company. I didn't really know what I wanted to do after college + the pandemic was happening so I took any position I could get. I took on a way more technical role than my title suggested (developed and implemented Tableau for the company, automated pricing updates with Python, etc...) and became very interested in data/programming in my time there.  A recruiter reached out to me on LinkedIn back in March for a my current job, which offered a huge salary bump, fully remote and a wide array of other benefits.

Interviews went great, and they made me an offer.

An important note is that I admittedly don't care about marketing nor hospitality, and my decision to accept this job was almost entirely based on the salary, WFH and benefits. The skillsets required on the application were in line with the stuff I enjoyed working with (SQL, Tableau, Excel, etc...) so I thought why not. 

Anyways, the Marketing Analytics team at my current job is just my boss (who's a Senior Director and in meetings 24/7) and I. My current duties are to run weekly/monthly/ad-hoc requests & reports that I pull from our DB. In my second month here, I finished automating all of these reports and am now working on implementing our unused Tableau subscription to live-feed workbooks for digestible visualizations. My boss and I meet once a week to go over these tasks and any other ad-hoc data requests that people have asked for. Occasionally my boss will ask me to create some slides for a certain request too. It's a *very* independent role which I like a lot. 

The data engineering/development part of my job I really enjoy, find interesting and take initiative to learn new material and optimize our data pipelines. But I've come to the realization that I don't care for presenting insights to the team and creating slides for upper management *at all*. I'm not sure if it's the department I'm in or the industry (probably a combination of both), but I am completely apathetic and almost dread doing the domain-specific analysis. 

This is where me disregarding industry has come back to bite me.

I know 3 months into a new job (especially remote) is very little time to familiarize oneself with a company, but as time goes on I find my interest in the technical aspects growing and interest in analysis shrinking. 

My boss emphasizes that he wants me to take on more of a role of presenting to the team, creating decks, etc... and I've been hesitant to tell him about my interest in the engineering side of our data (which could use a ton of work as everything is being done in a bloated Excel workbook atm) because a) I've been here for such a short amount of time and b) this *is* technically the role I signed up for - to be an analyst and present insights.

I don't know, just kind of venting and wondering what I should do. I'll probably end up riding it out for at least 9-12 months, hone my technical skills and begin applying to Data/Analytics Engineering roles.

Would really appreciate any advice from those that have been in this position before. Or just any thoughts on the matter. Thanks

TL;DR

3 months into new job as Marketing DA - no interest and/or knowledge in my industry, really dislike presenting analysis/creating PowerPoints and would much rather transition into a more backend, technical role.",analytics,2022-07-12 13:06:58,13
"The most important aspects of analytics are domain expertise, business acumen and curiosity. 

If you have those you’ll go far.",33,vxis3e,"


Is it a good career to go into? Are there opportunities for advancement and growth? Can somebody succeed in this program/field if they suck at math? What level of math is required? Are you required to become good at programming?",analytics,2022-07-12 12:00:31,49
Business analyst role or a business intelligence manager. These skills would fit perfectly for a lot of BI managerial roles,2,vxtx96,"Hi All,

First just wanted to thank this sub as, even though I'm not super active here, it was super vital to me during the time I was miserable as a CPA and looking to move into data/analytics. Simple advice I got from searching through threads here about what languages/tools to pick up, how to go about learning them, or even salary threads gave me all the motivation and direction I needed to make the switch. This career isn't perfect, but I'm materially happier now than I was grinding out long hours as a CPA for low pay.

Now that that's out of the way, I'm starting this thread because I want to now figure out what my next steps or how I should grow my career at this point. Having spent about a year now as an analytics consultant, I now have a better sense of what I'm good and bad at and want some advice on how to address my weak points so I can continue to thrive in this field. For context, I'm a Manager with a Big 4 firm in their analytics department. I help support both corporate strategy and M&A projects by ETLing large datasets the Excel monkeys can't handle and then generating reports/insights from a consolidated database.

The main thing I've realized from working in analytics is that my business domain knowledge is super valuable. Basically, me working as a CPA and understanding what stakeholders look for and having run so many ad-hoc scenarios/analyses means I know exactly what questions to ask, and therefore what data we need/how to structure an output that'll add value. Where I'm lacking, however, is when it comes to more advanced statistical models. For example, if I have to run a regression, my associates know how to clean the data/interpret the results far better than I can. I understand at a high-level what regressions, k-means cluster, etc. are and can Youtube/google how to write the script in Python, but I feel like I'm lacking in the theory behind this stuff or how to go about transforming the data to get it ready for these analyses. I'm wondering if there's a cheap way, hopefully short of doing a full-blown masters in analytics/stats, to address this weak point. The business world candidly isn't yet ready for most advanced analytics anyway from both a data/knowledge standpoint, but I don't want to bank on that not changing in my lifetime ever and want to be prepared for it should it happen.

Further, I think I'm confused on where to take my career/what I can reasonably advance to. Right now, I'd say I'm more of a ""businessman"" than a quant and my competitive advantage will always be that I understand business needs rather than being a wizard at stats or data visualization. I think of myself more as a ""business analyst who knows SQL/Python and can work with/manipulate big datasets and understands stats better than 90% of Excel monkeys in finance"" rather than someone who is ""a god at SQL/Python/Tableau/Stats and is trying to learn the business side"", if that makes sense. Given this, what are roles I should be looking for or where can I reasonably take my career if I decide I want to bail from consulting?

Thanks a lot for the continued help as I try to figure out my career out!",analytics,2022-07-12 20:58:30,4
"I 

Fucking

Love 

Power query",31,vxh281,For smaller datasets I'll admit it's quite efficient and simple to the point. Though for complex queries on large datasets it becomes a bloated mess. Maybe I'm just not patient enough to wait for every step to load.,analytics,2022-07-12 10:43:36,18
"6 months was enough for me to learn sql, r, and power BI, but I already had a 4 years experience as a data analyst using excel  before that, so learning programming was just a natural path.",7,vxqkgl,I have been thinking about getting into data analysis and was wondering a range of about how long it would take me to become employable studying 30-40 hours per week? I am looking to either go to college for 2 years for something else other than data analysis or doing self teaching mixed with possibly a bootcamp. Thanks!,analytics,2022-07-12 18:01:07,8
B2b or b2c: who has better vending machines in the break room?,3,vxxbru,"Analytics roles (DA, BA, DS etc.)",analytics,2022-07-13 00:30:28,3
"Thank you for this great write-up, went to your blog and instantly signed up lots of great information I am excited to read through.",1,vxvsd4,,analytics,2022-07-12 22:49:18,2
Eia.gov/opendata/browser/electricity/rto/region-sub-ba-data,3,vxs9xd,"Im looking for high time resolution data on energy usage/generation/import for California. I can find yearly averages but nothing high high res. I was looking for daily atleast and hourly ideally. 

Thanks for the help",analytics,2022-07-12 19:29:53,1
"What do you do in your current role? There’s almost always a way to make your current work more data-driven. Does your company have anyone doing people analytics work?

I’d recommend getting more hands-on analytics experience in your role before doing many more applications. Classes are great but companies love experience.",11,vx7vea,"I have a Masters degree in I/O so have statistics knowledge, however my program primarily used SPSS and didn’t really show how you can apply stats to HR/people related data. I currently work in HR with very limited data related duties (it’s a very old-school company). I’m interested in moving to a purely people analytics role. Id like to learn as much as I can and learn things like SQL, Tableau, Power BI, Excel dashboards, along with a refresher on interpreting data in general. I just don’t know where to start or look. I’ve been watching some videos but that doesn’t feel super productive and helpful. I’ve applied for a few analytics positions already and am not surprisingly getting rejections. 

So my questions are: 1) are there any other important programs/skills that I should learn in addition to what I mentioned? 2) any suggestion on courses or certifications to learn all of this? 3) once I’ve learned some of this, how do I show that in my resume? As in, without actual analytic work experience how do I show I have applicable skills? 4) and finally, where do you suggest starting on all of this? 

Side note: I’d also like to learn and test out if this is for me before actually getting in an analytics role. I love data and spreadsheets but I’m clearly not super confident or advanced in my analytic skills at the moment. I don’t know if I’m in over my head and if it’s unrealistic that I can get into analytics.  

Thank you in advance for any advice or suggestions!",analytics,2022-07-12 03:22:17,21
"Yeah it's in the admin settings under
'Tracking code' or 'tracking info'. I forget which one

If you need the GA4 ID, you'll find it in the same place, but listed under 'Data Stream'

Edit; ya, you're welcome too",1,vxdnm8,"Hey, my client created a property for their website on Google Analytics but didn't retrieve a Universal Analytics ID. Is there a way to retrieve this after the property has already been created, or do we need to start from scratch (create a new property)?",analytics,2022-07-12 08:16:50,1
"My personal rule is to stay in a role as long as I'm learning/advancing myself. Sounds like that may not be happening here, so you should ask if there's any other reason to stay.

Tactically from a resume perspective, if you start looking now, you can leave this role off your resume entirely. A gap of a few months in a resume is probably better than staying with a company for just one year.

Feel free to dm me if you'd like advice on demand forecasting roles - that's an area I've hired for in the past, and likely will again in the near future.",6,vx5op4,"I recently joined a quick commerce grocery delivery startup in their analytics team. My background is in advanced analytics in the pharma industry, and I felt this was the perfect move to diversify my experience and learn ecommerce.

However, the manager who hired me left the org before I could join, and there's been some restructuring. My interview discussions was about forecasting demand leading me to believe I would have a mix of ML and descriptive analytics work. However, my entire pipeline for the next 3 months is set out to build about 10 dashboards. While dashboarding is important, I don't enjoy doing it day in and day out.

Staying on the job will help get me experience with ecommerce data and dynamics but I would really hate to do this for a few months. Is there value in sticking it out? Just wanted to get your thoughts since I'm a little overwhelmed myself.",analytics,2022-07-12 00:53:06,3
"Does your company have any internal reporting that measures key business metrics? Try getting access to those and learn from what is the critical metrics that defines success for your company. Eg daily active users, or, average order size,etc.",1,vx0nnh,"Hi r/analyticss, 

I got a new job with an IT company that has an analytics team, and in the job I do a lot of interacting with customers. I have a good IT background but it's in datacenter and cloud, the most anayltics-ing I've done is SQL queries grepping.

I want to be able to talk to our customers about analytics and a) sound like I know a little bit about what I'm talking about and b) know when someone is baffling me with bullshit. 

Is anyone able to share any advice or links around what sort of things I can ask a customer to understand where they up to with their analytics efforts? My company partners with Adobe, Google, and Snowflake but I don't understand what a typical anayltics environment looks like apart from a very basic idea of raw data in a data warehouse > anaylze using ML models, python, otherwise stuff? > presenting results + visualizations provided by things like Tableau. By way of example if I were in a similar position in my old life I'd be asking customers about the IT infrastructure, suppliers, priority apps, data storage, backups, DR, HA around critical infrastructure, etc. 

I'm hoping someone is able to provide a high level overview of what I can expect to find out there, bearing in mind a lot of my customers are large, slow moving companies like insurance companies, although I hear there is a lot of work around AML and analytics-enabled service personalization going on around 

Thanks in advance! gtg screaming child",analytics,2022-07-11 19:56:34,2
"Could always go for a certificate from a university if you have the ability to. I am seriously considering it, either from a recognized university or community college. I like the structured learning a program like that offers. But also interested to hear what others think. If learning online, my first search would be udemy. But it takes awhile to discover worthwhile content there - there are some very good classes though. I would learn to use smartsheet. I’m in the process of doing that.",2,vwm3sf,"Crossposting from r/BusinessIntelligence 

I have been recently promoted into a people management position for a small team of data analysts. We  have a multitude of projects and are struggling to keep track, prioritize and handle the different tasks.

What are your recommended books, courses, etc. on project management for data projects?",analytics,2022-07-11 09:10:53,4
"The simplest way is to learn and use Tablrau Prep. It's purpose is to pull data together fr multiple sources and engineer them together to a dataset that will work for your visualizations. Other benefits include being part of the Tableau ecosystem which you are already familiar with. It connects easily to the products you have listed with the exception of ServiceNow. That is a monster of its own and unless your IT team has built APIs you can use, flowing data from ServiceNow is klunky at best. I have built several flows to go around the lack of api, but it's not pretty.",1,vwzqea,"Good afternoon,

I have been working with Tableau for about a year now and have worked very hard at being a proficient Tableau developer. With that being said, I still struggle with more of the back-end work that ultimately feeds to Tableau. By back-end work, I am referring to the process of data collection & manipulations and not as much on the visualizations. I am kindly asking how others implement a data flow system from start to finish. For example, pulling data from an excel spreadsheet into Tableau would be one of the simpler examples. I am looking more for some type of form that would capture the data and then feed it to a database that Tableau would then pull from.

For those interested, my company works primarily with Tableau, Microsoft products (such as SharePoint and Excel) and ServiceNow. It would be a bonus to be able to create systems using these types of softwares but I am open to all examples. I truly just want to be a more value to the company that I work for.

Thank you all for the help and support. Much appreciated!",analytics,2022-07-11 19:11:34,1
based on the title i'd say college      j/k,33,vwkqwp,"Hi I have an associates of arts degree and right now I’m working for a financial firm as a financial rep holding S7 and S63 I want to get into the analyst role but not sure if I should go the self thought route and take courses on coursera, freecodecamp, data camp etc. and work as a data analyst then take courses on CFI, Wall Street prep and maybe get my CFA and go the financial analyst or wealth analyst route role or would I be better if just going to college for 2 years more and get my finance degree?",analytics,2022-07-11 08:12:22,8
"Esri has lots of eLearning material, I'd look into those as a starting place",6,vwj68s,"Anyone know where I can learn ArcGIS? Online self paced or any other format!? 

TIA!",analytics,2022-07-11 07:02:11,1
"Caveat: This is for US Market

There are self-taught analysts, but what helped them is prior work experience in whatever field that’s relevant to the industry they’re in: for example, Marketing/Design person doing Analytics for Marketing Campaign or UX.

There’s absolutely no need to go to school to learn Python and SQL: subscribing to Udacity, DataCamp, or any other MOOC is more than sufficient. School is great if you want to augment your current skills with even more advanced skills: more deeper mathematics or business intelligence or strategy approaches to solving problems.

However, I’d recommend finding a job now, and work somewhere and see potential avenue of work with data. For example, given your field, working at an Ad/Marketing Agency and measuring how your content drives results. 

As minimum if you can learn how to use Google Analytics and Excel, then you can get any web related jobs for any sized company.

Python and SQL are great for slightly more larger companies that have a lot of data or at least infrastructure to host data (otherwise, how are you hoping to access data with SQL if they don’t have that framework).

All the best!",11,vw9aue,"I'm a 33 y/o freelance content writer looking for a career transition. I want to get into marketing analytics or data analytics as I found the field pretty interesting, and I have done SQL and a bit of programming during college.

I cant enroll myself into college again due to financial constraints, plus I'm in India, so it is difficult as it is.

I wanted to know if it is realistic to land a job as a self-taught marketing analyst? Also, what should be a realistic timeline to study things before doing portfolio projects, considering I have almost a full-time job? Thank you!",analytics,2022-07-10 21:00:23,15
"Honestly, not being funny but this question is asked daily (if not more than daily). Please read the other posts as they’ll be a lot of good info in the replies that will be directly relevant to you.

But the short answer is - it depends. Just apply for some jobs and see how you get on. In the meantime just work on some projects that YOU find interesting, don’t try to second guess what someone else would find useful as you’ll tie yourself in knots. 

Try to solve interesting problems, and have an enquring mind - these are way more important than any particular technical method.",3,vwnm7f,"So I have a bachelor's in economics and an associates in accounting. I worked content review at a social media company, tech support at Robinhood, tech support at another bay area startup that makes learning programs, and am currently doing data entry at an information management company. 

I'm taking classes at my old community College and working on projects. My questions are

What kinds of projects look good for someone applying to a business intelligence analytics position? Should they showcase queries in pandas, sql, graphs in matplotlib and plotly, etc? 

Do I need to learn scraping for data analytics? Because I read somewhere that I don't. I don't know if it's true.

What kinds of companies should I apply for given my background? What applicable domain knowledge do I have?

Do I need to apply for a job in another field besides analytics to gain experience? Because I want to go straight into analytics if I can.

Thanks",analytics,2022-07-11 10:14:30,3
Like what kinda employers exist for business analysts? Literally every company ever.,6,vwnl9d,"I’m thinking of Doing my specialisation in Business Analytics and What are the job prospects. (I like the Fin tech sector)if you all could suggest some good resources to learn about it, that would be great.",analytics,2022-07-11 10:13:18,3
Why is this list of random tools getting upvotes? Tf is wrong with this sub?,18,vw0z5o,,analytics,2022-07-10 13:59:29,14
"Pretty generic description, and description doesn’t really matter. Your interviewers may not even review it while deriving their questions.

What does matter is having stories that speak to your accomplishments. Come with 2-4 stories on hand that demonstrate skills required in this job description…that also speak to accomplishments. This way you can work those in to whatever question they ask you.

Make sure these stories are fairly universal so you can pull them in to different interview answers..because it’s easier to have a few ready that can be adapted than trying to prepare for every interview question ever.",5,vvv2s2,"Hi guys. I have an upcoming interview coming up but I have no idea what the position actually entails. It's for a bank. Could someone help me out?

 In this position, the individual will serve as a data testing, reporting, and analytical resource 

Responsibilities of the role include, but are not limited to:

* Perform detailed code reviews, identifying anomalies and other potential data quality concerns
* Utilize data analytics skills and specialized knowledge to independently research issues relating to data integrity driving to root cause
* Document data acquisition procedures performed and summarize relevant system access
* Supporting the end to end production of periodic performance tracking reports, including trend analysis for management, senior leaders, and key executive stakeholders
* Monitoring metrics and associated data to identify and escalate potential issues and providing recommendations for resolution to ensure optimal performance

**In this role, you will:**

* Participate in less complex development and design of methodologies and standards for review activities companywide in alignment with the risk management framework
* Ensure effective and appropriate testing, validation, and documentation of review activities for risk programs, risks, and controls according to standards and other applicable policies within Independent Testing
* Support and implement less complex initiatives with low to moderate risk and exercise independent judgment to guide risk reporting, escalation, and resolution
* Present recommendations for resolving more complex situations and exercise independent judgment while developing expertise in risk management framework and the risk and control environment

 

**Desired Qualifications:**

* Technology experience with any of the following:  SQL, SAS, Toad, PowerShell, VBA, DAX, Power BI etc.
* Experience gathering, analyzing and interpreting large datasets
* Ability to retrieve data from various information systems
* Experience in the technical build-out and/or support of databases, query tools, reporting tools, BI tools, dashboards, etc. that enable analysis, modeling, and/or data visualization
* Experience translating complicated data from multiple sources into insight, knowledge, and understanding for our business partners
* Ability to prioritize work, meet deadlines, achieve goals, and work under pressure in a dynamic and complex environment
* Excellent verbal, written, and interpersonal communication skills
* Ability to take on a high level of responsibility, initiative, and accountability
* Ability to summarize and recommend new strategies/solutions. 
* Strong analytical skills with high attention to detail and accuracy. 
* Ability to research, trend, and analyze data. 
* Ability to effectively communicate, verbal and written, with various levels of management, including senior leaders and executives.
* Experience in Audit, Compliance, Risk and Testing
* Critical Thinking and Analytical skills",analytics,2022-07-10 09:25:21,3
Does Google Optimize not solve the problem you're trying to solve?,7,vv3wdf,"Hi,

for AB Testing i use Kameleoon but im tired of using the Kameleoon Results as my primary source for Test Evaluation so can anyone recommend a source on the subject of A/B test evaluation with Google Analytics? With Google Analytics, the topic of sampling in particular i don't know how to manage. 

&#x200B;

Thanks",analytics,2022-07-09 08:17:33,4
Ask the recruiter to clarify your questions,11,vvjzlw,"I have an interview with a large bank soon. ""Independent testing"". No idea what it means and I have no one to ask to help me with it. I'm trying to prepare for the job but I can't figure out what the job description is asking for. Could someone help me out?",analytics,2022-07-09 21:59:36,3
"Not the hero we deserve, but the one we need. Thank you, kindly!",6,vuhk4o,"Hey everyone, longtime lurker here. I noticed that a lot of people really hate the new Google Analytics 4 UI so I made a free Data Studio template. You can get it here: 

[https://datastudio.google.com/s/qJvqv6KDmuc](https://datastudio.google.com/s/qJvqv6KDmuc)

I'm not asking for anything in return, not even your email address. Enjoy!",analytics,2022-07-08 11:36:20,11
"Yeah I work in HR analytics, DM me a link and I'll send some general suggestions.",2,vusyn8,"Hello everyone, I am graduating college this fall and I am hoping to land a data/business analyst job post graduation. I want to start sending out my resume to companies within the next couple of months to get ahead of the curve.I was wondering if someone in this subreddit with industry experience  could take a quick look at my resume and give me a few pointers(nothing extensive since I know we all have busy schedules). Thank you in advance.",analytics,2022-07-08 20:55:49,7
Source?,3,vua343," 

* To provide detailed information related to major factors (drivers, restraints, opportunities, and industry-specific challenges) influencing the market growth",analytics,2022-07-08 05:52:19,8
You have to be an admin and you have to unlink it from Google ads first.,1,vue8qu,"I'm trying to clean up some old properties and accounts and would like to consolidate a few properties under different accounts. All tutorials point to there being a ""Move"" button inside of the property settings, but I'm not seeing it anywhere, despite having full ownership of the properties.

Is this something that has been removed/disabled during the GA4 transition?",analytics,2022-07-08 09:08:09,1
"You know that it’s really common to move between data jobs?

Congrats on the position. You’ll be fine.",21,vusixp,"Just wanna say thanks for the help guys. 

I studied and trained to become a DA for the longest time. Reading this subreddit also pushed my improvement in all aspects everyday for 20 days since my graduation.You guys were very helpful and necessary in achieving my dream as a DA.

Sadly, my dream as a DA ends here since most companies want more experienced DA applicants. I have to leave my dream and accept a Job that was offered, as a  Data Engineering Analyst.

It hurts me to say this but 
Thanks for everything. You guys were amazing.",analytics,2022-07-08 20:31:09,6
"An eternal truth is that end users aren’t great at specifying what they want. There will likely always be a good amount of back and forth after publishing a new dashboard/report, so plan on that. In my experience, the best solutions are:

A. Get your team experience with the use cases end users face. If the person designing the dashboards understands the mindset of of the person who’ll be using the dashboard, they can proactively design the dashboard to fit the needs that user will have.

B. Build flexible data models/dashboards with the knowledge that the first draft will almost always need significant revisions. This can go hand in hand with A. (The stakeholder asked to see X and Y metrics, but I bet they’ll want to see Z as well. I’ll make sure that’s easy to add). The most productive way to handle this is to go through the report with the end user live and make quick tweaks with them. That way you can go through multiple iterations in a single meeting rather than waiting on emails/Slacks back and forth.

C. Rather than asking what metrics users want displayed, ask what pain points or problems they want the report to solve. For your example of call center metrics, instead of asking about metrics like average call time, etc. maybe ask questions like “are you hoping to better identify what might be driving call times up? Per individual or department wide?”

Basically, domain knowledge lets you proactively address stakeholder concerns without having to try and get it out of them in the form of clear-cut requirements.",14,vtpmrt,"I'm curious to hear what the methods or tools are that teams are relying on for this. My team does reporting in PowerBI on call center metrics, support cases, etc. We get requests from ops leaders about what they want to see but then an analyst goes and designs it in PowerBI and we go through like 10 iterations of refinements with leaders and it seems like we just can go on forever.

How does your team approach this? Is there any particular method you like for eliciting requirements from leaders in advance so that it's more clear exactly what needs to be built and what the finished product will look like?",analytics,2022-07-07 11:37:09,4
"Hey, I have been working as an analyst since 2016, instead of Tableau you could check out Google Data studio. Tbh, many companies use different software solutions, don't intimaded, I always say, even in interviews, that I might not know it now but i am happy to learn since analystics is such a fastly developing field. You got this.",10,vtkr3j,"QUICK UPDATE:

So my resume has nothing about Tableau or Python, so I can honestly say I dont have working knowledge - that being said, what could I do to understand the basics of tableau/python so I go in with something rather than nothing? R is the only thing that I have on my resume so I am going to hone in on that and try to understand it on a deeper level.

&#x200B;

I studied math and finance during undergrad and minored in economics. I used R in 2 classes only. I have excel experience but none in SQL, VBA. I dont have any python experience. The job description states i need working knowledge in python (panda) and tableau. I was thinking of doing the google analytics course and getting that certificate. How else can I prepare for this interview? I really love data science however I never had the opportunity to work in the field. I really want this and am willing to do whatever it takes to gain some knowledge on data analytics in the financial field

&#x200B;

&#x200B;",analytics,2022-07-07 08:04:51,23
Tableau is the number 1 tool used in Netflix for business users to access data.,6,vtu83f,"Hey all,

I am starting my first job next week at a fortune 500 doing analytics for their marketing team. I will be the only analytics hire on the team and will be taking the reigns I suppose, in terms of direction and composition of dashboards, kpis and quarterly reports.

Technologies I will be using will be a Python tech stack such as Numpy, Pandas, Seaborn, Matplotlib, etc. and SQL. 

I have personal projects and such that I have done but never any corporate experience of using data and such in real business cases. How should I be presenting my data, is jupyter notebook appropriate or should I use plotly and dash or Tableau,?

Or any general advice for me in terms of beginning my analytics career and/or any advice in terms of how I should be approaching my new role would be greatly appreciated.

Thanks y'all",analytics,2022-07-07 14:59:47,5
It’s a super well known issue so much that the IAB maintains a list of known bots and spiders.,16,vtim87,"Hello! I run a sociotechnical security firm called International Persuasion Machines, and I wanted to share some research I worked on a while ago, and hear what folks in this community think about the vulnerability, and how you solve for it today.

Basically, \*every\* web analytics platform can be easily spoofed with incredibly basic fake browser environments. If I want to fake any numbers on Google Analytics, Mixpanel, Oribi, Indicative, Woopra, etc, all I have to do is spin up swarms of fake browsers and fire the traffic off. This has serious implications all over the place - in one case I demonstrate how to defraud marketplaces with this ability, but also this has implications for people trying to fake their numbers, for grey-market services that ""guarantee"" traffic numbers, ad fraud, and so forth. In a longer explanation, I walk through and provide screenshot proof of how this fake traffic shows up on all these analytics panels. How well known is this issue, and how much do you all, as practitioners, do to mitigate fake traffic? Do you have custom rule sets or some browser extension or other toolkit you use to de-bot your numbers? Are there companies that specialize or particular platforms that specialize in de-botting these numbers?",analytics,2022-07-07 06:22:24,9
"If you don’t have analytics experience, you’re setting yourself and your team up for failure to go straight into a management role.",5,vu07bi,"I have an upcoming interview at a tech firm for a product insights manager role. The product that they sell is related to tax and accounting. As a background, I have training as a CPA in audit and have spent some time in finance on the buy-side. Would love if some ""insights"" :) can be provided by kind soles into what this role entails - what the day-to-day looks like, what they say in me, and how I can best help the organization. 

About the role:

\- develop and attract product analytics team 

\- data-driven approach to strategic decisions for products

\- work with design, development, and product to understand state of products and progression to goals, implementing solutions, and deriving insights

\- create data driven testing culture to define, interpret and implement test results

\- improve organizational metrics and identify areas for improvement to help customer experience

&#x200B;

About you:

\- strong financial knowledge and ability to define and drive business KPI's and customer benefit metrics

\- ability to influence within a matrix organization

\- partnering with product, technology, design teams to leverage analytics and technology to solve customer and business problems

My 10 cents on the role is defining a KPI and determining how to improve it and looking at other metrics which will lead to a good customer experience and furthering development on the product on areas which can be improved to work towards a better customer experience. Thanks again!",analytics,2022-07-07 19:48:16,7
"Prediction of movie success depending of main actor and/or production expense , director etc",1,vtlnsv,"My DA boot camp is wrapping up this month with a capstone project where we are presented with a set of data and we are to perform EDA and apply various methods to gain insight and try to answer some questions of our own making.

The provided data set is the IMDB database (hate typing that with that acronym lol). We are allowed to find additional data sets to complement this. 

Generally speaking, most teams are to pick a genre and do as described above.  Not looking for anyone here to do my work, but my questions for you are:

1) if you know of some, what are some other data sets I could use to complement this information about movies, where can I find more?

2) what are some interesting topics you can think of to pursue besides just comparing box office totals etc.

I realize these are kind of loaded, and I would like to avoid having you do my thinking for me, but if you can figure how to say it, I'd love some ideas to dive into and get my hands dirty.  I'm trying to avoid ending up with the same presentation as all the others in order to stand out. 

3) Maybe what are some ways I could stand out?",analytics,2022-07-07 08:45:48,2
"You can write in SQL, Python, R, Hive, Impala, Julia, Scalia, SparkR, and PySpark depending on what systems of compute you have attached to Dataiku. Assuming your data source is an SQL database you can write SQL. At a bare minimum, you can make use of Python recipes. Nice thing is you can start with Visual, inspect the SQL (assuming a DB), and write your own. Same goes for the Visual ML capabilities. If you use the in-memory Python recipes, depending on the algorithm you can click the view in a notebook and it will give you an approximation of what Dataiku used to build the model.",2,vtuxf3,As opposed to working with SQL,analytics,2022-07-07 15:32:07,2
"Wow, good question! Luckily data skills are highly transferable between sectors and all sectors need data analysis skills to a more or lesser degree. I think a natural start is one where you can use your commerce knowledge. Having some domain knowledge is always a HUGE boost to your career prospects. I would also look for commerce-related roles where you start applying and practicing your data analysis and bringing it into the role, rather than looking for a data entry role. You will much faster start doing more rewarding work (intellectually and monetary) that way.

Re picking an industry, thats more tricky. If I was starting again I would start at anything that catches my eye and/or has a lot of job openings.  EG you'll probably find a lot more roles at freight/shipping companies than animal rescue, for example. Then find those companies webites and for many of them, read their Annual Reports and particular any sections that talk about their plans. You'll spot a trend in what they are all aiming for and you'll see if that sounds good to you or not.

I haven't seen or heard that there's much of a linear progression in data work from Data Entry to something else and then something else.  For example, I am now a Senior Analytics Manager, and I still sometimes have to do ""data entry"" type of activities.  You're more likely to turn another job into a data job.

Re looking for projects to build your portfolio (and skills), I wrote some suggestions here: [https://www.reddit.com/r/dataanalysis/comments/vl4bus/my\_suggestion\_for\_building\_a\_portfolio\_when\_you/](https://www.reddit.com/r/dataanalysis/comments/vl4bus/my_suggestion_for_building_a_portfolio_when_you/)

What type of stuff are you studying as a Commerce student? If you weren't looking for a data role, what roles woudl you typically search for? That might give us some clues about which industries might be good options. 

PS for what its worth, you might say Ive switched industries about 3 or 4 times now.",3,vtszhz,"I am currently a commerce student going into my third year, and I am looking for my first data entry job. Certainly, people often have to switch between jobs to find the industry they are really passionate about, but I still hope to start my first data job in an industry that I, at least, do not dislike. Also, I have no previous experience in data, and I am looking for projects to build my portfolio. I suppose it may be easier for me to get a job if I can choose my projects according to the industry that I am going into?

So what are the best ways to have an overall understanding of different industries, which can at least help me find out industries that are definitely not a match for me? 

Feel free to share your insights!",analytics,2022-07-07 14:01:53,2
"If you get a job you likely won't come back to coursework ;)
You will be promoted to googling/stack overflow.

But to answer your question, i think it may be better to download the videos and transcripts you may want to come back to. Site usage policies keep changing.",14,vtcm0f,I just finished the entire certificate today and claimed my badge. I finished before my free trial ends.. I wonder if I can still access the videos if I need review in the future.,analytics,2022-07-07 00:15:31,9
IMO stay where you are the proclaimed golden child and get a high end advanced degree. Your networking opportunities should make it all worthwhile.,5,vtkyax,"Hey Reddit, 

So, I’m at the point where I think I should be looking for a new job. 

Would go from Senior Data Analyst -> BI Developer. 

2.5 YoE. 

I’m no longer challenged at my current job, except for every few weeks I’m under an insane crunch to produce results overnight for several days in a row. I am also 1 of 2 on our team doing this type of analytics for our entire enterprise of 280k+ employees. My boss fought hard to get me a promotion to senior with less than a year of experience. 

I no longer really work with SQL queries, nor do I really do much Power BI development. Everything is just kind of on hold until something big comes in. All I do is train people on how to use my products. I feel like I am starting to lose my edge a bit, and rusty with my coding. 

The problem is, I’m in a very visible position to upper leadership. I report, nearly directly, to our C-Suite and VP staff on their personal data team for our F50 company. They have moved mountains for me so to speak and the future here is golden if I stay. By that, I mean I have been *told* I will be moving up quickly. Also, in my position, I’m pretty much recession proof and have incredible job security. 

The other side of the coin..

I have an opportunity to work for an established startup. 10 years old, same industry, doing the same position. I would be the BI Developer/Architect at this firm for a few thousand people. I would also be the first dedicated BI employee at this company. They have had incredible growth the last few years and the trend would continue. The tech stack is the same. (SQL/Power BI). The biggest caveat, recession on the horizon and being the new guy with a cost center position. 

Pay:

F50: Best Health/Dental you could imagine. ~20 days PTO a year, 3% 401K match and both increase at 3 years. If I stay, they will also pay a good chunk of my MBA. 80K/year, will not likely see 100k+ for several more years in a HCOL city. 1-2 day hybrid office. 

Start up: Okay health/dental, 25 days PTO/year, 6% 401k match, no MBA reimbursement. 110-125k/year + 10% bonus. 1-2 day a week hybrid that will lessen to biweekly after I ramp up. 

TLDR; leave a job with a golden career track and less money up front, and go to a startup that will jump my compensation considerably.

Edit: also, I do not want to burn the bridge at my current job. I would like to come back one day and retire here after I have more experience. I have been here for a little over one year.",analytics,2022-07-07 08:13:49,4
"With your background in architecture and analytics have you tried looking into the AEC industry? I’m not sure if you want to leave that industry or if you didn’t enjoy the jobs you had and just want something more analytical. You could try looking at consulting/analyst roles at aec companies that look for people with sql skills or data visualization. These roles are usually at larger companies and are branched under consulting but some of the tasks they do are manage other companies real estate and do analysis on the sq ft of properties to see if companies need that amount of workspace and need to acquire more properties. 

You could also look into property tech type start ups who probably are looking for people analyze spatial data. 

I’m in architecture but if you’re interested in planning there are probably are also roles and I would try to look for planning firms that have consulting or analyst roles (pretty sure they exist, at least in the us).",2,vtevqv,"I want to know what industries are out there and who would look for someone with a non-STEM degree and design and planning background. I don't know what kind of companies to look for or what roles I could look into. For instance, are there urban planning companies that hire data people? What kind of companies need data visualization or data tools that need to be user friendly or visually appealing beyond a basic level? 

My current job involves mostly business intelligence, making dashes for sales and pricing data. It's good experience but moving forward, I'd like to work with different kinds of data. Where I work, there's lots of e-commerce companies hiring people proficient in SQL, but it sounds like more sales data to me. Not exactly opposed to it but not excited either.

Background: Undergrad degree in architecture + 2.5 yrs of design, planning, and construction work experience that didn't align with my goals. Online certificates for data visualization, data analysis, some programming (SQL, python).

Current role: I eventually landed a job as a data analyst for a startup, a mostly business intelligence role. I make Excel and PowerBI based tools for our clients.

Would appreciate any suggestions. Thanks for taking time to read this",analytics,2022-07-07 02:51:45,1
"This is a hot take, but no.

If you're doing this because you think it's interesting, I'd put it in the portfolio. If you're doing this because you want to build out your portfolio, choose something else. Solve business problem to recommend a solution. The result should be something actionable. Think moneyball. Datasets to consider, player salary, team net worth, then joining data sets together with hour existing player data to find insight.",7,vt1ix6,"For reference, I'm currently in university for Mathematics (Optimization/Operations Research) and I'll be starting a co-op term in the fall in supply. I plan on working in business analytics/operations research.

I wanted to start a side project on NBA analytics, where I determine the statistical progression of the ""Average"" NBA player over time (approx 20 years). So far, I have acquired the player data from basketball reference and now I'm using SQL to preform the analysis. I then plan on using a data visualization tool (most likely tableau) to showcase my results.

Would this be seen as a good project for a resume, especially at an undergrad level? I already have experience preforming analysis using Excel and Python (Not using Pandas/Numpy), but I want to showcase my skills in SQL and visualization, as those are important for a lot of positions. Any advice is greatly appreciated!",analytics,2022-07-06 14:46:49,8
"You need to seek *knowledge* here, not more statistical methods.

For example: do you have access to daily sales for a full year for other products in a similar category? If so, you use the curve (seasonality) from those and set the volume in accordance with what the 8 months for the product in question tells you.

Apart from this basic understanding of seasonality, you should ask yourself, the data, and the client if demand for this product is shrinking or growing. If you could compare these 8 months to a basket of other products to account for seasonal variance, that could provide a glimmer of useful knowledge. The company strategy and marketing plans are helpful as well.",3,vtcsts,"What forecasting method should I use when data is limited. I’d hope to use SARIMA but that’s clearly out of the picture. 


 I have sales data by product and SKUs from Nov 21' - June 22'. The client is unable to give data for anything before Nov 21'. Additionally, sales data is grouped by the week, NOT daily. I'd like to do forecasting but with the lack of longer term data, I'm unable to include seasonality or special day sales (i.e. increase in sales because of 6/6, 7/7 special day sales). With this in mind, what is the best forecasting method I can use? I know SQL, Python & R, PowerBI. 

Company profile: Footwear, accessories and apparel.",analytics,2022-07-07 00:28:47,4
"You know what doesn't look good to employers - when your answer to the question, ""why do you have such a big gap on your resume"" is, ""I, who went to a top 25 school (meaning 11-25 since you'd say top 10 otherwise) and never did internships, did not have anyone offer to pay me what I believe I am worth.""

I'd advise you to follow the example of Leonard nimoy. When he wasn't acting as spock, he would take any job he had the time and skill to do.

I started off making $12 an hour while working on my masters in finance (bs in business admin) and applied to 100 jobs after I finished my masters. I got one reply which led to me working at a bank paying $50k a year and I had to dress up every day. Now I'm making 6 figures a few years later and I couldn't tell you what a Bayes model is or how to do hello world in R. But I can do excel like nobody's business and have a rudimentary understanding of data structures and can convey that information to technical and business people alike.",32,vsueyc," Graduated in December from a great university. Top 25 in the US. Statistics degree. I sadly was going through the worst time of my life, mentally, because of many factors so I only got volunteer work in data analysis for a political campaign. I did get experience setting up R-Studio sessions within AWS to run Naive Bayes Models though, during that volunteer experience. 

There’s a job I could potentially get working for a Fortune 500 but it only uses Excel (Vlookup, pivot tables) for the most part and pays $50k.

I feel like I’d be shooting myself in the foot by taking it on as my first role and making me look bad on my resume but the thing is… it’s a contract. I could leave in 6 months for something else. If I do take it on, I plan on volunteering my data analysis skills for something online to look like gold afterward (I plan on doing this anyway).

I just don’t know if the job is too low tier despite it being a Fortune 500. 

I feel like I'm worth more than $50k.

Guys, I've never had family attend university before me so I'm truly just trying to figure out if taking this role would have companies in December avoiding my application.

 As of now, I get other Fortune 500 companies to interview me but I realized I sucked at the interviews and am finally doing better in them after a month and a half but because Summer just got here, I'm afraid of not getting a job due to fewer openings.",analytics,2022-07-06 09:43:43,31
"Take your new yearly total comp, divide it by 2000 and multiply the result by 1.5; that is the minimum.

If you want to get fancy you can do a 15 hour per week retainer blah blah blah. I don't recommend it.

Remember the value of a good relationship, but don't let the company off too cheap. Make sure it's a short term deal; you can push harder if they still need help 3 months from now.

I had a friend work a $3k a month retainer continued for 5 years that took him about 10 hours a month back in 2003 when $3k seemed like a lot of money.",15,vstvkm,"I have ~6 years of experience in BI/analytics and am currently the senior manager of data analytics at a tech company. I resigned a few weeks ago because I got an offer at a FAANG and they are pretty understaffed currently so we had discussed the possibility of me consulting for a bit until my replacement is hired and onboarded. My manager proposed an $80/hour rate at 10 hrs/week today…that seems quite low to me. Do any of you folks with similar profiles consult, and if so, what do you charge?",analytics,2022-07-06 09:20:17,7
"Google one helped me transition from admin to an entry level data role at a new company. 

At least got me started, I’m interviewing for a better job tomorrow so wish me luck lol.

For the money I found it helpful. You’ll definitely need to study at least excel and sql extra outside of the basics they lay out. It’s comprehensive but very surface level.",19,vsk5uc,"I want to learn further about either data analytics or data science. I'm still gathering information between these two fields. Which one is your recommendation between the two courses?   
Thanks!",analytics,2022-07-06 00:27:57,20
"Am I the only one annoyed by how they decided to move to GA4?

Unless I have missed it and such tool exists, Google needs to provide a migration utility that would allow us to **migrate** our existing analytics to GA4. Not doing so and expecting that we all start from scratch is poor form. No company respecting its clients would survive such tactics. 

I am already looking into alternatives with Matomo and Plausible.",7,vsrcv8,"Main reasons why you can already start benefiting by moving onto the ""next-gen"" platform already:

**\* Cross-platform tracking—web and apps**

‘Data streams’ are essentially different ‘views’ of your data that you can create based on certain criteria. For example, you could create a stream for all web traffic, a stream for all app traffic, or a stream for all traffic from a certain country.

**\* Analysis and reporting**

The new Google reporting section that provides a new toolset to run advanced analysis.

**\* BigQuery integration**

For example, export your data into BigQuery and then use SQL to run complex queries that just wouldn’t be possible in the GA interface.

**\* Better insights with machine learning**

GA4 uses ‘probabilistic matching’ to stitch together data from different sources and give you a more complete picture of your users.

**\* Improved data model**

For example, let’s say you have a website with a search feature. With Universal Analytics, you would track pageviews for the search results pages. But with GA4, you can track actual searches as they happen, regardless of what page the user is on.

**\* Purchase probability and churn**

GA4 uses machine learning to predict purchase probability and churn for each individual user, so you can focus your marketing efforts

Also in the article on our site, you can get a couple of GA4 pro tips to help with the migration, take a read if you're having trouble making the switch.

Any other crucial reasons you'd point to?

Besides the #1 of course: Google ain't gonna give you a choice",analytics,2022-07-06 07:30:11,18
"After p&l I would focus heavily on suppliers, especially since it’s a construction firm of that size. Executives can take those reports and do competitive bids/negotiate for lower rates. If they’re doing thousands of projects, I would focus also on defining KPIs for the field teams.",9,vsd91m,"Started a new job recently at a construction firm (20+offices, 1000s of projects/year) that has never used anything more than excel for managing costs/revenue/etc.  Initially they want to automate the PnL reports. The next idea I pitched to them was to analyze old project data to determine what industry, region, etc was most profitable/least profitable. 

Aside from this, what would you seasoned vets look at to provide some value for the board/execs?",analytics,2022-07-05 17:58:44,6
"Your submission looks to be asking about industry tools. If so, you are not the only one asking this question, try the search, the sidebar (lots of resources there), and [check out the resource collection on our community site](https://lookingformarketing.com/tools?utm_source=r_analytics&utm_medium=ai)

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/analytics) if you have any questions or concerns.*",1,vswcqg,"Got Governance? 🔐 📊   


Enterprise data governance relies on several key factors. IT leaders and their teams NEED to know where their data comes from, how it gets processed, and what value it ultimately generates.

What tools are you and your team using to manage governance? Are you and your team looking into new tools?",analytics,2022-07-06 11:05:47,1
"It feels like data isn't really needed here, right?

Either clinicians don't do jack shit, or they help an x amount of patients per day. Increase the number of clinicians, you increase the number of patients being helped per day, thus working through the backlog faster, thus reducing the wait times.

As for how much they reduce the wait times, it would be: (current patients seen per day / backlog) - (new patients seen per day / backlog) = number of days you reduce the wait time with.",6,vs0vbh,"I'm in the middle of an evaluation for a clinical team and a lot of the feedback I've gotten from everyone I've interviewed is that they want to see an improvement in patient wait times. Right now patients have to wait up to 2 months for an appointment with certain clinicians and that wait time impacts a lot of things. One of the solutions would be to add another clinician to help with the demand. The clinical team has tried to do everything to improve wait times except adding another team member. 

I'd actually like to show with their data the impact the number of clinical team members has on wait times, I'm not sure what type of analysis I can do for this. I thought of doing a regression analysis to show how much of an impact the number of clinical members has on patient wait times (i.e. showing a difference in wait times when they were down a team member or someone was on vacation, etc). But I'm not sure if this is the right approach for the variables that I'm using. 

Is there a better way of showing how the number of clinical team members impacts how long patients have to wait for appointments?",analytics,2022-07-05 08:44:09,9
"Some open data project could help, for example use public data and Tableau Public and build a dashboard, or Kaggle with open Notebook (Python)

The point is to prove that u could actually deliver useful insights with data",2,vs785n,I currently work in HR however would like to become an analyst. do you think a portfolio with a HR/People focus will be the best to transition into analytics within HR? Or should I branch out?,analytics,2022-07-05 13:22:42,2
"whenever these questions are asked, some people say yes, some say no. have fun with that.",40,vrvf99,"As above. 

Currently working as a Sales & Operations Analyst for a supply chain software company. Been here for 5 months.

I do forecasting, reporting based on various kpis , sales rep and product performance etc. Also business process improvement , documentation and mapping work processes.

I use salesforce , excel and power BI ( mainly ETL) regularly. I can build dashboards but its not a main focus of my role as they were already built.

Is it worth doing a Msc in data analytics to strengthen my technical knowledge and future job security or is it pointless given I am working in the field?",analytics,2022-07-05 04:13:40,15
You should be able to get 90k whatever you do with an mba. I don’t think you’d even need an ms to switch to something where you can learn analytics even if that’s not the job title. There are numbers you can put into spreadsheets in pretty much any job.,3,vsbhik,"I am 42 with a MS in Geography and an MBA...I make $90k a year. However,  I am bored out of my mind at my current job. I started an MS in Business Analytics degree at a reputable university in the Northeast of US. If I graduate with a MS in Business Analytics,  what are the chances I will land an entry job which will match my current salary!? I know nothing is a guarantee , but I would hate to graduate and get entry level jobs with $75k salary...I don't want $100k, matching my current salary will suffice. 
What do you guys think?!?",analytics,2022-07-05 16:32:07,5
"Not being funny, but that question gets asked (in some form) almost daily. Have a look at the myriad other questions and their responses first.",10,vrpm09,I’m currently working on a masters in data analytics from a state school in Missouri. I’m starting to catch on with the basics of python and I’m wanting to start working on some portfolio projects to bolster my chances of getting an analyst role in the hopefully not so distant future. What kind of projects should I look at doing? Can you point me to some examples so I can get an idea of what I’m needing to do? I’m also currently working on SQL on SSMS and Power BI. Any help will be greatly appreciated.,analytics,2022-07-04 21:32:36,15
"I'm sorry if this isn't the answer you're looking for, but when I see simple technical certifications on people's LinkedIn profiles (such as the five hour SQL joins course you recently completed), it suggests to me that the person's technical skills are probably very basic. I'm not sure if that's the message you want to signal to the world.

A much better tactic is to create a blog (ideally using your real name, e.g. joesmith.com), where you write articles on different SQL queries. This (a) shows you have a genuine interest in the topic and (b) makes it seem like you know what you're talking about. You can then link to this blog on your LinkedIn profile.",37,vr3pzw,"Hi guys,

I have been learning SQL on and off over the past few months and recently just completed a few courses including joins on DataCamp.

Once a course is completed, you are 'awarded' a certification of completion. Are these certs worth putting on your LinkedIn profile under 'Licenses & certifications' and resume when applying for jobs or is this essentially useless? 

When applying for jobs, some online applications ask for linkedin profile url. Again, would these datacamp certs be worth placing on your linkedin and resume since it 'proves' that I know SQL joins as well as just general visibility for recruiters for example or should I just leave this off since it does not count for actual real world experience, etc.?

What has been your general experience with this? Graduated w/ a BS in Biology from a few years back and currently work a non-IT related job in healthcare right now but am looking to potentially transition into an analyst role and I live in/near a very large city in the USA by the way if that matters.

Thanks for any feedback/advice I can get on this, greatly appreciated in advance!!",analytics,2022-07-04 02:44:06,19
Almost certainly SQL. Potentially on data governance and security. Looker and Power BI are vastly different in just about all aspects.  Many things that are straight forward and easy in PBI are quite difficult or virtually impossible in Looker.,3,vrftf6,"I'm a Data Analyst/Business Intelligence Developer interviewing for a Senior Data Analyst role. I work with Microsoft Power BI and Microstrategy.


I'm going to have a functional interview soon with a Senior Data Engineer soon, and I was wondering what I should expect.

About the job description: It's a role that will use Looker as their BI tool. The candidate will own the main metrics for the company, design dashboards, data manipulation using SQL, use Jupyter Notebooks (Python) to share analysis, have soft skills like stakeholder management and all that. (I have experience in all these)

Their data stack includes Google Big Query, Looker, dbt. (I've not used any of these but I'm confident i can pick these up)

I'm wondering what to prep for the interview, what would this person be looking for, what to portray and come out on top.

Thanks again",analytics,2022-07-04 12:54:37,11
"Wish I could follow posts, how can I get something to notify me when someone else comments here?",1,vrs43x,"Hi all, 

Recently i was learning about web scraping and i tried it out on some cryptocurrency website, and i was wondering what kind of analysis project can i do with cryptocurrency. Like if the full on analysis project is like business question, data clean, data extract, data visualization, what will be the business question for the project?

Any advice would be much appreciated, Thanks in advance.",analytics,2022-07-05 00:17:29,6
"Didn't have a problem without a university diploma. I've changed my profession to data analyst from a software developer at 29 years old. Presented my skills at my first interviews as a data analyst, done some various testing tasks in Excel - that was enough to get a job.

Sooner I developed some portfolio in Tableau to show my dashboard developing skills, it definetely helped me at interviews.

What I want to say - every company, every interviewer was interested solely in my ability to solve their problems. Where I got my knowledge and skills - at some university or on my own - it didn't matter to them.",12,vr4wou,"I've recently finished my apprenticeship lv3 in business administration but this was not quite what I was looking for. I like my job and the people I work with - but I don't think it's my passion. I deal a lot with customers and generating reports/processing orders/booking stock and a lot of excel and generating data for customers/managers. Also a few project on the side made by my own incentive that have helped based on data that I generated and this bit is what I enjoyed doing the most but it's  not really  ""my job role"".  


I'm 27 years old now and I had to quit uni due to ££ and move abroad and start from scratch and then decided to do an apprenticeship and finally finished it but not feeling that spark and I hate it because where I'm it's a nice office with nice people...but the earnings are just average and not much of an opportunity to scale.  


What would you do in my case?",analytics,2022-07-04 04:04:15,32
"There are time options available, I recommend doing a Google search for "" entity relationship diagram"".",3,vr5n1p,"Hi, right now my company is using Power BI to visualize table relations. I am exploring if there are other open source tools to achieve this visibility so I can enrich my dataset and see how things are connected. Any tool that you find quite handy to use in your experience?",analytics,2022-07-04 04:50:04,7
"I may be a bit biased: 

Accounting will suck your soul out your eyes and turn you into a joyless waif. Your coworkers are going to be some of the weirdest mother fuckers on the planet especially as people start to leave because they realize they hate their lives. The Big 4 will trod on your soul and happily work your health away into anxiety and high blood pressure.",35,vqqt3h,"So originally I was planning on doing accounting but I would have to double major to meet 150 credits so I decided to double major in accounting and MIS 

I started looking into MIS and I think data analytics,BI,data engineer etc route looks good (I already know sql and some python; learning R, power BI etc atm)

The thing is accounting has stability and the almost guaranteed progression and exit opportunities from big 4.

If I go down the analytics path I will probably continue with more stats coursework and try to get a MS in analytics 

Alot of ppl mention that analytics pays way more but out of undergrad the pay seems to be almost same but I'm not sure. (If anyone knows please help)  but If I get a MS in analytics the pay seems to be way more but I'm not too sure why 🤔

I'm also not too sure about the opportunities in analytics since it seems to be a fairly new field.

I just need help whether and how to keep both options open or should I go fully down the data analytics path 

Thank you 

Tldr: accounting or analytics?",analytics,2022-07-03 13:52:57,19
Put your work in GitHub. Or a personal blog site. Either one is fine.,12,vqmspz,"Hey, can you guys tell me, how to showcase SQL projects? and where to find them, is it Github? or somewhere else?",analytics,2022-07-03 10:38:24,15
Yes. I am an R guy but we make a lot of money on Azure consulting. Dax is very cool and you can embed R or python scripts in it. I frequently end up using ggplot to get around things that aren't easy or doable in power bi. It's a great skill to have. Ryan Wade has a book at Apress on integrating them and I believe GuyInACube has a video on it. If you ever need any help feel free to ping me. Admittedly I use R and my python is rusty but at this point theyre mostly interchangeable,4,vqsvbn,"Hi,

I am a BI analyst used to excel / DAX / Query and I wonder if basic pandas/numpy/plt knowledge could become usefull in the context of dashboard development/day-to-day data analysis.

I've recently started the 'python data analyst path' of datacamp and I really like it. But for now it's more of a hobby , decorralated from my actual work as I feel like everything I do in my job will be a lot easier and efficient using microsoft tools.

Has anyone had the experience of slowly learning python for data analysis and progressively use it at a 'microsoft shop' ? what did you use and how did it turn out ?

Thanks for reading !",analytics,2022-07-03 15:33:51,4
"Run [Visual Studio Code](https://code.visualstudio.com/) off your computer and get Jupyter Notebooks. If you need something running in the cloud, you can use [Google CoLab](https://colab.research.google.com/notebooks/intro.ipynb).

Now, if you need something visual with a whole suite of tools, you are looking for a cloud computing platform: Google Cloud Platform, AWS, Databricks, etc. But these aren't cheap.",1,vqm06k,"I have a ton of ideas to analyze open source data and obtain useful insights. I am data analyst by trade but only we versed with Cognos analytics. I just started learning about all the free open source tools and resources that are available to extract and analyze(like oracle cloud, etc). But I do not know how to integrate various tools and automate data extraction, data analysis in real time to get results based on data that is added real time. Has anyone does that? If so can you plz share steps briefly 

Dm me if you have any questions",analytics,2022-07-03 10:00:38,4
"1. It can be hard to say if this career overall isn’t the right fit for you after only one gig doing it. Unless you’re like absolutely miserable and can’t take it anymore, I’d give another org a try before writing off the career. 
2. You have a year experience now. Have you tried submitting job applications for better gigs? The hardest thing is when you have no experience, now you have something. You could even wait another 6-12 months and have a good shot at a better gig. 

The MBA route is a tough sell imo. You’re early in your career and they’re expensive. Especially if you’re not fully sure what you want to do, it sounds more like a way to buy time which won’t necessary help you get a good job in the end.",18,vq65js,"I'm currently working as a low level data analyst for a John Deere dealership making 50k. I Have a BS in data science and experience with Python, R, and SQL but my proficiency has weakened as I spend 80-90% of my time using tableau and a tiny bit of SQL. 

I have several issues that have bubbled up over time and I have some big decisions to make.

1. I have had several coworkers and my brother (a data scientist working for JD financial) tell me that I am worth more than 50k. I believe this as well, but my programming skills & confidence have also deteriorated. 
2. I really enjoy my job. The data we use is fascinating and we are doing really cool things with machine and agronomic data. I get excited for the future with this company.
3. I get a 3k raise every 6 months. That's really what's kept me put. I started at 45k a year ago. While 6k a year isn't too bad, it would take 5 years to get to 80k. 
4. I am using datacamp courses to rekindle some of my ability in python and sql but I'm not sure where it would be best to invest the majority of my ""learning time"" because I don't know which direction I want to go!
5. I want to go back to school, but I almost wonder if Data science is not for me. I'm tempted to just get an MBA and use my hard skills in analytics to leverage a management trainee position that usually starts of around 90k. 

This turned out to be more of a  journal entry so don't feel the need to have all the answers, I'm just hoping to get additional perspective on my situation from fresh eyes

&#x200B;

tldr: Lots of options facing a young entry level analyst, no clue where to turn",analytics,2022-07-02 18:26:41,19
"I used the certificate as a good foundational spring board. I needed structure and this course provided that for me. I did a lot of extra learning in addition to the course. The biggest help was having the case study at the end, because it gave me a really good blueprint to use for other case studies.",16,vq2uol,"I'm not too sure how the course quizzes as I've only watched about an hour of the course on YouTube, but I'm finding that a lot of the talk is filler and I feel I'd be much better off watching courses of SQL,  Tableau, R, Excel, and more separately to learn more in the time I'd be watching these videos from the Certificate course. Has anyone went this route and then just took the Certificate course and went through the quizzes or should I just go with the entirety of the course?",analytics,2022-07-02 15:27:05,51
"That first sentence is a doozy. 

Also, shouldn’t you know what type of job this is since you’re far enough for an interview?",3,vpi0af,"Hi here , 

I have an I interview with wells Fargo soon about a risk analytics consultant ( credit risk modeling ) 
I wanted to know if anyone of you guys work under this title , how you like it and what is your salary range ?

Thanks",analytics,2022-07-01 20:01:53,4
"Without any dataset to work on, you are using hunch. It’s not analysis",14,vpboip," Although you do not have a data set to test, you are asked to make a probability calculation even though you do not know the preferences of the participants. How can we go about doing this?",analytics,2022-07-01 14:36:05,5
"How many tables do you need to paste into the document? The reason I ask this is it's sometimes quicker and cheaper to simply copy and paste the data instead of spending two days coming up with a beautiful automated solution.

I used to work at a company who built and maintained a highly complex and expensive machine learning system to extract keywords from around 1,000 texts. It would have been much quicker and significantly cheaper (magnitudes) to do it manually.",7,vp1g1h,"My team is tasked with producing a large text based (Word doc) report annually. Our primary responsibility is the calculation and pasting of tables. The calculations are automated through SQL, but does anyone know of a way to automate the table creation/writing process?",analytics,2022-07-01 06:56:24,11
"Every 3 to 5 years, I would use it to go to the European equivalent of the Joint Statistical Meetings in North America (or to go to JSM).  In three days, you can get a great overview of lots of things you've never heard of before that you should study and try out.  Two years in a row means a lot of overlap, and it's not as valuable.",15,vohpye,"Our company gives us £1k a year for personal development. We can spend this on pretty much anything we want, as long as we can relate it to our job somehow. 

Others have done things such as language courses, Adobe courses, Python, Textbooks, etc. 

How would you best spend this money if you had it? 

I know it’s down to me, but just looking to brainstorm and see if there’s anything cool I haven’t thought of.",analytics,2022-06-30 13:13:43,10
Sounds like a good opportunity for you.,2,vorep5,"I was invited to attend data hackathon  by Chase and Walmart. I’m happy that they invited me but most of the participants major in a STEM field such as CS, Data Science, Applied Math, etc and I major in Business Analytics which is not a STEM field so I’m wondering if I should attend these hackathon events? I’m just a bit worried I may feel out of place since my major is not a STEM major, but I know how to use Tableau/SQL database, and write Python/R codes but not at an advance level.",analytics,2022-06-30 20:59:10,3
Google Ads / adwords pixel for targeting users.,2,vorb00,"Hello,

I have some questions about google tag manager and analytics. How can we track users that have used our e-commerce function on our website and send them deals in google ads and social media? We already use google analytics, gtm, facebook pixel, etc. I just need a direction where I can go, thank you.",analytics,2022-06-30 20:53:28,2
Landing a new job,2,vosn9r,Fellow analytics professionals! What's top of mind for you and your team going into the second half of 2022?,analytics,2022-06-30 22:08:08,3
"It’s your manager’s responsibility to point you in the right direction and/or to give you the tools to learn. The sink or swim managers who don’t think it’s their job to help people grow shouldn’t be hiring people without the experience they expect you to have. IMO it sounds like you are driven and that’s exactly what a good manager looks for. 

See if your company has any learning materials for junior members or try and read recaps/summaries of what the other departments are doing. Use google and find your industry specific sites. They often have acronym lists or definitions of specific industry words. 

Another good option could be to look for a job with a better manager.",8,vocak1,"I’m seeing that stakeholders are busy people who want you to take the directIons laid out and follow it promptly, while also taking the time to ask questions and collaborate them with how the finished product should look. 

This is my first analyst job and I’ve only been working it for a few months now. Many times, the terms they use are very industry specific and I don’t understand what they’re referring to. All of my coworkers have worked there for a few years now and have more years of analytics under their belt. My title says says senior but I was hired and am paid as a junior. I enjoy my work but feel completely lost on the business end of things. I’ve been reprimanded a few times now for being too reliant on my coworkers and for my ability to follow along. My manager conceded that this is part of the learning process but made it clear that I need to get my act together. 

How do I go about understanding the business end of things more clearly?",analytics,2022-06-30 09:18:40,7
Managerial =MBA,5,vobbu6,"Wanted to know which would help more later on, if one is open to moving into managerial roles later in their career.",analytics,2022-06-30 08:38:12,13
What kind of projects can be built with this? Curious.,2,vo55vq," 

## On the 27th of June, 2022 Facebook and Instagram’s parent company meta announced that Instagram reels APIs will be made available to a number of endpoints on the Instagram Platform for developers.",analytics,2022-06-30 03:29:08,4
"Given that Anaplan is an obscure tool/platform and they make many promises about how easy and transparent their training and tool is, and you just went through their 
homegrown certs, you would have no problems in the interview. Good luck!",3,vo398s,"I have four months of experience in Anaplan and a Model Building Certification from Anaplan . I have got an interview in 2 days . What should I be preparing for . This is for an Anaplan Analytics job .

Please let me know If I can go through some videos since I need to talk about my project and the have only worked on a single project few months back and had to change my project. 

Thanks for any advice",analytics,2022-06-30 01:22:55,8
"Former recruiter, turned analyst here:

Email them. Nothing to lose.",15,vnro9v,"Got a recruiter to call me. Good job. Low pay but I'm a recent grad and I suppose that's fine.

After being asked a few questions about myself, I was then asked what I was looking for in a new role. I told them hypothesis testing, linear regression, etc.   
I think I went too deep. The role being offered is nothing more than:  
""Interpret/analyze data and provide insights to determine impact, trends, and opportunities. Develop reports and deliverables for management and communicate with all levels of stakeholders""  


I love that it the descriptions also has slight project management duties within it so I'd take the lower pay if that is the case. I still want the job if possible so I'm not sure whether to apply directly to the company now that she told me the name or if I should email her. 

&#x200B;

Thoughts?",analytics,2022-06-29 15:06:40,6
"International remote work creates challenges related to government regulations in the other country.  It's unlikely that a company in one country would hire a single employee in another country, and I doubt any company hires without considering the company.  Even a company like Google, with offices in 10's of countries, will pause before hiring in a country they don't already have offices in.

Contract work, where some organization in the other country deals with local regulations is more likely.  And that happens.",8,vnghr0,"Hey all,

I'm from Brazil and wanted to know what is the perspective for analytics or data science remote jobs around the world, specially junior or mid-level.

Is it something tangible? Or am I dreaming big?

What would be the skills that I must show to be able to compete in a world-wide manner?

Also, are there some good websites specifically that share analytics or data science remote jobs?

Thank you all!",analytics,2022-06-29 06:59:45,3
What is the difference between a visit and a unique visitor. What’s the difference between a page view and a visit.,29,vmzjsn,"When you think about web analytics, what is the basic metric that you find yourself explaining over and over? 

One thing that your colleagues, new hires or even customer need constant education about.",analytics,2022-06-28 15:19:34,55
"The chi-square test is testing whether you can reject that null hypothesis that all of the percentages came from the same probability distribution.  It's like saying, ""I'm going to flip coins, but I have four different songs I'll be singing for different flips.""  If the test comes up significant, that tells you that at least one condition has data that did not come the distribution that generated data for at least one other condition.  To find which 2 or more are different, you do post hoc pairwise comparisons.  The pairwise comparison to the control group will show you the lift over control.",3,vneavb,"I have several groups to compare, a control group, a group contacted by email, a group contacted by phone, and a group contacted by mail.

I want to measure if there is a significant difference in the % that bought a product, and I had thought to use a chi-squared test in R for this.

I feel that the expected values used in the chi-squared test should be those of the control group. However the expected values are determined from the entire dataset (as normal). Has anyone experience with a similar question? And does it seem logical/possible to use the control group as the expected values.

Thank you for any advice you can give me!",analytics,2022-06-29 05:09:56,2
A great github for SQL is github.  :),36,vn4ksr,"I am the head of analytics in my company and I see too many similar requests from different business users which require slight modification and then the query gets lost in abyss, is there any solution for storing and searching through existing sql queries written by all analyst? Something like GitHub for sql?",analytics,2022-06-28 19:27:14,25
"Learn SQL, then just use proc sql everywhere in it. Occasionally you’ll use its actual stats functions but the documentation is pretty extensive so you’ll be fine.",13,vn12vo,"How hard is SAS to learn in a professional setting? Or what do you guys recommend to start with to get a good grasp on it. All experience I have is when I was in graduate school which was not that granular. 

TIA",analytics,2022-06-28 16:31:58,10
"Welcome to the channel and the industry. Suggest you check the side bar, utfg with your questions + reddit and you'll find the dozens of posts like this posted monthly.",5,vmj08l,"Hello,

I'm going to to do a masters degree on big data and I wanted to learn Python ans SQL before the course starts in 3 months. Any sites that could be usefull? It can be either courses or other sites of relevant information. Thank you for your time.",analytics,2022-06-28 02:24:11,8
"Ad Pro here, and I think you (and the prior commenter) are making some assumptions in error.

First, you called this ""website data"", but I suspect it's actually ad data since impressions are views on ads vs page views on a website. If this is ad data, did it come from an ad server? If so, it may be possible (even probable) that the ad server is able to track post-view conversions (via cookies or device id), which would explain the lines with revenue and no clicks. In my experience, the majority of conversions happen post-view because something like only 16% of users ever click on display ads (my primary area of focus).

Second, if you are using an ad server, it's entirely possible to have days with no impressions/clicks but with revenue. Conversion latency is something my teams look at regularly. Depending on the product and price point, we may see average time to convert from last touch as days or weeks. The same cookie tracking that enables post-view allows conversions to be registered well past the last touch date. I've seen default values set at 90 days, in some ad servers!

All that to say, there's nothing about the data you're describing that looks weird or incorrect to me. It sounds like all the data I've seen everyday for the last 15 years. I'd recommend asking some questions to make sure you have a solid understanding the data before you start trying to fill ""gaps"". Good luck!",10,vme99m,"Hi all, 

So I came across a data set that has information regarding revenue of a website, such as impressions and clicks. 

Impression being how many times the ad was shown, and a click when a user actually clicks on it. Rev is how much that generated. 

I found the data to have missing values in certain rows, like blank revenue but has 0 clicks and a number of impressions. Other rows that have blank impressions, 0 clicks, but has a revenue associated with it. 

Would it be best to 0 out these cases or would it be best to imputate with median or use linear regression to predict on these missing fields?",analytics,2022-06-27 21:20:02,3
"I think you know plenty, IMO. Understanding how to develop useful KPIs can be important. I think you will learn more about what you are wanting to learn on a job better than you would through any course. Have you attempted any interviews yet? Id say apply for some jobs and see how interviews go and that will give you more direction if you are actually lacking or not. You sound ready for an entry level role with the right support from more senior teammates to me.",19,vm5euh,"I started the Google DA Coursera course at the end of January and completed it in March. It served its purpose in terms of a Marco understanding of data analysis , but I found a few sections lacking depth. I’ve spent the last few months learning SQL & Tableau. I think I’ve got a decent enough grasp of Tableau to perform job functions but still don’t feel fully prepared for Excel & SQL in a data analyst context.

I consider myself lower-intermediate with SQL and about the same for excel. I have a grasp of basic select, aggregations, grouping , case when, joins (though I want to fully understand use cases for self joins), and cleaning functions like substr, concat, etc. I wanted to hold off on learning Window Functions & CTE’s until I understand the most frequent use cases for subqueries & when to use them (aggregating on different levels?). I plan on taking the Udemy Zero to Hero SQL course , if anyone can provide some feedback on that.

Excel is so weird to assess because I feel fully comfortable using it but underprepared for a data interview. I prefer making visualizations on tableau. Does anyone swear by a particular resource? I want to cover everything I would experience on an interview",analytics,2022-06-27 13:42:48,30
Lol this is not a dilemma my bro.,9,vmk8nk,"Please help.

So I just graduated this month as a computer engineer and I've always been excited to become a DA and maybe learn from a senior DA.

 But I'm still on my 4th course in the Google DA course. I also learned basic mySQL and Google sheets (Advance formulas/functions) recently and a little bit of tableau.

 I've been applying to a lot of jobs while so and a lot of interviews have passed.

Finally, now I might get an offer to become a Research Executive or Data Engineer.

I need help. Both roles require data and analysis. With the DE job they will require us to use BI tool

What do you guys think? Should I accept non-DA roles? So that I could learn? ( That's what the DE role said I had to learn to clean and process data before I could become a DA in the future )OR just keep looking for straight up DA roles. 

*Research Executive Role - requires me to formulate topics and help a team do the research and maybe visualize the data ( FMGC, services, market, consumer)",analytics,2022-06-28 03:43:31,4
Maybe the easiest way for me is to pull the event names and respective event parameters in a Google Data Studio table.,2,vmjw20,"Hi! Can anyone help me with exactly where to see historical event parameters values and counts in GA4?


When exploring events under Engagement > Events > Event name I know you can see the event parameters in the ""Events in the last 30 minutes"" table, but if I want to see event parameters for an event that happened more than 30 minutes ago, say yesterday, where does this data exist?


I've spent hours on Google Support chat and I think the time would have been better spent banging my head against a wall.
I would love nothing more than to be shown that I've been the idiot the whole time, but no amount of research has shown me where to see this (what I think to be extremely basic) information!",analytics,2022-06-28 03:21:22,1
"This is great! Didn't know about it, thank you.",10,vln9on,A search engine dedicated to finding datasets: [https://datasetsearch.research.google.com/](https://datasetsearch.research.google.com/),analytics,2022-06-26 21:45:55,5
"Google will always be the most accurate—we do our best to come up with our estimates to be as close as possible, but GSC/GA will always be the standard :)  
We recognize the importance of these, and even have multiple integrations with GA/GSC throughout our toolkit; have you integrated your Google accounts yet? They can really amp up your reporting!

P.S. You can always use r/Semrush for direct questions about our toolkit!",4,vm0i7l,,analytics,2022-06-27 10:10:57,1
"I've done all three, though my experience was I was only a ""financial analyst"" because I was doing data/BA stuff in a finance cost center. When I was a BA there was more project stuff going on, gathering requirements etc. Currently a Sr data analyst and my time is all writing SQL and creating Tableau reports. It's my favorite of all of them.",19,vll0ha,"I know job titles and the roles and responsibilities can vary between industries and companies, but I'm curious on what would be the best option career wise?  I started a financial analyst job a few months ago and am thinking of what I want my next job to be and the career I want to work towards.  I'm in my early thirties and prior to this financial analyst role I was a legal/operations coordinator for 4 years.   My industry is travel/hospitality. I have a degree in international business with a focus in finance and marketing. In my role we only use Excel, Powerpoint, Cognos and Hyperion.  We do not use SQL, Tableau, Power BI, Python, and R etc. I want to avoid doing additional expensive/time consuming schooling such as MBA, MSDS, etc since I do not want student loan debt as I am debt free now. And as of now I don't have a desire to get a CFA.  

&#x200B;

What I'm looking for career wise, is lots of job opportunities (ex. won't become obsolete lots of jobs out there to apply), good pay and work life balance, and remote/hybrid. Currently I make approximately $66K in HCOL 45-50 hours a week, and I would like my next job to be in the $80K+ for HCOL. Should I stick to finance or try to pivot to a more data analyst role for career longevity? If I learn SQL and a data visualization program well (Tableau or Power BI), do I have a shot for data analyst roles? I'm concerned that if I switch to a more data analyst role it would be harder since my degree is business and not CS, statistics, economics, etc.  And would I hit a career bump by not being able to reach a data scientist role since I may not qualify for a MSDS program? Or is it better to get a more general ""business analyst/sr business analyst"" role and then pivot to something like analytics manager or product management or consulting? Just thinking if I should start studying SQL, etc now if I want to some time next year try to pivot to a data analyst role or a senior analyst role?",analytics,2022-06-26 19:41:13,23
this is a wendy's,7,vmgj3i," 

## Just a few months ago, On 25 April 2022, Elon Reeve Musk bought Twitter for $44 billion dollars, with this gigantic deal, and now Twitter became a privately owned company.

 **This controversy between Twitter and Elon took a shape of the privatization of social media platforms. If we remember correctly then before buying twitter completely, Elon musk has suggested some features to the Twitter board indirectly through Twitter polls and tweets, and one of them was the** **“edit button”, which allows users to edit abusive, hatred, and harmful comments totally from the tweets even without deleting them.**",analytics,2022-06-27 23:37:57,2
"Anything related to AWS, Azure, Apache Spark would be good if you want to do something with those tools. Any project related experience (personal, Kaggle, etc.) would be good on a resume. Python institute also has certifications for ~$100 to very your Python experience. 

There are a lot of free online courses as well, honestly too many to list off. I would just search something you’re interested in (starting with the fundamentals) and expand from there",2,vlulmh,"Hello I want to switch from accounting to data or business analytics , is there is good certifications that can help me to get a job in Europe (France , Belgium )?? 
thanks",analytics,2022-06-27 05:41:28,1
"You are not really going to gain domain knowledge till you get a job in the field. No college/online class is going to give you real world hands on domain knowledge with confidential operations information. 

You don’t have to worry about domain knowledge till you are looking for more senior roles. No one is going to expect someone with zero experience to have domain knowledge. 

I have 10+ years of airline domain knowledge. I have more domain knowledge than most of our local management, which is why I can be a great asset with the analytics I provide.",15,vl4mxw,"repost from data science, so apologies if you saw this twice.

Tldr; Perhaps what i would really like to learn from this post is more insights on domain specialization (specifically business). I have gone through quite a few reddit posts and everyone thinks having domain knowledge is important, but strangely enough i dont see much posts talking about it (what to specialize, why, etc). All i see are perhaps 3month roadmaps saying you must learn statistics, machine learning, programming, excel, etc. Not trying to belittle learning those, its just that since everyone thinks domain specialization is important its weird that theres few posts about it.


Detailed post:

Hi there, was wondering if anyone has a similar educational background as mine - i will be a Y2 business degree student this coming august and i have decided i would like to take a second major in data science analytics (DSA). Ironically though, i am unsure of which first major i want/domain i wish to specialize in.

(1) So far, im thinking of either finance or operations management, but would be happy to hear about stories from other majors like marketing in terms of how data science is applied to your field. Any other resources that could help me gain more insights about each field would be appreciated too - ive got one more year before i need to declare my majors


As for why i would like to major in DSA, i am currently in a repe internship and i realised how important data is in this field (finding comparables, and making sure that you are not overpaying especially with the recent cap rate compression). Even if i do not become a data analyst/scientist, i do think it would help me greatly if i had knowledge in this field.

P.S. Maybe my initial premise that specialization is good might be wrong too. keen to hear whats your take (should you expose yourself to multiple departments or just specialize in one)


Just in case if anyone wishes to provide me other advice:

About me: ive completed automate the boring stuff on udemy, google data analytics course and am halfway through udemy complete data science course 2022. Looking forward to taking andrew ng's new course when im done with that. 

Would be looking to join a club at winter that would provide mentorship and guidance to making a machine learning project. Exploring the option of doing a part time data analytics internship to get my foot into the industry.


Thank you and have a good day!",analytics,2022-06-26 06:22:40,9
"If you can use pivot tables, vlookup, index match I’d say you’re ahead of 95% of excel users and therefore at “advanced” level.",140,vkl0au,"When I read job description for some entry-level analytics roles, I saw that employers asking for “expert-level Excel”, “expert in Microsoft Office Suite”, “advanced Excel”, or something like that. I’m wondering what do they actually mean by this? I know advance Excel formulas such as Index, Match, nested formulas, and I know how to build dynamic charts. In terms of Office Suite, I am familiar with Word and PPT (at beginner level) but not any more beyond that. Having said that, I’m wondering if my limited proficiency in Office Suite is enough for me to land the job and should I still apply and wow them with my other skills such as SQL, Python/R, and Tableau? Or should I not apply?",analytics,2022-06-25 11:19:37,63
"Find an industry that has a lot of money but not a lot of competition, something really unsexy like  property management, or international shipping, or farm labor accounting, network in that small field. 

There are a lot of old people in these professions that are not computer or data savvy. And usually there are like 4-5 firms that dominate, so job hopping for higher pay is quite easy if you maintain a network and keep your ear to the ground. 

So find a niche, become an expert, work on your presentation skills and don’t come off as a know it all asshole.",34,vkhpkp,"Title basically.

I just move into analytics and am about to finish my Master's in Data analytics. 

If you were to start your career all over again in analytics, how would you scale it to make sure you can maximize salary and growth?

* How long to stay loyal to a company after attempts of promotional opportunities to no avail? 
* How long within a company would you expect a promotion if not outlined?
* Anything that can be done OUTSIDE of education to continue to upskill? Certain projects? Other certifications?",analytics,2022-06-25 08:41:40,15
"Without knowing much more about the role or industry, I'd say what you have covers it",6,vkfwq0,"I have a job interview coming up soon. It requires ""foundational data analysis and working knowledge in Excel & SQL"".  


In college, I got experience writing models in R and know descriptive statistics in Excel (pivot tables, index match). I'm well-versed in hypothesis testing (ANOVA), linear regression (p-values, R-squared), and making graphs using ggplot2.  


I DO know SQL but I know that the statistics stuff is what is going to get me the job.

This would be my first job out of college. What are they looking for when they say they want foundational data analysis?

&#x200B;

Thanks everyone!",analytics,2022-06-25 07:12:39,7
You can YouTube/Google most excel operations on the job. The key is knowing the names of those processes so you know what to search in Google. So maybe an introductory course would be good. Again you can find many free introductory courses on YT,6,vkc2qi,"I am 22M with 1YOE as a data analyst; *Snowpro Core Certified, Tableau Desktop Specialist Certified, and Dataiku Advanced Designer Certified.*

**Skills:** *Snowflake(SnowSQL, SQL), Python, Tableau, Dataiku DSS*

* Do I really need to be an ace in MS-Excel? I think I can do everything with Python.
* Which ETL tool should I learn? Matillion, Fivetran, DBT?
* My current goals are to ace stats for now. What else can be done or any helpful resources for Stats?
* Any other tip?",analytics,2022-06-25 03:23:27,11
"I'd say tech: SaaS, web analytics, product startups",9,vknnre,"I have 2 years of experience as a mechanical engineer working mostly in MEP. Recently I decided to become a data analyst. What specific domain in data analytics should I target that is interesting and has the most job opportunities and money for me?
For example: Banking? Construction? MEP? Software? Web development? Healthcare? …

Thank you",analytics,2022-06-25 13:28:18,5
"Much like with most of the relationship questions posted to Reddit, the correct answer is ""communicate about needs, don't assume"".

As a hiring manager, I'm probably a little unusual in that I like to coach my candidates a bit on how to do the interview with the panel. After all, work is not a closed-book test, it's a team sport; if somebody can take coaching well and perform better because of it, that's valuable to have on a team. For the roles I hire, I am usually looking for strategic understanding and leadership, so I coach them to stay high level with just enough details to build confidence in their rigor, and save the details for Q&A.

If nobody from the company has given you advice yet, there's no problem asking the recruiter what works best with the hiring manager. During the interview, keep one part of your brain watching reactions to see how they're following and adjust accordingly; you can also be direct in the moment and ask questions like ""do you want me to go any deeper?""

Now, all that is great general advice for any aspect of the interview. I will say with regards to  ""simple talk"" vs ""complex talk"", the important thing is to be clear and speak in a language your interviewer understand. If you feel like you need to ask this question, then nine times out of ten the right answer is to start out ""simple"" and follow their lead if they want to use more jargon or technical terminology.",7,vkn572,I have an upcoming interview and want to know if I should keep my vocabulary simple for the most part and then every once in a while throw in some advanced language.,analytics,2022-06-25 13:03:02,3
I enjoy analytics and very much dislike software dev,6,vkmhoo,,analytics,2022-06-25 12:31:08,2
"Your problem is going to be the websites blocking your scraper, so you'll want to use a bot which is difficult to detect. Have a look at Puppeteer-Extra, especially the stealth plugin. You should also consider routing your bot through random residential IPs. Have a look at BrightData.",3,vkeh5b,"Hi, I am going to be starting my thesis very shortly and looking to scrape some data. I don't have a coding background but completed the 'automate the boring stuff' course on udemy. I want to know what type of data is easier to learn to scrape - google play store reviews, tripadvisor reviews, Twitter feed, Facebook group posts, comments etc? And please direct me to some resources? Thank you",analytics,2022-06-25 05:54:31,5
I would think most people use Python to do the cleaning,7,vk04we,"I’m pretty good with using SQL to join, extract, filter, and analyze data.  The one thing I think I need to work on/learn is how to clean data.  Such as formatting it consistently, removing duplicates, and wrangling/transforming string fields.  Any online courses and/or video recommendations would be extremely helpful!  Thank you for reading and happy analyzing!",analytics,2022-06-24 15:25:57,11
"Yes, proficiency in Excel is a default expectation in analytics because it's been around a very long time, is pervasive throughout industries, and is relatively scalable up to a certain point. It's also very flexible and used by people and stakeholders with varying degrees of expertise. 

Python is good to know as well, but keep in mind that there won't be as much support and as many frameworks around Python as there are for Excel. There's a recurring meme about analytics workers being asked to provide something in Excel despite all their efforts to build and maintain things in \[insert non-Excel tool here\].",82,vjrv8o,"So I'm doing projects to beef up my resume as I have  a college degree but no experience. Right now I'm teaching myself numpy, matplotlib, and pandas. I'm building jupyter notebooks and I'm going to teach myself scraping soon. 

My problem is excel.  I've decided I want to be a data analyst and eventually a data scientist, but I don't want to build excel projects and sharpen those skills unless I have to. I'm pretty familiar with it but I just don't want to do excel for a living. Unless it's the only way.

This would also be my first white collar job, so will I be able to choose my own tools, or will I be able to choose which tools I want to use? Do you get the choice of which tool you want to use? I've only worked jobs with strict and rigid rules and protocols for everything, so these types of jobs are new to me.",analytics,2022-06-24 09:05:42,50
"I have never heard from anyone I ever worked with that they had formal training in Excel.  I have included use of Excel in my intro stats class, but it wasn't a course on Excel.  People study tutorials on Youtube, lots of StackOverflow.  There are websites that say they'll teach you everything you need to know.  And play with it.  Excel and Google Sheets are available online, so your Linux box running firefox will support you fine.",28,vjth8h,"Pretty much the title says it all.

As a youngster, I *had* to live with Linux because I could not afford more recent hardware. I was so poor I needed Linux to run my dumpster fire of a desktop system. But now, at every opportunity, I choose to use Linux because my mental health *enjoys* the open source tools and the unique level of stability. I *especially* like the rock-solid nature of Debian and XFCE, on which I am currently learning both R and Python.

I was noticeably less productive on R running atop the Windows environment. Last night, I nuked my Windows 10 partition with Microsoft Office 2019 and have never felt better since. It was cathartic.

But the question lingers: Can I skip the formal training in Excel and land a job in this field?",analytics,2022-06-24 10:18:09,32
What's your location and is the job remote?,4,vk2xri,"hi guys  


did 3 years of mech eng, dropped out. finished CS associates, then on my last year of data science BS currently. I started an internship as a ""performance analyst intern"" mainly working w sql start of my ""junior"" year into my data science BS.   


It seems clear like they want to hire me after I finish my degree, so I will have 2 years of internship experience at that point. I have a lot of other random job experience like 2 years at a high up manager position at a customer service job.  


How should the internship and other stuff to affect initial salary for this first job? (if affecting it at all)",analytics,2022-06-24 17:48:03,16
"You don't sample datasets (unless it's billions of rows and your machine needs to), you sample participants",1,vk742d,"Hi guys. I have a quick question. When it comes to sampling techniques (stratified, simple random, etc.) are these usually done on super large datasets in industry?  


How do this relate to regressions if someone were to be building them for datasets?  


Where do these datasets come from in let's say, the banking industry?",analytics,2022-06-24 21:45:25,5
"Run.

It's not going to get better, and your future is too bright to spend it being ground into dust for pennies.",41,vjeuiw,"I got a job of data analytics at the beginning of the year. When I was on board, there were three colleagues on my team: 2 FTEs and one contractor. After only one month and a half, it left only me in the group. I took all the team's work without any support and worked overtime for almost two months. Then the new joiner came, but he resigned in the 3rd week. I love the job; that’s why I’m still there. But since I knew I was lower-paid at least 25% than others, I felt hurt by this unfairness. Now I’m really discouraged to work and feel disgusted. I don’t know if I shall be obsessive with my passion on the job or it is time to leave………….",analytics,2022-06-23 20:32:16,15
"Pick a tech stack that you want to learn. SQL, visualization is a must, dbt can be nice, pandas/R if you want to do data science. Python or another programming language is you want to take the data engineer path. 

Certificates are nice on an intern CV, but most people don't care about them after that. Demonstrating that you can use it in a real project to deliver impact is what maters. 

If you want to stand out from all the interns/juniors do something that matters and is visible. Try to do a project that saves a real life problem. Build a report for a non-profit organization or public dashboard that will have active users. Build a dashboard for your hobby that others can use. Write an article about your analysis that others can read.",17,vjhiqu,I am a junior CS student looking for tips to have a desirable resume. Are you going to get the google data analyst certificate? Are you going to build project and if so then what are they and what are the public data source that you will use?,analytics,2022-06-23 23:07:35,7
"Each workplace is different so ymmv.

That said, Excel and Pivot tables, and basic formulas I would call table stakes. You should definitely learn it but it’s not noteworthy and in really strong analytical organizations you will rarely use it (outside of helping less analytical folks and basic data exploration), or if you’re in a very small company that doesn’t have databases.

You’re going to distinguish yourself with Python, Tableau, SQL, statistics, etc.",7,vjlzbs,"If so, how often do you make them, and whether knowing the basics of pivot tables is more than enough to do the job because other BI tools such as Power BI and Tableau are more powerful? Or knowing advanced-level Excel pivot tables is required in your job?",analytics,2022-06-24 04:12:35,15
"I don't know about courses since I don't use them. But I know a book called Hands-On Data Science for Marketing, by Yoon Hyup Hwang, which uses both R and Python, for example.",5,vj4ub9,"Hello all, are there any courses that teach MMM and uses python? I want to learn the basics and the underlying analysis involved",analytics,2022-06-23 12:22:55,3
grats dude! getting that call that you got the job is one of the best feelings in the world,23,vie2ss,"https://www.reddit.com/r/analytics/comments/vgtqyu/job_interview_tomorrow_theyll_give_me_a_dataset/

The dataset they gave me was waaay simpler than I expected. A snapshot of some costs for a few similar departments, and I had to make recommendations on how to reduce costs. I came up with two ideas in the first few minutes and then panicked for the rest of the hour because I couldn't come up with anything else, but apparently that was all they were looking for. Presentation of my results went well, lots of discussion after that all went well, politely stood my ground against some of their ideas about the date that I didn't agree with, and had a great conversation. 

So anyway, I'll definitely be hanging around here in the future. Glad to be a legit Data Analyst.",analytics,2022-06-22 13:04:17,15
"Start applying for the jobs you want now.  Hiring is weird: I've been hired by mistake and I was hired once in an effort to make trouble for the person who was slated to replace my hiring manager.  Both good jobs, but hiring is weird.

While you apply and be open to jobs, there is a lot to be said for being in school where you end up working closely with a professor who supports you.  Your connection might be more important than the name of the program.",8,vj218l,"Hi I have a B.S. Political Science with a minor in Statistics. 

I've worked for three years at an international business non-profit as a research analyst. I've wanted to break into data analyst/business intel roles for a while and have considered bootcamps like Springboard. 

I have an opportunity to get a M.S. Artificial Intelligence from an accredited (but not highly ranked) school. It would be very cheap and I would be able to live with my parents while getting the degree. An alternative is the M.S. Computer Science although the AI track is really more of a data analysis/data science track, just with a bit more AI/ML/NN content. I would be doing a work study with the director of the program who is a Prof. of Computer Engineering. The school does not offer a Data Science, Analytics, or Applied Stats M.S. within the Engineering Dept.

1) Is the name of the degree (AI) a potential issue for analysis/analytics jobs?

2) It is essentially an applied stats course with opportunity to concentrate in 'Data Sciences & Analytics'. Would it be seen positively for roles that aren't explicitly AI/ML related?

3) Any other considerations pursuing AI vs. CS? Many of the courses are interchangeable between programs.

Thank you!",analytics,2022-06-23 10:18:38,14
"Lmao. Learn Sql.

Vba is not worth it at the expense of learning sql, r or python.

Most data analyst roles have a sql assessment. unless it’s a dinosaur company still working on decades old spread sheets with macros.

And don’t work for those dinosaur companies that make you do that. The worst thing in this industry is to become obsolete and have no one who wants to hire you because your skill set isn’t up to date",55,viq4z2,"I wonder if I should learn Excel VBA? I have always wanted to become either DA or BA but I’m not sure if what I have learned so far are enough for me to land entry level jobs. I am familiar with SQL, Tableau, Python and R but most comfortable with SQL, Tableau, and R, so I was wondering if it’s still necessary for me to learn Excel VBA? Or knowing advance formulas and know how to build charts should be more than enough for me?",analytics,2022-06-22 23:18:41,27
"With your background it’s not even close, data analyst is the answer. I mean this in as friendly a way as possible, but with your current experience (as I read it) you have no chance at a SWE position. Competition doesn’t matter if you aren’t qualified and don’t have the skills. Also from my experience the bar you have to pass in interviewing is higher in engineering than in DS/analytics so the number of other candidates is far less important than your own skill level.

I would be somewhat concerned about the mathematics, but if you grasp/can apply statistics and probability you probably can advance on the analytics or DE end of the DS spectrum. 

For entry level data analyst the sql skills will be the most important thing along with analytical reasoning/quantitative problem solving.",11,vipirz,"So I have a bachelor's degree in economics and an associate's in accounting. I've taken upper division statistics courses and up to calculus 2, which I passed with a D. I'm terrible at math, statistics I'm ok. for job experience, it's just odd jobs honestly. I've taken courses in Python, C++, and SQL. I've worked on a couple small projects for data analytics (simple queries with pandas/sql and matplotlib).   


the problem is that I see that job postings on Linkedin (I'm from the bay area of california, san jose silicon valley) have over 1-200+ applicants for some positions. just way more than software engineering positions for which I can see like 12-14. much lower on average. to my knowledge, my background is better suited for data analytics and eventually maybe data science. however, my skillset as of now has almost no software engineering to its name and no projects done for this field either (yet, at least). the biggest thing is that I will be competing with people who have computer science degrees.   


so, how is the applicant pool for entry level data analyst positions? are they strong applicants? how competitive is it compared to software engineering? which do you think I should go for? Please advise.",analytics,2022-06-22 22:38:30,10
"Monthly active users. 

Hours watched or Usage.

Churn rate.

General marketing funnel conversion metrics. 

Channel conversion rate. 

Basically from a marketing standpoint you would want to know how many people you are targeting (in each segment) and how of many of them convert. Which is the best performing channel to acquire users. What is the cost of acquisition. How many of these users stay with you. Are they really using your product and finding it valuable. What is the net promoter score. How many people are leaving and why and where. What are the substitutes. How do the stand in relation to my product. What is the correct subscription cost. Will lowering it increase my user base or will it make my product look cheap. Will I be able to retain customers even if I raise the subscription cost. At what price is the optimum revenue.",4,viotur,"I just had an interview for a data analyst job and I really like the company.

There will be more interview stages coming and a case study so I was hoping to know what KPIs are usually used in product analytics, specifically in subscription-based business models.

Background: I worked in corporate finance before, but FMCG and not tech.

Thanks guys",analytics,2022-06-22 21:56:33,4
"You have a class of 100 people with an average grade of 50, and a class of 10 with an average grade of 90. If you average 50 and 90, you get 70, but the real average should be closer to that of the bigger class

.EDIT: my example is a case where average of averages is bad. In samples of same size, it can be ok.  re-reading your post, if you're comparing equivalent metrics across two customers, I believe that could be ok, depending on the metric. Are you able to share examples?",27,vhzk3x,"Are there any mitigating processes to consider to make less bad? Case: I have average 30 day app usage data for a customer to use as a KPI for analysis and two groups of customers to compare. 
Thanks

Edit: A lot of great answers in here, thanks all for getting involved and providing advice. To summarise,  weighted averaging can help, understanding your comparison groups and ensuring that you are comparing truly comparable customers. There's also the acknowledgement that averages of averages may not be inherently a bad metric as long as the caveats are fully understood.",analytics,2022-06-22 00:41:02,11
"I think the key thing here is going to be stakeholder management. For example, when you’re asked “what trends stand out to you”, that can be an opportunity to probe further. The ambiguity of the question could be two things: they could either just be expecting you to parse through a massive dataset on what’s essentially a fools errand, or they have something in mind subconsciously already.

Also, you’ll find a lot of times managers and leaders do have access to the dashboards, but might not know how to use them. If you’ve built them, a great opportunity to build trust could be working sessions where they ask and you answer. It’s possible they have questions that aren’t answered by the off-the-shelf reporting, and updates may be as easy as adding a new field to the SQL or as extensive as building a new integration for a new data source. 

All in all I’d encourage a mentality shift from “Im not good at my job” to “I need more info from you”.  I don’t believe aimless analysis is a good use of resources if there’s not a goal in mind, so I’d encourage politely probing for what the goal might be. For example, maybe they’re actually interested in the performance of a few products in a specific market. That’s something actionable you can dive in on.

Edit: one more thing: I’d actively advocate for yourself and get yourself in those meetings where goal discussions take place. You want to position yourself as a valuable decision making resource and not a faceless dashboard creator if you want to continue to advance. Sometimes leaders aren’t always aware they have the people they need already and need a gentle reminder.",3,vibcqx,"I (25F) graduated with a degree in Marketing Management in 2017 and found a job at my current company right away. For the past 5 years, I have been assisting the Marketing Manager, managing a dealer growth/rewards program, overseeing our sponsorships, content creation, trade show planning, etc.

Some quick information about the company. We are the USA office of a multinational B2B company. I don't want to say our exact product, but for all intents and purposes, we will say it is tractor buckets. There are only about 25 employees in the USA. There is no IT or BI department. The USA office is pretty heavily managed by our headquarters in Asia, and we have no say on pricing, and we do not know our profit margins.

Around 2 years ago, USA management noticed that I work well with Excel, and they asked me to help with analytics (in addition to marketing). Around this time, I started sending basic weekly and monthly dashboards with information on customers and sales mangers. Last year, I tried to send emails to USA management every few weeks with various insights. Like breakdowns on how X products are performing compared to previous years, whether X incentive program was successful, how our customers are shifting towards X product lines, etc. There was some feedback on this information, but nothing ever seemed to be done with it. It honestly seemed like a stressful waste of time.

I have noticed that I do much better when I have clear instructions and goals. For example, there are times where management will come to me and say ""we are considering X, what do you think about this based on the data?"" I can provide detailed insights and reports on this with no issue. The problem is when my only instructions are ""tell us if any trends are standing out to you."" We have 2 divisions, 75 customers in 30+ states, and 2,000 products. I don't know what to look for!

I taught myself Power BI a few months ago and have been able to create dynamic dashboards with all sorts of information. Our management and sales managers have access to these so that they can always see sales, how we are tracking toward our goals, top customers, product information, etc. I am not sure how often this information is used.

I am so lost at this point.

I am not sure how to do my job. Or I do, and I totally suck at it.

Please help...",analytics,2022-06-22 11:03:37,4
"Your submission looks to be asking about industry courses. If so, you are not the only one asking this question, try the search, the sidebar (lots of resources there), and [check out the resource collection on our community site](https://lookingformarketing.com/courses?utm_source=r_analytics&utm_medium=ai)

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/analytics) if you have any questions or concerns.*",1,vi20v8,"I’m about to finish my bachelor of marketing and I want to start my career in business analytics.
What courses should I start to take?",analytics,2022-06-22 03:26:41,1
+1 cause tools are great,1,vi4wt2,"Hey there! I’m building an insight-first website analytics tool (something between GA and session recordings with focus on detailed content analysis) for UX designers, CRO specialists and marketers.

If you are in one of these groups and would like to **take part in the short usability session**—let me know in a comment or send me a message!",analytics,2022-06-22 06:09:25,6
"Addition, subtraction, multiplication, division",81,vhfa7k,,analytics,2022-06-21 07:58:41,35
"Former SAS programmer here. The only companies using SAS are in industries that are highly regulated, like insurance, banking, life sciences, perhaps some energy. If you are selling into those, then you could be fine, but just realize the big tech companies and damn near everyone else have moved to using R, Python and of course some version of SQL. They have made those choices originally because they are just as powerful but also… free. You will not do well competing against a free and powerful technology (that was phrased weird but it’s true).

I would not plan on being there for 5 years, but I would take the job if you’re looking to break into tech sales. Learn how sales works, hone your craft, and then be looking to exit 18-24 months into carrying a quota.",25,vhhf9r,"I’m interviewing for an Account Executive sales position for SAS, and already local to their HQ office near Raleigh, NC. Would be selling into healthcare and life sciences. 

SAS is a prestigious tech brand in my area and has consistently placed on Fortune’s Best Places to Work and other ranking lists (...taking with a grain of salt). I’ve always heard their WLB, benefits and office perks are great.

However I’ve been reading the company has fallen behind and no longer an analytics segment leader. Recent reviews from the development side are negative for outlook, and sales reviews aren’t as favorable either. 

If I am given an offer, it would be a huge jump in comp and my first SaaS sales job, which would look good on my resume since SAS is an established large tech brand especially for my local market and may open future opportunities in Big Tech for me. It would definitely be a major jump from my current smaller less known company.

I am just cautious that my quota would be unattainable, and I’d be setting myself up for failure by joining a sinking ship. Would want to stay and grow at least 5yrs in my next company. 

**Thoughts on SAS? How are they perceived on your development and functional side?**",analytics,2022-06-21 09:34:30,9
This is too vague,10,vhsy60,"Hello, I was wondering if jobs in analytics specifically a business analyst is good for a big picture person. In high school and college so far I enjoyed creating the presentation more then the actual project. Idk if this is too vague but would appreciate advice.",analytics,2022-06-21 18:21:57,9
"So, the blockchain is only related to crypto in that crypto uses the blockchain to process transactions. I don't think companies like IBM have any interest in crypto, but they most definitely are interested in the blockchain.

The blockchain provides a public ledger of all transactions for a good. Any transactional good that could possibly be damaged by lost data, or a nefarious or negligent actor with access to edit those transactions are good candidates for the blockchain. For example, a house's title, or medicine within a supply chain.",8,vhly07,"I tend to look at LinkedIn jobs weekly just to see what is out there and this is something new I have seen pop up. Is there anyone in the community that knows a little more about this role or is even possibly in this role? 

This really intrigues me especially as larger companies like IBM and Deloitte add these types of positions, even though I haven't heard of them offering services related to Blockchain or crypto.",analytics,2022-06-21 12:53:45,3
"I also have a degree in business administration. In my opinion, people in business administration should know about data analytics. In a lot of cases, it doesn't make sense to manage a business without data analytics anymore. 

I'm now becoming a business professor, and I see more and people schools saying that analytics is going to be a requirement to have a degree in business. I'm developing a course on marketing analytics to launch in Fall, for example.",9,vhat8b,I have graduated in business administration and data analitics has caught my attention. Do you guys think that someone from a background in business can enter this kind of profession?,analytics,2022-06-21 04:08:49,14
"If you're comfortable going the apprenticeship route I'd recommend looking for an employer who are looking to take you on through a DA apprenticeship. It would give you a decent grounding in the core principles of data, stats, writing basic code etc.",3,vhkjae,I’m 20 and I’m currently leaving my hairdressing apprenticeship for career in data analysis. How do I begin?,analytics,2022-06-21 11:51:25,4
Hash the data or don't include it in the data set accessible/available to your users,3,vhsaha,"I am currently looking for ways to do self serve analytics like mixpanel, amplitude on top of datawarehouse, we are evaluating reverse ETL + mixpanel, however we have some PII  data that we are not comfortable sending to 3rd party cloud, what are my options?",analytics,2022-06-21 17:47:50,3
NDA?,1,vhs9v3,"I am currently looking for ways to do self serve analytics like mixpanel, amplitude on top of datawarehouse, we are evaluating reverse ETL + mixpanel, however we have some PII  data that we are not comfortable sending to 3rd party cloud, what are my options?",analytics,2022-06-21 17:46:58,2
"In that time, I’d be looking at production rate per product and then sale price per product and see if there is any argument to increase production of specific products to increase profits. Look at declining production trends for potential to update production methods, see downward trend of product sales to evaluate product marketing or
Competition. Look at seasonal or monthly variance that could give better production or sale strategy eg. Fabricate product d only during y season as sales are low most of year. I would also make sure the results are presented in some good tables and charts to show capability to demonstrate the information.",112,vgtqyu,"Context: just got my CS degree, got an interview for Jr Data Analyst on the Continuous Improvement team for a big company. 

First interview went great, next interview tomorrow is as described above. It will be done in Excel and if I had to guess, the data will be something like production amounts, costs, revenue, sales, etc. Business-y stuff. 

Any tips on things I should look for first to identify any interesting patterns or anomalies?

I've been working through a simple Udemy course on Data Analysis in Excel so I'm comfortable with pivot tables, pivot charts, vlookup, and all the basics.

For more context of where I'm at, I've also got a minor in math and just missed getting a minor in stats by one course.",analytics,2022-06-20 12:34:05,30
"There may be cases where it could work, but I think something like software or data engineering would be less social. I have meetings every day as an analyst, and sometimes full days of meetings where I have to contribute. If I moved up in leadership it would be even more.",11,vh81zp,"Is analytics a good option for someone who is not very social?

I am the kind of person who spends entire weekends without talking to a single person.I work remote and never discuss anything with co workers other than work.I have also been told that I am unusually quiet.


Should I pursue this?",analytics,2022-06-21 01:00:42,8
"Do you mean multiple users that have admin level access and are making a general mess of things? You need to lock this down and centralize governance with a dedicated team. 

Will require senior level support, but it shouldn't be too difficult once you mention that your company web analytics platform is basically unmanaged and potentially opens you up to all kinds of problems, including GDPR violations.",2,vhj77z,"I'm working in a large organization and there have been a number of individuals, vendors, contractors etc. who have been in charge of the GA accounts. It's a mess.

How do you build a governance process for GA access? I'm leaning to dumping all these accounts to see who screams, there is no documentation around accounts and why they have access.",analytics,2022-06-21 10:53:43,3
"Appropriate value of A based on what? Without a dependent variable, why are you changing the value of A?",3,vh5gup,"I was working on a task where a particular item ""A"" has different price points as shown below:

|Item|Price|
|:-|:-|
|A|400|
|A|495|
|A|450|
|A|550|
|A|614|
|A|571|
|A|600|
|A|572|

Which statistical method do I use to arrive at the most appropriate value for A. I've tried to simplify the use case. Could you please guide?",analytics,2022-06-20 22:11:35,6
What kind of data would you want to pull from each account and experiment and are they linked to the same GA account or view?,1,vgoklz,We run experiments on several client Google Optimize accounts that provide access to our account. Looking for a way to get combined data of all experiments we are running on a regular basis for our own KPI/metrics. Ideally importing everything/certain things into a spreadsheet on the click of a button or regularly?,analytics,2022-06-20 08:49:31,2
"It doesn’t make much sense to me, but Google has a big data center in the Ashburn area. And plenty throughout Texas. Maybe the location you’re getting is a data center of where the email literally is?",5,vgc4xk,"I use the free version of Streak for Gmail to track when my emails are opened by recipients.

I live in Australia, and Streak is telling me that some of my emails are being opened in Ashburn, VA (USA). I know that the recipients of these emails are not in Ashburn, VA; they are in Australia.

I've also had it tell me people are opening the emails in parts of Texas, USA, and I'm fairly sure that the recipients are not in Texas.

Can anyone explain this?",analytics,2022-06-19 20:43:42,7
It might be useful to mention in which corner of the planet you are.,1,vg3ex7,"Greetings!

I have a masters in analytics and a bachelors in marine biology. I really wanted to do marine data science. I just wanted to know if any of you knew of any companies that would be a good entry level or even internship possibilities and if anyone on this thread is doing marine data science or anything close to it how is it like!? 

I applied to lynker heard really good things about them. 

TIA!!!",analytics,2022-06-19 13:06:31,1
"If you are ok with an extra step, GA4 has a native connector to bigquery, so you could set it up and then have a gsheets connection from gbq, that would be automated and allow you to check historical info too.

If that’s too much of a hassle, ga4 to data studio, transform things there, then manually export to a sheet.",6,vfvsfk,Anybody have a favorite tool for getting ga4 data into sheets?,analytics,2022-06-19 06:59:05,5
"I’m not saying this in a way to sound rude, it’s said with love..    you need to set healthy boundaries,  no one will set those boundaries for you.  If you end work at 5, then end work at 5..    be polite, but if something feels like they want you to do it now say “thanks, I’m absolutely able to handle that, will dig in once I’m back in the morning 🤗”..     my whole career (12 years in data) Ive felt this pressure and only just realized that their goal is to get as much output as possible from me, and it is my responsibility to create appropriate guard rails.",57,vfcpbx,"I’m beginning to resent the fact that I’m being pulled into meetings at the end of my shift and expected to work after hours. I’m a temp, hourly employee, who does not get overtime. 

I understand this field requires overtime sometimes, but does this hold true for contractors? My coworkers in those same meetings are salaried, entitled to vacation time, have more flexible schedules, etc. I don’t. 

My onboarding paperwork asked if I’d be willing to forgo breaks to work. It was a voluntary thing and I put no. Other than that, nothing in my contact states anything about this. 

I should also note, no one is asking me to stay later but it would reflect poorly on me to leave- it’s one of those unspoken expectations. 

I don’t feel like it’s okay to bring this up, because I’m new both to this job and this field. It’s a big reputable company and I need the experience.

*edit* I love that this sub exists, these responses were exactly what I needed to hear. Thanks everyone!",analytics,2022-06-18 11:52:58,13
"I'm still sticking with analytics but I can understand. 

Three years back I was scared of analytics and data in general. I didn't anything beyond mean, median, mode. T-test, z-test would make me shit my pants. I became a developer and still wouldn't understand things beyond basic. Like only thing I understood were classes and objects. And then I was an okay okay developer. Couldn't do much. Couldn't learn much either, was not good at understanding.

Then I did my MBA. It made me a better thinker completely. I learnt a lot about the world and how it works. I understood systems and possibly what thought would have gone behind it. Why things are the way they are. And most importantly, it changed me completely. I could now understand things better. I'm a much better data analyst/scientist and developer. I do everything now.

Maybe try to be patient. And learn how things really work. Find why analytics is being used where it is used. Get attached to problems and not ML techniques. I think this would help.",17,vf4bna,"Hi, I am slowly getting an anxiety that may be analytics is not something good for me. I am not able to unspkill myself and attending interview makes it more clear that I have lot to cover with limited time to learning and my efforts are not at the best.

The big question, people who left analytics in recent years if you are ther? where did you move to and how did you adapt.

Skill set - Google analytics, tag manager, Power BI, Marketing analytics.",analytics,2022-06-18 04:32:26,20
Can you get a relevant job on campus? Some universities have research or teaching assistant positions that look good on your resume while you're still at the uni,6,vfeowy,"I know there are a ton of post on this, but it does not hurt to make another, lol.

Been applying to jobs/internships in analytics or data, but not getting anything back. I am using LinkedIn courses to learn more about Excel and making reports.

I would like to have someone mentor me to make projects or reports on raw data; is there something out there that can help me with that?

I like to look at Government data or Auto industry data. Ive been looking at US Dept of education, but it seems so like so much comapred to the data we get for learning in class.

Any tips or Youtube videos/course that will guide me with working on raw data to to a clean report? 

Thank you",analytics,2022-06-18 13:32:41,5
"This is my direct line of work. 

Workdays offering is pretty new, and somewhat limited by the ability to get outside data into Workday (they will sell you PRISM for this piece.) I have no doubt if Workday prioritizes it, this product will improve within the next 3-5 years (just like Recruiting and the rest of their modules have) but it’s certainly nowhere near best in class. 

By way of comparison check out the 2 market leaders in the People Analytics space, Visier and One Model. I’ve deployed Visier twice, and am midstream in deploying One Model currently. Happy to answer specific questions via DM.",9,vfa522,"I've gotten the sense that there aren't a ton of HR/people analytics folks on here, but figured it wouldn't hurt to try asking this: does anyone have experience with Workday's core People Analytics offerings? My team did a demo this week. My initial impressions are that WD offers a lot of out-of-the-box metrics. The salespeople claimed that there's a lot of customization in terms of labels, titles, etc., but didn't actually demonstrate any of this, so I'm skeptical. IMO if your audience doesn't understand your KPIs/metrics then your dashboard is useless. I use Tableau a ton in my work, so I'm used to having a lot of flexibility when it comes to dashboard design. Thanks!",analytics,2022-06-18 09:46:46,2
What is your undergraduate in,5,vettrj,"Well I can't decide about my master's I have researched a lot day and night to take a decision, even I have read some topics here on reddit. I think I have no great skills in programming and I haven't learn Python or R yet I try to learn SQL properly. I have a business background but I enjoy a lot algorithms and logical functions. I feel really comfortable analyzing metrics and economic situational enviroment, so I think I would enjoy both master's

Can you give advice and share some of your experience about this,  Data Science or Business Analytics in your experience and opinion.",analytics,2022-06-17 17:09:40,43
"Myself being a supply chain analyst in an IT, spends  most of  the time around Power BI, Python, SQL and excel. I never had to built any software for managing the supply chain.",6,veofhk,Hello I was wondering what skills should I gain to perform well as a supply chain analysts. I am good with excel and SQL. But I’m not sure what else would be good to bring to the table. I was looking into diving deeper into Java and Python I want to be able to build a SCM software tailored specifically for the Hopsital I’m working at. TIA!!,analytics,2022-06-17 12:47:59,3
"Lead Analyst. Middle ground. This way you're an IC and you'll lead people. Plus, of you switch the next logical position would be analytics manager.",46,ve6ewv,"Given the choice of title with a promotion, which would you go with? Which would have better career options down the line? Naturally I prefer to be an IC so I lean more toward Senior DA, but can’t help but think Analytics Manager may be a better title for future roles?

Note also that “Analytics Manager” would be a little grandiose at my org, where there are only 2 analysts (one of them being me) and 1 data engineer.",analytics,2022-06-16 23:17:54,30
"Senior Analyst here.

I work directly with stakeholders, prioritise my own workload, define my own briefs, work on projects independently or lead more junior analysts, deliver insights with no or minimal review. I inform my manager as to what's happening in my area of the business rather than vice versa.

Ultimately my manager has very little involvement in my work. Occasionally I'll check in with them for guidance, or they'll inform me of other conversations around the business and reprioritise my work. But most of the time i have very little oversight.

I mentor and train junior analysts, but don't manage them. I'll often help make decisions that affect the department (data strategies, roadmaps, new tools and resourcing etc.) but the final decision isn't mine.",43,vdhlgy,"Hi All,

Recently transitioned into a Senior Data Analyst position and am wondering on the industries sentiment on the fundamental differences between the two job titles? 

So in your experience/opinion - what's the difference between the two job titles?

Thanks!",analytics,2022-06-16 02:02:03,12
Bigquery is where you store your data for future or you pay for GA4 a you have data retention for 50months if I am right.,3,vdk3mf,"I’m not clear on what specific data points are removed from GA4 based on data retention settings. Is it ONLY user-identified tidbits used in retargeting and stuff, or is it ALL information such as basic website stats like traffic count data, page stats, e-commerce non-user-specific info like product performance? I.e. NO data survives 24 months. 

If it’s EVERYTHING, where can we store our long-term data for free with similar reporting? GDS or bigquery for longterm dashboards?",analytics,2022-06-16 04:45:23,1
"Let me fix it for you: 

PM: hi, can you guys tell me why this query is giving me a different result from the dashboards? Why is there a difference? Are you doing something about this or is this reported? Query is showing 205.05 and the dashboard shows 169.90.

MC: The DB data is in USD and the dashboard shows it in GBP.  205 USD=169.9 GBP according to the rates from here: paste wiki link

&#x200B;

PM: kthnxbye.",116,vcxjcd,"Project manager : PM

My colleague : MC

*Pm messages in our slack help channel for our data products*

PM: hi, can you guys tell me why this query is giving me a different result from the dashboards? Why is there a difference? Are you doing something about this or is this reported? Query is showing 205.05 and the dashboard shows 169.90.

Query :
>select total_revenue_usd from our tables

Dashboard:
total_revenue_gbp

MC: Well, like the name of the columns suggest, one is in American dollars and the other one in gb pounds.

PM: that is obvious, I am not asking that, I mean I did the conversion and it is not the same.

MC: could you provide the rate of conversion you used to check with the data?

PM: I didn't do any conversion, the dashboard is showing a different number from the query and I don't understand why.

MC: did you or did you not convert the value then?

MC2: *pastes the conversion rate table from confluence*. If I use these rates I'm getting the exact 205.05 amount in dollars.

PM: OK then this table clears my doubts. *peace out*


This job is something else man

Edit: mates, it's just a jest. We had a good laugh from it, unstrength thy niples.",analytics,2022-06-15 08:47:45,45
Hey there! Have you checked if your data is sampled? And are you sure there are no other filters or dimensions that differ from one report to the other?,4,vdfzwf,"Hei as the title suggests im having problems in google analytics
For example if I look at the report from 3-10. May 2022 the sessions number is 400, but if I compare it by dates the number of sessions drops to 375 (for the date from 3-10 of may).
So it looks like: 3.5 - 10.5 sessions 375
                            24.4 - 10.5 sessions 300
Compared to if i dont compare dates: 3.5 - 10.5 sessions 400

And i wonder why is that happening and which one is acquirate now?",analytics,2022-06-16 00:07:46,4
"What do you have to offer for analysis??

You’ve peaked my curiosity.",1,vdmc69,"I know actually nothing about analytics, but I am curious  
Which games are most often selected as an answer on the subreddit r/tipofmyjoystick  
I think this would be good information to put on that subs wiki since most common games will be instantly found  
Since the name of the game is in the flair, it should be easy to collect the data, right?",analytics,2022-06-16 06:43:23,1
"Google sheets + Google data studio. Data studio isn’t the best product on the market, but totally free. You can make custom metrics inside the tool from your data coming from Sheets. I’ve done this with my personal financial data, and make tracking & budget reports for fun",4,vclncs,"What are the free tools available for transforming data, storing data, and performing ETL on data?

From my understanding, you can use **Google sheets** to store, transform, and visualize data for free.

You can also use **Github** to store and visualize, but NOT transform data for free.

Are there other tools to recommend? I'm on a project with a limitation on the use of costly software.

Are there good options, like storing on Google Sheets, and using a Javascript API call on the Sheets to create a visualization on the fly?",analytics,2022-06-14 21:00:11,6
Sounds like a scam,96,vcblly,"Just received on offer from a recruiter for a position which requires 5 weeks of training that I would be required to pay
fo out of pocket($750.00). He said I would be reimbursed when I received my first pay check. They said the training was required even though I have 3 years of experience in an analyst role and will be graduating next spring. Obviously I turned it
down, but I’m curious what everyone’s opinions are on these types of offers and whether or not anyone has had a similar experience.",analytics,2022-06-14 12:36:25,23
Which analytics tool and tag management tool are you using?,2,vcrv4a," Hello fellow redditors. I'm quit new to analytics and have been amazed by this new world that I have found out about. I have a question about how to track my website users.

  
Situation:  
I have a website for selling a web panel for sending bulk sms. Users come to my website, choose a plan, pay for it and then login into my website. When they enter username and password, we send that data to the login page of another website and they are actually logging in in another website also managed by me (which offers bulk sms services). 

  
Questions:

* How do I track successful and failed logins? (People enter username and password on first website and if validated they land in the second website)
* How do I exclude people who have logged in to access the other website from my website visits in analytics?",analytics,2022-06-15 03:59:21,4
I think this question is more for a data engineer maybe? Generally an analyst will have this metric ready for analysis / query,1,vcqj0n,"We're running a visibility campaign on external websites to promote our product - I've used UTM links to track performance but it only shows the number of users on Google Analytics, I want to be able to track **clicks** on each banner ad, how is it possible please?  

Will event tracking via Google Tag Manager do the work? How so? 

Thanks in advance for your help.",analytics,2022-06-15 02:26:00,4
"I have never heard of an agency covering those things for contract roles. But I would guess it’s because you’re not actually an employee, so you’re stuck with worse insurance, shitty PTO, second class status all around relative to normal salaried employees.",7,vca2uj,"I've been looking around for a new role in data analytics. I've talked to in-house recruiters, as well as recruiters who fill W2 contract roles for an agency. In my experience, the W2 contract roles pay 20–30% more than the regular W2 roles. Is there a reason for this? It's my understanding that the agency would cover payroll taxes, benefits, etc., like an actual employer would, so I can't figure out why the pay would be higher. Am I missing something here?",analytics,2022-06-14 11:27:54,6
"Definitely list them in a Technical Skills section, but also include some detail on how you used them in your previous role. For example, you could say “Used Python and Pandas to automate monthly reporting, reducing overall lead time by 34%.” The goal is to highlight that not only do you know the skill, but also understand why it’s valuable to the business.",11,vbr6cm,"Hi guys,

I graduated with a BS in Biology from a 4 year public university a few years back and have been working in a clinical like setting conducting hearing tests to clients. The transferable skills from this job to any analyst position would be presenting the data (what their hearing test means) to the client so that they understand where they're at and building rapport/relationship with them as well for trust, etc to eventually sell them something (interpersonal/communication skills). 

I've also self taught myself Python, JavaScript, and SQL over the past ~2 years and have gotten to an intermediate level with all 3 languages.

When applying for analyst positions that requires presentation skills (presenting data & findings to clients) and most importantly SQL/Python, how would I present that on a resume to get HR's / hiring managers attention?

Could I literally write

Skills: Python, SQL

to show that I know python and SQL and that's it? Or would I need to show actual proof on a resume (whether its a link to your GitHub or a 'project' section) that I am competent with these languages such as in depth projects that relates to the analyst role that I am applying for? 

If I can just write down 'Skills: Python, SQL' on my resume and that's it and just expect to solve 'leetcode' like questions during an interview to prove competency, that's no problem. But if I actually need to have projects, etc. listed on my resume just to get past HR, then that's an issue because now I have to figure out what to make even if I know I have the skill. What are most people doing?

When applying for an analyst position that asks for SQL/Python, what are most people doing to show that they have these skills when writing their resume?

Note that I am not specifically focusing on data analyst positions where portfolios are the 'norm', just analyst positions in general that might also asks for SQL, etc. in the job description.

Hopefully my question makes sense, thank you all for any help/advice I can get. Greatly appreciated in advance!!",analytics,2022-06-13 17:52:29,6
"I switched from marketing to analytics. I’m in the US in a MCOL city. 

2004-2016: finished my BA in Communication and then did various roles in marketing, PR, digital marketing. Data analysis was a small part of my job, I didn’t get any training in it, but looked for any opportunities to dig into whatever data I could get my hands on, and tried to teach myself how to use Excel to gain insights. Salary ranged from $27k to $78k. 

2016: moved to my first analytics role. I had been part of that marketing team for about 3 years at that point when they did yet another reorg, the VP recognized that I seemed interested in data, and moved me into an analytics role (under an experienced analytics manager). Kept my salary the same ($78k), through annual reviews/raises my salary was $88k by the time I left in 2019. 

2018: realized my analytics role wasn’t very advanced, and coupled with my liberal arts BA, I had a lot of skill gaps if I wanted to continue an analytics career. Enrolled in a MS Data Science program part-time while continuing to work full-time. Just graduated this year. 

2019: landed a new role doing product analytics for a travel tech company you’ve probably heard of. Still here to this day, total annual comp (base, bonus, equity) has ranged from $120k - $153k.",29,vbd5tx,"Hey guys, I wanted to start a thread related to people sharing their experiences and advice surrounding improving their salary and/or career progression in the field of analytics. Please share what you did to improve salary or get promoted, and any details about that process. If you can share your title/salary progression as well, that would be great.

For example, this could include advice such as getting a Master's degree, switching jobs, taking initiative at work, learning python, or any other specific things you did that you felt helped you get from Salary A to Salary B. Would love to hear you guys' experiences!",analytics,2022-06-13 07:13:26,29
"Any job with a good track record is going to help, you should just learn SQL now it's not too difficult",14,vbnhyz,"Hello all, sorry in advance if this is not the correct place to post.

I've graduated and have been trying to find a job but don't have any experience in the field. I see some jr. positions on indeed and one place has contacted me with a jr database analyst position, but it seems more of a shipping/logistics role and doesn't say anything about SQL. Is it worth it to get jobs like this just for the title/experience and learn SQL on the side? Is that a decent way to break into the field? I've been applying to data analyst-type jobs for about a month now and don't really hear back and I assume it's from the lack of experience. Most jobs seem to want 1-3 years experience which I don't have so I'm getting a little desperate for anything, especially since this at least has the title.",analytics,2022-06-13 14:52:29,10
Following,3,vbsnzv,"Hey all,

Like many others, I have to transition all of our clients (about 80) to GA4. We're currently using Universal with GTM. I'm looking for an excellent guide, the best method, etc. and also something that explains the pitfalls. All of our clients are setup using cross-domain e-commerce tracking.

&#x200B;

* Is this going to be easy or a nightmare?
* What kinds of issues am I going to run into?

Thanks for any and all info.",analytics,2022-06-13 19:11:24,11
"What do you like about analytics, and what did you *dislike* about web development?",2,vbq572,"Hello everyone, I graduated with a CS degree in December. My original goal was to pursue web development. Found out quickly it wasn't for me. I jobshadow someone who was a healthcare data analyst and find what that person did very interesting. I browsed through couple subs frequently to find out main skills I needed to learn before applying.

My question is with a CS degree and basic knowledge of excel, SQL, Tableau can I start applying for jobs or should I pursue the google certificate? Also i recently just got a job in a datacenter. I took this job because I need some income to pay the bills. Not sure if I should add this to my resume as workforce experience. For now I left it out as it's non-related. I do have 3 SQL/Tableau projects on my github and resume.",analytics,2022-06-13 16:58:31,3
"I knew her personally, we worked in the same research lab. She never seemed like a scammer to me.

Data analytics is not hard, anyone who is smart enough to use computers comfortably should be able to pick up the stuff she is teaching, such as sql queries and data visualizations. The trick to become wildly successful at it is to be able to hide your mistakes, or spot other's mistakes ;)

Almost every profession can benefit from computers, coding, and automation. A doctor who knows how to code will create a lot more value than one who doesn't, so it is not uncommon to earn a lot more once you learn such skills.",3,vbdk9a,"Hi, I learned about the existence of data analytics strangely through this data analyst on tik tok who makes videos about breaking into the field. She has turned it into a job of sorts, and has free and paid classes where she helps people get jobs in the field. It's all over her linked in page and she often boasts how she's gotten her students new jobs in data analytics in 2-3 months. She also says she started in a completely unrelated field and just did the khan academy sql course then did some projects in Tableau, edited her resume/LinkedIn and applied and was able to get a salary of 70k - this took her 2 months. In I think the 5 years since then she's moved around some and now gets paid like 150k.

Just wondering is her path and advice an abnormal success story? 

How long does it typically take someone to land a job in data analytics from scratch? 

Is doubling your salary in ~5 years common or a fluke?",analytics,2022-06-13 07:32:20,10
Does your school not use handshake?,5,vbgk1w,Are there any companies out there that hire college graduates with no work experience. I seriously don't understand why they call them entry level positions even though they ask for years of experience.  What companies out there hire data analysts with just a bachelor's degree. The location I want to work for is either remote (anywhere in the United states) and pennsylvania (excluding Philadelphia). The types of data analyst positions I am looking for are the ones that primarily use software in business intelligence. Any recommendations?,analytics,2022-06-13 09:47:38,10
Can you leverage your analytics team or analysts for generating those insights. I am always very skeptical of insights from engineering.,1,vbgy4n,"Hello.  

I work as a data engineer at a SaaS startup that offers a suite of products to our customers. 

A project that I have been working on recently is bringing product usage metrics into our operational tools like Salesforce and Hubspot. The intention is to allow customer success team to have more data-driven conversations with the customers. For example, highlighting the ROI from using our products, upselling, etc.  

I do believe that we can unlock more value from our data and am looking for ways to do it. For example, fighting churn, cross selling, upselling, etc. 

Are there books, whitepapers, courses that the community recommends that would help me in this direction? Specifically, I am looking for resources on data-driven marketing / customer success.

Thank you for reading.",analytics,2022-06-13 10:04:35,9
It entirely depends on what direction you want to take your skillset...,1,vbfl6r,"Good morning/afternoon/evening,

I recently started a new job (in Analytics) and now have access to an incredible learning platform that has courses for almost any technical skill one can think of. I obviously want to use this to my maximum advantage, so I am trying to make a list of skills to learn while I have it. At this point, I have the following general things on my list:

Tableau

SQL

GCS

Python

R

I am looking for other technical skills that you believe to be worth learning so that I can take full advantage of this learning platform. It is perfectly fine if your recommendation is niche/doesn't directly apply/etc. I really want to use this platform for as many skills as I can, as you never know when one of them might come in handy. Give me your worst!

Thank you so much,

Himhefferies",analytics,2022-06-13 09:04:26,3
"Depends on the business use case. You can use a viz to highlight a particular piece of information, show differences or trends. Data Visualizations is an entire field that sometimes gets overlooked, but it can compactly and concisely communicate info quickly.

Edit- This question should be asked to be clarified. Sometimes vizs are for certain groups- who is gonna be using this viz- an exec who doesn’t need granular info or a regular employee who could use granular data and gain some meaningful info from it. Once again heavy on business use case.",5,vb3cye,"Hey y'all, I have a question regarding my latest interview. I was asked this question:

""Can you explain how you would go about using visualization tools in order to present findings? ""

&#x200B;

I don't have real work experience yet so I'm not sure how I would go about this. I genuinely don't. I've made visualizations within R in university and I've started educating myself on Tableau with 10-12 visualizations.  


Can someone shine light on what they are seeking with this answer? How would I answer it if I don't have experience? If someone is kind enough to provide a template of what to say, I'd be so grateful lol.",analytics,2022-06-12 20:59:46,5
Are you sending these events to GA4? Confirm that event requests are being sent properly.,1,vb8jql,"Hi guys!

&#x200B;

I did a GA4 about a week ago. When i did it, i made a custom event (in GA4, not in tag manager) what sowed up at the next day. I created several more but since then these newly made events are not showing up.

&#x200B;

When i go to create custom event, all of my created events are there. But if i want to make them as a conversion, only the firstly created event is there. Enhanced measurement is turned on and i don't know what did i wrong.",analytics,2022-06-13 02:55:19,2
"Scope varies from project-to-project. Most likely you will be part of a team of analysts but you may have different assignments than others. And sometimes these assignments are individual (small projects) and at other times they will be team based (big projects).

I've seen that in a big project, the lead analyst will usually guide the juniors regarding analysis. Also, they would focus themselves on presentation bit more.",19,vaks5s,"I will be applying for jobs in this field, just wanted to know whether Data Analyst work Solo or in a team? Does it vary from company to company? or ""Team"" ones are usually consist of experienced people and vice versa.",analytics,2022-06-12 05:18:59,9
"Find datasets you would enjoy working on (i.e. sports team you follow, demographic data from your hometown, etc) and apply what you've learned in your courses to these datasets. This will 1) show prospective employers that you are taking the initiative to learn and apply technical concepts, 2) help you practice the skills, 3) perhaps you will find a niche you want to go into. 

It's up to you to determine how fancy you want your portfolio to be (github, squarespace, building it yourself). Lastly, I actually recommend you to try to delay your graduation of you can and secure an internship (I know its easier said than done), but it's one thing I regret not doing because it made that first job much harder to get.",13,varyp3,"Hello everyone, I was hoping to see if you guys could give me some advice on my next steps. So I am currently in my senior year of a Bachelors of Science in Accounting. I picked my major think it was the right field but after getting through a bulk of my classes I’ve realized I actually enjoy the analysis and logic of accounting, not accounting itself. My degree is set up mostly accounting courses, but I will have a few statistics and computer science courses under my belt as well. 

I am set to graduate in May 2023, and I am wondering, what are something I can do to set myself up to become an analyst after graduation? I have been working on the Google Data Analytics certification and using DataCamp to learn R and SQL. Will setting up a portfolio with a few projects be enough to land a job? Or is there something else I should be focusing on?

Basically I am just wondering, what should I be doing in this next year to prepare myself to land an analytics job after graduation?",analytics,2022-06-12 11:21:56,7
"If you want to move in to people analytics ,more than completing any course I would suggest practice it!  definelty learn Excel advanced (index ,match, stretch learn VBA also you need this skill to automate some boring work) and one visualisation tool like power bi or tableau.(create some sample visualisations both in Excel and on the visualisation tool for you to showcase in your resume) Then update your resume highlight the responsibilities of your previous job where you've used data to make decisions or chnage your strategy and also mention the impact it created. get an understanding of some important metrics and what data you would use improve these metrics etc. You should be good to go.",3,vag9jw,So I've been in an HR role for a year now and want to get into the analytics side of things. I also want to move broad so I've been looking at Postgraduate Business Analytics degrees. I've also started with the Google Analytics Certificate from Coursera so far I'm really enjoying it. Any advice from fellow analysts here,analytics,2022-06-11 23:52:12,5
"No, you’d use a UTM tracking code on the back half of the URL and measure the amount of visits that URL generates",3,vakivm,"Can I put analytics on an ad on someone else’s website?
If I can, then how? And if not, is there any way to count the amount of clicks that my ad gets?",analytics,2022-06-12 05:03:05,1
"I used to hire data scientists, and one of the things I was looking for was do they really understand what they’re doing, or are they just using libraries to produce generic insights. 

I don’t want our insights to be based on what the libraries are able to do, but rather we decide what insights we want and then make the libraries (or custom code) produce those insights for us.

I think if you can explain that you don’t apply a library to a problem and work backwards, but rather start at the beginning (the problem), use your statistics knowledge to come up with the solution (the algorithm), and then either find a library which supports that algorithm or come up with a custom solution. And of course you’re able to do this through your years of using software to interpret and process data.",15,vaczdv,"""Can you explain how your statistics background can provide insights or better practices for dealing with our data?""

I majored in statistics but since I don't have experience, I sincerely don't know how being knowledgable about sample size and such will be useful as a data analyst. I know that the question was referring to basic statistics.",analytics,2022-06-11 20:07:27,3
The imposter syndrom comes and goes. IMO it never goes away completely. Just accept the fact that most of us are winging it,25,v9sxlf,"Working as a data analyst for 3 years now and just got an offer for the position of Sr. Business Analyst at a product based start up. Pays way above my expectations for this role (as much as an experienced Data Scientist) and the interviews and all went well too (5 rounds of technical SQL and Case studies with senior people and various managers from the company). But I feel like I need to learn a few more things to be job ready as I expect there will be some expectations given my position (Senior) and experience. My technical skills are no problem, I'm more concerned about the Business Analysis part of the job as I have never professionally done those. My previous jobs involved preparing lots of reports and dashboards and automating them using scripts. 

I asked my hiring manager the job description and this was roughly what he said -

1. 25% of the time we may have to prepare some reports and dashboards and automate them.  
2. 75% of the time we spend in optimising the business using data. This may be finding why a certain metric is behaving in a certain way and then provide actionable insights to optimize that metric. Also, testing new features using AB testing or any other statistical methods, preparing proof of concepts for those tests and presenting the findings to managers of several teams.

I just wanted to know how I could better myself in the second part of the job and if there are any good resources online that'd help me to be job ready and not feel out of place when I join it in a few weeks. I have picked up these two books and thinking of reading these but just wanted to hear from some more knowledgeable people here before delving deep. 

1. How to start a business analyst career by Laura Brandenburg (read this and liked parts describing the job but seemed a little outdated)

2. IIBA - A guide to business analysis body of knowledge - thinking of starting this next but would love to know if anything else would be a better idea

Thanks!",analytics,2022-06-11 01:11:47,19
"Your best bet is to land any corporate job in in field you want to become an analyst in, and then use that to integrate analytics projects into your job or volunteer for analytics projects to gain experience.",32,v9bnu2,"What is a position lower than Data analyst that will help me transition into the field to get some experience for a few years before advancing? I am coming from the EMS field, but have a Masters in Health Informatics.",analytics,2022-06-10 09:30:53,46
"Any analyst job will help land another analyst job. But someone with more relevant domain knowledge will be valued more. I have no confidence working as an analyst in the healthcare industry. I’m sure I would be fine, but I have 14 years of domain knowledge in the industry I currently work in, which makes doing the job a whole lot easier.",17,v9dp9m,"Hi r/analytics,

Just as the title states, is taking ANY analyst job for any industry at any company going to make getting other analyst jobs easier for the future even if your current analyst position does not necessarily transfer over to your new position?

Ex: Getting a job as a risk analyst in the IT industry and then later on applying to be a data analyst in the healthcare industry. Industry and job title/duties are very different but skills needed to analyze data transfer over.

In this case, would it be best to take the first analyst position I can get or just wait until I get an offer with the title/duty that I am actually looking for?

Just something I was wondering about, thanks for any help/advice I can get on this. Greatly appreciated in advance!!",analytics,2022-06-10 11:03:20,5
The big tech companies do interviews in the fall for internships and new grad hires the following summer.,1,v9i99w,"Hello, so I will be doing an MSBA in well known university at east cost, and we will have opportunity to do summer internships, currently I have no relevant experience in this industry. I do know basics of SQL, Python, tablue, and good excel and by summer I will be done with half of my masters too. So when should I start applying for summer and how much should I expect (i am asking this so that I can budget my living according for whole year). 

Also, is it possible to get an internship with this background or I might face hard time and should improve (ofcourse I will, but any particular skill that would be suggested?)

Thanks",analytics,2022-06-10 14:37:48,2
"Financial side everyone and their dog is using analytics, on health care not so much. There are lot of issues around data available, interoperability etc... basically it's industry of closed systems which doesn't really lend itsel to analytics. It's also an industry that keeps buying closed systems with 5-10 years support deals so I don't see anything changing any time soon.

There are areas like drug discovery where analytics is heavily used but healthcare as whole I'd expect to be massively behind digital product companies for example. Think of Uber, Netflix etc.",5,v9c214,"Healthcare and financial services are always touted as two industries ripe with opportunities for analytics. Data analysts/scientists working in these 2 industries, what has your experience been like? what are the pros and cons of working in them?",analytics,2022-06-10 09:49:13,3
"I was at a startup that used Mixpanel. I can't speak much for the other two, but I will give a warning about services like these.

We used mix panel to easily log and store what users were doing on our app. Unfortunately, we didn't put any effort into storing the user data separately.

We thought we could always just get the raw data from mixpanel at anytime because we were paying customers.

That, unfortunately, was wrong. They offer you access to a severely rate limited API to get your raw data. If, however you want a more reliable and scalable access to your own data, you have to pay even more.

It felt just wrong to have to pay to access your own data. So we worked hard to move off of mixpanel and store our own data. Now we have a BI team that can query data and create dashboards ourselves.",1,v9bpy3,"Considering Heap, Crazy Egg, and Mixpanel for WP analytics. Important to us are heatmaps, time on page, time watching a video, every single click/engagement and which registered user committed which actions.

Does anyone have any preferences or warnings amongst the three above? Are there ones I should be looking at that I'm not considering?",analytics,2022-06-10 09:33:25,4
"If you have the technicals, I would recommend just apply for data analyst jobs and see how things go. I transitioned from working as an accountant to a financial analyst, and financial analyst to data analyst by accident (10 years ago), so it's quite doable if you already have the technical requirements met.",7,v8wkm2,"Hi everyone, 

I currently have 2 years of experience as a financial analyst at a large fortune 100 tech company. I'm about 6 months away from graduating with my MS in Business Analytics. I have applied to numerous data analyst jobs and have been struggling to get traction. I consistently get recruiters calling about SR Financial analyst jobs, but that's not the path I want to take.

My position requires heavy excel modeling, SQL for data gathering, Data Viz for presenting, and just overall analysis. I'm not involved in the normal financial reporting, month-end type finance. 

My coursework has been heavily in SQL, Python, R, nodejs, and MongoDB. However, I'm not being recognized as a potential candidate. I can't find a network to help with a new position but I'm still hopeful. 

Any recommendations or feedback is welcomed! 

Thank you",analytics,2022-06-09 18:47:49,12
I would try to find a planning/reporting analyst role or something that doesn't support or work with a pure contact center. They typically only care about the 3-4 kpis and Excel and phone tool reporting is sufficient. Other departments will ask more complex questions or have more complex answers that will lend to better tools but may not require a ton of knowledge of them up front.,3,v8ysik,"As is much of the rest of the US, I am looking for a new position. 

Background: 10+ years in a government call center. 3 years as an analyst in the its PMO,  promoted to Supervisor of the PMO for 2 years. I left the agency after moving into a different role and was entirely unhappy. 

We primarily lived in Excel. We didn't use SQL, we didn't have that much data. I have completed online interactive SQL training. I wanted to bring us into dash boarding, but we didnt use Tableau or PowerBI. I tried to bring it but support was low and I didn't have the time to do it all myself.

Ask: Where do you suggest I go from here? Anybody with this type of background, what roles did you apply for? Any other suggestions? 

I am feeling a bit stuck, getting interviews for Real Time and Workforce Management analyst roles but nothing yet. 

Thank you!!",analytics,2022-06-09 20:50:53,7
"It’s going to vary by team/company and type of marketing they do. But my previous role was marketing analytics, so I would recommend, roughly in order if importance:

- Excel, especially functions (index, match, vlookup, if, sumif, etc), pivot tables, creating visuals
- how to put together a succinct PowerPoint presentation 
- a visualization tool like tableau or PowerBI 
- industry specific platforms, like Google Analytics or SalesForce. I’m sure there are others, but that’s what I’ve used. 
- the statistics behind a/b (hypothesis) testing 
- SQL
- Python or R will be helpful but not always required 

Also soft/transferable skills, like good communication, how to identify problems to solve and the right data source to use, project management, how to collaborate with others.",4,v982bw,"I had posted a few days back in this sub know what I should study to get into marketing analytics. But I find myself getting into the rabbit hole and end up getting overwhelmed.

Making a career transition is quite important for me from financial perspective so I would really appreciate if the kind folks of this sub can let me know what things I should study to become job ready for a marketing analyst role. 

I currently have a DataCamp's annual subscription. And I have applied for Google's Data Analyst certification scholarship and waiting for their reply.

Thank you!",analytics,2022-06-10 06:43:15,3
I work in healthcare research and pretty much everyone uses R or SAS. I am the only one who uses Python.,43,v8i7dr,"I've been working as a data analyst just over a year now and have only used SQL, Python and Tableau to conduct my analyses. Python has such a large library, it can already do statistical functions. I don't think our data science teams use R either. Am I missing something? Should I be learning R?",analytics,2022-06-09 07:35:11,43
"I think this depends on the database you are using, for MS SQL I think it asks you on import and for BigQuery from GCS it does use the first x records to infer the data types.

If your CSV has 200+ columns, a bigger question would be why is all this in a single table or are these columns even useful?

IMO, if this is for learning I would bring in a smaller data set or copy this CSV and bring in fewer columns",6,v8kx6l,"**Edit: a CSV I am importing as a flat file**

I have 247 attributes (columns) and 829 records (rows).

I did MAX(LEN(column)) on each column when I was running into an error about truncation. I found some strange things like:

1. Two columns where the max length was 57 for both, for one, SQL decided it nvarchar(100) was good (as it should) but for the other, it decided nvarchar(50) was enough.

I don’t mind fixing the problem manually but it can’t be a pain when you have 247 columns to deal with, does SQL only test a sample of rows or something for max length?",analytics,2022-06-09 09:39:09,20
"I have autism too.

It may depend on your educational background. My undergraduate degree was maths, and my maths textbook reading speed is 2-3 pages per hour, because the content is so dense. So I'm reasonably used to reading very dense stuff that's not rewarding to read.

With an analytics textbook, try a similar approach: read only the bits you strictly need to read (depending on if you're doing a qualification with set reading, say, or it's more a case of ""I need to learn about X""). Go paragraph-by-paragraph, or sentence-by-sentence if it's super dense. For an online textbook, you can highlight the section you're about to read and then de-highlight it as you read it, to keep track. I also make handwritten notes (this works better for me than typed notes) and try rephrasing the content I read as if explaining it to someone else. Make sure you do the exercises, too, because they'll break up the time.",8,v8ab60,"This is something that's affected me since before I was interested in analytics but since some of the best resources are textbooks it haunts me still. I'm doing a short course for analytics atm but just can't read the book. 

I'll stare at a page and read and eventually the thoughts inside my head will just drown out the words on the page and I'll realize that I'm just looking at words while being absorbed in my own thoughts. 

I don't think I have ADHD. I do have autism. 

What do you guys do to stay focused? I don't even know if I'm reading textbooks correctly I'm guessing it's not a sit-down and read cover to cover sort of thing but is more of a reference material. 

I want more in depth knowledge but can't get it :(. 
Thanks guys.",analytics,2022-06-08 23:18:49,10
"Sounds like to me that the manager is pretty good to go. I would spend as little time as possible on that assignment and look for areas of inefficiencies elsewhere that you can improve on. 

For your boss I would simply outline the situation that manager does x,y,z and that you’d be happy to help him with his process if so requested but other than that your talents are better used elsewhere.",2,v8jzxv,"  

**My Background:** I am a new hire a financial tech company. I started a week ago, working on the data team. I have experience with query writing and basic understanding of data management but am by no means an expert. 

**Context:** I've been tasked with helping our reporting/business intelligence manager. He is converting a KPI deck that was previously presented in power point into PowerBI. Some background on the data - there is a 'data lake', which as I understand, is where raw data is being housed from flat files, applications, etc. This data is in tables. There is also a data warehouse, which has tables and views. However, the warehouse is a new feature at this company.

I am tasked with trying to help find features/enhancement/opportunities to the data warehouse views to help support his conversion project.

**Issues:**

1. The warehouse isn't really being used by anyone besides this one particular BI manager. Other people are utilizing the data lake. 

2. The BI manager doesn't really have a vision beyond just copy/pasting these reports into PowerBI. I feel like in order to even start work on number 1, there needs to be a vision/business requirements on what should be built.

3. There is no VP right now of my department. I'm not a data architect, so I'm not sure what kinds of questions I should be asking, and who I should be collaborating with to figure out what needs to be built.

**My questions:** I've asking the BI manager how can the data team support you in this effort? What kind of opportunities or efficiencies can we work on to make the data more streamlined/accessible for you? I’ve asked him to think about how can the data team pre-process data upstream, add different data markers or flags, etc. His answer was asking me to optimize his query, which I think is not really taking good use of the opportunity the data team is trying to provide. 

He did mention that not all the data points are available in the warehouse, and he might need to look at the lake for some. So that is an actionable item for me. I think I am asking the right questions, but I can’t seem to pull any good answers/direction out of him besides this.

I guess I'm really looking for some perspective from this group to see what kind of support would you want from your data team when building out a reporting layer/data warehouse? What kinds of questions would you want them to ask you? What kind of Input/Opinions would you want to give to the data team?   Any tips or insight to put me on the right direction would be helpful.",analytics,2022-06-09 08:57:46,5
Was this not covered during your interviews?,1,v8osd1,"Hello r/analytics,

Does anyone mind telling me what a product strategist / strategy analyst does exactly? Like day-to-day?

I just want to prepare best for my new role coming up. Thank you in advanced!!!!",analytics,2022-06-09 12:30:48,2
"I spent my first year in finance on a team that reviewed individual credit reports by eye and manually audited our calculated attributes (the ones then passed into our models).

That experience with the granular details of the data that was then aggregated for use in driving the rest of our business served me incredibly well for the remainder of my time working in the credit/finance sector.

If you’re at all likely to stay in your field, then I would recommend taking advantage of this chance to acquire domain expertise that’s pretty difficult to come by otherwise.

To add on - I moved up from my ‘read credit reports’ job by demonstrating that I could answer questions for people in other departments, which was for I learned and practiced business analysis.  If you’re able to access collected data beyond individual claims, you are in an *ideal* position to learn the industry and start forming and testing business hypotheses.",3,v83hlk,"So this is pretty curated advise I’m looking for. I just started last week at a good company, with great benefits, better pay then I could get else where, etc. The issue is it’s doing insurance claims and can’t be consider for internal movement for 18 months. I just graduated with a degree in Business analytics and statistics issue is I’m not using any of it and wondering if I’m wasting time not pursuing getting experience else where. 

The company I’m at is a place I’d like to stay just getting to a different position. So I guess I’m wondering what can I do in the mean time to be able to get hire for a BA position or if I should just look for other jobs to get experience. I get this comes down to very personal decisions that others can’t answer but hoping for some input if anyones been in a similar situation? Apologize for this being a pretty broad post and probably redundant just at a bit of a loss.

Any advice is greatly appreciated.",analytics,2022-06-08 16:53:48,4
"select a, b, c from table where condition = 'X' and d is in (select d from table2)

that's about as complex as my day to day gets. maybe add a join.",64,v7v6aa,"I know it's a super goofy question but as an SQL newbie, I'm really curious.

When you're working on data every day, how complex or long does an SQL query get on average?",analytics,2022-06-08 10:34:44,25
"The field is broad in the sense that there are a lot of specializations, some of which require a lot of code, for example predictive analytics, while others require SQL and knowledge of visualization tools such as Tableau, for instance BI / descriptive analytics. 

I don’t know whether R is necessary if you know Python. I’d rather know Python applications to data well, and know how to use SQL beyond extracting data (data modeling, advanced functions).",28,v7qhjn,"Hi there, I'm an electronics major student, was self learning data analytics, those who are working in this field, I wanted to know how much programming do you actually use in data analysis in your day to day work? I know Python (and it's libraries Pandas, matplotlib), is it enough or should I need to learn R too?",analytics,2022-06-08 07:07:24,24
"My guess is , they're trying to move away from dashboarding and instead focus on providing data points for you to create your own dashboards in Google data studio.",5,v84saf,"It's a pretty big change, and Google is forcing us all to transition to GA4.

I'm not sure what motivated this decision

&#x200B;

I appreciate any input!!

EDIT: Can you please explain in layman terms, I'm not a very analytical person, please be kind :P",analytics,2022-06-08 18:00:51,10
Might get more answers in r/careeradvice or r/recruiting,4,v81262,"I have a lot of interest in certain jobs that I apply for. Especially those in healthcare. Some like asking for my desired salary. I have a certain number I would like to have but I would totally be willing to take less pay if the job made use of certain skills I want to develop.

Employers, Would you be skip out on candidates who do this?",analytics,2022-06-08 14:56:15,14
"Got both of my most recent roles, the necessary data was contained in a combination of SQL Server databases organized in Star schema, so a decent amount of the work was writing the queries to filter, join, and aggregate the desired data (using either SQL or R (Markdown)

Somewhat frequently I need to supplement the data with an additional csv file generated as a reference from a website - I’m working on how to pull that data via API so that it can be scripted to refresh whenever the process is run.

So far my experience has been that while it’s good to have a ‘template’ code so that analyses can be run for different clients without starting from scratch, there’s *always* some client specific nuance that requires modifying something in the code to run properly.  Client A might identify a certain attribute in one field, but Client B means you have to evaluate two separate columns, etc.  it helps to keep your code modular so that you can change how an attribute is calculated in one section and then have that change carry forward through the rest of the code automatically.

Like - in the first section of my code I set an attribute ‘denials’ - and that field is used in calculations for the rest of the code.  I can change how that value is set at the start of the process, but I don’t have to make any changes later in the code.",3,v84k21,"This is a really beginner-level question. Most of the time, we were provided a CSV file for academic projects. Then we imported the data to R studio/ Jupyter/ Power BI for analysis. One time, I wrote queries in BigQuery, opened the result in data studio, and analyzed the result there. So overall, the process was very static.

Now, I was wondering how do things work in real life. What tools do you use for connecting with the data sources? Is it automated? Or do you sort of ETL and analyze it whenever necessary? I hear a lot about stuff like Spark, and Databricks. How do these things work together? Sorry for the really basic questions.",analytics,2022-06-08 17:48:40,8
No,6,v850wp,I keep reading articles about this and I am unsure how true it is. Is it true or not?,analytics,2022-06-08 18:13:24,6
I have a feeling you should not need another masters degree to transition into business analytics. I think you might be able to parle your previous experience into an analytics role.,5,v7selx,"I have an master degree of accounting (not US degree) and CPA.  VERY interested in analysis and have done some when I was an internal auditor.  I am thinking of getting a master of business analytics.

I am making 85k and wish another master would offer more opportunities with better pay.  Is it a good idea?",analytics,2022-06-08 08:34:18,7
I started at 40k and now two years later just accepted a position at 120k. 70k sounds pretty great.,38,v70501,"I graduated this spring and was offered a position as an automation and reporting analyst. I live in the Midwest and I was wondering if $70k sounds about right for a starting salary? 

Thanks!",analytics,2022-06-07 09:16:10,29
"These are my two assumptions

1) Marketing

2) Powerbi and tableau are low code/no code, specialized tools for analytics reporting. Easier to learn if you're not a programmer.",12,v7bygt,"I recently joined a startup. As we ramp up sales and business activities, the founder has asked me to start building reports for tracking user signups, revenue, churns, and stuff. At first, it was mostly related to CRM and user analytics then it branched out to product KPIs, financial analytics, and even engineering productivity. It felt pretty exciting because I'm touching on a lot of aspects of our business.

I spent a lot of time building out these ""real-time interactive dashboards"" on PowerBI for the team. Each team member is pretty appreciative of it. Though quickly I've found myself managing 30 dashboards with hundreds of tables. I actually built a metrics directory for everyone so they can find the metrics they want to see but there're just too many of them so there're more and more overlaps between these dashboards.

This makes wonder why companies don't build a centralized website for these metrics so teams can just go to their respective section, but rather they have to build these separate BI dashboards that can quickly get lost and lose popularity?

I may be very biased since I've never worked at a company larger than 1000+ people. Is this because the technical limitation of the BI dashboard (PowerBI, Tableau, etc) or it's because of security, division of labor or some of the other things?",analytics,2022-06-07 17:14:00,12
"If you have enough relevant data, you can run a correlation analysis with multi linear regression to give you an estimate.",2,v7fp32,"Hi,

I’m trying to estimate the amount of traffic I would get if I were to translate my website in German. 
I have a rough idea of the big lines but I’m not sure how to proceed for the calculation.

So far I’m just using Google analytics to estimate the traffic by countries in Germany, Switzerland, Austria + rest of the world. I was thinking of combining those numbers with few competitors that already have their sites in German to get an estimation.

Any other idea on how to proceed is welcome 🙏🏻",analytics,2022-06-07 20:24:43,2
"If you're set on getting something to get into the higher roles, data entry would probably be the one I suggest. You seem to have the desired skills that I use as a data analyst daily as well.

My personal suggestion is to focus on SQL, tableau and excel as your main skills while applying to data entry AND data analyst jobs. 

If you get one in data entry, keep building up skills in the aforementioned list and use them at this data entry job to develop your skills. There is a lot of functionality in excel that data entry would be a perfect sandbox for.

I landed my first job with about 2 weeks of sql, my only tableau project was one I had spent a weekend building my first dashboard and writing a blog as far as what I found out, where you could utilize it and possibilities for future additions to make it even more useful.

Python is an added wishlist item, especially as a junior analyst/fresh. I would add it in later after you feel confident in your sql, tableau and excel skills and looking for the next thing to learn. You can dive into APIs, statistical analysis and lots of other stuff at that point with a solid foundation.",4,v702rr,"Hey guys, I'm currently working my way through some online courses to pick up skills like SQL, Tableau and Python, but I'm curious if there are any low level jobs out there (like data entry or similar) that might allow me atleast some daily active experience in some of these skills, even Excel if that's all.

I don't mind learning in my downtime but I know I would be alot more invested If I was sharpening all my skills throughout the days.

Also I have almost no *work* experience involving software and the like but I do possess a great deal of technical experience already so I don't believe I would have too difficult a time showcasing that value.",analytics,2022-06-07 09:14:03,4
"Following, since I don’t believe it’s common for R to be the choice for ETL.",1,v76md6,"Anyone have any suggestions for cloud ETL automation services where we could have R scripts hosted in the cloud to run ETLs on specific schedules? 

This would be at an enterprise level. Thanks.",analytics,2022-06-07 13:24:53,3
"I think if you have a solid understanding of everything you've written, you have more than a basic understanding of SQL.

You could add subqueries to your list too.",57,v68t1j,"I've been working on my SQL skills. I'm trying to get a sense of how much progress I've made, plus I want to know what I should be focusing on in order to level up in the job market. IMO basic SQL means:

* Run-of-the-mill SELECT FROM WHERE
* JOINS
* Aggregates
* Knowing when and how to use HAVING
* CASE/WHEN
* Sorting and grouping

I've also been working on WINDOW and date functions, which I sense are considered more on the intermediate side, but I could be wrong. 

Thoughts?",analytics,2022-06-06 10:28:45,30
"The marketing mix models that I'm more familiar with use some form of regression, but then it depends on the data that you have and how advanced you want your analysis to be. 

For example, if you have time-series data for many variables related to investments and returns, and maybe variables that describe what happens between the investments and returns, I certainly would think of running something like a VARX (vector autoregressive model with exogenous variables). But that is well beyond what many companies expect. 

I also have seen models that include Monte Carlo simulation, but I don't know much to tell about this approach.

There is also the possibility of running experiments, or something close to that, to better establish causality. But, again, one would need to know how far the company wants to go for something like this. Superior analyses can be much more complex, time-consuming, and expensive. Many companies are happy with simple approaches.",3,v6om15,"I’m working with a client who’s running an offline operation that’s primarily driven by online marketing (fb, google, twitter etc). They had asked me to analyze their marketing performance and recommend a strategy. So far, I have been able to do basic ROI calculation further broken down by geographic regions, time etc. 

I was wondering what else I can do? Are there any existing marketing spend optimization models or marketing mix models that can be used here? I have decent analysis skills but don’t know much about marketing analytics. Any insights would be greatly appreciated. Thanks!",analytics,2022-06-06 23:41:03,1
"Do both. Finish the Google course and do projects. At the same time, apply for jobs.",3,v6pitg," 

Hey guys! This is my first post please be nice haha.

I would like to ask help from the Data Analyst community for guidance on my next road map.

Bullet points for info about me:

* Currently about to graduate from my BS: Computer Engineering course
* Taking up Google Data Analytics Professional Certificate (3/8 finished)
* Mistook Web Development as Internship
* From the Philippines

Lately I realized that I find this profession amusing & fun in terms of helping a business with data-driven decisions in fueling for their success. I look forward to become as skillful as I can and someday maybe transition to Data Science/Business Analytics.

So should I continue to finish the Google course then conduct data analysis projects (for experience) or should I just sharpen on the go with a job (which I will still apply for). If you have any advice I would really appreciate them.",analytics,2022-06-07 00:40:06,2
What makes you think your models aren't correct/helpful?,1,v6h28b,"
I have been staring this problem down for days....weeks. 

You are trying to predict when a vehicle will hit x miles (200,000 in my case). You have data on the truck's mileage on a weekly basis for 28 weeks. 

What type of model would you build?

I've done multiple time series. Which was super difficult for my new doing this kinda thing. I've also tinkered with ARIMA. 

I kinda think I'm way overthinking this...


## I ended up using Prophet and it ended up making this stupidly simple and I am only mildly annoyed. :)",analytics,2022-06-06 16:44:06,4
"You wrote a lot about the ""data analysis"" part of it, but not about the ""marketing department"" part of it. 

Usually, people organize by marketing topics. For example, marketing people need to understand segmentation, and then they can use something like clustering for that. 

I'm still developing my courses for that, but some books that I'm checking are:

\- Marketing Analytics - Data-Driven Techniques with Microsoft Excel, Wayne L. Winston

\- Hands-On Data Science for Marketing - Yoon Hyup Hwang

\- Marketing Analytics Based on First Principles - Robert W. Palmatier, J. Andrew Petersen, Frank Germann",8,v635tx,"I am planning to switch to the role of a data analyst in the marketing department. I am a freelancer so I'm being self-taught at the moment. I have decided to learn Python, SQL, MS Excel, and Tableau. I have studied SQL for a bit but need to brush up my skills.

The problem is since I have a full-time job, I can allocate 2 hours/day and a bit more of off days and weekend. But the field seems so vast, I would really appreciate if anyone can guide me on how I can go about planning my studies. I have a DataCamp subscription and a few Udemy courses for Python and SQL. Thank you!",analytics,2022-06-06 06:08:47,20
"I wouldn’t put anything on your resume or LinkedIn that you aren’t comfortable talking about in an interview.

If you want to talk about it, try redoing the project even if you have to brush up on some past learnings.",3,v6dw4r,"I have a group project in university before graduating where my role was to make use of Random Forest in R.

I honestly don’t know how to interpret my steps because I began setting up the model (it’s one of the simple random forests) when I didn’t know anything about it. None of us did. 

Im not completely sure if it’s setup correctly, even, but I do know that it gave me an expected number for RMSE when comparing to my teammates. 

Should I include this on my resume? On my LinkedIn, at most?",analytics,2022-06-06 14:15:57,2
For someone with limited experience? Absolutely,1,v6d6uh,"I recently graduated university. Know a lot of R, learning Python right now, wrapped up a good knowledge of SQL, have a portfolio up, did analysis consulting for a political campaign, and I have the AWS Cloud Practitioner. 

Would y’all like to see a the Tableau Desktop Specialist? It’s only multiple choice and not hands on so I’m debating on whether it’s worth it.",analytics,2022-06-06 13:45:12,2
"I did UX/UI through CF, and if you are considering them for the structure, I'm afraid to tell you that you won't find much of it there. 

Tutor: Can't talk with, but they critique your work. But they aren't required to give long, thoughtful feedback. And ask them one question and they say, ""just submit it and i'll let you know"". But then the critique comes and you are lucky if they addressed the questions you had before even submitting. 

You do get a mentor, who you can talk with for an hour a week. But, at least in my specific program, the mentor is not required to have first-hand knowledge of the course materials. They know and work in the industry; but they don't know anything about the actual documents or lessons their student is reading through CF. And your mentor and tutor don't even know each other. So you can't use your time with your mentor to better understand your tutor's feedback. 

My mentor calls were helpful, but useless in actually processing the material/lessons. 

If you are new to the discipline you are taking on, I'd recommend finding a local person you can meet with and take a smaller intro course online to get a feel for the material. Once you know you like it and can do the work required, then a costly bootcamp may be necessary/feasible to take you to job ready. 

&#x200B;

But I would be highly skeptical of CareerFoundry supplying that for you. They are basically a ""teach yourself"" bootcamp.",2,v6aqls,"I just recently felt the need for a career switch and I was thinking of analytics. For background, I have no related experience or a degree. I stumbled across a few options and was wondering if anyone had any input on CareerFoundry's data analytics program. Also, I know I can learn everything on my own and there are online resources, but I also need the structure. Any input or experiences would be appreciated.",analytics,2022-06-06 11:55:30,2
I don't know about this situation but if you define your career based on the pay of others and their titles you will never succeed.,2,v6dnkp,"I have a potential job that will use Excel & Tableau.

Not too sound arrogant but the school I went to is really well known. 

Every close friend of mine had secured a job paying $60k-$90k after graduation. 

Truth be told, I don’t care about money like that. I spent a LOT of time figuring out what makes me happy in college and I have clear direction moving forward despite the “set back”. 

Thing is, I want it to be a glide & not a grind as a much as possible. 

Idk if I should take the job due to no SQL. Friends push for more but idk that they understand my position. As a first gen, I understand it even less.

It’s in healthcare though and I love that.",analytics,2022-06-06 14:05:18,9
How do I come up with tasks that I need to do? My manager gives me them.,10,v5qawa,I've been in business analytics for about 2 years now but still can't exactly grasp how do you come up with projects to do? What exactly prompts you to look into one or another thing? What is it like to decide to analyze something and then present to the management with a solution.,analytics,2022-06-05 17:16:38,6
"Show that you have learned solo since getting out of college. Use your resume. Use your portfolio. Get a job, any job, to try to get a foot in the door somewhere. After you have a job, searching for your next one is much easier.",22,v57aqi,"Low key messed up by not getting an internship in college. I was going through a major mental rough patch in life and impostor syndrome, looking back, was what got a hold of me. 

I did participate in analytics tournaments though and have been actively adding to my portfolio and doing leetcode on SQL & Python. I also volunteered for a political campaign for analysis consulting.

I'm definitely eager and I realized that I’m not a dumb nut. I just felt like I was bc of all the high achievers at my university. 

How can I show on a resume or cover letter that I'm very willing to learn?

Edit: I majored in statistics and had leadership spots in orgs at the community college before transferring. I’m also studying Tableau just a bit.",analytics,2022-06-04 23:12:25,28
"Data analyst or any analyst roles, data scientist, Research roles like research assistance/associate, statician, biostatician. Analyst roles are easier to crack other roles require strong education and experience.",10,v59lu5,Above+ if there is some extra req. for that role pls mention it too.,analytics,2022-06-05 02:12:59,2
"Not at all required. I only have a BS, and it's not even in a tech/computer field. Sure there are going to be employers who want to see specific degrees, but at least with mine they cared more about work experience and work ethic.",31,v4mpki,"I read people's profiles on LinkedIn and noticed a pattern that most business/data analysts have master's degree. Although there is a mixture of analysts with a BS and MS in BA/DA, it appears to me that most analysts (50 to 60% of them) hold a Master's degree so I'm worried whether I will be able to get an entry-level role in this field? I have always wanted to become a data analyst but the thought of going to graduate school terrifies me because I don't have the luxury to afford a master's degree, especially where I live, living expenses are so over the top.",analytics,2022-06-04 04:11:46,47
"Progress is not really a SQL database in that it uses nothing like ANSI standard SQL.  MS SQL Server, Oracle, IBM DB2, MySQL, PostgreSQL all use something very close to ANSI SQL.  Learning *one* of those is essentially learning them all.  I strongly encourage you to learn one of them if you're seeking a data job.  99% of businesses use one of these for their database needs, not Progress.  Good luck.

Edit - sniped by OP changing his description.  Damn your eyes OP, you made my list!  /s",28,v4istt,"I’m learning SQL at the moment before applying for jobs in BA/DA field. I was wondering if I should learn how to use multiple SQL database systems or only focus on one database first? Because for me, I find it easier to work on PostgreSQL I feel kinda terrified of using multiple database systems which will make me feel very confused and stressed out.",analytics,2022-06-03 23:20:22,22
"That's the intended behaviour of the default channel report, it groups different sources of traffic into these predefined groups.

There are automatic groupings for organic search engines as your screenshot shows. 

The Referral grouping will likely contain a list of the websites that have sent traffic to your website. 

You can look at the Page referrer by building an Explore Report and adding Page referrer as a dimension to the Row and Total Users profile Sessions as the Value.",3,v534ya,"Hello Everyone,

I hope your day is going well. Currently, my Google Analytics dashboard is not displaying the exact websites my users have come from. For example, I can see X users came from social media but not the specific website. How do I fix this?

&#x200B;

Image: [https://drive.google.com/file/d/15wzs\_Dr4QmI4Mn2gn-acQSqgDtpQ3M2B/view?usp=sharing](https://drive.google.com/file/d/15wzs_Dr4QmI4Mn2gn-acQSqgDtpQ3M2B/view?usp=sharing)",analytics,2022-06-04 18:42:08,1
"GitHub repository readme files are a good place to show your work. You can attach screenshots and write up what you did. Im trying to figure out good projects as well. I did some that showed off intermediate Sql knowledge, showed ability to create a db using AWS, create tables and import data, then connect to the DB and query it and import that to tableau and publish visualizations.",6,v4w3c0,I need to build a portfolio to attach to my applications to show my work. I was wondering what are some projects that would be appealing to hiring managers? And what site should I use to showcase these projects? I've never really done anything like this before.,analytics,2022-06-04 12:24:38,5
Excel,7,v4ml0g,"I will graduate from college with a bachelor's degree in BA this year and was wondering what are some of the skills I should possess before applying for entry-level jobs in the analytics field? So far my skills are Python (a little bit, I knew how to do loop statements, functions, print receipts, extract data on Panda modules and AppJar, and that's about it), R (mostly do math and statistics, my most used library is tidyverse), SQL (populate tables, manipulate data), Tableau (data visualization), Microsoft Office, Google Suite, statistical analysis, descriptive/predictive/prescriptive analytics, and data mining. Is there anything else that I should be aware of?",analytics,2022-06-04 04:02:31,8
"You can realistically expect $50-65K based on the info provided. Keep in mind that even if they bring you on as a salaried employee at an equivalent yearly basis to what you are being paid as an intern, you'd still be in a better position since you would get benefits. 

And while it may be difficult to fully appreciate now, there is significant value in getting your foot through the door to start building professional experience. That should be the main consideration when it comes to a first real job.",3,v4tv9e,"I'm currently a couple weeks into an internship and I'm really loving it. The work is challenging and fun and I'm getting to learn new skills and own a project. I've had several discussions with my boss where it's obvious we both have a mutual interest in transitioning me to full time employment at the end. 

My biggest question right now is: what should I be expecting in terms of salary? As an intern I'm making $25 an hour which is $52k annually. I live in a medium cost of living area.

To give a bit of context about the work I'm currently doing, it involves some advanced analytics/unsupervised learning in python and I'm the only one on the team (aside from my boss who doesn't have time) that can do this work. My boss has implied they want me to continue doing this kind of work. The other's expertise are SQL and some ETL and BI tools. So I do bring something different to the team.  

They are planning on hiring a statistical analyst on the team and the salary range is available on glassdoor. It's $90k - $120k. Now, I recognize that the person they're looking to hire for that position is going to be expected to have more experience, more statistical training, and preferably a masters, etc so I'm not expecting to make that much. 

With all that said what is a reasonable amount to be targeting?",analytics,2022-06-04 10:40:37,6
"From the tests I’ve done, you need to know 

- order of operations (select, from, where, etc)
- how to aggregate (sum, average, count, etc, and how to use group by accordingly)
- all the different joins and when and how to use them 
- using CTE to create a table and then query it (instead of nested queries) 
- when and how to use WHERE vs HAVING 
- what is CASE WHEN and when and how to use it 
- CAST AS - what it is, how to use it 

I think that’s the majority of what I’ve been asked to do. I’d also know how to use lag/partition by since that’s very handy. I don’t think I’ve had that come up in an interview test for me personally.",3,v4q75s,"Basically I have a ton of experience with digital ad products already, I'm very proficient in Excel because realistically, most processes in advertising are just done in excel because most people don't have to know how to code to do most tasks. 

Our analytics team is pretty weird in the sense that nothing we do uses SQL directly(nor Power BI nor Tableau). We use a proprietary ad platform that ingest api data from a bunch of different ad platforms and combines them. The extent of SQL usage we get is by creating calculated fields in the proprietary platform that basically uses a dumbed down editor for sql that has a limited amount of funtions and commands. Most of the job is product knowledge in order to clean up data in platforms to get it to work in the proprietary software.

&#x200B;

This I feel puts me in a weird place skillwise as an analyst because I have no way to practice excel IN work even though I'm an analyst. Right now all I've been doing is using Pandas in place of tasks where I'd normally use excel power query to practice that.

&#x200B;

Is there a site that could give me an example of things I'd be quizzed on regarding SQL skills and knowledge in an interview?",analytics,2022-06-04 07:38:08,6
masters in Data Science (probably),1,v4wwco,"Hello everybody, I think I didn't express my doubt correctly before. I am a bachelor's student in marketing analytics finishing my degree. I am figuring out what's next for me now. I am learning business intelligence and tools such as Power BII, Tableau and in-depth knowledge of SQL. I have really strong statistic (all the process of hypothesis testing, Pearson, universe composition, etc) knowledge and skills, financial maths and econometrics/ sales analysis based on KPIs and basics of data modeling/ mining.

My background is mostly focused on statistics, sales analysis, and business and database modeling, but my programming language knowledge except for SQL is low, I have no idea about Python, Machine Learning, Cybersecurity, Complex algorithms and such.

According to your experience which would be my next step? I prefer working in the IT area, technology, innovation and predicting modeling data and visuals in general. Also what kind of companies could try to get into them and the average salary for a Junior (expecting to get six figures after some years of experience). Any advice is very welcome and appreciated.",analytics,2022-06-04 13:04:30,3
GDP is a seasonally adjusted annual growth rate you cannot sum them up.,1,v4sukv,"Has anyone worked with Macroeconomic data processing in Excel?

I have a question, I would like to calculate the accumulated GDP in a period with the quarterly or monthly data that I have; however, when I use the accumulated rates formula in Excel that I usually use for interest, the result I have for the three months is not identical to the quarterly result released.

I would like to know if I am making a formula error in Excel and if there is any other way to calculate. Also, can I use quarterly data for this type of work (they are more detailed and complex), or do I necessarily need to use monthly data to calculate cumulative variance?

Thanks.",analytics,2022-06-04 09:49:50,1
"Data science, business analytics, etc. I would say none are required and you’ll probably get bored of  sales analytics pretty quickly if you want to do technical work though. There are only so many ways to look at a slow moving b2b pipeline.",1,v4mofe,"I just landed a role 4 months ago as a Sales and Operations Analyst with a Supply chain technology company that provides EDI( electronic data interchange) for businesses.

I have worked in sales as an account manager for 3 years before that and decided selling wasnt for me. I applied for the sales analyst role as I thought suited my temperament more.

My current day to day role involves data analysis of sales data, forecasting, territory planning, determining close rates, win/loss rates, ad hoc reports, compensation analysis, salesforce administration and optimization and contract administration/deal desk management. Im also documenting training requirements and processes. Im also the first analyst in the region so will be acting as sales analyst lead and help with training and onboarding new analysts thats come on board.

So my role is a mix of commercial/tech/data/L&D.

I use Ms excel, salesforce , Jira, confluence Ms power BI heavily. My company has been training me up and so far are happy with my work and determination. I feel this role will open a significant amount of doors for me.

My undergrad and graduate certificate is in an unrelated field but was science based so there was a significant data, reporting, research component etc.

Is it worth going for a masters degree to help with prospects down the line even though I have now landed the role? I feel I got the role by dumb luck. What would be the most suitable masters degree for a sales analyst? A tech, data or commercial based degree?",analytics,2022-06-04 04:09:30,2
We have used loader.io in the past to blast servers with traffic to see how they respond.,2,v4dv1d," friends, I have my web wordpress pages on my own server and soon I will launch a campaign that I hope will get a lot of traffic to one of them,

my question is if my server and connection will be prepared to receive high traffic?... do you know any way to test it to see if it has a problem with high traffic and thus fix it before it is suffering because it fell or something?",analytics,2022-06-03 18:15:58,2
My past encounters with people who attended St. Joseph’s (if it’s the one in Philly) is lol. Don’t waste you’re time. That school only holds weight within the Philadelphia job market and is stupid expensive.,3,v41pmw,"Want to connect with people pursing/pursued masters in business analytics at:


U Massachusetts Amherst,
Ut dallas,
U of Illinois Chicago,
U North texas,
St Joseph's University 

Thanks!",analytics,2022-06-03 08:20:13,4
"Given that you’ve already done some analysis and visualization, you’ve already done some analytics work. So once you pick up SQL, you could keep doing that. At the same time, you can look into data science/machine learning/predictive modeling and see if that interests you as well. 

Job titles are so subjective. What one company calls data analyst, another calls data scientist. Do you want to keep analyzing and visualizing data, or do you want to build machine learning models and basically be an extension of a software engineering team?",8,v3uz7t,"Hey!

I’m a UK newly graduated doctor with an MBBS and management BSc. I’m looking to move into a data career. 

I love Python - most recently, used it to analyse + visualise Electronic Health Record data at a very large hospital, preparing a report for discussion with a senior doc. 

What is the most realistic for me to purse between data science via analytics? Given:

- I’m not sure whether I want to spend another 2 years in education doing a masters (already spent 6 years!) - heard this is required for data science roles 
- With analytics, I understand you also need SQL (which I have a little experience with, easy to pick up after Python) and Tableau + Excel. These latter 2, particularly Excel, seem so boring, esp when it seems like I can already do it using Python (Plotly and Pandas etc). 

I’m ready put in some work, but wanted thoughts on which direction to do that in!

Thanks for taking the time to read this.",analytics,2022-06-03 01:43:39,9
Have you tried using the debug function in GA 4?,1,v3y49y,"Hi all, when testing eCommerce tracking through the real-time report in GA4, is there any way to view whether the value actually pulled through? 

I can see my purchase event has been triggered but I can't see whether it pulls through the value. Any tips?",analytics,2022-06-03 05:17:48,3
If the man wants pie give him pie (or woman),78,v3hc2z,No matter how much I explain how horrible they are and better ways to viz always lands back on pie…I give up,analytics,2022-06-02 12:53:58,51
"Do you advertise online? 

Do you have a leads form or something like that on your website?",1,v3x8wd,"Did anyone else get a surge of traffic May 18 at 8pm with no referral data and the only thing looked at was the homepage?  
I got 656 visits from all over the world from like 8-10pm and then the next day, back to normal with like 2-5 visits per day.

Analytics broke for a moment?

Line",analytics,2022-06-03 04:25:26,10
"I have a degree in finance, worked in the field for 3 years. My advice is to be careful and do research before taking a job. There’s a billion different “analyst” positions out there, but only a handful that actually do real analytics. Avoid “operations analyst”, “performance analyst”, that sort of thing. Those are mostly excel monkeys. Aim for Data Analyst title or if you like dashboarding and report automation, business intelligence analyst. Quantitative Analyst is also a good job usually, but can be hard to become a quant if you don’t have a masters or pure math. 

Ultimately I left working for a big bank and now work for a startup that’s fintech, lending, so adjacent but not directly in the banking/Wall Street area.",8,v38kxd,"Hello all,

I am about to graduate with a degree in business analytics and was wondering about the possibilities in the financial industry. I have always been fascinated with the stock market and think it would be a great place for me to take my career. Any thoughts, experience or tips are appreciated!",analytics,2022-06-02 06:12:36,11
"Google has a large list of Partners for putting together your [stack here](https://cloud.google.com/find-a-partner/) although I suppose they are all probably going to recommend Google products lol. 

If you are bringing in external consultants to set it up and you guys aren't really sure of the pros and cons then I would probably assume its best to hire consultants that can give you guys an end to end solution and not make decisions before hand if possible. 

I'm not a data engineer either and I am also in a small company and they took a chance on me as a data analyst and their ""stack"" was an absolute disaster. 

I've been slowly trying to streamline it and figure out our ETL process and found some great resources on [mode analytics here](https://mode.com/resources/), check out their white papers in particular [this one](https://mode.com/maturity-model-assessment/)

They have articles about particular ETL solutions and what not too, they don't provide data integrators and are in the business of analytics tools so they are in the space and but aren't suggesting their own products in these whitepapers in terms of data integrators. 

Good luck and feel free to let me know if you have more out too, im trying to learn as much as possible.",1,v33l71,"I joined a smallish company last year as an analyst. The company has no data storage system, most of my work so far has consisted of importing various Excel spreadsheets into Power BI and doing transformations and reporting there. 
Now I've got to the stage where I want to propose a data storage solution to the directors but I'm not sure what would be a good solution. 
Intuitively felt Snowflake would be a good solution, but not having much exposure to the data engineering side of things I'm not sure if maybe a plain SQL database on Azure would not be better/more cost efficient.
Whichever system we decide on, we'll be bringing in external consultants to help set it up. 
Any advice on how to decide on the appropriate systems?",analytics,2022-06-02 00:49:30,2
Probably excel if you don’t already know it well,12,v36in2,"I will graduate from college this year with a degree in BA and I am actively looking for jobs in BA/DA field. I am thinking of learning a new software over the summer, so far I have learned SQL and Tableau. Is there any other software that I should learn? After I learn the basics of SQL and Tableau in two weeks I will start sending in my resume.",analytics,2022-06-02 04:17:03,13
"Since your duties as an analytics engineer will be creating logic for data transformations, writing lots of queries, and building data models, being an expert in SQL is a must. The other skills for this role may include Python or R, working knowledge of dbt, data engineering and BI tools knowledge, knowledge of standard software engineering practices like version control, auto deployment processes, etc.  
I recommend practicing on stratascratch platform for your interview. This platform provides thousands of SQL, Python, and R interview questions from real companies to practice.",5,v2whqg,"Hi y’all,

I’ve been a Data Engineer for around 1 year and 4 months and my background is more technical than analytical/mathematical.

But, before it, I’ve worked as a Revenue Operations Analyst and handled work with dashboards, some basic metrics and data quality.

What do you recommend as most important skills to this role? I’ve been trying to understand it.",analytics,2022-06-01 17:49:20,5
What system are you using?,2,v2xn2d,"My manager wants a unique key column with leading text and zeros, that autoincrement with each new entry. I don't know how to handle this with the leading zeros.

Example:

CS-0098

CS-0099

CS-0100

CS-0101

I'd normally just concatenate it on but I have no idea how to make it ""roll"" into existing zeros when it goes from 2 to 3 digits",analytics,2022-06-01 18:47:00,3
"Well, I can tell you that the way that I learned was through other people I worked with. I started in Visual Basic in Excel. I created macros that turned into tools that my department used to do their jobs. 

Basically I made their lives easier by reducing the work necessary for production agents and improving quality scores through automation. 

Next I learned how to write SQL queries. SQL is a Data Analyst's bread and butter. You can skip straight to this step in almost every scenario you would be starting from. 

Make sure you have a firm working knowledge of SQL. It is how you will pull almost all the data you need. (Of course, this depends on your company). 

Next I learned Python. Python is way more versatile than SQL, but also much harder to use. SQL will typically be used in conjunction with a database (or data frames generated by Python). Python is a language you can use to create and clean the data you will be working with. You can also use it to manipulate the data. You can pretty much do everything in Python that you can do in SQL, but it is much harder. 

I use the two in conjunction with each other on a regular basis. 

Python is deep and you can do a lot of things with it. Even create visuals and charts from your data. But it takes a deep knowledge of how it works and I would strongly recommend finding someone in your company that you can go to when you run into issues. 

Hands on experience is the key to getting good at these skills. Finding ways to apply SQL and Python to make your coworkers lives easier will nearly always guarantee you more projects to on which to hone your skills. 

Last tip. Something else that doesn't get talked about a lot are the soft skills. You need to be a good communicator. Don't just take the specs from your customer and build them exactly what they are asking for.

Use probing questions to find out why they need this and what it will be used for. Often you will find that the solution they thought they wanted was not the best solution for their problem.",9,v2e2es,"Hey guys, I am pursuing marketing management as my post-graduate course. And as of now I am an intern learning the ways of social media marketing. I was asked to do a report on the website of the organization that I am in, and I used a few free analytic websites to generate a report.   
This whole thing has piqued my interest and I want to explore more and learn analytics.  
Please guide me, much thanks!",analytics,2022-06-01 03:45:54,8
"Offer 1, the exposure to AWS and Azure is worth it IMHO.",17,v1y6sp,"I understand that this can be a little subjective and ultimately this will depend on my preference but I'm not looking for someone to make my decision for me. I'm looking for perspective and insight.

I have two offers, both similar roles, basically ad hoc analytics, some data science, some dashboard building. Benefits are basically the same in terms of 401k and insurance so I'm leaving that out

Offer 1)
110k 15 pto this would be at a data center where I could get AWS and azure exposure and they'd help get a solution architect or dev ops cert for either if I wanted that but not really needed. Seems like it would be doing internal work until I get comfortable, then dealing with external clients. The work seems interesting and this dept is in it's early stage.

Offer 2) 115k 19 pto, would be for insurance company dealing with external clients. Some travel. Not a pro or con in my book. It seems to me, they want my help automating their excel and power pivot processes into something more streamline. In my experience, this is usually not the most fun work. It's a pain to understand someone's work. It's not really analysis. And no azure or AWS exposure. There would be SAS exposure. 

Is having data center experience valuable? They mentioned if I wanted to work extra or do some backup functions, I could learn other data center/IT type work, working with severs and using the terminal. I'm not really looking to get into IT, but the cloud exposure seems useful. Plus, I feel I could develop more skills versus the insurance company, I feel I'm being hired for my expertise, so the learning would be limited. It makes me uneasy to be in a spot where I'm the ""expert"" vs the data center, they have engineers and developers and data scientist that could probably do circles around me. But pay and pto bump is nice for insurance. I already tried to negotiate the data center job but not budging. 

My question, which area do you believe has opportunities down the road? Those in data science/analytics for insurance, do you like it? Do you feel like your learning technically? Do you feel like you could go to a different industry easily? 

I have a few friends in data science in insurance and they seem to do well. But, I feel cloud exposure is useful for any company and will be down the road. I'm in my mid 20s so learning is important to me. The friends in insurance seem to have pigeon holed themselves into the insurance data science field, which isn't necessarily bad, I just think I'd get bored and also think if I'm unsure about my future path, then the data center work is good. 

Thanks in advance!

Also ps to anyone not looking for work, I'd encourage to apply because DS and analytics seems insanely hot.",analytics,2022-05-31 12:39:26,8
"I wouldnt call GA4 a storage, for storage you can set up a dwh and make custom dashboards.",6,v1wmi9,"Calling all marketing/analytics experts:

What's your tool stack for marketing, user journey, analytics, & customer support / CRM etc?

We're a small online subscription biz based on WordPress; looking to upgrade our stack to a more robust / connected system.

Main factors: Connectedness(integrations), Simplicity, and Cost.

So, this is the stack I'm considering:

- Klaviyo (Email, SMS, CRM)
- Google Tag Manager (Tracking)
- Google Analytics 4 (Storage)
- Google Data Studio (Reports)
- GetEmails.com (Email & Attribution)
- DeadlineFunnel (Evergreen Funnels)
- Hotjar (Heatmap / CRO)
- Zapier (bc idk why yet)
- HelpScout (Cust. Supt)
- ManyChat (FB msgr)
- Agorapulse (Social Media management)


What about y'all? What have you found to work well and also play nicely together?

Klaviyo caught my eye bc of their emphasis on their integration ecosystem with our other tools: Stripe, ViralSweep Giveaways, ManyChat, HelpScout, WordPress, Google Drive, AWS.

Your expertise, experience, and current tools would be mighty helpful y'all :)


P.S. - still wishing for a dashboard that connects email/sms marketing, customer support, & social media. (i.e. Mailchimp, Intercom, and Hootsuite) or (i.e. Klaviyo, HelpScout, and AgoraPulse) etc.",analytics,2022-05-31 11:20:49,4
"Somewhere around 50-70k. 

The issue won’t be the salary; you’ll make that much *if* you find the job. 

Finding your first job is infinitely harder than your next one. So you’re facing a pretty big filter.",30,v1ffcw," I am graduating next year so I will start looking for full-time positions.

I have two internships, 1 at an F500 company and the other at a small startup.

My degree is in Data management and Analytics and I live right next to Seattle.

What type of salary should I be expecting once I graduate? My resume is linked in my comment below. Also would be amazing if I can get any tips on how to improve my resume.

Thanks yall!",analytics,2022-05-30 18:56:03,29
Note taking. Read up on Cornell note taking methods. I suggest this for all of my interns and new hires.,23,v1342h,"Hey, I have practiced R,SQL, some python(but mainly R) ,Data studio ,Power BI
What are the simple or intermediate type of tasks like building xyz dashboard etc which I should know as an intern.",analytics,2022-05-30 08:48:21,10
Yes Google Analytics 4 is that bad.,3,v12ug4,"



##Google:

* Google May 2022 Broad Core Update Is Live. 
* Google Ads Smart Display Campaign and Optimised Targeting Changes. 
* 7 Updates coming to Google Performance Max Campaigns. 
* A Summary of Updates from Google’s Marketing Live 2022 Event. 
* Google Analytics 4 is Receiving a lot of backlash from Marketers and Industry individuals. 
* Google Unveils Ads Manager Tool for Video Ads. 
* Google Ads now have a new carousel layout for Product Ads and also testing many new layouts for E-commerce listings. 

##Tiktok:

* Tiktok partnered with Social Media Management Tools like Later, Sprout social to help their users auto-publish Content. 
* Tiktok launched First Stage of Live Creator subscriptions this week. 
* Tiktok added new editing features to help you create better Videos. 
* Trend: Music Artists all over the platform are calling out labels because labels are stopping music releases due to marketing. 
* Tiktok Adds New Lead Generation Feature to it’s Business App. 

##Instagram:

* The platform is working on Reactions For Videos and a drop-down menu for Homepage. 
* Instagram launched their new Font called “Instagram Sans” for Stories and Reels. 
* Instagram was down this week for some users. 
* Another Instagram Link Bug was occurring this week which stopped users from visiting other websites from the app. 

##Twitter:

* The platform agrees to Pay $150 Million in Privacy Settlements.
* Twitter’s latest update will make third-party apps better. 
* Twitter rebranded it’s media website to help creators with more resources. 

##Meta:

* Meta updates it’s privacy policy with few changes and Facebook’s data policy is now Meta’s privacy policy. 
* WhatsApp now allows Indian Users to Access Governmental Documents in App. 
* Meta introduced Facebook Graph And Marketing API v14.0. 
* Meta Introduces Recurring Notifications for Businesses on Messengers. 

##YouTube:

* The platform will start rolling out YouTube Shorts Ads from this week! 
* A Possible update is coming to YouTube Comments UI/UX. 
* YouTube Analytics Now separates Data by Video Type. 

##LinkedIn & Snapchat:

* LinkedIn shares new insights into remote job opportunities in selected industries . 
* LinkedIn shares insights on new feed algorithm. 
* Snapchat rolls out ‘shared stories’ feature to make it easier to share memories. 

##Marketing:

* Wordpress 6.0 is Here with New Improvements And great features. 
* Bing Tests Branded Sidebar Navigation Tool. 
* WooCommerce and Tiktok Integration is here! 
* Meta and Spotify is bringing back Political Ads silently for mid-term Elections. 
* DuckDuckGo’s Search Deal Prevents Browser From Blocking Microsoft Trackers. 

##Reddit:

* The platform launched a new “Reddit Talk Host” program. 
* Reddit shares insights on creating better Ads. 

##Others:

* SEC questions Elon Musk over late Twitter disclosure.
* Twitch pledges to make Ban Notifications more transparent. 

##Updates Worth Discussing 

1. What do you think about the new Wordpress Update? 

2. Is New Google Analytics Update is really bad? 

3. Instagram’s working a lot on brand transitions. So, Is this right time to start on platform when all these transitions are happening? 

4. YouTube Short Ads will make impact like Tiktok Ads or not? 

—

Hey, I have been sharing these updates from last month. If you want to receive them every week you can subscribe through link in my bio or drop 👋 in comments. More than 200 Redditors joined last month, Only 10 subscribers away from hitting 2,000 subscribers. 

**Can we hit that today?**

Join r/Marketingcurated for more!",analytics,2022-05-30 08:35:19,1
"You should be good. I have a 2021 m1 Mac book pro for work and it does everything I need it to. I mostly use excel, Jupyter notebooks, and tableau. Only thing I would suggest is learning how to use virtual environments to manage your python packages.",4,v1bxlt,"Im trying to invest in a laptop, and considering to use SQL/python/powerbi  etc. leaning towards macbook pro M1 chip.

Is there sth i should be aware of? Would the software runs smoothly?",analytics,2022-05-30 15:45:50,7
Sounds like your df regions might have duplicate on_codes.  You can check by using the validate keyword argument and pass in “many_to_one”. My suspicion is that you have many to many.,10,v0yhrt,"Hello, I am currently working on the preprocessing of a database and I am doing it with pandas, I am sort of a newbie doing my data analysis prep and work with python and I am doing this so I can practice. I've read the documentation, watched vids and read blog posts about merging and joining with pandas and I feel I understand it, however, in practice I am not able to achieve the result that I want and I do not know why. I can't find the answer, so if any of you can help me figure out what I am doing wrong I will very much appreciate it:

I have two dataframes: the main one that I assigned to variable df (this df is  5.931.992 rows) and a second one who just contains information about regions and it's called regions (107 rows). I want to merge my regions information to my main df by a foreigner key that we can find in both dataframes, this foreigner key is call ""op\_code"", I use the following code:

    df_merged = pd.merge(df, 
                         regions, 
                         on ='op_code', 
                         how ='left')
    df_merged

I am using a type of left join because in my resulting dataframe I want all of my 5.931.992 rows and a new column with the information for the region when it is available. I might be understanding the workings of merging and joining wrong, but according to everything I read THIS should be the way to do it, however, when I run this df\_merged ends up having 11.605.440 rows. 

Can somebody please help me so I can get the results I need bc this dataframe is obviously duplicating information and I don't understand why.

Thank you so much!!",analytics,2022-05-30 04:51:38,12
Maybe you can concatenate and create a new key,12,v0hpbm,"Hi, I’m currently working at an agency and we’re trying to manage a data warehouse for our HR department using Snowflake. One of the issues is many of our sources do not have a primary key such as Emp ID and rely on employee name. How can we integrate all these different sources into the data warehouse so that they connect to each other without redundancy. If there is any other info I need to include please let me know in the comments, thanks.",analytics,2022-05-29 11:51:21,11
"Might be worth posting a redacted CV as it is likely down to how you market your limited experience.

I also believe this sub has a vast amount of resources to further add experience",1,uzrnsn,"A little background of myself: I am a recent graduate with a Master of Science in Health Informatics (Aug 2021). I have education experience with Oracle SQL, Tableau, SAS, and some project management with ProjectLibre. I have been a first responder for the last 5 years while attending school so no real professional data experience.

My question is: How do I land an Entry Level Analyst job. I have been applying all over the place and have only gotten one interview so far which did not advance any further. If anyone can help guide me in a better direction, I will be extremely grateful. I want/need to get out of my current job sooner rather than later.


Edit: [Redacted Resume](https://imgur.com/a/kOGNxiu)",analytics,2022-05-28 10:09:48,8
This sounds like plain old incompetent leadership combined with company culture issues.,38,uz53ii,"Hello all, 

I am a new analytics manager for a Fortune 500 company. I have a few comments/questions regarding my role. 

While my title includes analytics, I find that my team actually analyzes no data, but seems to be a BI team mixed with being a help-desk. We do some basic ETL and supposedly manage changes to some Power BI and excel reports (however I have almost zero design input and have no control to make measures/change the data model). In addition, a lot of the time I am getting a bunch of emails and Teams messages about random requests and basic excel questions/projects that should be the user’s responsibility. It seems like much of these requests come from users who are just lazy and don’t want to learn or use some basic excel (pivot tables). With that said, I report to someone who is non-technical, and really offers no support or guidance as to what my role is. I had to go into excel and delete some cells for him the other day. In addition, my boss still has full control over the dashboards/reports despite no technical knowledge of data visualization, and just spews out a bunch of changes he wants and what visuals I should use. 

As an example of something that bothered me, I suggested changing a visual that was using a second Y-axis, and even that was denied. I feel like my knowledge/education/experience is going to waste. 

Is this just how the profession is? I am not sure if I am being negative, or if my feelings are valid. I have worked at smaller companies/startups before, but never a large company like this. 

Thanks!",analytics,2022-05-27 11:54:28,26
"Fully remote job for a larger company without an established analytics department. Or a job you're overqualified for but can ""skill down"" to. I get paid a full time salary and work 10-15 hours/week. My previous job, same concept. Looking for a second full-time job now. (Shout out to r/overemployed)",2,uz4u01,"I've been an analyst for about 12 years, first a business analyst and for the last ~6 years a data analyst (SQL/SSRS/Tableau), but I've also built a business in the last 2 years that makes more than I make at my day job (business does around $120k vs my day job paying around $95k).

However, rather than going full time at my business and eating into money I could use for growth, I would like to find a way to go part time as an analyst (15-25 hours per week) and spend the rest of my time on my business.

I have looked at upwork (it's where I hire for my own business) but I'm a terrible advocate for myself in applying for jobs generally, moreso that style where it's mainly single task based work.

Is there a third path anyone has found? Are there agencies perhaps that might hire a person to support a client that only needs a part time commitment? Is it possible to hire someone to act as your advocate for building a career on upwork?

Appreciate any insights or experiences!",analytics,2022-05-27 11:41:38,8
"That's a pretty odd one. How are you confirming that it's not tracking the specific permutation of ""UK + Chrome""? 

i.e.: GA Audience->Technology-Browser+OS Report, w/ Country as a secondary dimension - showing 0 count

Or are you able to reproduce the behavior on multiple devices?

Definitely a curious one, since it seems so particular. I've run into region-specific issues from time to time, particularly when using consent management tools, but haven't seen region+browser specific.

Any chance you'd be willing to link the site?",2,uyxnel,"Traffic not being tracked in Google Analytics from users in the UK using Chrome  
Users in the UK using Firefox are tracking fine   
Users outside the UK using Chrome or Firefox are tracking fine.   


Any ideas as to possible reasons?",analytics,2022-05-27 05:57:18,2
no,9,uyqbij,,analytics,2022-05-26 21:44:50,13
"Always ask what the range for the position is . If they don't budge, it's a red flag.

Sadly not from US so not sure if you were too high/low or correct",25,uyjnct,"So, I just had an interview today with one of the biggest retailers in West Coast as a FT Data Analytics.  I'm a fresh graduate, no working experience in DA yet, but I have some projects in DA software such as Tableau, Alteryx, PowerBi and Query, Phyton, and UiPath. Even tho my previous background was accounting, I do have a strong passion in Tableau and Alteryx. 

So, I answered the salary question 
$34 per hour or more than $70k per year.
I live in Calif. I'm not sure if that's too low or high for fresh grad (state school).

Edit: Socal location. Yes I do have the Alteryx core certification and currently working on Tableau desktop certification.",analytics,2022-05-26 15:46:02,45
"Yes, I enjoy my work. I’m a data scientist/advanced data analyst on a product analytics team at a tech company. 

Stressors … honestly I don’t find any part of my job *that* stressful. 

I’m measured on progress on my projects, am I regularly completing tasks. I wouldn’t say my boss has any hard metrics on that. Just that it’s clear I’m doing something. Many of my projects are longterm so it’s not like I’m closing X tickets per week or anything. 

I have found growth by 
1. Looking for opportunities at my job. Previously I worked in marketing. No one on the team was really analyzing the data we had, I was curious, so I started digging in. Taught myself Excel so I could make sense of the data. Started sharing my insights. 
2. Based on what I did above, I was moved into a marketing analytics role. At that point I started getting some formal training in software like PowerBI. 
3. I realized I really enjoyed analytics and working with data and wanted to learn everything I could. I knew my job wouldn’t teach me everything, so I enrolled in a masters of data science program part-time while continuing to work full-time. While a masters isn’t necessary for a career in analytics, it is necessary for some advanced roles, especially in data science and machine learning. Plus I was just excited by the field and wanted to learn all I could and also go as far as I could.",9,uypang,"Hey friends 👋🏼 

To give a little background, I’ve been working on call centre customer support roles for the last four years and am ready to up-skill into another role.

One thing I hate about customer facing roles are the constant needs to hit metrics and the stress involved.

For those working as data analysts or scientists:

Do you enjoy your work?

What are the stressors?

What kind of KPI’s are you measured by?

How have you found career growth?",analytics,2022-05-26 20:45:17,10
"Are you brand new to analytics? I was when I started the courses (I'm 7/8's of the way through it), and I've found it to be a pretty good introduction to analytics. I also know I'm going to need to take individual courses on Excel, SQL, Tableau, R etc. to really get a handle on using these programs for data analysis.

Also, they don't teach Python in the program. I've had to learn that through other online courses.

hopefully it'll help me land an entry level job once I've earned the certificate.",38,uybqit,"I have a Bachelor’s in Biology (not much math in my degree besides algebra and trig). I’m 22 years old looking to go into tech specifically data science. 

I was thinking I can get skills in python, R, and SQL from the certificate course and apply it to data science maybe.

Do people land jobs often from this Google cert? Will we be underpaid for not having a traditional degree?",analytics,2022-05-26 09:39:06,31
Data governance and data engineering are very distinct domains - which is it you need help on?,3,uyccdc,"Need to know what kind of skills in data and engineering are needed to do ETL, governance, warehousing

Planning to store historical data from analytics tools as extracts somewhere",analytics,2022-05-26 10:07:09,3
Better pay. Better job locations. More opportunities in more industries. I'm trying to find a new position as a full-on analyst after getting hired as an engineer who's evolved into one at my current job.,1,uy8ijo,"Say you have a pathway to be director of analytics or even a data PM role over time compared to something like a senior analytics engineer and possibly data engineer. Two separate pathways that lead to different outcomes.

How did you decide for your career that you wanted to choose one over the other, assuming you have equivalent experience in both up to this point?",analytics,2022-05-26 07:11:03,3
"In my opinion, before you get to data, you need to understand what you are analyzing. Maybe you don't even have the data that you need.

There are many factors that can affect sales. You probably should have some understanding of psychology, communication, competition, economy, the company's strategy, or anything relevant that can affect sales. We didn't have data about pandemics, but when the pandemic started we had to consider that to understand what happened to sales. It was not data that was telling us the story. 

It's like a medical doctor when trying to define an action plan for a patient who had a decline in health. The doctor can use data, exams, etc. But the doctor needs to know which questions to ask based on his knowledge about medicine, which data should be collected, how the body works, etc. Digging into data is not enough to be competent at delivering the correct solution. to the patient",6,uy01hu,"*some context -  I'm in marketing.*

**eg**. telling a client conversions dropped 50 percent since conversions was 100 last week. this week its 50 conversions.

versus

correctly connecting the pieces together and explaining why the conversions dropped and what factor(s) exactly caused it based off of digging into the data and knowing where to look? And delivering the correct solution or action plan in response to the drop based of data? Are you supposed to look beyond data as well?

\----------------------------------------------------------------------------------------------------------------------------------------

Is the correct term im looking for data story telling? if it is, how do i become an expert at what im trying to articulate above? What do i need to do to get to that expert competence level in accurately explaining the why as opposed to just the what? What do I want to focus on studying/learning/praticing/doing?

&#x200B;

Any insights, advice, and feedback would be appreciated!

&#x200B;

Thanks!",analytics,2022-05-25 22:03:44,4
"Do you know how to calculate these 
- mean
- median
- percent
- rate
- lift 

I think that’s most of the math I used in my first analytics role",71,uxgi77,"Hey friends 👋🏼 

I understand that data science is certainly the more statistics heavy of the analytics fields and that you’d be expected to be able produce a variety of models etc.

If you’re using Python/Spreadsheets to clean data and build dashboards/charts as an analyst is there much interaction with math?

Additionally, for what level of maths is involved is it fair to say the tools do all the rote calculations and it’s more about understanding concepts?",analytics,2022-05-25 05:41:48,28
"Hey there - just gonna add some context to my answer first up that I’m coming at this from a UK finance position, and so that might mean some differences. Also that I think Risk and Compliance as a term can cover a few different things here too. 

At my previous role (credit risk/data analyst for a retail lender), Risk and Compliance were commonly referred to as “second line”. We in the credit risk function were “first line” and initiatives and projects would be led by us. Along the way and more so as we got closer to implementation, “second line” would offer advice on anything we may have missed, and also bring a conduct risk point of view to the table - namely mostly they’d hold us to account for doing the best thing for the customer, and use their experience of applicable legislation or working with regulators to ensure everything was above board. This would also help provide assurance when it was time for audit functions (internal and external, “third line of defence” for us) to get involved.

The analysts in “second line” would often check our work for want of a better phrase. The negatives to the role were that they weren’t really innovating beyond making new reporting dashboards from time to time. It was more of assurance that any data use to support new initiatives was based on solid ground.

The pro to the role was told to me by a senior manager who moved from first to second line. He enjoyed the fact that he could get involved across all aspects of risk - application through to recoveries, conduct risk, market risk… you name it. However, he eventually moved back after missing that innovating work life as opposed to more “checking”.

I dunno if that’s really helpful honestly, you may have already gathered as much from job descriptions and alike. But just thought I’d throw in my two cents worth.",3,uxdefp,,analytics,2022-05-25 02:16:50,3
"Currently experiencing this as an HR analyst. People *really* love beautiful and simple data visualizations. Also, if you can automate existing reports and make other people's lives easier, that's win.

From a soft skills perspective: the bar is really low, so anything you do that's different is seen as amazing. Communicate with purpose. Try not to oversell or get carried away in your excitement. Ask questions! If your team currently uses visualizations and you want to improve upon them, show before and afters with the same data. Dont assume you know best; ask for preferences and/or offer options.",6,ux6mfv,"Looking for examples of quick wins a marketing analyst can earn in a corporate environment. The role is wide open. Data sources are just now being fully explored, BI tools are just now being implemented.

How do you package and sell a project in a new role in a wide open environment?

(Have over 10 years of marketing experience before hand)",analytics,2022-05-24 18:50:39,14
"Just gotta learn SQL. My path was performance analyst -> data analyst (learned SQL using YouTube) -> data scientist.

Learned SQL, then python. I use way more SQL working as a DS than python, and as a DA I used exclusively SQL.",35,uwuskd,"I am currently a Financial Analyst with 3 YoE and I'm exploring a career in Analytics. Does anyone have any advice or any resources they would recommend for someone who is interested in this career path? I am open to going back to school for a Master's degree to make this transition.

I see that a lot of people recommend the Google Data Analytics certificate, but are there any other courses or posts I can reference?",analytics,2022-05-24 09:27:49,40
"Is there a reason why you're getting a master's first, instead of a fulltime job after graduating? I don't know your background, so you might already have a lot of work experience.

If you do choose to go this route, I highly recommend working in a relevant field on the side, either part time or as an internship.

Time and time again, this sub has highlighted the importance of experience over education in getting a good role.",3,ux78u2,"Hello! 

I just graduated with BA with a concentration in accounting. However, I wanted to pursue my master's in Master of Information Systems with an emphasis on Business Analytics at Cal State-Fullerton. I have very little background in programming as I only know how to code in Jupyter Notebook. But, I have a strong passion for data viz and cleaning. I had the Alteryx certification, and I'm currently working on the Tableau certification. I love Tableau so much! I think I can spend all day with Tableau with no problem, but Phyton/SQL is probably my weakest subject. I didn't enjoy coding so much.

Do you guys think I'm choosing the right major for the master's degree? 

&#x200B;

Also, I have a  lot of spare time during the summer. I'm planning to get the UC Berkeley Business Analytics Certification (2 months to be completed and cost about $2300). Do you guys think it's worth it? or should I get Google Data analytics certification? Not sure how much it costs for the Google one.

I hope someone can give advice on this. Thank you!",analytics,2022-05-24 19:25:04,10
"I went through something like this at my first job. 
My only advice is to ask yourself: Are you putting enough time to learn everything? Are you using it efficiently? Are there any online courses you could do during the weekends? 
Maybe take one month just for studying and go hard with it. Be honest with yourself 
At the same time, you could start applying for new positions. Maybe this is not the right place for you. I changed my first job after 5 months because I was a new grad and they threw everything at me. I thought I was useless, dumb, and dragging my team down 
But at my new job, I’m happy and I feel like I’m part of the team. My ideas and skill sets are taken into consideration. 
ps - your company can replace you tomorrow. You control your stress and happiness. If you don’t like a job, leave it. We only live an average of 80 years on this earth. Why waste time stressing abt stuff !!!",3,ux5egl,"Hello all,

I recently got a permanent position at a company I worked at for over a year prior and they were really excited to have me and very happy with my work.

It's been about 3 to 4 months of training and for some reason, it's not fully sticking yet, and I even sometimes forget things I SHOULD know. I'm not sure what it is. I used to be really good at my old tasks and learned quickly. 

My new position is very complex and there are a ton of systems I'm learning on and many rules to follow that all intertwine. But then certain things are just general and exclusive to the rest. I feel like I need to see the same things many times before feeling comfortable, and the pressure is on. 

I think maybe I'm pressuring myself too much though, and making myself freeze. However, I'm disappointed in how slowly things are sticking. They said they understand that it's a lot and they'll keep working with me, but I'm taking too long on some tasks as well. Anyone else experience this? Any tips that might help me succeed? It's been a huge source of stress lately. Any insight appreciated!",analytics,2022-05-24 17:43:58,6
Python/SQL/Tableau sound like overkill for the ask. I’d build the dashboard as a series of charts/tables in excel - at least there’s a prayer of a chance that someone can interpret/fix a workbook after you’re gone.,28,uws1rd,"I started as an intern at a small supply chain packaging company last week a couple days after graduation. During my interview for this position, I was able to convince them that I am a ""data person"" because I have a degree in math & stats. But in reality, I understand that my degrees don't mean shit because I have no experience working with actual data.

  

Yesterday, my boss sent me an email asking me to create an Excel spreadsheet where the supervisors are entering their hours per customer each day along with the volume completed by customer. He wants a daily snapshot with weekly and monthly roll ups that we can track. Ultimately, he would like this to be part of a dashboard to give everyone visibility to the company's daily performance versus goals by customer and overall for the facility. 

&#x200B;

I have so many questions to ask, and there's so much stuff to learn. The only tools I am somewhat comfortable with are Python, R, & Excel. 

1. I have never made a dashboard or visualization in my life. The company is not familiar with software like PowerBI or Tableau. Should I learn those tools while I'm being paid and try to convince my boss to get a Tableau license? 
2. No one here is really comfortable with Excel, and when I use it I know I could be doing them way quicker, either through keyboard shortcuts or VBA, which I also don't know. 
3. No one in this company is familiar with programming like Python.
4. I don't know any SQL, but I should learn it. Should I incorporate SQL in this project? 

The email from my boss is the only thing he sent to me about this project. I'm not worried about the instructions being vague or general because I don't like being handheld, but I just feel so lost at this stage. I don't know what tools to use for these questions and I have no one to go to and ask for help directly. I don't want to email my boss because it would be burdensome. But if I need to learn something, whether it be a programming language or something specific to the business, I'll do it.",analytics,2022-05-24 07:25:04,26
"Amplitude is just a platform that captures product analytics, whereas Looker is a BI platform that can also accommodate analytics akin to what is already in Amplitude. The difference is that in Looker you would have to build out the dashboards themselves vs. Amplitude that stuff is already built out. 

We use both but are funneling more and more users on to Looker as we are able to marry together product analytics with other data.",2,ux4571,"So the startup I'm working for has me in charge of choosing our company's analytics vendor. And to be honest, I'm overwhelmed. I will try to set the stage to the best of my ability:

* We are a healthcare company which Product, Engineering, Clinical Operations, and Clinical Project Management teams.
* We currently use Amplitude, Looker, Fullstory and Segment (the last of which I've been told is a ""data gatherer"").
* We care a lot about funnel metrics--where users are dropping off and retention in particular.

From the feedback I've collected, the CEO wants to depend on a single vendor, versus the 3 or 4 that we currently use.

My question is: what are the fundamental differences between Amplitude and Looker? I've been told that Looker is just a BI tool and doesn't have the product analytics suite or data backend like Amplitude. What does this mean? I've seen Looker visualizations before and they don't seem to lack anything compared to Amplitude.

Some people have recommended Heap and Pendo too. Does anyone have insight into all of these? I know Fullstory has some visualization capabilities, but the main value seems to be session recording/playback.",analytics,2022-05-24 16:35:44,2
Maybe the open source data science masters?,2,uxcrmv,"Hi everyone, i am data analyst, but my position is different from other analytics positions out there. I know sql, python, pandas etc.. 

But i want to take full analytics course that will cover everything to be sure i can leave my job and be data analyst in bigger company.

Any recommendations?",analytics,2022-05-25 01:27:24,8
Is there a dictionary to confirm the definitions of each metric? Perhaps there are cutoffs/exceptions.,2,uwwwv0,"I'm running a paid search campaign and these are the metrics below. I understand the difference b/w both metrics but I don't understand why there is a large disparity?

Avg time on page = 5 minutes. 

Avg session duration = 19 seconds

Bounce rate is 95% and pages/session is 1.06",analytics,2022-05-24 11:01:53,3
"I am taking business analytics and I have options to take electives such as fintech and financial data science

I reckon if you do finance completely, you'll have to learn coding outside of uni to be able to learn analytics.",3,uwy6ai,"Hi guys,

So I am graduating this year.
My background: undergrad of accounting and business finance.

However, I am interested in pursuing a field related to data and finance.
I don't have any experience or skills in programming tho.

My options for postgraduate are either
1. Finance
2. Fintech
3. Data analytics/data science
4. Business analytics

I have no idea on how to proceed as I am interested in all of the above. Which of the above would be the most promising?",analytics,2022-05-24 11:57:32,6
"There's plenty of overlap between a data analytics title and a data science title; it's not standard across organizations and honestly many companies don't even know why they choose one title over the other.

You will definitely be able to work as more of an ""analyst"" (to your point, more focusing on the analytics lifecycle) with a MSDS. I'd stick it through since you're already so far into it.

Honestly recruiters probably aren't going to look at your resume and say, ""This DATA SCIENTIST wants to be a data ANALYST?! What in the heck!"" They probably don't even know the difference. They'll see you have an MS in a data-related field, and you can use the phone screens and interviews to describe how you want to provide value as a data analyst.",21,uwhbjn,"I am struggling with this question and I would love some advice, if you have any.  

I have a bachelors in statistics and I am currently a data analyst with about a year of experience under my belt. I am also going into my third semester of my masters in data science, which I am working on part time while I work full time. I have also complete the ibm data science professional certificate. 

I can’t decide if the data science path is really for me. 

I love statistics. I love probability. I love taking raw data and creating fun visualizations out of it. I have work experience in SQL, R, and Python, but I have enjoyed most of the work I’ve done that revolves around analyzing the data to identify, explain, and visualize trends in it. Using tools like tableau, spreadsheets, thoughtspot, mode, etc, have all been exciting developments for me in my career and the stuff I’ve enjoyed doing the most. 

On the other hand, I am doing a data science masters right now. And it’s just making me more confused. I was operating under the assumption that analyst work leads to data science work.  But data science seems to be much more heavy on the programming / engineering side of things, which I don’t enjoy very much. I’ve made a couple small machine learning / NLP models and I enjoy learning about models and what they can do, but I don’t enjoy making them. I don’t like really programming-heavy tasks, like writing algorithms or building large programs to complete some big data science task. 

I guess I would just really like some advice. I’m really torn on what to do. I feel like I’m more suited towards analytics work (and I find it more interesting), but I’ve already invested time and money into my masters (even though I’m really not looking forward to the classes I have left, which are the very programming heavy and theory heavy coding classes for data science) and I’ve just had this weird association with data science being the final evolution of an analyst, rather than them being two separate fields that have some overlap. 

If I stick with just analytics, what do I do next? Do I look for an analytics masters program? Do I look for certificates to improve my analytics skills? Do I solely focus on my career? 

Has anyone else been in a similar situation and is willing to offer advice ?",analytics,2022-05-23 20:08:52,12
"How deep are you into your degree? If you’re not deep, you should probably just switch majors. Accounting isn’t exactly relevant.",2,uww1oo,Currently an accounting major doing math courses on the side. Don’t know much coding but looking to learn whats needed to become a data scientist,analytics,2022-05-24 10:23:32,6
"Workspace is the best self service option for most end users. Recent releases have been improving the flexibility of quick segments, so a lot of things that would have required an actual segment can now be addressed on the fly.

If you have central oversight/governance, see if they have a power user training framework you can adopt. If not, you might have to take the lead and invest some time in identifying and training a few users who can then field some of the demand. It's not a trivial task, but could be a good development opportunity.

I have gone down other paths, including Adobe data warehouse scheduled exports to a DBMS and/or S3 followed by transformations to feed downstream reporting. This also works well, but is significantly more work (upfront and continuous), so it's not something I would recommend as a first or only approach.",2,uwu4v1,"In my organization, I'm a guy who knows how to generate insights with Adobe Analytics. I'm usually doing it for myself, though.

Lately, my colleagues have been asking me for help with it. I don't have the bandwidth to throw reports at them ad-hoc. So I've been trying to create a data product or two that will help them get what they need without having to know how Adobe Analytics works.

Here's what I've been trying:

1. Creating a workspace project and sharing it with them. **Problem:** Users still have to tweak filters and create their own segments to be able to answer their own questions with a workspace project. Or, I have to create 150 different workspace projects for each of my colleagues.
2. Adobe Analytics connector in PowerBI. **Problem:** Extreme limitations on API calls. 50k rows max per call means that trying to pull a timeframe of longer than 90 days results in very long data source refreshes that I'm worried could hang from time to time when scheduled. Bonus problem: segments are having no effect in the v2.0 implementation of the Adobe Analytics connector.
3. Adobe Analytics report builder. **Problem:** Very targeted insights only, due to filtering requirements. Means I would be answering colleague questions ad-hoc with it. Back to my bandwidth problem.

If our data were in Google Analytics, I'd create a report in DataStudio and be done with it.

How do you handle this in your organizations? Creating data products from Adobe Analytics data that non-analytics users can explore on their own?

Thanks for any insight you can offer on this!",analytics,2022-05-24 08:59:00,1
"I don't know that this is the best solution, but you could use a Custom HTML tag, and write a little JS snippet for the page that fires the tag on IntersectionObserver.",1,uwr6ot,"Hi guys,

I need some help with Google Tag Manager and Google Analytics.

I want to send a pageview to Google Analytics if a user is viewing an image in a gallery on an article. The user is staying on the same URL, when he is viewing an image.

I created a trigger group which consists of two events

• Consent given by the user to use Google Analytics  
• Event which is fired when an image in a gallery is being viewed

This works so far so good. Unfortunately the trigger group is only being fired once on a page. When a user open the gallery and views the image and scrolls to next image, he is staying on the same url. In that case the trigger group is only being fired once.

How can I achieve that the trigger group is being fired multiple times on the same page without removing the second trigger (consent)?

I found out that I can create a second trigger group and add two image views and the consent as a trigger. But this solution means that I have to create 100 new tags, which is kind of crazy. Is there any workaround?

Thanks in advance",analytics,2022-05-24 06:44:42,1
"Hello, i am looking for a analytics tool, which will suit our clients, we are agency focused on marketing automation, but some tool miss an analytics part.

I will definitely check this. It sounds great.",4,uw1cvn,"Hi, one of the creators of Objectiv here.

We're a team of long-time data geeks and have spent a year developing an open analytics tracking standard, so data models built on one company's data set can be deployed and run on another.

It’s open-source and integrates with common/open tools in the modern data stack.

Check us out on GitHub: [https://github.com/objectiv/objectiv-analytics](https://github.com/objectiv/objectiv-analytics).

We've just launched publicly and are super curious to hear what you think about the taxonomy, and what you would like to change/add to make it work for your team!

—

Some history: we’re Ivar, Tom and Vincent and we have a shared history of crunching analytics data (early team founded app store analytics company Distimo, acquired by App Annie).

For the last two years our team has modeled on existing analytics data from Google Analytics, Mixpanel, etc for 50 companies. Every time we found that their data teams really wanted the same things from their models: prevent churn, deep personalisation, better conversion and higher user engagement. But they had to all do a lot of cleaning and preparation first, and then build/train models from scratch; because analytics tracking is self-defined in every company, there was no way to take pre-built & trained models off the shelf. So we wanted to fix this.

We started by creating an open analytics taxonomy: a standard way of tracking product data. To develop the first version, we looked at the 50 companies we worked with and mapped their product UIs and data teams’ requirements into one extensible data format.

To help engineers with tracking instrumentation, we now have a low-effort tracking SDK on various platforms that validates the instrumentation against this open taxonomy in IDE, runtime and CI. Currently on plain JS, React, Angular, React Native, and working on more platforms.

We are not building a typical SaaS analytics platform, but data goes straight into a self-hosted data warehouse. Data teams can then use our modeling library Bach to model directly on the raw data in a notebook, using pandas-like operations that are translated to SQL.This gives access to a growing library of data science models that people can share, which we call the open model hub. Bach can also output the SQL to run it in production without lock-in, e.g. to feed to a BI tool, dbt, etc.

Right now, we support Postgres for storage, and are finalizing BigQuery integration, with Redshift and others to follow. To run this at scale, it integrates directly with Snowplow.

Next steps are extending the open analytics taxonomy beyond product analytics (think marketing, payment, CRM data) and to extend the library of pre-built & trained data science models.",analytics,2022-05-23 07:22:47,3
"Accounting/Finance, can always pivot into analytics later, but in my experience the pay will be significantly better in accounting/finance. Analytics may have better work/life balance however.",8,uwb2y0,"I'm trying to think whether I should go down the accounting route or analytics route 

Accounting would be an accounting major and analytics would be related to MIS and while I could double major, I would prefer getting a minor or two within stats, programming or accounting if MIS 

Which one should I go with? 

I will probably do a MS analytics or DS if the analytics path",analytics,2022-05-23 14:37:52,19
"It really depends on what you define a job to be good. Also, the details of the role (amount of 'analytical skills' required) depends on the company and the team. Might be a better question to ask the hiring manager or someone in the team",1,uw6al7,"Hello, grad student here!!

Digital journey analyst: basically optimising customer journey with analytics. I'm confused cause it's a role with so many different names... Customer journey, customer insights, service journey etc.

I'm not a very tech person so I've never heard of this kind of position.

&#x200B;

Is it a good paying job? Does it have opportunities for progression? Especially in UK.

Does it require a lot of analytical skills?

&#x200B;

Thank you. Appreciate any inputs.",analytics,2022-05-23 11:02:56,1
"The percentage will vary.  

GA4 has built in data modeling.",2,uvw2mw,,analytics,2022-05-23 02:08:59,2
"It's not significantly more advanced than Plausible or Fathom, but Matomo can be self-hosted I think. The UI isn't as nice as either of the other platforms though.",3,uuscpb,"I need something I can host myself ideally. I’d like and alternative to Google Tag Manager and Google Analytics combined in a single product. Plausible and Fathom have open source self hosted solutions, but they are really basic (too basic).

I’ve stumbled on Scale8 doing a quick GitHub search. It seems quite new, but super easy to use. Only issue is I’m not really sure how best to host it.

Has anyone used Scale8 and know how best to configure it and set it up? (Hosting). I’d like to use digital ocean if possible.

If anyone has found other solutions please link in comments. I’m not interested in cloud offerings for now.",analytics,2022-05-21 10:55:49,20
"I used coronavirus simply because it was a new hot topic and not too many grads had any projects on it. 

Other than that, if finding data is an issue, center for Medicare or Medicaid. All their data is open and free, you just might have to do some digging, but could make some serious projects out of the stuff in there",3,utm9ja,"For analysts working in the healthcare industry, which kind of data would be great to see a new grad having messed around with on a portfolio?

I really want to get into healthcare data analytics.

Data on mental health would also be super nice but I can’t find anything genuine, I feel like.",analytics,2022-05-19 22:30:48,8
"Cool to see another WGU student. I completed the MSDA recently.

As far as your questions:

- No you are not too old to begin a career in analytics. People from all backgrounds and walks of life got into analytics roles and pursuing a technical degree like you are would be a plus for you.

- I would just look for normal jobs with the title Data Analyst/Marketing Analyst/Business Analyst/Business intelligence analyst. Those are some major areas of analytics so it’s up to you what you want to focus on, but don’t worry about the job description requiring 2-3 YOE, just apply.

- For a lot of entry level roles, basic statistics will do. Basic stuff like mean, median, mode and maybe testing for significance with p values but it really depends on the role and job description. But definitely learn SQL well, you will use that in most positions to query data. Python is a big plus but not necessary for entry level.


I’m curious why you want to pursue analytics instead of software development? SWE is something I may want to go into in the future, do you not enjoy the work?",6,utgrvp,"Hello! I am currently about 2/3 through my WGU degree for Software Development. I was a school teacher for about 6 years and realized in 2021 I had enough and wanted to follow my lifelong dream of being a computer programmer. I am 35 years old and very interested in software development and data analytics (I have discovered through my coursework). I live in the USA and have very limited tech background but I am currently interning for an indie videogame development company using C# primarily (which I know doesnt really help with DA).

Anyways, I have a few questions that are likely easy to answer.

1. Am I too old to BEGIN a career in data analysis with basically no background in tech.
2. what are entry level position titles I should look for? jr data analysis positions seem to want the coveted 2-3 years experience - where do I get that!?
3. I am currently brushing up on basic SQL skills and will be ""deep diving"" python soon. What else should I learn to prepare to enter the world of DA? I see ""study maths, stats and programming"" but what math? and stats can be super broad... so I just would like a little more focus because I am extremely stretched as far as mental bandwidth goes.
4. thanks. sorry for not using the search function but my children are monsters and I need to feed them lol",analytics,2022-05-19 17:09:28,14
"Personally I wouldn't pigeon-hole my first job experience into software I don't plan on specializing in long term. You're right, the BA role will probably open a wider variety of jobs for your next move.

What does each pay, are either remote, and where are you located?",16,uta1n9,"EDIT for result: I ended up taking the BA role. Thanks for the input!

**TLDR:** Marketing Data Analyst using tools like Amplitude, Segment, and Customer.io to work with clients, present to stakeholders, help drive growth at their companies **VS** business analyst using tools like SQL and spreadsheets, gathering business requirements, working with/presenting to stakeholders, helping merge data infrastructure through project management.

So... I've been trying to make a career change into data analytics, and after hundreds of applications and a lot of LinkedIn networking, I finally have two offers. While they both involve data, they are very different, and I'd really like to choose the one that will set me up for a better career in data analytics and data science.

1. Marketing Data Analyst: The employer is a small firm that does business to business contracting. I would be assigned a project or two at a time, work with clients to help them use a data tech stack that we suggest, analyze data for them, and meet with them to go over progress and assist them with implementation of infrastructure. Sounds great, right? **The only caveat**: I would **NOT** be working directly with the foundational data science languages, like SQL, spreadsheets, or Python. Rather, I'd be working with tools like Segment (for gathering data), Amplitude (analysis and reporting), and Customer.io (targeted marketing campaigns). My experience wouldn't be limited to these tools, but would be heavily based around similar tools. I might eventually work with SQL and direct data management, but initially, it'll just be using Customer Data Platforms and various visualization tools.
2. Business Analyst: Bank employer acquiring/merging another bank. Traditional BA role. It'll consist of gathering requirements from stakeholders, synthesizing them into tangible business requirements and questions, working with data and software stakeholders to get the appropriate data from the databases, merge various databases, and report to stakeholders. I'll get to work more directly with SQL, and will be helping two large companies merge their data infrastructure. It would look great on a resume.

**So, which would be better for a career in data?** The marketing analyst role sounds very, very interesting, as I'd get to work with a large variety of clients. I'm just afraid that I wouldn't get to become proficient in the most basic data skills like SQL. The business analyst role seems much less appealing in terms of the variety and excitement of the day-to-day, but appears to be a much more traditional entry into data.

What would you choose based on your experience in the field?

Tools I mentioned:

[Segment | #1 CDP to Manage Customer Data](https://segment.com/)

[Amplitude | #1 in Product Analytics](https://amplitude.com/)

[Marketing Automation for the Whole Customer Lifecycle](https://customer.io/)",analytics,2022-05-19 11:38:52,21
Create a custom user field and upload it as part of the CSV. Then use that field as the condition for cohort inclusion.,2,ut3e5k,,analytics,2022-05-19 06:25:46,1
"Based on your post I'd suggest ""immature data"" but I assume you haven't shared enough to give an adequate answer.",13,us8s02,"This might be a stupid question, but I often see an effect where there appears to be a recent trend in data that is a false trend caused by recent data being incomplete. This might be caused by, say, a delay in a database being updated or by a future event affecting how we count things now.

Is there a word for this? It comes up so often, I'd really like to be calling it by name!

Edit: an example. We receive transactions from customers at random intervals. All transactions must be unique. If they are not unique it is a sign of fraud. But the interval between 2 non-unique events is also random (or non-predictable). So, when we see the rate of measurable fraud decreasing, there needs to be a caveat that it might be illusory. Giving that illusion a name would help people understand it in all the different contexts in which it comes up.",analytics,2022-05-18 02:16:27,10
"CAR and STAR and FARM are all just ways of structuring your responses. Any of time you get any question that starts ""Tell me about a time when"" or similar, structure your response by first explaining what was going on, what you specifically did, and what happened as a result (and best yet, what you learned or took from that situation). The acronyms are all just slight variations of this same thing.

Situation Task Action Result. 

Challenge Action Result. 

Frame Action Result Meaning. 

And, for me at least as a hiring manager, it REALLY helps you to give well structured answers like these. Saves follow-ups, shows you can think through a problem on the spot, and that you can communicate clearly and concisely.",5,urvuin,"Hello guys,

Don't know where to ask this so here we go... 

In the middle of a couple job recruitment processes. Both involve a round of interviews having guesstimate, case studies and problem solving questions. The first two are easily available on the internet but would love to find something good on Problem Solving questions. Especially using CARE or STAR method as that is what's expected in my interviews. Any material will be highly appreciated.",analytics,2022-05-17 13:41:54,5
"Data Science, Manager (lead a team of data scientist, product and marketing analytics)

14 YOE

Masters

VHCOL

230k+20% bonus + 100k RSU",38,ur117l,"Hey guys! I think it's beneficial to do salary threads every once in awhile, especially in times like today when companies are increasing pay to attract new talent. This will help us as a community in knowing what the market is offering, and helping people negotiate what they're worth. I wanted to kick off a thread to discuss our pay with important factors along with them. The format is -

1. Current Title (can expand on what you do)
2. Years of Experience in Business intelligence/Professional work
3. Educational Background
4. Location/Cost of Living
5. Base Salary + Bonus

I'll start us off -

1. Business Analyst (Creating Power BI/ Excel reports, maintaining reports, gathering requirements).
2. \`2 years in analytics/BI
3. Business Degree and transitioned into BI
4. Smaller city in the USA, LCOL
5. $70k, no bonus.",analytics,2022-05-16 10:18:29,181
[removed],3,urn4q1,"Should clicks on a link that displays in an email, on a social media site, and on my site all use google analytics/the same analytics? 
Right now just the click from my site uses google analytics but I want to see the sum of the clicks across all devices",analytics,2022-05-17 07:01:07,4
"I had good outcomes with -

‘In six months, after you’ve hired someone for this role, what are you hoping that person will have accomplished that will make you feel like you made a really good hire?’

It’s just another flavor of ‘what makes for a successful employee’ - but it has gotten interviewers to open up a bit about their expectations and hopes, and can lead to a conversation about what the biggest obstacles are that the team is facing.

I like to think it’s a good way of demonstrating that you’re going to be someone who thinks ‘big picture’.",19,urbnzp,"Have my first phone interview for a remote data analyst position. I love that it's remote and that it revolves around SQL.   
I'm currently thinking back on which scenarios I can use in case I get asked behavioral questions but I need questions to show interest and get specifics.

Do y'all happen to have any that have worked well for y'all in the past?   


Much, much appreciated!",analytics,2022-05-16 18:50:14,9
"I did this for a startup several years back. It involved identifying the best time to schedule people to meet business needs, identify people who are getting the axe due to performance, figure out if PTO requests are an issue, create contingency plans in the event of any major disruption, Quantify top performers for raises and bonuses etc. My experience was very broad due to the startup nature of the role, but it was very eye opening at the same time. Best part,  I got to work hand in hand with COO, Sales Director, HR Managers, even the Finance/Accounting side. I was trusted with a lot of information about the inner workings and strategy of the company that most employees weren't purview to. Worst part, Managers can lay squarely on your shoulders the fate/livelihood of people when they need to trim expenses. Some people don't mind that, I do remember not enjoying that part at all.Hope this helps

Best of Luck!",14,ur16tr,"I work in people analytics and am on the hunt for my next role. I've found a couple of pretty intriguing listings for workforce or ""workforce planning"" analytics. In the past I've seen some comments here that make the work sound pretty bleak (e.g. tracking the time that employees spend on breaks), but I've read elsewhere that it can encompass skills gap analysis, talent strategy, and other stuff that I truly find fascinating.

Anyone have experience in/thoughts about workforce analytics?

Edit: I should add that I'm aware that ""workforce analytics"" and ""people analytics"" (aka HR analytics) are often used interchangeably. I do get the impression that workforce analytics is sometimes viewed as a discrete field, though. I think it tends to have a greater emphasis on planning and budgeting for your future workforce, as well as the skills/development needed to get there.",analytics,2022-05-16 10:25:19,9
"Sounds like an excel sheet with some formulae to sum it up at the end.  Buttons can use Excel Visual Basic if you really want them though that may take more setup than it’s worth. All this can also be programmed in a relevant programming language(sounds like an app to me), but that also may be more trouble than it’s worth.",2,urkp4z,"First of all, I'm no programmer but I can figure out things if I get a mere idea. Unfortunately I don't know how to create such a certain thing or if there even exists something that I am looking for. I would prefer to use excel but I am open for using other software like MS Access...

I am a pool-billiards player trying to analyse my matches. Therefore I am looking for a convenient way to do so.

The way I want to analyse my matches is the following:

Having an interface where I can 

&#x200B;

1. enter certain data like a scoreboard with buttons that are connected to an if condition that can fill certain cells with a number (e: no mistakes made -> fill cell for mistakes with 0).
2. enter certain data like a scoreboard with counters with buttons for raising/lowering the score which will be put in a certain cell (e: 2 balls potted -> fill cell for mistakes with 2).

After entering the data for game 1 I want to enter the data for the next game in the following row via the same interface.

Then I want to do simple calculations like average mistakes per game and put those stats in graphs and track my improvement, which will be fairly easy.

I was just wondering, if there is a way to do it easier than entering all the data cell by cell.

I am not a native speaker, so please excuse me for my possibly unclear phrasing, but I wanted to reach a wider audience.",analytics,2022-05-17 04:52:38,3
Triple exponential smoothing is the classical approach when seasonality is likely strong.,23,uq3n38,I have 24 months or 8 quarters of sales data by region and product categories in an excel file. What are the different approaches which i can use to forecast the sales for next 12 months at region and category levels. Please guide,analytics,2022-05-15 03:56:47,10
"Gapminder, maybe?",1,xhnu8a,,Stats,2022-09-18 11:15:04,1
"1- By effect size do you mean the actual coefficients or just what you see in the visualizations?

2- Are you visualizing with participant id points linked or identified with the same color for example?

3- Are you visualizing all 3 fixed factors in facets with ghost lines or just one at the time?

Effect size and significance don't always go hand in hand. That's why publishing based on p<0.05 is so trash.",1,xfo42b,"Hi!  
Why do I get significant effects but extremely low effect sizes (almost unnoticeable in the visualizations). 

Context: I ran a hierarchical mixed model with 3 fixed factors and ""participant"" as random factor.

Could it be that I did something wrong?",Stats,2022-09-16 03:56:44,5
"So are you creating a playlist with 15 randomly chosen songs?

Are you trying to calculate the odds that those 8 songs that played in a row are on the same album? 

Finally how many albums are there? 200 ?",1,x86v99,"I'm trying to figure out how to find the chance of the following scenario occuring: 

I have a playlist with 3000 songs on it. The playlist contains a full album, which contains 15 songs. After shuffling the playlist, 8 songs, all of which were on the one album, playing in a row. 

How would you apply 2 different ""variables"" into the same problem? 

I made an attempt and came up with the number 2.5x10^-34. I am in no way confident with this answer. Can I get some help?",Stats,2022-09-07 07:40:14,2
"For (c), think about what operations you do to each data point to calculate z scores. What happens to the mean and standard deviation of your data when you do each of those operations?

This is related to other parts of the question. Part (b) boils down to, ""what happens to the mean and standard deviation of your data when you multiply each observation by 60 and then add 5 to every observation?"".",1,x13cb0,,Stats,2022-08-29 16:31:21,1
"Not sure what they want you to tabulate, but here's the data:

- 80 people total.
- 16 women (15 students, 1 teacher).
- 60 were students who paid 1600: 45 men and 15 women.
- 6 were male servants.
- There were 14 staff members: 13 men, and the previously mentioned teacher.

Your breakdown is:

- 13 male teachers paying x
- 1 female teacher paying x
- 45 male students paying 1600
- 15 female students paying 1600
- 6 male servants paying zero

Find x to get the amount the teachers paid.",1,wus8pv,,Stats,2022-08-22 05:24:21,3
"Okay, so you need to know that for two rvs X and Y: E[aX + bY + c]=aE[X]+bE[Y]+c

Now do the same with y_2, just replace the t with 2 and compute the expectation.",2,wq3s7v,,Stats,2022-08-16 12:51:59,3
What’s an outlier? Sounds like a silly question but there’s no real answer in the sense of something that’s clearly defined and accepted.,1,wnp0nz,"Ok I’m a math teacher and it’s been a minute since I’ve done this sort of stats so can I get help? I may not have enough info on how average dog weight and height is gathered by societies like the AKC to find this but I’m curious if anyone can help. 

My big shiba who I’ll post a photo link to later is 50 lbs in weight and 21 inches in withers height (front paw to “shoulders”)

Various shiba ink fact sites say the breed males is “on average are between 17 to 23 lbs”. Not sure if this is implying confidence interval or if it’s a 21 mean with some sort of variance/standard deviation of 3. They also have a withers height of 14.5-16.5 inches. 

Again I have no idea how to interpret these “statistics”. Help appreciated on that.

Can I conclude my shiba inu is an outlier for a purebred?",Stats,2022-08-13 14:23:31,3
"In R, you can get a prediction interval (which is what you want) using:

>	predict(model, newdata, int = “p”)

The int = “p” bit means you want it to give you a prediction interval. (The default confidence level is 95%, which can be changed with another argument, if you want). If you say int = “c” instead, it would give you a confidence interval instead of a prediction interval, which would be similar to what you see in the ggplot.

You could have the “newdata” be sequence of values along the x axis, and then plot the prediction interval as two new lines. This is easier to do with base plots, but definitely possible with ggplot.",1,wlaynw,"I'm looking at some data for a personal project (fantasy football scores) and trying to apply my basic knowledge of statistics to better characterize the data.

I have 9 samples (9 years) of data of points vs rank, and I've included log fits (y\~log(x)) in the scatter plots (for reference, each sample has 32-35 observations). I haven't included the r\^2 value for each plot, but they range from approximately to 0.584 (in 2021) to 0.869 (2017). I understand r\^2 doesn't imply goodness of fit by itself, but combining it with the plots makes me feel comfortable using these curves to approximate the distribution of the annual data. I am open to better ways to find a fit, but I want to avoid overfitting and would like to keep this somewhat elementary without diving into a deep rabbit hole:

https://preview.redd.it/q7dkd8gxmyg91.png?width=1128&format=png&auto=webp&s=72230844104a16194d3c8c8db6160d80be84f699

Overlaying all of these points and fit lines to visually examine repeatability/variance, there is no clear shift in the data, as expected. Outside of systemic changes to the overall scoring, I would expect the distributions to remain consistent, which they appear to be (this is a small sample size, though):

https://preview.redd.it/i64p2kkanyg91.png?width=1128&format=png&auto=webp&s=add34ffccb4aef0f7d1ad8ce7abbc05fbcfefd0c

Making the assumption (which I understand is not proven in any way here) that the y-values are normally distributed about their means (which I will assume to be predicted by the trendlines), I am trying to understand how to properly interpret or construct a confidence interval.

I am using R for this analysis, and the 95% confidence interval shown around the log fit curves does not seem to be what I'm looking for. For example, the plot below has 14/34=41.2% of the observations outside of the 95% confidence band around the trendline. My interpretation of this is that the confidence band only applies to the fit line accuracy itself and not to the prediction of the data. Or maybe the confidence band is relevant to the full population, instead of the sample?

[r\^2 = 0.869, 2017 data, n = 34, y\~log\(x\)](https://preview.redd.it/kdnaulrvpyg91.png?width=978&format=png&auto=webp&s=5caee9d67f510548533360b11fb99323dad9f1e7)

&#x200B;

The goal here would be to generate a curve with a CI around it for predicting values for future years.

I feel like I'm probably missing something significant or going about this incorrectly, so I'd appreciate any advice or explanation. Is there a way to combine the error in fitting my data with a curve with the expected variation about the predicted y values to generate an encompassing confidence interval/probability space?

&#x200B;

EDIT: In case the images dont show up, theyre uploaded here in order: [https://postimg.cc/gallery/vnL8X6P](https://postimg.cc/gallery/vnL8X6P)",Stats,2022-08-10 15:36:12,5
"https://docs.google.com/forms/d/e/1FAIpQLSflzjw1k5DetmnOVfnfL4iH17WQd67t2mdne2dvcueFyOEbCg/viewform?usp=sf_link

If the link above does not work ^",1,w0b989,,Stats,2022-07-16 00:49:09,1
The CLT applies to the sampling distribution of the sample means. So it applies to the distribution of data not the design itself.,1,uyijeh,Struggling a bit with this concept as one of its assumptions are said to be independence of samples,Stats,2022-05-26 14:53:44,4
Need help with stats exam !,1,uxykw1,,Stats,2022-05-25 20:32:59,1
"Male, shoe size 13, 6'1""",1,uwq8ie,"I’m trying to procure a data set of shoe sizes and heights of men and women to compare how one effects the other in each gender.

So if you could share with me your shoe size and height it would be appreciated.",Stats,2022-05-24 05:58:36,2
"The difference is in the assumption that you know which way it varies.  That changes the probabilistic description (hence why we have one- or two-tailed distributions).

As an example: what is the fewest times you must use a balance scale to identify the odd sprocket in a bunch of 12,
a) if you know it’s heavier
b) if you only know that it’s different, but not whether heavier or lighter?",1,uucpi0,"Hey everyone I was wondering if someone could help answer a question I have regarding the F test when comparing the equality of two variances. 

Background: I was studying the F-test recently and was a looking at comparing two variances of ""returns"" from two samples split by a certain event. First testing weather the variances of returns is different before vs after the event, led to failing to reject the null hypothesis as the value for F was within the critical values. Thus it can't be confirmed that there was a difference in the variances. Then I proceeded to test weather the variance of returns was greater before the event than after the event, using the same significance level. This time the value F was greater than the critical value and I rejected the null hypothesis. This means the variance of returns is greater before the event than after. 

My question is, why can it be concluded that there is no sufficient evidence to indicate the variances are different (step 1) but there is enough evidence to indicate one variance is greater than the other (step 2), if one is greater than the other than by definition wouldn't it have to be different?",Stats,2022-05-20 18:31:28,1
You want to do a post hoc test: tests that are typically done after you have a significant result. The most common  for ANOVA is turkey's post hoc test for comparisons. So that's probably what you'll want to do.,1,upq8rv,"Hey all, I've been stuck at my computer for hours trying to figure this out. I did my ANOVA to investigate a relation between mental toughness, workload and the three groups of people that scores for these (they're all lumped together as one variable), but now that the result is significant I need to conduct follow up t tests to find out which groups are statistically different from each other. I'm not sure which one to use and I'm so confused, it's been two hours. 

Any help would be appreciated, thank you x",Stats,2022-05-14 13:57:30,5
"A Chi-Squared test is going to be sensitive to your small sample size (i.e. one of your groups is <~50). So you could perform a Fisher's Exact Test (better with smaller sample sizes) to ensure that the differences in usage across the 2 groups is statistically significant. 

Then you can report just as you outlined above, but with more confidence that the findings are valid. You can run a Fisher's Exact Test in any common Stats platform like SPSS, JMP, or in R or your preferred Python platform. You can even use Excel if you download a free plugin that adds functions for common statistical tests. Just trying searching ""free Excel stats functions add on"". Here's an example of one: https://www.real-statistics.com/chi-square-and-f-distributions/fishers-exact-test/

As far as communication and visualization of the findings go, if you are concerned about percentages misrepresenting the differences in the sizes of the groups, you could consider using your group size as a scaling variable. For example, 2 pie charts (one for each department) with the percentage for each group represented, but with the size of the group dictating the overall size of the pie chart. That would simultaneously show the differences in the usage rates (the pie chart itself) and the group sizes (differences in the size of the pie charts). Adding the group sizes above or below the chart would also be helpful in making sure it's clear that there are large differences in group sizes.",1,ul7kj9,"Hi all, with the caveat that I’m not proficient in statistical matters I was recently ask to pull users softwares usage across my company and outline the differences between groups. Aside from listing groups percentages is there a sensible way to represent the size of a group? I don’t want to use percentages only as I found a bit underwhelming and quite misleading i.e. “50% of people in Finance use X as opposed e to 25% in customer support” in Finance there 10 people in CS 1500! Thanks lads and be nice with a newbie like me 😬",Stats,2022-05-08 11:11:44,1
spam,1,uft9ya,"Hello pay professional vetted tutors to handle your; ✓Assignments, homework, Online classes,Online exams, projects and Dissertations at very affordable rates.


NB:  VOUCHES AVAILABLE.

WhatsApp +1(424)377-0855

Discord  Dr. Paulettel{Verified Tutor}#3107",Stats,2022-04-30 22:54:53,1
Is this fee for service or are you just being nice/bored,1,u7cbws,"Hi Guys
Reach out to me for help with statistics
 related tasks.

Email:statisticianjames@gmail.com 
Discord:statisticianjames#2668
WhatsAPP: +19169314934",Stats,2022-04-19 11:44:50,1
Got it! Needed to add a fixed binwidth and center!,1,u317y8,"Hello! 

I think I am having a problem with my ggplots.. I am trying to plot circular data which ranges from 0-180. 180 degree equals 0 degree.

I am using the following code to plot:

  `ggplot(nearest) + geom_histogram(aes(x=nearest$Angle), fill = ""#56B4E9"") + theme_minimal() + coord_polar(theta = ""x"", start=0) + scale_x_continuous(n.breaks = 18, limits = c(0,180))`

There is a gap from 175 to 5 degree. There is however data in this range. How do I have to change the code to solve the issue?

I am offering a 15usd amazon coupon, if somebody solves the issue for me 😊

Thank you so much!

&#x200B;

https://preview.redd.it/ziyf4zp39dt81.png?width=836&format=png&auto=webp&s=e46c207d868539d81e62b0118ed7f0871d376dfb",Stats,2022-04-13 14:42:51,1
Whit not use a correlation test? Pearson's sounds appropriate,1,u0vbdy,"I'm currently working on a project that is asking us to test the correlation between two datasets (Sales vs Projects). I've run a linear regression on R and got the summary in the first pic. Had to run it again since the intercept wasn't significant and got the second summary. My question is, would it still be valid to say that the datasets are positively correlated even though the residual standard error is extremely large and also increases after dropping the intercept variable?

Much appreciated of any help!

[First summary](https://preview.redd.it/onkouv71qss81.png?width=710&format=png&auto=webp&s=b584be0271dc1ade7304f71dc651d47084262d2a)

[2nd summary](https://preview.redd.it/mjjl9nb0qss81.png?width=732&format=png&auto=webp&s=97358e9924c8d08c9735fd27e50536cd23017e5d)",Stats,2022-04-10 17:41:37,4
"I agree with you on this one. Think of each draw as independent, which means that picking one card has no effect on the other.

For the first card, there’s 3/12 or 1/4 chance you get an ace. For the second card, it’s still 1/4. Same for the third. 

Individually, each ace has a 1/4 chance of getting chosen. However, where you’re right and he’s wrong is that the question is asking about 3 aces in a row. Getting three aces in a row is more rare, and you multiply independent events. Specifically, it’s (1/4)^3, so you’re correct.",1,u0sln2,"A friend and I got into a little argument about probability involving cards and we decided the best way to go about answering it would be to ask reddit. So here's the problem:

Say you have a deck of 12 cards, 3 of them are aces. What is the probability of getting 3 aces in a row given that you return each card picked and shuffle the deck after each turn.

I think it's (1/4)³ and he thinks it remains 1/4.",Stats,2022-04-10 15:20:42,5
"Si te gustan los videos🎬 del canal apóyanos suscribiéndote📑 y brindando un comentario💬 y un “me gusta”👍, gracias:
https://www.youtube.com/channel/UCn0v3DhIceFUACWxR9fU30Q",1,u0gql5,,Stats,2022-04-10 05:37:59,1
"Hey, depends what you're looking for. Fbref has tons of player, team, and game specific data. Transfermarkt has transfers/player data. I'd start with Fbref, it's great!",1,tsx442,hey all. I'm looking for good sites that offer free soccer/football data. Any suggestions would be appreciated thanks,Stats,2022-03-31 03:53:02,1
"The way you would calculate a probability like this is in two steps: A = the # of distinct hands you could have which are ""triple aces"", B = the # of all possible distinct hands you could have (whether triple aces or not). Then the probability is A / B.

B is usually easier in most problems, so start there. For this, you are dealt 19 out of 80 cards, so the number of hands you could have is ""80 choose 19"", which is a [combination](https://en.wikipedia.org/wiki/Combination). In this case it's [1159120046626942400](https://www.wolframalpha.com/input?i=80+choose+19) possible distinct pinochle hands.

A is more complicated. We usually count by multiplying the ways of meeting each of the conditions. There are 4 aces of each suit, and you need 3 of each. Using combinations again, there are ""4 choose 3"" ways of being dealt 3 of the 4 aces of spades, and also ""4 choose 3"" ways of being dealt 3 aces of every other suit. Then you need to fill out your hand with 7 remaining non-ace cards, which would have ""74 choose 7"" possibilities. Wolfram alpha calculates this total as [460692240384](https://www.wolframalpha.com/input?i=%284+choose+3%29%5E4+*+%2874+choose+7%29) possible triple aces hands.

Finally, A / B = 460692240384 / 1159120046626942400 = .000000397 (pretty unlikely). [Getting a royal flush in a 5 card poker hand is about 4 times as likely.](https://en.wikipedia.org/wiki/Poker_probability)

I'm about 85% sure I've done the math right, but I'm 99% sure this is the method you would want to use.

EDIT: I just realized that I messed up the math. The final step should have had ""64 choose 7"" not ""74 choose 7"" (there are 64 non-ace cards in the deck). This would change the final answer to actually make it less likely.",1,tjuqn1,"I'm not that good at math. I simply cannot wrap my head around calculating the odds of this. I was playing a 4 player card game called Pinochle, particularly the double-deck variation so the results when googling anything were very limited. 

In this game, there are 80 cards. 10s, Jacks, Queens, Kings, and Aces of all 4 suits, and 4 of each kind. Each of the 4 players is dealt 19 cards, leaving 4 aside for something referred to as ""the kitty."" I want to know the odds of being dealt something called ""Triple Aces,"" meaning that I have 3 of each suit of Aces (3 Ace of Spades, 3 Ace of Clubs, 3 Ace of Hearts, 3 Ace of Diamonds). For that matter, you could calculate the odds of Triple anything, whether it be 4 of each 10, Jack, Queen, King, or Ace. 

I simply cannot wrap my head around the math of it. I'd appreciate any and all insights or if someone is willing to crunch the numbers for me. Thanks.",Stats,2022-03-21 21:22:29,1
"Sum of normals is normal as well, 1- cdf of that normal at 21 is the answer",1,tgygvm,"Hi, I saw two approaches how to solve this problem yielding different results. How would you solve this common problem:

X \~ N(7, 49), X \~ medical cost per patient

Compute the probability that the medical costs for five patients are more than 21 euros.",Stats,2022-03-18 03:42:13,3
"There are two things happening here. First, it's probably true that lightbulb lifetimes are not actually exponentially distributed and that textbook exercises just choose an exponential distribution. The actual distribution of real-life lightbulb lifetimes probably looks about like you've described.

Second is why the textbook chooses to use an exponential distribution in the example rather than some other distribution. One very convenient thing is that the exponential distibuton's density is simple. Another is that the exponential distribution is memoryless. If X~exp(theta), then for any time t>0, X-t|X>t ~ exp(theta). In other words, if a lightbulb's lifetime is exponentially distributed, knowing that the lightbulb has survived t time so far doesn't give you any information about the lightbulb's remaining life. These properties can make solving certain exercises easier",2,tcqoxx,"I cannot understand how the exponential distribution function models the lifetime of a lightbulb. All the examples that I see will say something like ""The lifetime of a light bulb follows an exponential distribution with mean lifetime of 100 days"". None of these type of examples make any sense. With an exponential distribution the probability density is greatest towards 0. But then if the average lifetime of a light bulb is 100 then I wouldn't expect the majority of light bulb lifetimes to be near zero. It doesn't make any sense and none of the examples explain how this distribution actually models problems like these. I expect the distribution would look more like a right skewed normal distribution with the mode nearish to the 100.",Stats,2022-03-12 13:57:59,3
"I think alpha in your cv approach should be .05, and your p-value should also be .05. is this a one-tailed test? If so, your cv is z>1.65. So you’d retain null in both.",1,syusga,"Hello resident stats wizards! lol

I am in an intro to statistics course, which I am really enjoying. I am presently working on an assignment for a unit on hypothesis testing and I could really use some help. I am getting confused, but luckily its not due for a couple weeks. Ive attached an image with my work on this question.

Can someone let me know if im on the right track here? I feel pretty confident about the critical value approach, but not so much the p-value approach. In fact, Im at an all stop on the p-value approach.

To determine is proportion of returning customers will be greater than 25% 

Pt A: Critical Value Approach

Ho: p<= 0.25

Ha: p>0.25

a = 0.10, p-hat = 0.3 (x = 12, n = 40, x/n = 0.3) Z-alpha = 1.282

z-test = (0.3-0.25)/sq.rt(0.25\[1-0.25\]/40) = 0.730

If z-test <= z-alpha, we reject Ho. We observe Z-test, 0.730 < Z-alpha, 1.282; do not reject Ho.

Part B: P-value approach

From the standard normal distribution (Z) table, Z-test = 0.730 = 0.2673

From the standard normal distribution (Z) table, Z-alpha = 1.282 = 0.3997

P-value = 0.5 - 0.2673 = 0.2327

If the p-value < Z-alpha, reject Ho. We observe the p-value, 0.2327 < Z-alpha, 0.3997.

My conclusions should both be the same in the end, however I think Ive messed up somewhere along the way. Any help would be greatly appreciated!",Stats,2022-02-22 11:37:16,5
[This](https://youtu.be/KLs_7b7SKi4) may help.,1,suolq4,,Stats,2022-02-17 06:17:22,5
Look into a two sample t test or a nonparametric alternative.,2,scalhy,"Hey all,

Looking for some help with statistical tests if someone good help me!

I'm doing an experiment comparing microplastics in 2 different beaches. The samples are in milligrams per kilogram (mg/kg), I have 90 samples in total, 45 from each beach (3 sample sites per beach with 15 samples taken at each site). One beach (45 samples) is near a road, whereas the other beach is not and I want to see whether the road makes a difference on microplastic concentrations.

What statistical test would you recommend in this scenario? I've looked at Pearson's R-Value test but I'm just not 100% confident that is the correct one.",Stats,2022-01-25 03:24:50,2
You can use sumifs on excel with critera on time(months) to capture  the periods,1,s990aq,Want to analyse average percentage change of call volume for period of Jan - Apr vs May - December. I have absolute numbers for each month. How do I do it?,Stats,2022-01-21 04:49:01,3
"Please don't.   There was a phase maybe 2 yes ago where every fucker made racing bar graphs of every fucking thing possible.  Anything that had a time axis became a racing bar graph.    Want to see stats on countries with most beer bottle caps? Racing bar graph!  Wanna see stats on how many fidget spinners have been rammed down kids throats by angry dad's?  RACING BAR GRAPH!  

The don't contribute anything to this sub. 

I used to block the spam farms that used to churn them out on this sub and others but as soon as you blocked them, more under a different u/. I quit half a dozen subs because it infuriated me.",1,s45lhl,,Stats,2022-01-14 16:18:05,2
"You should check for normality. But most parametric tests (eg t test, ANOVA) are robust to violations of normality. If you're worried, analyze with parametric and non-parametric tests and see how, if at all, the results differ.",1,rhug9v,"This might be a question I am overthinking a lot. I have a dataset of ~60 participants. Using a much, much larger dataset, I transformed each individual participant's values into z-scores based on some key characteristics. However, the participants in my study have a disease state and those in the larger dataset are healthy normals.

As I am comparing individuals within my study (i.e. 40 vs. 20 participants), should I be using parametric tests exclusively because all the data have been transformed into z-scores? Or should I still be checking for normality in each variable because I don't know if my specific population (the disease state) follows normal distribution for each variable? Thanks for your help.",Stats,2021-12-16 09:35:48,2
"t-tests and the like are used to decide whether populations have different means and those population means are unknown. The t-distribution gives us an estimate of uncertainty that the sample mean is equal to the population mean. If you *truly* have the entire population, there is no need to use inferential stats; you would just calculate the population means and directly compare them.

But consider: do you *really* have the entire population? You're probably interested in making inferences about unseen observations, such as whether a department will be more truant in the future. Those unseen observations are also part of the population. You can also think about the truancy rate as a random process generated from the individuals in that department. They were late some percent of the time, but if this were an experiment that could be repeated, you would expect that percentage to fluctuate over repetitions.

In other words: just treat them as samples and use t-test, ANOVA, etc. It's very rare to actually have the entire population of interest.",1,rgjfgr,"I've got data for the store I for which I work. I'm looking at absenteeism by department, and I've got those numbers pulled for the past 12 months. For example, I've got in the grocery department, in August, there were x scheduled shifts, there were y absences, and I've got the percentages of y/x. I'd like to know if the absent %, by department, is not random, i.e. is one department consistently more truant than another, or was one month worse, etc. Can I still use like a t-test or a chi square even though I technically haven't sampled anything but am using the whole population, instead?

I took a quantitative research class in grad school and showed up for like half the classes (I was in liberal arts, wth did I need statistics for, right?), and now I regret that decision, lol. It's like I half remember half of the things. 

To that, do any of you know of a good book that covers this sort of testing, etc., that's not a textbook, or if it is a textbook, not one that's obnoxiously expensive?",Stats,2021-12-14 15:44:08,1
What are you interested in? Politics? Sports? Demographics? It sounds like you can choose literally anything.,2,rc1i6f,"I am studying a higher certificate in statistics and need to identify a research problem. Does anyone have an idea? It needs to be a descriptive research problem using descriptive research methods and data collection by means of a questionnaire.

Anything would be helpful, I'm out of ideas...",Stats,2021-12-08 14:30:36,4
"It might depend on what your hypothesis is. I would probably use boxplots of the mean likert rating by session, and you could use linear regression to see if the response changes over time.",1,r8vv96,"Hello, I am working on research for a case study. I have a single question session outcomes rating ordinal likert scale. I want to compare the scores across six sessions. What analysis would be best to use? & What would be the best visual representation for this data?",Stats,2021-12-04 11:32:38,2
What's the command/question?,1,r7ez99,I am stuggling with a R command that I need for a word problem. Thanks so much!,Stats,2021-12-02 12:19:51,6
I don't think you can do ANOVA on sheets or Excel. You can do a t-test. If you're trying to learn stats I highly suggest using dedicated stats software like R or SPSS.,4,r6nmvk,Okay so I know google sheets is awful and I should have used excel but does anyone know how to do ANOVA on sheets? It’s a school account so I can’t use plugins.,Stats,2021-12-01 13:00:03,1
"It’s important to consider what the statistic actually says, and whether it pertains to your question. 

1. What are the odds if you marry a college educated woman, that you will end up getting divorced?
- The statistics in the paper provided do not make any kind of statement about the likelihood of divorce, only the proportion of divorces initiated by women. So, you can not infer this from what is provided. 

2. What are the odds a man initiates a divorce amongst all married people. 
- This can easily be found using a proportion given in the paper provided. Perhaps re-read the information in the paper. It might help to draw a venn-diagram.

Other questions to consider: 
- Do these proportions include same-sex marriages/divorces? 
- If so, what proportions are two women and two men? 

I would assume they’re not included, but it would change things if they were and your questions only consider hetero marriages/divorces.",1,r4qc9n,,Stats,2021-11-29 00:25:13,2
"""help"" or ""do my homework""?

Explain what you've tried and where you're confused",4,qzf30u,,Stats,2021-11-21 23:34:09,1
"“Team average rank” is made by averaging their Total offense rank, pass offense rank, rush offense rank, total defense rank, pass defense rank, and rush defense rank.

“AMV” stands for average margin of victory. 

For “record” teams start at 0, and every loss is ten points.

“SOS” or strength of schedule is based on how the committee favors conferences.
SEC - 10
B1G - 15
Big XII - 20
ACC, PAC 12, Independent - 25
Group of 5 conference - 35

The Avg. is an average of all of these numbers/rankings.",1,qnox1d,,Stats,2021-11-05 17:44:36,1
"So, have a Bland-Altman plot comparing two measurement tools measuring the same things. 

It appears there's a relationship based on the actual grade of the ""New"" measurement system, I'm thinking that this would best be corrected by applying a linear correction factor to the ""New"" System given that we're looking to augment our old system (New one is capable of making many millions of measurements and has no turnaround time from the lab... better faster decisions, the lab is the gold standard, there is likely some sampling error in the lab values however based on how samples are taken that would show up as a consistent difference in the means)

The mean difference is well below a number that would be of concern, however the slope of the suggests to me that I need to apply a proportional factor to ""Calibrate"" the new to the old, but that the new detector can likely be used in place of the old one when this test gets repeated with the corrected factor (ULA/LLA should be \~+/- 3% to be acceptable

&#x200B;

Am I thinking correctly here?",1,qkinno,,Stats,2021-11-01 10:11:34,1
https://en.m.wikipedia.org/wiki/Central_limit_theorem,2,qiftz0,hi 🙋🏻‍♀️ i need some help. i'm having trouble grasping the concept of this,Stats,2021-10-29 08:50:00,3
I did a bland Altman plot to test statistical variance between to measurements one being a “standard measure”.  Sounds like this is a similar situation?,1,qhr8y6,"So, I've got two instruments. One measures a mineral content by gamma rays, the other by XRF. 

I can only sample each location once, and while I can get thousands of data points from the gamma method, the XRF can only be used once at each location (Destructive sampling)

I suspect the XRF is subject to high sampling error, plus the known machine error as the collection is done by hand wheras the gamma ray instrument mineral content calculation was developed empirically, but essentially follows the theoretical with an offset for background radiation and a coefficient for the actual detector efficiency. I know that my Stdev on the error of repeated XRF data is at least 4.2%

Being mineral data where there are few ""Low Values"" in the deposit, the distribution has a non-normal distribution with a fairly heavy negative skew

**I am trying to find out if the Gamma measurement is ""Good Enough"" to replace the XRF method**

The shapes of the graphs spatially look very close and pass the eyeball test for the most part

I have looked at  [Bapdf.PDF (york.ac.uk)](https://www-users.york.ac.uk/~mb55/meas/ba.pdf) and kind of understand the general gist of things. 

Given that I've got paired data, my first instinct was a 2 sample T test, of course that assumes normality. There's a relatively low number of samples n=60, but probably not enough to be comfortable using a paired T test. 

Also looked at some scatter plots, for 3 of the 4 areas sampled there is a decent correlation between the two methods, the 4th shows a very weak correlation. (0.755, 0.861, 0.680) and 0.138 for the bad one. Even removing outliers the 4th one does not improve. 

I then looked at the errors between the two measurements and performed a Wilcoxon Signed Rank Test on them. I am suspect because I would expect the means to be same and the medians to be different, but the Wilcoxon makes an estimate of the median.

Trying a paired T test, despite the non-normality gives results very close to the eyeball test. 

I'm leaning towards just comparing the Bland paper, if the S.D. on my XRF samples is 4.2%, then if the 95% CI for the XRF-Gamma falls within the 2 S.D. of the XRF, I can surmise that the gamma system is at least as good as the XRF system.

**Is my approach sound? Is there something super simple i'm missing here?**

**Data of one good match and one poor match**

Good:

002122R	002122C

26.9086	31.6112

25.9372	26.0424

20.1626	21.3552

27.7477	24.8304

24.9726	28.9558

26.5295	23.6312

27.0672	34.2722

28.2661	23.3552

27.2073	25.7270

22.1429	26.0744

24.2849	19.7932

26.8091	26.3094

24.6101	23.6528

25.3932	21.0662

23.1220	17.4122

27.2879	23.9052

26.9289	29.4108

21.8905	27.7452

27.6540	19.5088

23.9503	30.4582

28.9496	24.1838

24.3084	32.2448

28.2911	22.9320

29.8437	32.0580

24.6758	24.3406

21.6676	22.4862

23.3531	28.5168

27.5150	34.1026

25.0211	29.1928

26.6565	21.5778

&#x200B;

Bad:

002124R	002124C

27.6723	19.7418

26.4039	24.9818

25.2982	30.6018

25.3024	23.4682

27.4754	30.2016

24.1571	23.4596

26.3391	30.8926

22.4909	27.9496

22.5801	27.0789

26.8395	26.3554

22.6395	30.5364

27.9441	26.1996

22.6540	24.6990

23.4946	35.3808

21.5055	29.3768

26.4529	28.9442

26.9598	26.3082

25.6962	24.7870

23.9282	27.8298

19.0391	21.3234

18.9592	20.0518

20.0776	32.1854

17.1199	28.8314

25.0054	34.7550

24.3322	29.7776

26.8774	17.6990

22.9232	16.0454

21.4934	28.5820",Stats,2021-10-28 09:50:16,1
Maybe Chi Square?,1,qfdls8,"Hi all,

I have some data relating to ecology where we are interested in the levels of disturbance (low or high) and the relationship to diet in birds (fruit-eating or insect-eating).

I have specified two variables as 'forest' and 'diet' and given each a binomial 0 or 1. For forest, 0 is low disturbance where 1 is high disturbance. For diet, fruit eating is 0 and insect-eating is 1.

I am unsure of how to visualise statistical significance of levels of disturbance affecting the diet of birds. Any help?",Stats,2021-10-25 03:19:33,2
Terms/topics?,1,qeowsz,"Does anyone who visits this sub do gig/contract work? Or can anyone recommend someone for me? I have a specific question I’d like guidance/help with. Gave it a shot revising stats resources I could find on the internet but it’s been so long since I took a stats class, zero chance I get it done lol. Any answers are appreciated. Thanks!",Stats,2021-10-24 02:39:18,2
What do you mean by “information on many observations”?,1,q7h1h8,"How do you call a dataset of this kind (i.e., one where you have information on many observations in one specific year, 2004 in this case)?",Stats,2021-10-13 11:10:47,1
Possibly two-way ANOVA.,1,q6ysr4,"I've got some count data that I need help figuring out that to do with it.

So I've got some group count data for three different methods: 1. Photos, 2. Aerial Surveys, and 3. Boat Surveys.

Groups are defined and placed into one of the 4 categories:

1. Calves and Adults - CA
2. Juveniles and Adults - JA
3. Calves, Juveniles and Adults - CJA
4. Adults only - A

I want to test if there's a difference in group size across for each group type (CA, JA, CJA, A) within each method. I've done a Kruskal-Wallis test to do this... not sure if there's alternatives.

Can I use a Poisson Regression ?

Additionally I want to test if there are differences in group size across for each group type (CA, JA, CJA, A) across each method (Photos, Aerial and Boat surveys). I've done a Friedman Test to do this ... not sure if there are some alternatives or if I okay with both options.

How do I really test the nested component for this question?

Thanks so much",Stats,2021-10-12 16:50:28,1
"Not much difference in the color:  
\#39d0ee color RGB value is (57,208,238).

\#2acaea color RGB value is (42,202,234)

It's probably just a difference in the eye dropper tool and how it picks up the image color from the display, including the brightness and temperature color of your monitor, assuming that is part of the software.",1,q0qfhz,"Take the bar graph below as an example:

    ggplot(iris, aes(x = Species, y = Sepal.Length,)) + geom_bar(stat = ""summary"", fill = ""#2acaea"")

I am trying to use the colour #2acaea. But when I use the eyedropper tool in Powerpoint on the resulting image, it says the colour is #39D0EE. 

But, if I use the eyedropper from this [website](https://imagecolorpicker.com/) for example, it does say the colour is #2acaea.

Which one is right, and why the difference?

Edit: Looks like exporting the image with `png()` fixes the issue. The Powerpoint eyedropper hex not matches the intended one. This problem only applies to the plots as they appear in the R window. Any idea why?",Stats,2021-10-03 13:47:58,4
"Without looking at distributions of data it is hard to say what models would be helpful. 

How are you measuring eye inflammation? More technical information on the data would be necessary.",2,pywbzq,"Hello, 

This seems like a great place for some much needed stats help. I am working on a project but seem to have gotten myself into a pickle. I need to fit patient data to some sort of predictive model. 

I have been measuring eye inflammation ( I ) over several months and watching how it changes with the frequency of medicine ( F ) for each patient. Over time, the eyes start to heal and require less frequent medication. 

The medication is expensive and if we can calculate how much people actually need over time we can save them a lot of money. 

How would I calculate a single model that explains I in regards to F over time?

Thank you!",Stats,2021-09-30 17:13:40,7
"If you are conducting a hypothesis test or confidence interval you choose the significance level. Your sample size will be important to your margin of error— the width of your CI. Additionally, sample size will effect the power of your study— the ability to detect a difference if one exists.",2,pxjgl0,"This may be dumb but I thought significance level was choses by whoever was conducting the test. Thus, SL would be independent of sampling size since you could arbitrarily choose a SL of .05 or .10 . 

My interpretation of the question may be wrong but in the case that sampling size does affect SL, I would have no idea how to explain it. 

Any help?",Stats,2021-09-28 17:11:36,1
"I have tried determining a cut off using the 4/(N-k-1) formula. The cut off would then be about 0.006. This means that like 120 would be classed as influential outliers (a significant proportion of people that would then need to be cut out). I hear that most researchers use this graph format to determine which cases are actually more influential. I would say the first 2 highest cases are obviously outliers, but what about the rest?",2,psiecp,,Stats,2021-09-21 06:09:20,8
"I may not be understanding your question, but classification analyses like Latent Profile (or Class) Analysis can tell you if multiple distinct groups or one single group fits your data best.",1,owvklf,"I have a couple of thousand data points, and I need to test whether they're dispersed randomly or in groups along two axes. What is the best test for this? I feel like it should be obvious but I'm not finding exactly the right thing by googling.

Thanks!",Stats,2021-08-02 21:08:26,2
"Government data maybe? Every state and country tracks data, some even release it! Covid or something would be great data for slr, mlr analysis. Another idea is price of graphics cards in relation to price of Bitcoin. All kinds of low hanging fruit or there if you know what to look for and where the data is. As for government https://www.in.gov/health/data-and-reports for example. I doubt they need original ideas, they likely do need different data than examples you can find the solution walked through for already. Government reports are hit or miss because they might exclude outliers or curate data, but the sources are there and if your results are close then you succeeded. As for graphics card prices vs bitcoin's, you shouldn't have a hard time finding that on your own.",1,oll7zq,"Hi Everyone,

My professor is being kind of a stickler and wants us to find data online to create our own linear regression models. He specifically said not to use sites like Kaggle and to find the data ourselves. I cant seem to find anything with a good enough continuous dependent variable to use. Do any of you have any suggestions for sites with queries or dashboards i could get good, useful data from? Any help would be greatly appreciated, thanks!",Stats,2021-07-16 10:09:29,4
Id round up since its .819 the nine makes you round up,1,oggpmj,"I'm getting an answer of 0.819, and there are multiple answer choices for 0.81 and 0.82. ARGH! Which one is correct? THANK YOU!!",Stats,2021-07-08 14:17:45,5
[removed],1,o9xxqs,I need to calculate the standard error from two samples. One has standard deviation 20 with a mean of 7 and other has a standard deviation 12 with a mean of 5. What is standard error?,Stats,2021-06-28 18:18:31,3
Legitimate Prof has helped me. I can vouch for professorgeorge90@gmail.com. From papers to Graduate Stats.He is the real deal. Find him on Dischord @prof George or email. Half up front then the other whe. done. Uses PayPal. Never had an issue,1,o5cy39,,Stats,2021-06-21 20:13:35,2
"As always, it depends. Need more information on the data format and intent of analysis",1,nyrsxk,"Hi guys

Would you be able to do so on single figures given to you by a hospital each month? For the past two years? Or would it be pointless?",Stats,2021-06-13 01:20:14,2
I can help you around with the concepts but cant do your hw if thats what you are looking for,1,nqwvx7,"Hi - I am looking for a stats tutor to help with a probability course in a masters in statistics program.  I just need someone I can think things through with and check my understanding on various topics.  I learn best when I can talk through things.  Please let me know if you are interested in some zoom tutoring around these topics: 

Set theory

Random variables

common distributions 

Transformations and approximations

joint distributions 

limit theory",Stats,2021-06-02 14:29:27,4
"I have quite good experience in stats
Lmk if you need help",1,n6ea1u,Looking for someone who understands and knows stats to help me with my stats homework questions. Message me or drop your snaps if you are interested,Stats,2021-05-06 11:39:01,2
"Linear with high variance up to about 50, then random after that.",2,mvu4ui,,Stats,2021-04-21 18:15:15,11
Cross validation?,1,msan21,"Long week at a new internship where I'm a bit out of my element statistically and the boss is out today. I uses Rstan to fit a model predicting parameters of a normal and mixed normal distribution to my data. I have parameter estimates now, and I graphed a posterior predictive check to see how well the models can approximate the original data and it looks good. I am drawing a blank on how I would quantitatively say which model (the single normal or the mixture) fits the data better. Any advice would be appreciated.",Stats,2021-04-16 12:32:16,1
"Edit: I'm assuming this isn't homework. If this is homework, the answer is probably a much simpler ""reject H0 because p < 0.05"".

Recapping to make sure I understand: you have several different test groups (barefoot + a few different kinds of shoes) and several response measures you were looking for differences in, of which jump height was one. The ANOVA F-test with jump height as the response came back significant, and pairwise t-tests revealed differences between barefoot and other shoe types but no differences between any two other shoe types. Is that correct?

If so, what to do from here isn't really a statistics question, it's a scientific question. Is it interesting to you that wearing shoes changes jumping ability over being barefoot? If so, then you're done. You have an answer to your question. If not, it may be that you were really only interested in the differences between shoes and only included barefoot as some kind of a control for good measure.

If you were really only interested in the differences between shoe types, then you can do one more thing: remove barefoot, and run the ANOVA F-test again. It's mathematically possible that an F-test can have a p value indicating significance but that pairwise t-tests all have p values indicating a lack of significance. You would interpret this as ""there is probably some difference among all of these groups but I can't say specifically which groups are different"". Removing barefoot allows you to use the F-test to conclude about only the differences between shoe types, instead of having barefoot possibly give you an uninteresting positive result.",1,mqshes,"I don't know where else to post this so maybe you lovely people can help me.

So I am trying to find out if Different footwear types have an effect on Jump Height, Velocity, Power and Force. The Hypothesis is that different footwear has an effect on Jump Biomechanics.

I ran an ANOVA and found that there's a significant difference between jump height when barefoot compared to the other footwear conditions but no significant difference anywhere else.

Do I accept or reject my hypothesis?

I've spent so long writing that my brain just cannot compute anymore

TIA",Stats,2021-04-14 08:05:36,2
"As /u/just_start_doing_it said, you probably don't need to worry about age related literacy improvements over seven days.

But, let's say you were testing something like an alternative 3rd grade curriculum, and there was a year between pre and post tests. In that case, you could use a two sample t test where one sample was the pre-to-post improvements of the intervention group and the other sample is the pre-to-post improvements of the control group.

Edit: I *think* you can do this with a two way anova, the effect you're interested in would then be the interaction between group and time.",2,mou0i2,"This is frying my brain. I just don’t get it. Perhaps someone could advise me on what stats test to use? I think it is a two-way mixed ANOVA. 

So here is the problem:

Say I want to test an intervention aimed at improving child literacy. 

So I test the kids literacy before, then give them the intervention and test again. To compare these, I’d use a paired samples t-test. But, we know that kids literacy improves as they age anyways. So I want a control group. 

So on day 1, I test both group 1 and group 2. Then, I give only group 2 the intervention, and on day 7 I test them all again. 

I’d then use a two-way mixed model ANOVA right?",Stats,2021-04-11 09:34:17,2
I dunno. Some sort of regression?,2,mnrdiz,,Stats,2021-04-09 14:41:37,4
"You shouldn’t. But be careful because people use test and validation interchangeably and it can be confusing. 

You should do all your training/tuning on the training set. This can be a standalone set of data or, more likely, the result of resampling (maybe even nested resampling). 

You should do all your model comparisons on validation set(s) (which some people call test set) - again this can be a standalone set of data or many (which match up to respective training sets) if you’re using resembling. 

Then on your chosen model you do the final accuracy assessment on your test set. You (should) *never* then go back and change anything or you’re “leaking information” from your test to your model selection/training/tuning.

You must imagine your test set is some future data you’ve not yet acquired, and you want to know how your model will perform on this new data. If you use information from this data to inform model selection/tuning etc, then this isn’t really future data and you’re not really assessing accuracy on future data - you’re assessing it on the process of training etc on known data.",1,mgl66y,Why should I use test error instead of training error to compare which statistical model is best?,Stats,2021-03-30 10:23:52,2
Likely Mean because scores should be approximately normally distributed.,1,me5fo9,Which one would yield better results for ACT or SAT scores? Mean or median?,Stats,2021-03-26 20:13:47,3
"I recommend reading this page. It's helped me with this very problem. [http://davidakenny.net/cm/mediate.htm](http://davidakenny.net/cm/mediate.htm)

The important part is this:  
 

**The Steps**

            Baron and Kenny (1986), Judd and Kenny (1981), and James and Brett (1984) discussed four steps in establishing mediation:

**Step 1:**  Show that the causal variable is correlated with the outcome.  Use Y as the criterion variable in a regression equation and X as a predictor (estimate and test path *c* in the above figure). This step establishes that there is an effect that may be mediated.

**Step 2:** Show that the causal variable is correlated with the mediator.  Use M as the criterion variable in the regression equation and X as a predictor (estimate and test path *a*).  This step essentially involves treating the mediator as if it were an outcome variable.

**Step 3:**  Show that the mediator affects the outcome variable.  Use Y as the criterion variable in a regression equation and X and M as predictors (estimate and test path *b*).  It is not sufficient just to correlate the mediator with the outcome because the mediator and the outcome may be correlated because they are both caused by the causal variable X.  Thus, the causal variable must be controlled in establishing the effect of the mediator on the outcome.

**Step 4:**  To establish that M completely mediates the X-Y relationship, the effect of X on Y controlling for M (path *c'*) should be zero (see discussion below on significance testing).   The effects in both Steps 3 and 4 are estimated in the same equation.",1,mbtqu8,"Hi all!! Very new here so sorry if this is the wrong place to post. 

I am currently working on my thesis and am a little confused about how to interpret results.

Firstly I did a stepwise multiple regression.

IV: Body Dissatisfaction, materialism, and internalization (of thin-ideal)

DV: compulsive buying

Internalization was excluded in the regression and BD and Materialism were both included and accounted for 20% of the variance in the final model.

Do these results mean that I cannot do a mediation analysis with:

Internalisation as the IV

Materialism and Body Dissatisfaction as mediators

Compulsive buying as the DV?

When linear regressed together and alone internalization and Compulsive buying are significant.

I ran the mediation and Internalisation is a significant predictor of variance in body diss which is significant to compulsive buying. internalization is significant to materialism which is significant to compulsive buying but the direct effect of internalization on compulsive buying is not significant.

What does this mean? / Does anyone know of any references I should look to for a bit of clarity?. Any help would be greatly appreciated!!",Stats,2021-03-23 18:02:43,2
"All measures we make in the real world as discrete because our precision can only go so far. However, if there are more than a handful of possiblevalues, I'd treat it as continuous",2,mbjqrn,Is this discrete or continuous?,Stats,2021-03-23 10:20:35,1
"Words may be used differently in different contexts but ""regression line"" often refers to a graphical representation, while ""regression equiation"" is the algebraic expression (something like y=2x+1). ""2"" and ""1"" would be the regression coefficients in this case.",1,mb5hxm,"Hey could someone explain the difference between the  
Regression line  
Regression equation  
Regression co-efficient in excel  
Thank you",Stats,2021-03-22 20:27:49,1
"I would call it a 3rd or 4th degree polynomial, if I confirmed/ calculated the regression.",2,ma0dxh,"Hi.  I'm trying to find a way to describe the relationship between these two variables and the best I can come up with is ""noisy, sigmoidal"", as I could roughly draw a 2-bend, S-shaped line through the points.  I don't need to go into detail about the relationship and all the various ups & downs; I'm just looking for a brief term that sums it up.  
Cheers, J

https://preview.redd.it/xm9m7bzcreo61.png?width=741&format=png&auto=webp&s=f5b5474cf062b9c98242067a6c852aaec1f27126",Stats,2021-03-21 09:34:43,3
"When I'm stating the average (mean)  I usually include either the (+/-) standard deviation or 95% confidence limit.

Other than that, what do you mean ""correct"" your mean?  Why would you want to ""correct"" it?",2,m4wok0,"Hi.  Do you use any measure to ""correct"" the mean? Is there any measure in the literature? If so, I'd be glad to read some papers.

Very basic alternatives I thought about:

* Measure = (mean + median) / 2
* Measure = mean \* (1+ (median - mean) / mean))",Stats,2021-03-14 08:04:18,8
"How big of a sample size do you have for each temperature-salinity combination? By summarizing your ""10 lived, 10 died"" data into just ""50%"" survival rate, you'd be losing valuable information.

You may want to use a classification model like logistic regression. It will allow you to assign a ""probability of survival for an individual"" at each of the temperature-salinity combinations.",1,m1af1y,"I have the average survival rate of larvae measured at three different temperatures (12C,20C & 28C) combined with three different salinities (5, 15 & 30) over the course of 21 days. I have nine figures in total (i.e. survival rate over 21 days at 12C with salinity of 5, survival rate over 21 days  at 12C with salinity at 15, survival rate over 21 days at 12C with salinity at 30, and so on...). My question is - what stats test should I use to analyse the results considering I'm looking at effects of both temperature and salinity over a certain period of time in the same test?   


I hope that made sense!",Stats,2021-03-09 09:42:24,3
It may be that canonical correlation analysis could be helpful,1,m0r29w,"hello, 

Basically, I am doing a research paper for a class in which I am measuring the association that personality has on certain ""risky"" or cautious behaviors during covid, what I have in mind is assigning point values to the yes or no questions regarding the behaviors and comparing them to the already number based personality metrics.  my question is, what the hell do I do with the data, I have already come to terms with doing a simple correlation between the two scores to see if higher scores in x personality trait mean a higher response of yes in the risky behaviors department.  my question is what other methods of statistical analysis would be useful in my situation?",Stats,2021-03-08 14:48:16,1
"TL;DR make more than one visualization of the same data.

You might have more agreement between you and your peers using a visualization other than a histogram, such as a kernel density estimate. If you can't do that, try making several different histograms.

Histograms are sensitive to their particular choice of bins. For example, if you combined pairs of adjacent bins into one, the distribution would look unimodal. If you divided each of these bins in half, there might appear to be more than two modes. If you shifted the cutoffs between bins half a bin to the right or to the left, the dip in the third bin might still be there or might disappear.

Every program that makes histograms has some heuristic for determining the number and locations of bins, but there's more than one way to do it.

Kernel density estimates are sensitive to the kernel width, but imo not quite as much. And just like you can make more than one histogram with different bin choices, you can make more than one kernel density estimate with different kernel widths.

Looking at the same data with multiple histograms or with a histogram and a kernel density estimate will give you a better idea of whether the dip between peaks is a fluke of this particular visualization, or a feature of the data.",1,m0d7ef,"I'm confused as to whether the shape of this histogram is bimodal or unimodal. I've had 2 people tell me it's bimodal, while 2 others said it was unimodal. If it were bimodal, I would think that the 2nd peak would be a bit larger, which it is not - therefore I don't think I'd classify it as being bimodal? But another opinion would really help to consolidate my thoughts!

&#x200B;

https://preview.redd.it/hdv96a3oasl61.png?width=1018&format=png&auto=webp&s=60cec58ce90fc2679db1cbc90780565c4d65e7d1",Stats,2021-03-08 03:47:52,4
you just have to,1,m0dqpi,,Stats,2021-03-08 04:26:59,5
"How are you measuring polyphenols? Mean numbers in each groups? If so, T-test would work. If you're measuring something a bit more involved, then it might be something a bit more complex.",2,lwr5kf,I’m looking at the differences in polyphenols between organic and non organic apples in 2 varieties.  And I have no clue which statistical test I require. Any help would be appreciated,Stats,2021-03-03 03:56:40,1
"Strange request, but I'd like to compile my own heat map table with the data of player characters I've recorded, but I'm at a loss for what exactly this is representing, and what the data points represent. Any help is appreciated.",1,lvsrbw,,Stats,2021-03-01 20:26:47,4
Why shouldn’t it include outcomes where only 1 number is rolled 10 times? There are no outcomes where you don’t get any repeats. So what’s the difference between getting 9 repeats or getting 10 repeats? Why shouldn’t 10 repeats be allowed?,1,lufrlq,"tl dr had an exam, got that question, now arguing with friends on result, wanna share with you guys

Question:  
""Dice was rolled 10 times and results were written down in ascending order. How many different outcomes could there be?""

Running a python code with all possible outcomes gave me a count of 3003 numbers. The problem here is it included 6 outcomes with the same number (e g 111..111,222..222, etc). Based on wording in this question should those be included? 

If this whole answer is wrong, what should it be ?",Stats,2021-02-28 06:13:49,4
"You have to do a two sample z test for proportions.  Find the test statistic ( z value) and then look up that value in the standard normal table to find the p-value.  Since it is looking for different it is a two sided test (for the p-value).  And then if the p-value is less than 0.05 (or whatever the level of confidence is) then it is statistically significant and they are different
Pm if you need more help",1,lsdvpn,,Stats,2021-02-25 11:55:01,2
"Maybe not ELI5, but ELI20 on a calculus course.

Two functions are said to be orthogonal if the following is true:

Let f1 and f2 be functions on a arbitrary interval.

If the integral of the product f1\*f2 over an interval is zero, the functions are said to be orthogonal over that interval.

so

int(f1\*f2) dt, t=a..b.

if that integral is zero, the functions are orthogonal.

&#x200B;

&#x200B;

As for what it means.

In cartesian coordinates (x,y,z), the basis is three vectors that are all at mutually right angles.

These are a complete set, a minimal set of vectors that can represent all points in cartesian space.

&#x200B;

In the same way, you can have orthogonal functions. A set of orthogonal functions can be chained together to approximate any function that satisfies the dirichlet conditions.

&#x200B;

So if you have one polynomial already, and want to find a function that is orthogonal to it, you will have to solve the integral function with respect to one of the functions, with the RHS = 0.",2,lqejpx,"Hi,

I am trying to improve the predictive power of a multiple regression model. One of the scatterplot show a quadratic relationship between my Y and the variable and I would like to add this to the model.

So far so good.

The problem I have, is that in R it is always suggested to use the function poly, which according to the help function of R, is a function to get the orthogonal polynomial of something:

poly(x, degree)

Although I can add the variable manually to the model without using this function (which is very easy, I have just to add:

\~ x + x\^2

I want to understand what an orthogonal polynomial is and why it is suggested to use this function in R instead of ""normal"" polynomials. Please explain like I am 5 :)",Stats,2021-02-23 02:41:06,7
"How much they make per policy:
22 (this is the premium of policy) - (300000*0.00000045) (this is the cost of payout) 


That gives profit per policy, so to get the profit from multiple policies just multiply the unit profit by the number of units",1,lnha70,,Stats,2021-02-19 07:35:03,1
Have you tried asking your professor for help yet?,1,ll621y,https://ibb.co/r272dww,Stats,2021-02-16 08:23:29,2
"there is a story as to why i started this:

a friend noticed i talk about pizza a lot, and challenged me to keep track of how much i actually eat. so i started July 1st 2019 and initially was just going to do 6 months.   
by the end of 2019, i thought i would do at least july-dec 2020, but my brother encouraged me to do all of 2020.  
now a month into 2021, i am continuing to collect the raw data but i want some statistics.  


(also, my goal for 2021 is to eat less pizza)",1,lgyca0,"this link is a spreadsheet on which i have collected my pizza eating habits from July 1st 2019.   
it is organized a couple different ways, but i have no idea what i am doing when it comes to stats.

 i am hoping that some one can help me organize and present the data, and maybe come up with some trends (historically and maybe even predictions)

[https://drive.google.com/file/d/17wvpXT-rsj342Hl\_SR-Nl2qlr3d6yVs-/view?usp=sharing](https://drive.google.com/file/d/17wvpXT-rsj342Hl_SR-Nl2qlr3d6yVs-/view?usp=sharing)",Stats,2021-02-10 10:11:24,1
"I think density function could also refer to other types of functions besides probability, but yeah I'd read them as being interchangeable in the same text. If you're taking a probability course, usually it's pdf or PDF as opposed to the cdf or CDF. The CDF is just the cumulative sum of the PDF for all prior points.",1,lg98u0,"Are these two terms interchangeable? The best I’ve been able to come up with to answer this question is this quote from the first line of the wikipedia article on PDFs:

“a **probability density function** (**PDF**), or **density** of a [continuous random variable](https://en.wikipedia.org/api/rest_v1/page/mobile-html/Continuous_random_variable),...”

&#x200B;

which seems to imply these are the same, but would appreciate confirmation.",Stats,2021-02-09 11:03:05,1
"If one variable was just a simple transformation the other, then yes you’d have autocorrelation and would trend towards -1 /+1.... unless the calculation introduced a third variable to transform Y ( say daily exchange rate, temperature, etc.) then  it makes more sense.",1,lcwan9,"I am not an expert in statistics, but I was reading a paper that measures correlations and I just stopped by the study design in that paper as it doesn't make sense to me. The problem is that it studies the correlation between x and y, where in fact x is calculated from y such that: x=y+z

My understanding is that when we study correlation relationship, we look into 2  independent variables. So with x=y+z,  we already know that they are correlated. 

Any thoughts?",Stats,2021-02-04 18:47:12,1
https://support.google.com/docs/answer/40608?co=GENIE.Platform%3DDesktop&hl=en,1,l8nq20,"I am a high school student doing a stats/bio project. My project analyzes heart rate values. I need to bring a bunch of .txt files with timestamp and heart rate values into google sheets. I could just copy and paste, but there are like 100 .txt files, so it would be really tedious. Does anyone know of a script or something I can do? I have like zero coding knowledge.",Stats,2021-01-30 08:16:09,1
"Here's what I've come up with so far:

I counted the number of articles per year and the number of unique journals per year for each of the four years with data.  Then I found the frequencies that a journal had X number of papers published per year, where X is between 1-8 papers.

For example, about 70% of journals have one paper published in them per year, while about 1% of journals get 6 papers published. I don’t actually care which journal it is, only the distribution of papers. I used Excel’s random number generator and weighted it based on these probabilities to simulate a year’s worth of output. 

=MATCH(RAND(),D$3:D$10), where D3:D10 lists the fractional probabilities of each X occurring.

A big input to this is the number of unique journals that get at least one publication in a year.  We subscribe to ~300 journals from this publisher, but for whatever reason only about 130 see a corresponding authored publication from our campus in a year.
Each unique journal has the chance to have 1-8 publications in that simulated year, so as that assumption goes up, the projected number of papers increases as well.

Once I got the model working in Excel, I converted it to Python.  This way, I can easily loop through and simulate 10,000 years of data in 15 seconds instead of clicking through one year at a time in Excel.

[These are the results] (https://ibb.co/N2mX6pZ) of 10,000 simulations for a year with 135 unique journals. The model predicts a median of 195 articles published, with a standard deviation of 11. This means, with these inputs, we have 95% confidence that between 173-217 articles will be published this year (2sd method).

Questions:
Is this reasonable?  Can I model the distribution of papers based on the past four years of data?",1,l285gw,"I have four years of academic publishing data from my campus, and I'm trying to build a model that will take that information and predict future publishing output.  I know I won't be able to be totally accurate, but I'd like to ideally build a Monte Carlo simulation that runs thousands of times and gives me a histogram of results (similar to 538 and their presidential election predictions).

The data consists of ~170 Journal Names and the number of articles published in that journal per year.  I can see that some journals are popular to publish in year after year, while other journals may have a year where no authors from my campus publish.

Journal Name | 2019 | 2018 | 2017 | 2016
---------|----------|----------|----------|----------
Journal A | 6 | 1 | 1 | 6
Journal B | 4 | 6 | 2 | 0
etc

The reason for this post is, now I'm kind of stuck.  I know some statistics, math, Excel, and Python, but I'm not sure how to codify this information into a model.  I think modeling each individual journal with their range and st dev would be a start, but the distributions are not normal and are almost random year over year.  A few journals dominate with multiple articles every year, but then there is the long tail of 45-60% of output with only one article in a journal for any given year.

How would I add other variables that might also predict publishing - grants, usage data, faculty size, etc?  

Any help is appreciated.",Stats,2021-01-21 14:23:54,1
"Is your data ""paired""? As in, the same people looked at both pictures and portraits? Or is the data completely independent from each other?

If it's the former case, you can just get the average difference between fixation time at potraits and pictures, and take that as your dependent variable.

If it's the latter, you can do a simple regression model adding an extra variable indicating if it's a portrait or a picture, then check for significance of the coefficient associated to that variable.",1,ksm9ja,"I want to see if when we perceive a face we look at it differently whether it is a picture or a portrait. I have two indipendent variables 2x3 (picture/portrait and eyes/mouth/background) and one dependent variable (fixation time). I have applied an ANOVA so that I could look at the post hoc comparisons, but I'm not sure it is correct. The sample is very small and it's a preliminary study.",Stats,2021-01-07 13:42:21,2
"Jets officially parted ways with Adam Gase, per source. Gase went 9-23 during his two seasons as the Jets’ HC. Jets now back in market for another HC.

***

posted by [@AdamSchefter](https://twitter.com/AdamSchefter)

^[(Github)](https://github.com/username) ^| ^[(What's new)](https://github.com/username)",0,kq2opx,,Stats,2021-01-03 23:04:09,1
"If you're testing for a relationship, then a one- way ANOVA is an obvious possibility. You're testing the null hypothesis that there is no relationship (effect), against the alternative that at least one group is significantly different to the others.",1,k9gru0,"Hi, if I wanted to carry out a test to determine the relationship between a continuous independent variable (days) and a categorical independent variable (divided into four groups) would I use multinomial regression?",Stats,2020-12-08 17:23:49,2
"Probably means 99 and a fraction... 99.1, 99.2, 99.3 and so on. 

Sometimes when dealing with very large number of samples a 99th percentile can include dozens, hundreds, thousands etc... of data points. Maybe your application needs to filter all but a handful of samples, so you take an ever smaller portion. Since there's no whole increments between 99 and 100, you take a fraction until you're left only with the number of samples you're interested in.

&#x200B;

Or at least thats my take on it.",1,k8p0il,Thai might be a stupid question but I recently founded that I’m 99+ percentile on something. I tried searching but I don’t know exactly what the “+” means. Is there a commonly understand meaning for what exactly 99+ percentile is,Stats,2020-12-07 13:42:16,1
"I don't have much of an idea, although I think it'd help contextualise it for people with more knowledge if you explain the way in which you've recorded ""Stress"" 

The topic sounds pretty interesting though, best of luck!",1,k7n83w,"Hi, I'm writing an article on the correlation between stress and GPA. I sent out a questionnaire to have people record their stress in the fall of 2019 and now as well as their GPA in the fall or 2019 and now. My problem is, I don't know how to find the correlation between stress and GPA with that data. Any help would be appreciated.",Stats,2020-12-05 21:48:49,1
Lol grow up kid,3,k6znwt,can someone do my stats hw for me ill pay you! It's due tonight and you have to score an A or B for me to pay you!,Stats,2020-12-04 19:55:23,2
"Try clustering. In case you need a book, look up Introduction to statistical learning with applications in R by Gareth.",1,k3da7e,"Hey there! Trying to figure out what method of analysis to use to analyze a large sample from many hospitals. I’m new to research, so I’m not sure what method to use here. I’m attempting to see any association between a group of hospitalized patient’s in their symptoms. So, for example, if patients who have nausea also report vomiting. I would also like to add more variables to this if possible so maybe looking at the association between 5 or so different symptoms. I assume there’s some likelihood ratio or something to analyze this properly, but I can’t seem to figure out which one. Any help would be greatly appreciated!",Stats,2020-11-29 10:26:53,2
"Agreed. Some of the videos are neat, but none are stats.",3,k27y65,"Apologies for the negative post, but thought as a (now ex-) user of this sub, the spamming of racing bar YouTube videos is kind of killing /r/stats, r/bigadata and a whole host of others.   I used to enjoy seeing legitimate and thought provoking stats questions appearing in my feed, but if I get pissed off again seeing yet another shit racing bar graph posted on something irrelevant..... It ain't worth my blood pressure.   They're not informative.  They're not statistical.  They don't provoke ideas, thoughts or discussions.  I've down votes them, but to me it just ain't worth sub'ing here or some of the other subs anymore.  

I wish you all well (except the robo-twats who keep posting that crap) and I'm out.  

Peace all.",Stats,2020-11-27 12:33:54,1
I would be interested,1,ju00yp,"Hi, does anybody know of any online reading/study groups for statistics? 

It would be ideal if they were to discuss books like Allen B. Downey's ""Think"" series or David J.C. MacKay's ""Information Theory, Inference, and Learning Algorithms"", both of which can be freely downloaded.

If such a group does not exist, would anybody be interested in starting one?",Stats,2020-11-14 03:32:28,1
Did you get help?,1,jkmpr1,Will compensate please PM ASAP!,Stats,2020-10-29 17:43:05,1
Unidemensionality is usually assumed when looking at empirical data. It's condition however is dependent upon an agreed upon meaning of the variable/construct being demonstrated.,1,jj64d1,,Stats,2020-10-27 10:55:44,2
"The distinction between ANOVA and ANCOVA is an outdated way of thinking, but was useful back in the day when calculations had to be done by hand. 

Nowadays you have computers to make things easy, and there's no reason you can't compute F-statistics on any number and combination of categorical and continuous variables. 

Most analyses performed in scientific literature do not make the distinction. 

For a typical ANOVA analysis, what you want is a ""LM"" (linear model) not a GLM. If your data is not linear in the sense of linear correlation with outcome, often a log transformation or similar can help. 

There's really not much shortcut other than doing the reading and trying to understand what ANOVA does -- otherwise you risk reporting incorrect results to your advisor....

[This](https://www.datanovia.com/en/lessons/anova-in-r/)  looks like a decent intro with R in mind.",2,jbqx3s,"I want to test to see if two continuous variables differ in their response to a continuous independent variable. Someone suggested GLM, but I read that this assumes linearity - my data are not linear, and from what I understand I can't use ANOVA because all data are continuous. I read somewhere that ANCOVA would be an option, but afaik ANCOVA assumes linearity. Also the data are not normally distributed. 

Any help greatly appreciated! 

Thanks in advance <3",Stats,2020-10-15 09:30:10,4
I don’t think there is any moderation on this sub. You should volunteer!,2,j2xtnx,"I swear I've blocked dozens of accounts that spam the same low effort, boring youtube content farms, to the point where I think I miss genuine questions. I don't want to unsub but it's ridiculous.",Stats,2020-09-30 17:04:51,2
Cheaters are not welcome on this sub,1,j1i5bn,If anyone has taken stats before and got a decent grade message me I will pay you to help me out,Stats,2020-09-28 11:49:49,2
"What's the outcome if you put your variables into a data frame (my\_df) and execute the following?

    model <- lm(formula = dep~ind_1+ind_2+ind_3+ind_cat, data =my_df)

I'd expect it to return me a linear model with an ""intercept"" term, a coefficient for one of the numerical independent variables and one coefficient for each of the categories (except for one that will be taken as reference). If this returns an error there may be something wrong with how you stored your data.",2,j1a21o,"Trying to run a regression using R. I have the dependent variable and 3 numerical independent variables along with a categorical independent variable with 9 sub categories making a binary system seem impossible. The model is predicting profit based on sales, discount, quantity, and a sub category consisting of 9 options.... very vague but any help?",Stats,2020-09-28 04:11:30,2
"Yes.  Essentially you are looking at a correlation of a measure with itself, separated by time, assuming that is what you mean.  But there is not enough information here. Are you looking at a correlation between a specific item based measure at different time points, or some other type of measure?

The value of test retest basically indicates the maximum bound of variance that could be accounted for with your measure.  So a coefficient of .6 would indicate that only 36% of the variance in your measure at time 2 could be accounted by your measure at time 1.  This is not very encouraging in terms of being able to replicate the relations with your measure.  In other words, it doesn't appear that you are measuring the same thing to a high degree at these different time points, which is a problem.  Unless of course you would expect there to be some reason why a measure would vary to a large extent over time.",1,iyp2ut," Does low test-retest reliability of a task lead to less reproducible results? If so, could you please direct me to good sources? I have found a lot of contradictory information.",Stats,2020-09-23 20:03:36,1
"Hello.  I am learning about this very topic for my dissertation.  I'm novice in SEM, but thought I would respond since no one else has.  

I believe you need to look into using weighted least squares to interpret categorical variables.   I hope this gets you pointed in the right direction.",1,itcw6d,"Hey guys, 
I'm currently trying to compute a Structural equation model using R. I assessed three independent variables at two different time points and want to know if they improved depending on an intervention participants did / did not receive. Additionally, I have another control group, making it three groups I want to compare. BUT I don't know how! The group would be an exogenous categorical variable. Do I have to use dummy coding and if so how?",Stats,2020-09-15 10:02:28,1
Thanks,1,irty6j,"Hi everyone! Hope you are all doing well.

I am writing to inform that we have introduced a course on ""DATA ANALYSIS"" for beginners. The course will cover various aspects and stages of data analysis beginning from sampling of data to probability distribution, regression etc. It is a free course and videos for the classes are uploaded every second day. If you are interested, please check this page!

[https://www.youtube.com/playlist?list=PLQ6H-ehJcYK7zxo6KYo\_7Ud7EJiHxw1rZ](https://www.youtube.com/playlist?list=PLQ6H-ehJcYK7zxo6KYo_7Ud7EJiHxw1rZ)

We have some other courses running on our channel as well. We would be glad if you could visit :)

Thanks!

P.s Please bear with the audio in the first video. The rest of the videos are all fine. Happy watching :)",Stats,2020-09-13 00:34:19,1
"What you are looking for is a [proportion comparison test](https://www.cliffsnotes.com/study-guides/statistics/univariate-inferential-tests/test-for-comparing-two-proportions) where the ""hypothesized difference"" is 0 (you want to prove the difference is not 0). In the link you have a formula for a statistic ""x"", which follow a normal distribution. If its value is outside the (-2, 2) range, you can say with good confidence that the proportion has changed.",1,ilpj0i,"I'm trying to analyse two simple sets of data. 
Did a person tick a box in one month's Vs did a person tick a box in another month. 
So first month I recruited 56 people 25 ticked yes 44.64%. next I rechecked this in a subsequent week 13 recruited 9 ticked a box 69% how do I comepair these to show if the has been an increase in yes ticks",Stats,2020-09-03 01:21:12,1
You can use anova or a two sample student t test. Either one should give you the same analysis.,1,ibnbxh,"Hi guys,

So I have a statistical problem. I have an experiment with an eyes-open and an eyes-closed condition which was performed in the beginning and the end of an EEG experiment, giving me four conditions: Eyes-open(beginning), Eyes-closed(beginning), Eyes-open(end), Eyes-closed(end). I partitioned each of these conditions in small intervals (30 intervals) and calculated the power spectrum for each of these intervals. I obtained a matrix of size trials x channels x frequencies. Now, I wanna investigate if there is a difference between the eyes-open and the eyes-closed condition by doing an ANOVA (only for a particular electrode let's say). A repeated measurements anova is the way to go here right? Moreover, I am actually only interested in a specific frequency range. So what I did is to average over trials, then selecting only the subset of frequencies I am interested in and averaging over it, then only selecting one electrode. So I obtained one value for each of my 4 conditions (see above) and compared them via a repeated measures ANOVA. Was this correct?",Stats,2020-08-17 14:26:44,1
inflection point,1,hu6gmg,,Stats,2020-07-19 12:29:29,1
2 sample t for means?,1,hphy4m,Ok. Not too hard but I can't think.  I want to compare the mean SAT scores for one college with the mean scores for the US. Which test?  Thanks!,Stats,2020-07-11 14:17:53,4
"[Reddit Enhancement Suite](https://redditenhancementsuite.com) will let you do that, I believe.",2,hm9v2h,"Last few weeks/months, this sub has been trashed by crap content from [u/datavtworld](https://www.reddit.com/user/datavtworld/).  Is there any easy way of block their content so I don't have to leave the sub, but no longer seeing endless reposted spam racetrack visualisations?

Reddit help functions is rubbish at showing how to block content from a specific users in either a specific sub or across the whole site.


UPDATE/EDIT:. Got it!! Woohoo! Much cleaner feed.

I use a 3rd party app mainly but the block function is only really effective in the official Reddit app. I've blocked them there and seeing the effect on all platforms.  As u/Zeroflops points out, select user name and you'll get the pop up functions menu.  Block there.    

Thanks for the help! At last, a whole load of subs become less spammy!",Stats,2020-07-06 08:36:25,8
"It might be the t-value, not the p-value. if your model is Y = aX1 + bX2 + ..., basically, you are conducting a hypothesis test on each of your individual independent variables (x1, x2, etc) to see if the slope is 0. If the true slope is (statistically) 0, we can conclude that there is no relationship between that independent variable and Y (the dependent variable).

The t-value gives you the value on the t-distribution curve, where the null hypothesis is that the slope = 0. The p-value is the probability of being at that t-value or somewhere more extreme.

Therefore, a large t-value means you reject the null hypothesis, or that your true slope for that dep. variable is not 0 (it is a significant variable).",1,hklu4m,"Apologies I’m not the best at statistics.

I’ve ran a regression and I’ve somehow come out a with a p value over 1.0. I’ve read online that o values can’t go over 1.0 how am I getting number a such as 7",Stats,2020-07-03 09:12:39,1
Or learn to use R.,1,hhi2js,,Stats,2020-06-28 10:08:23,1
Elaborate?,3,hh5li0,,Stats,2020-06-27 18:12:10,1
Data source: Gapminder,1,h0jqld,,Stats,2020-06-10 13:42:18,1
Hi can you tell me what you used to make this plot please??!!,1,gvwkud,,Stats,2020-06-03 08:59:17,2
How high of a [mobility score](https://citizenfall.com/2020/07/11/the-journey-of-the-third-passport/) can you get?,1,gqk5i1,,Stats,2020-05-25 14:52:33,1
Sure,1,gka1jb,,Stats,2020-05-15 07:24:24,2
To be fair haven’t we tested a lot more people than the other countries?,1,ghabw2,,Stats,2020-05-10 14:46:38,2
It said China with 100 cases but worldwide 50,1,g7d6ch,,Stats,2020-04-24 10:59:59,1
"Hi there,

There are the base functions `quantile` and `cut`:

    qt <- quantile(sales_df$revenue, probs = c(0, 0.25, 0.75, 1), na.rm=T)
    
    sales_df$revenue_level <-
      cut(sales_df$revenue, 
          breaks = qt, 
          labels=c(""low"", ""avg"", ""high"") )

Hopefully you can see that the `cut` function takes a vector of break-points (`breaks`), which we have fed from the `quantile` function with your specified cuts (quartiles, omitting the median).

If you are having trouble searching for R documentation online, you could try [Rseek](https://rseek.org/), [Rdocs](https://www.rdocumentation.org/) or [this site](http://finzi.psych.upenn.edu/nmz.html) (if you use DuckDuckGo, you can use the bangs !rseek, !rdocs and !cran respectively).

Regarding your analysis, if the ""so-and-so"" (outcome variable) is continuous, you'd likely want to make a box-plot, while if it's discrete/categorical, you'd be left with making a contingency table.

For the appropriate hypothesis test, you'd likely be looking at an ANOVA for the former (continuous outcome) and a χ^(2) (Chi-squared) test for the categorical outcome. This is without knowing much else about your data and desired analysis, so YMMV.

&#x200B;

Edit: I thought this was /r/rstats — that is more appropriate for R related questions",1,g799w5,"I would like to categorize my data column ""revenue"" and categorize these into ""low"", ""avg"" and ""high"" preferably like this: 

low = <25th percentile
avg = between 25 and 75th percentile
high = above 75th percentile

Once I have categorized I want to test wether for example companies with a high revenue are more so and so ... 

I am very new to ""R"" and I cannot find this online. Please, how do I do this? :)",Stats,2020-04-24 07:25:01,3
"I can't find any sources being referred to in this video for why they have chosen these statistics, nor do they seem likely to be based on anything. They also don't seem at all consistent even in what constitutes the end of humanity.  Events like a a supervolcano eruption and engineered pandemic would wipe out modern society but likely not the entire human race, while big crunch would likely happen well after humans completely die out and finish off everything else.",2,fxxw8x,,Stats,2020-04-09 11:35:39,3
"I'm guessing this is for a school project, rather than a publication. If you're at a university, you probably have a student license for STATA or SPSS you can find on your school's website. I'd recommend R because it's free, but I don't like the syntax, and you'll probably figure out the drop-down menus just fine for the first two..

It would've been helpful to get demographics so you might be able to run some regression where you control for certain things like age, race, gender, etc. But basic correlations may suffice, depending on your project.I mean, technically, you could look at t-tests to be super simple.

Always be able to report the demographics of your sample. You'll have the ability to speak to representativeness, interaction effects, etc.",1,fpld8k,"Hey guys, I'm trying to figure out which stats tool to use for a project, and was hoping you might help! 

We've disseminated anonymous surveys via social media which ask wether subjects have experienced abuse (Y or N), and how how they relate to prosocial behaviors (likert scale, 16 behaviors). We're hypothesizing that those who have experienced abuse are more likely to display prosocial behaviors. What are your thoughts? How should I analyze this data?

Thanks in advance for any responses!",Stats,2020-03-26 15:41:58,2
"Nrow(df[df$event == 3,])",1,fp88v6,"I have a data set consisting of variables household ID (HHID), event (i.e. 1, 2, 3, 4), and few other variables. I want to count number of events = 3 (using: sum(event==3) ) for each household.  The sum() function works fine for all households but not within household. How can I do this?

Here is a hypothetical data:

df:

HHID                      event

1                              1

1                              1

1                              3

1                              1

1                              2

1                              4

2                              3

2                              3

2                              4

2                              1

3                              1

3                              2

3                              4",Stats,2020-03-26 01:57:22,2
"I'm trying to do a two way anova using lmer, with local (H,M,L) and larvae (N = no larvae, Y = Yes larvae) as predictors, and the number of eggs laid in certain locals as the response. So, the code for my anova looks as such:

Local.lmer <- lmer(Rafts \~ LocalType+LarvaePlot + LocalType:LarvaePlot + (1|Block), data = Local)

anova(Local.lmer)

&#x200B;

It then produces the table below. My issue is this: based on the difference in the M local when larvae = N vs. the M local when larvae = Y, it looks like there definitely should be an interaction. Is there something I'm messing up, or am I misunderstanding what it means for there to be an interaction?

Type III Analysis of Variance Table with Satterthwaite's method

Sum Sq Mean Sq NumDF DenDF F value    Pr(>F)    

LocalType            5829781 2914890     2    35 10.6420 0.0002452 \*\*\*

LarvaePlot            751000  751000     1    35  2.7418 0.1066929    

LocalType:LarvaePlot  323153  161577     2    35  0.5899 0.5597995    

\---

Signif. codes:  0 ‘\*\*\*’ 0.001 ‘\*\*’ 0.01 ‘\*’ 0.05 ‘.’ 0.1 ‘ ’ 1",1,fo07u0,,Stats,2020-03-23 23:17:17,1
u/Number_Bot !Covid19,1,fmon6w,,Stats,2020-03-21 15:43:54,3
"maybe Elements of Statistical Learning. Take a look at the pdf (legal, from the website of one of the authors), see what you need to know to follow it (maybe not anything) and ask for more resources.

It might help to read something more basic on regression first,",1,fczw93,"Could someone recommend some books or websites to give myself the equivalent of at least a minor in stats? I need to learn more stats than I ended up taking in my formal education, but it feels silly to go back to academia just to take 3 or 4 classes. Mainly I just need to know the data science type stuff, which seems to focus heavily on regression analysis, making good inferences from graphs (I'm horrible at eyeballing things where I come from a theory-heavy background), and whatever would be a prerequisite for those things.",Stats,2020-03-03 12:25:39,3
"According to the [Q4 2019 earnings report](https://s22.q4cdn.com/959853165/files/doc_financials/2019/q4/FINAL-Q4-19-Shareholder-Letter.pdf), choosing to watch **2 minutes** of any Netflix title counts as a ""view"" as opposed to the earlier metric of 70% of a title. I've compared 2 minutes of watch time vs the runtime of some of the longest titles on Netflix. You can find the source article here: [https://urbanpapyrus.com/new-netflix-view-count-method-weird-infographic/](https://urbanpapyrus.com/new-netflix-view-count-method-weird-infographic/)",1,f3f2h7,,Stats,2020-02-13 12:11:07,1
">  Is Pearson readable?

Which Pearson, and when talking about what? 

If Karl, then ... don't believe him on df in the chi-square, but mostly he's solid enough\*. keep in mind a lot of modern stats wasn't in place when he was writing, so some of his writing may seem a bit off for that reason. If you mean Egon, then generally, yes, lots of his stuff is good (Neyman & Pearson's classic paper, for one example).

\*(though with a bit of tendency to claim his work on stuff was original with him when not uncommonly a fair bit of it was actually done by others - some of it maybe he hadn't seen but some of it he definitely had to have seen)",1,f06y47,"I'm trying to re-stimulate my interest in Stats & Probability by studying early works in the field. I'm reading a little Bayes and Fisher but I need more recommendations--the older the better. Is Pearson readable?

Also, in Oystein Ore's work ""Cardano: The Gambling Scholar"" I learned that the earliest modern work of probability is Cardano's ""Liber de Ludo Aleae"" but I'm having difficultly finding a PDF of the Latin original on the internet--any help?",Stats,2020-02-07 00:05:26,1
"Ready for a massive generalization?

Physics treats math as a tool to understand reality.  Stats treats math as the underlying reality.",2,f04ask,"I've switched from a physics job to a stats job recently, and there's something different about the way things are done. I can't quite put my finger on it though. My only thought is that it is a difference in requisite accuracy? I.e. a physicist is looking for THE correct answer to a problem, whereas the set of possible answers to a statistician's questions are broader.",Stats,2020-02-06 20:12:38,1
"It is hard for me to answer because I find your question imprecise, but that's ok and understandable. 

What 'X is statistically significant' usually means is that there is 'statistically significant difference between population mean of variable X and Y (quite often Y being zero, for example in correlation: if correlation coefficient is significantly different from 0, then correlation *exists*). 

So I'm guessing your question is something like:

Is it possible to assess if students taking exams in order: A, B, C, scored higher (or lower) on 'big exam' Z than:
* students taking exams in order: C, B, A (or any other order)
OR
* all students, on average, without knowledge of their 'small exam' order

If what you mean sounds more or less like that, then yes, it's possible. If it is something else, then it's an open question. 
If you can specify which case exactly you want to investigate, it will be much easier to help. Also, what kind of statistical software (Excel counts too) is available to you? 

Cheers!",1,eq1t8t,"Hello all, I’m a 4th year medical student trying to crunch some numbers regarding data for one of the big tests in medical school. My question is: is there a statistical test that assessed for a significance based on the order in which a process is completed?

To put it in context, we have several smaller, subject-focused exams we take before this bigger test, but they are taken in any order. I’m trying to determine if there’s a statistical test that can examine the order such smaller tests are taken to see if there’s an effect on the score for the larger exam. Any help is appreciated!

If this isn’t allowed, please just delete this.",Stats,2020-01-17 08:29:00,6
Hii did you find the data ?,1,eig507,,Stats,2020-01-01 01:29:53,2
"What do you mean by ""total population variance""?

You want to calculate the confidence area around a mean estimate. Why not use this http://onlinestatbook.com/2/estimation/mean.html",1,e721zl,"Hello r/Stats

I am struggling today. I have a problem for which I know there is an easy answer, but my brain is completely jacked up. I have a list of wells, and each well has a series of measurements. For example:

|Measurement 1|Measurement 2|Measurement 3|
|:-|:-|:-|
|10|11|10|
|12|14|9|
|20|14|13|

I am going to use the average of the measurements as my final measure, and I would like to record my 'confidence' in this measure. I figure that wells with a low variance in the measurements will have a higher confidence than wells with a very high variance. Which metric should I be using to measure the confidence?

We can assume that I know the total population variance. One idea I had was to use the p-value of the chi-square statistic.. But I'm not sure that's the most robust method. I can't remember.",Stats,2019-12-06 10:57:47,1
Stats and data science in particular are good skills for jobs these days.,1,e5lr3o,"I’m considering a stats minor, I am in industrial engineering and have a business minor already. I have one semester left before graduation and one class away from a stats minor(it will not be an easy class). I really do not want to do it.. the motivation and senioritis is bad and I have other heavy classes I need for graduation. I have a job already and won’t be using stats heavily.. I just do not want not having the minor to hinder me should I want to change jobs down the road.
Any advice?",Stats,2019-12-03 11:58:24,1
"Hey coolercode, 

&#x200B;

You're in luck! I just so happen to be working on this for one of my PhD courses. Unfortunately, it is in Python, not C++, but I'm sure it will give you a good base for your code. 

&#x200B;

Cheers!

&#x200B;

 [https://github.com/Candylockk/Solving-the-Vehicle-Routing-Problem](https://github.com/Candylockk/Solving-the-Vehicle-Routing-Problem)",1,dta0xk,I got question on how to solve VRP as I do not know how to begin. Need help in C++ coding ):,Stats,2019-11-07 21:18:58,2
"Sorry, this makes no sense as a straight up count, you need to be normalizing by population.",1,dq7i8e,,Stats,2019-11-01 10:34:29,1
RemindMe 2 days.  Population discussion.,1,dpoaio,,Stats,2019-10-31 07:26:46,3
"I'd put all the histograms on a common scale (in fact I'd probably use a dotplot (a stripchart in R) or similar, so I could see all the points)

Do you have any specific questions you want to ask of the data?",1,djrcv1,"Those are frequency plot of 3 classes results to one of my assessments... I'm trying in make some quick statistical analysis in order to have some information on how each class did... But I'm no expert... what information could I see by doing this? what would u say? 

I think this may be a stupid question... But I'm sure if this is really useful at all..

thanks!

https://preview.redd.it/b9sjv84lact31.png?width=712&format=png&auto=webp&s=f8ed5251f9c6c8b93178c6a1fe97176dc9ebc080",Stats,2019-10-18 11:07:44,1
India and china re gaining its old position,1,d7t44f,,Stats,2019-09-22 09:53:57,1
Dominated by US,1,d7cjdu,,Stats,2019-09-21 09:17:49,3
"You are reading it correctly, but you shouldn’t ignore the confidence interval of that OR: the difference between the groups is not statistically significant. So it actually just shows, that most likely the first shot isn’t effective by itself and the chance of hospitalization is therefore equal in both groups.",1,czgh7r,"I’m trying to figure out this table (link below), specifically what doesn’t make sense to me is when the age group 6-11 months the OR is .91 for comparing no dose of DTP vaccine vs 1 dose. Does the author imply that having no DTP vaccine in this case lead to a lower rate of hospitalization then having 1 dose? 

[link to table](https://cdn.jamanetwork.com/ama/content_public/journal/jama/4907/joc31278t3.png?Expires=2147483647&Signature=sOFMVSO-BczkPtPHHAzSDjR7W~9eLd9U3~aTYY9KoMOLyxkpOYZAXMy8ObOHp2I0vYKyojHny53qb3zbKrYqzT-aWE2sOhOzcK-tDiEIAYDxVdfMj0T6GZ6lYCAqlfTdPbB~oY-QVFgrHyQ7ct46zOWfjpgYyzSskZBmTWhqacuXjZc~Nkct2BZwGwlGXmlgJ3UuBnZWUgtbQkNTOwaG~6L7CxlRLNSx4RMU4Qnw7ERgUp7VhFbP6TA8XfUcuSQ3bHjDjgdVcNZWOx~EQKRi5AVlDXetMURfcr8Tgl~m-MCGPkMxrZ-GseAeKQ0jfGMZPZlrM1OCJGwwncsyFRYjOA__&Key-Pair-Id=APKAIE5G5CRDK6RD3PGA)",Stats,2019-09-03 22:15:53,1
"Most questions ≠ most popular. I’m not saying the association is definitely wrong, but we don’t know and I’d argue that it would be better data science (and more honest) to title the graph with only what it’s *actually* measuring - and then let the reader make their own interpretation/conclusions. In other words, the subtitle should be the title - probably paraphrased (e.g. proportion of stack overflow questions by language) - and the subtitle should specify details (e.g. % every month etc).",1,ct2e0p,,Stats,2019-08-20 10:12:03,1
I think you need to take another attempt at explaining this.,2,cgvnjj,"So trying to work something out. Not sure exactly to work it out and then if I can, I would like to put it in excel, but I can figure that on my own. 

If there are 16 things that we need to find 8 combinations of 2. I know that is a combination. I think I know how to account if we find out 1 or more pairs. And then how could we account if we find out 2 things that are for sure not a pair? Does this make sense?",Stats,2019-07-23 10:29:47,3
"You're trying to have one player win all their matches and one player lose all their matches, but the player heading for getting black and the player heading for getting gold have to partner each other! What happens in that match?",1,cgrumh,"My dad plays a regular doubles tennis match with three other players. The format is that they play a set, then switch partners, so that after three sets every player has partnered every other player. 

If a player wins all three of their sets, they get a 'gold'. If a player loses all three, they get a 'black' and have to buy the drinks after.

They realised that every three set session ends with either a player winning a black or a player winning a gold, but never both.

I can 'prove' to myself fairly easily why this is the case just by thinking through the permutations. But I wondered if anyone has a neat way of proving this in an easily communicable mathematical way, and if the problem scales or transfers into any more interesting mathematical problems.

(I may be posting this in the wrong sub, in which case apologies and any redirection appreciated).

&#x200B;

Tl;dr - neat proof required that all groups of three doubles tennis matches, where partners are switched after every match, ends either with one unbeaten winner or one always-beaten loser.",Stats,2019-07-23 05:15:46,1
"The P-value is, by definition, the probability of your extreme observation.
You can either have a fixed cutoff -- an alpha value, and ask, does my phat fall on the outside of the location of the alpha, 
or you can ask, how far can my cutoff go before my observation slides inside my cutoff.  You derive you P value from the latter.",1,cet7n1,"**setup to question:** Not sure if others will agree that \[this explanation\]( [https://www.youtube.com/watch?v=S4wmS0a0Ams](https://www.youtube.com/watch?v=S4wmS0a0Ams) ) of hypothesis testing is good (I think it's phenomenal), but I do have a question about hypothesis testing, and specifically, null model/p-values. CC of the video says that if there's a specific p-hat that is far enough outside the standard deviation, then it will determine your p-value and whether or not you accept or reject your null hypothesis.

&#x200B;

**My question:** Why does one extreme p-hat indicate ""significant evidence""? is the p-value actually inferring that we're finding several p-hat values out in that region of our distribution, or is it really just that this one p-hat is so crazy that we must consider it ""significant evidence"", as I currently understand it?

&#x200B;

thx",Stats,2019-07-18 07:30:09,1
"I'll assume this is for a grad school application? If so, you're right, it won't hold as much weight as a letter of someone you've actually done research with (ideal situation) or a class you took with a professor. That being said, I only had two letters of people I conducted research with, the third one I had just taken his class, and it was enough to get into every school I applied to (he was a full professor though, and a pretty well-known one).",1,brpicw,"Just wanna say i got an A for my first graduate stats course and im hype.

Thks to the community for keeping me thinking about new problems and providing useful input to posters. 


Question actually: my teacher is a phd student, so he’s only an adjunct lecturer technically. He said he would write me a recommendation letter but that it probably wouldn’t hold so much weight since he’s not an official teacher, is this true?  
  
He told me since i tutor the undergrad stats to ask the board for a rec letter and they’d prob approve. Would this help even though they dident really teach me or see my work in action?",Stats,2019-05-22 07:32:49,4
"The smaller number only means significance when talking about p values in comparison to alpha values (so p < α would be a significant p value).

Here, you are given a correlation (R =-0.86) and critical values (Rcrit1=0.349 and Rcrit2=0.449). Correlations indicate the strength of the relationship between your variables, so more extreme numbers are stronger relationships. That means in this case, you want the absolute value of your test correlation (R) to be greater than (or rather, more extreme than) that of your critical R values.

Since |-0.86|>0.349 and 0.449, your R value is significant.",1,bjqs1c,,Stats,2019-05-01 21:20:27,2
"Wish I could say I'm great at Stats, but that's a lie. Shoot me some questions and maybe we can work it out together? ",1,balbiy,"Would anybody be willing to tutor me in statistics, I need help grasping the concepts as quickly as I can. Or if I could just ask you some questions that would be of some help as well. It’s STA2023 so fairly basic for someone experienced. Any help would be appreciated. Thanks ",Stats,2019-04-07 14:24:15,1
Are you able to dummy code the variables?,1,aweliy, The problem I have now with my data set is that a lot of the variables in the data set are  character variables . so How to find co-relationship between categorical  variables(of character data type) in R ? possibly using co relationship matrix? ,Stats,2019-03-01 22:58:04,1
"There’s literally loads of packages doing all sorts of stuff. 

If you just want to start with some simple correlations between parameters then try pairs, or corrplot. You can make more sophisticated stuff with GGally. Then there’s data exploration packages such as DataExplorer, among many others. ",1,aw6602,"I have been learning R for a while and now I have reached a topic about Data Exploration , where at the task at hand I have to identify the correlations/relationships between columns. It's really confusing to me and I am sorta lost , is there anything that can help simplify this concept for me , so I can proceed ?",Stats,2019-03-01 08:30:23,1
" 

### Abstract

We investigate to what extent online A/B experimenters “p-hack” by stopping their experiments based on the p-value of the treatment effect, and how such behavior impacts the value of the experimental results. Our data contains 2,101 commercial experiments in which experimenters can track the magnitude and significance level of the effect every day of the experiment. We use a regression discontinuity design to detect the causal effect of reaching a particular p-value on stopping behavior.

Experimenters indeed p-hack, at times. Specifically, about 73% of experimenters stop the experiment just when a positive effect reaches 90% confidence. Also, approximately 75% of the effects are truly null. Improper optional stopping increases the false discovery rate (FDR) from 33% to 40% among experiments p-hacked at 90% confidence. Assuming that false discoveries cause experimenters to stop exploring for more effective treatments, we estimate the expected cost of a false discovery to be a loss of 1.95% in lift, which corresponds to the 76th percentile of observed lifts.

&#x200B;

Link to the study:

[https://papers.ssrn.com/sol3/papers.cfm?abstract\_id=3204791](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3204791)",1,aszuoh,,Stats,2019-02-20 23:29:34,1
They're the same. ,3,asyda3,what is the relationship between the mean median and mode in a normal distribution 8 need help,Stats,2019-02-20 20:52:50,1
"Yes, your interpretation in the edit is correct.",1,ap3z5v,"Hello. Just trying to figure some *post hoc* letters out at the minute.

&#x200B;

https://preview.redd.it/yjrmd15gvqf21.png?width=718&format=png&auto=webp&s=0ab6c90f742272a8d49a628c9fac5d2774740607

I find these difficult to get my head around. I can only really tell that A is different C. In fact, is C different from all of them? Can someone summarise this for me? Essentially there are four woodland sites and I got the least amount of beetles at C.

&#x200B;

Edit: Is this correct? Adwell isn't different from Emmington or House, but is different from Skittle Green. Emmington isn't different from Adwell or House, but is different from Skittle Green. House isn't different from any site. Skittle Green is different from all sites except House. That means that Skittle Green differs significantly from Adwell and Emmington? 

&#x200B;

&#x200B;

Much appreciated.",Stats,2019-02-10 06:42:02,3
Typically the factor analysis  (or principal components analysis) would be used to obtain a smaller number of dimensions to reduce the number of independent variables in the regression analysis. ,1,a7pggn,If you had a 20+ attribute model why would you perform a factor analysis and then do the regression on the factors instead of the raw data?,Stats,2018-12-19 12:09:16,1
Stats,2,a717my,Stats ,Stats,2018-12-17 09:20:37,2
"They looked at around 400 vehicles, and found that the weight of a vehicle has a statistically significant relationship with its fuel efficiency (mpg). In fact, they found that every added pound results in a loss of 0.007 mpg. You should ask for a plot of their data, as this will reveal if their assumption of a linear relationship between weight and mpg is correct. 

TLDR: They’re giving you a hint to try and get lighter trucks. ",2,a62kum,"hi guys, i'm an EMT/Firefighter i just got handed this sheet from someone at the city i can't make heads or tails of it, it's supposed to tell us about replacing some Trucks. i'm supposed to explain it to my boss. i would appreciate not looking like a jackass. so if you could give me like 4 or 5 major take aways you would be helping me out alot thanks!

https://preview.redd.it/dzatoypf17421.png?width=1324&format=png&auto=webp&s=0d297854f7620169817f2b185226b97b3142d714",Stats,2018-12-14 00:12:13,1
"I understand bin size is ultimately up to user judgement.  You know what the purpose of your histogram is, and if you tinker with the bin size I'm confident you'll quickly narrow in on a size.  Also, if your data set will continue to grow your bin size may need adjusting in the future.

Also, bin sizes tend to be in multiples of 5 or 10.  I'm not sure why, but it's probably a standard that extends from precedent set long ago.",1,9u8rwz,"I am building an app for poker players to track their performance.  And one of the things we are doing is creating a histogram of session data.

$ on the X-axis and count (number of sessions that fall within a histogram bar's range) on the Y-axis.

And the problem I have is that I do not understand theory behind sample / grouping the data points into ""like"" bins.

I need to create a programatic process that looks good when the user is only on their third data point as well as when the user is on their 500th.

Are there any general equations / resources that you guys can point me towards?  For instance, I know that Excel automatically generates the number of bins and their range from a sampling of data when you tell it to make a histogram.

What governs this process?

Thanks so much guys!
-md500",Stats,2018-11-04 17:29:20,2
"Usually, with correlated data, PCA would help to remove correlated effects and provide a low dimension dataset that has new ""features"" that are not correlated. The purpose of PCA is, as you are saying, to find the direction of highest variation, so the first PC should have the highest variation.  Are you saying the first 6 principal components are equally sharing the variance explained?",2,9u7bu2,"I'm presenting on a evolutionary genomics paper for a class and I'm trying to scrutinize the research groups methods since their results seem kind of strange. They did a PCA to find correlation between different measures of convergence, but they flattened their data three times into 6 principle components and claim that variation is shared equally across the components. Doesnt this defeat the purpose of flattening the data if each principle component only spans a small amount of variation?",Stats,2018-11-04 14:30:12,5
Totally.,2,9tl6zb,Stats ,Stats,2018-11-02 08:40:00,1
"Chi-Square Goodness of Fit test with specific proportions?

Something like this maybe for the Chi-Square test, with H0 being no statistical difference between the observed and expected values for your sample (40, 45, 15):

[Imgr Link](https://imgur.com/a/OvZ2Wy3)",1,9rtrxz,"Can someone please solve this question

a manufacturing company sold items in three package size namely large,normal and small.Experience has shown that they are in a ratio of 3:5:1.Sales return from a particular region which shows that 40,45,15 dozens of diffrent package respectively.Is this package significantly diffrent from previous ratio",Stats,2018-10-27 05:07:40,2
"Median * 1.5 does not equal 1.5 standard deviations from the data.  There may be a language issue here, or perhaps we don't understand the same statistics.

IQR * 1.5 in either direction of the median will give you 'fences' as seen in boxplots.  Data points outside of those fences aren't automatically marked as anomalies, rather are categorized as such for further investigation.  They may be perfectly valid, or they may be worth removing or other action.",1,9rau7k,I have been tasked to create alerts based on anomalies. So I thought using IQR\*1.5 but this only flags extremes.  Is it possible to use the medaian(quartile 2) \*1.5 to get data 1.5 standard deviation  away from the data or am I completely misunderstanding?,Stats,2018-10-25 07:33:01,1
"Assuming that the control group is a control on the same dimension as the experimental groups (e.g. in a drug trial, control group takes no drug, with experimental groups 1, 2, and 3 taking 3 different drugs) this should be a 4x1 anova. While your control group does not receive extra treatment, they're still a group you're analyzing. 

Does this make sense/help?",1,9p4n0y,"If I have an experiment with a control group and three levels of an experimental group, what analysis do I use to test it? My inclination is a three by one ANOVA, but not sure if that is correct.",Stats,2018-10-17 18:21:26,1
"The simplest, most general approach is to convert each metric to z scores (subtract the mean, divide by the standard deviation) and then take the mean of the z scores across the 5 metrics. This assumes that each metric is more or less normally distributed or can first be transformed to a normalish distribution (such as log conversion). ",1,9nkxbi,"I am trying to create a composite score across 5 different metrics at different scales. I'm not sure how to approach this. 1.Should I standardize all the metrics then sum them or should I normalize all the metrics and sum them? 

2. What is the best way to use this to establish a letter score to the range of composite sums?",Stats,2018-10-12 07:52:12,1
"So its been a while since I've done this but I'll give it a go.

Prob A wins in the first round = 0.5 x 0.3= .15

Prob B wins in first round = 0.5 x 0.25 = 0.125

Prob C wins in first round = 0.7 x 0.75 = 0.53

15 + 12.5 + 53 = 80.5, so 19.5% of the time the tournament will need to go multiple rounds

15/80.5 = .19

.19 × 19.5 = 3.63

15+3.63 = %18.63 = prob of A winning

12.5/ 80.5 = .16

.16 x 19.5 = 3.03

12.5 + 3.03 = %15.53 = prob of B winning

53/80.5 = .66

.66 x 19.5 = 12.84

53 + 12.84 = %65.84 = Prob of C winning

65.84 + 15.53 + 18.63 = 100%

",1,9a089g,"Hi, I have a logical problem. I'll explain with sports (does everyone like tennis ?): I know the probability of winning  each player against each other. I want the probability that they win the tournament. Let's have 3 players :

Player A have 50% odds to beat Player B

Player B have 25% odds to beat Player C

Player A have 30% probability to beat Player C.

​

I want to rank the players with a probability for being the best :

Player A : X%

Player B : Y%

Player C : Z%

​

With with A+B+C = 100%.

​

I fried my brain, thinking to much to this stupid problem. Any ideas or suggestions to help me to see the light ?

Thanks",Stats,2018-08-24 12:07:30,1
"You have to use Bayes Theorem to calculate the posterior probability. It breaks down like this:

`P(fault|red light) = P(red light|fault) * P(fault) / P(red light)`
That reads as “Probability of fault given a red light is equal to ...”

`P(red light|fault) = 0.9`
`P(fault) = 0.02`
`P(red light) = 0.02 * 0.90 + 0.98 * 0.10 = 0.018 + 0.098 = 0.116`
*^ you should be able to figure out where these last numbers came from*
————————————
`P(fault|red light) = 0.9 * 0.02 / 0.116 = 0.15517`
",2,92zhp1,"A high school stats question - but I cannot get the answer in the textbook.

\- On a certain type of aircraft, the warning lights for the engines are accurate 90% of the time.

\- The lights show green for normal and red for trouble.

\- There are problems with the engines on 2% of all flights.

Calculate the probability that there is a fault with an engine given that the warning light shows red.

(The textbook claims the correct answer is **0.1552**)",Stats,2018-07-29 17:32:26,10
"You should check if the differences in genre are still statistically significant after adding the other variables you want to include in the model. But there is reason to believe they would be. I would keep it. 
As a general rule, if you want to predict future ratings, you should not discard explanatory variables just because they are not statistically significant. I would keep them if it makes sense to believe that they are related with the rating (for example, if a certain theory argues so). ",2,91lt7s,"I'm doing a project where you make a regression equation to predict IMDB rating. One of the aspects I want to use is Genre So when I look at the broader category of Genre, there are 10 categories. Some are statistically significant, some are not. Do I need to drop the entire category of genre in my final regression equation, only use the statistically significant genre categories, or use all the genre categories no matter the significance?

https://preview.redd.it/ac01dk5gwyb11.png?width=1072&format=png&auto=webp&s=836e0dd41f76e4a58a32a3819e7bfad200964d4b",Stats,2018-07-24 14:55:47,2
"Well this is a bit tricky because there isn't a whole lot to go on. For regression you basically need two sets of continuous data. There are two variables the y or dependent variable and the x or independent variable. You are trying to see if the independent variable can predict the dependent variable. Basically does y depend on x. The y variable you are looking for is the probability of receiving a kidney. Now for example, if you could get the age of people who receive a kidney transplant and the likelihood they had at getting the transplant, you could see the relationship between likelihood of getting a transplant and age of recipients. This can be visualized as a simple scatterplot and you can determine the fit of the data pretty easily using linear models. You can determine the relationshop in any stats program (R) or excel. 

You can basically regress the likelihood of getting a kidney with almost any other variable. But this is just a CORRELATION it does NOT mean the x variable causes the y to occur. It's just a relationship between the two variables that you can draw inference on.

I hope this makes sense, I've had a really long day. Also I'm on mobile so I apologize for any typos. If you have any questions I'd be happy to help. ",1,8yosf9,"Out the probability of receiving a kidney transplant?

I’ll admit I always hated math but now learned to respect it and try to understand it so bare with me.

I’m trying to figure out the probability of me receiving a kidney transplant & having it successfully be implanted...hope that makes sense 

What do I need? ( I know a ton of info but what exactly) 

Also umm, I’m horrible at math and came here to ask. Can you guys help me figure this out? I’m asking because I’m at stage 4 and I’m tired of listening to theories and stuff and want to see solid numbers of such. I rely on statistics because of my stats professor who showed me how much it is now needed to project things.",Stats,2018-07-13 16:20:57,2
"The range for where 95% of the means fall is, by definition, the 95% confidence interval. You haven’t done anything to your data, only resampled it.  So you are living the null hypothesis. This is an empirical determination of what a t-statistic calculates for you mathematically if you assume normality. 

The nice thing about the empirical method is that it makes absolutely no assumptions about normality or any other distribution.  This is useful when you don’t know a priori what the true distribution is or how to describe it.  The trade off is that it takes more (computational) work than performing a t-score lookup.",1,8tqheo,"I've been working through a book **Modern Data Science with R** and I have a conceptual question about bootstrapping and confidence intervals. 

Say you do a bootstrap for a mean 1000 times. How do you get the 95&#37; confidence interval? According to the demonstration in the book, you simply calculate the .025, .975 quantile. Can anybody explain why this is so? I'm wondering why this process doesn't include the familiar steps of calculating a confidence interval like you'd do in a t-test.

Just in case there are any R users that want a reference to a specific example of the book exercise I am working with, it is here:

[https://mdsr-book.github.io/instructor/foundations-ex.html](https://mdsr-book.github.io/instructor/foundations-ex.html)",Stats,2018-06-25 06:24:33,2
"Just in case more information about the continuous variable is needed: it's a measure of white matter integrity for a single region. The possible minimum and maximum of this measure are 0 and 1 because of how its computed, but the values for this region, for this subjects goes from .37 to around .48. ",1,8tpncl,"Hi!  

Scenario: 62 subjects did a selection task composed of 128 items, that can be divided into four conditions, because each item has a cue and a target (also other options but thats not of interest in this analysis) which can be affectively positive or negative (2x2 factorial design, within-subject). I found a significant within-subject effect hinting that cue-target affective incongruence difficults performance (accuracy is my dependent variable).

Now, here comes the issue: I also measured with neuroimaging the integrity of a certain area of interest in the brain. So I have a continuous variable at the subject level. The hypothesis is that this area is responsible for solving cue-target incongruent tasks.

What I want to do know is evaluate the interaction between the within-subject effect in accuracy and this continuous variable. I'm aware this requires a multilevel analysis (I'm using R), because I have to analyze the subject and the item at the same time.

Any help would be appreciated! Thanks!",Stats,2018-06-25 04:11:03,1
"How to understand that we are going correct direction, that changes world did are positive? I have chosen some predefined set of statistic, but maybe you can advice what I should look for.",1,8p092b,,Stats,2018-06-06 05:36:22,1
"This sounds like a fun project! The approach you take really depends on the data & what you're trying to test, though. Are these GPS data from a single individual or separate individuals? The same species? How do populations of that species [arrange themselves spatially?](https://imgur.com/JSMmjMK)",1,8mhybq,"I was wondering if someone could point me to a good paper or online source that discusses how to carry out regression analyses when the dependent variable is spatial distance, and thus always 0 or greater. When I carry out traditional regression, the best fit relationship can end up predicting negative distances, which violates the real world constraint. For some background, I am analyzing GPS data and trying to test hypotheses about what would / could predict distances between individuals at any given time. I thought of transforming the dependent variable from its raw current state \(a continuous non negative value\) into a binary variable \(e.g. 'A is within 5 meters of B'\), and then carrying binomial regression, treating this binary variable as a count of instances. But it would make me feel better if I didn't throw away the richer information contained in a continuous variable. Any tips very appreciated!",Stats,2018-05-27 07:07:40,1
"Meh. If a quick youtube video does a better job than 99% of stats text books and professors, I think your discipline has an existential crisis you're not acknowledging.",1,8iwis1,,Stats,2018-05-12 07:55:39,1
"Too late for your exam probably but here it is:
X2 goodness of fit is for comparing distributions.
X2 test of independence is to see if 2 variables are independent/associated or not (pretty self-explanatory.
According to my teacher X2 test of homogeneity is pretty much the same as goodness of fit.

If that doesn't help then maybe this will:

Goodness of fit = 1-sample with 1 variable
Test of independence = 1-sample with 2 variables
Test of homogeneity = 2-sample with 1 variable

And of course the formula for all of these is the same:
X2= (obs-exp)^2 /exp

Degrees of freedom are different though.
",1,8imj40,"I have to use Minitab for a data analysis exam soon, but I'm not 100% on the difference between the types of X2 tests. They seem to be called different things. There's cross tabulation, association, goodness of fit, and independence. Only the first 3 are in Minitab. Could someone please explain the differences and applications?

Much appreciated. ",Stats,2018-05-11 02:52:12,1
"Yes, you should add it for exactly the reason you stated. ",1,8i5gqe,"Hello there! I am defining the fixed effects of one linear mixed model and I realised I have the following problem:
The outcome variable is the combination of two variables. I know that one of these two is influenced by a third variable. Let's say, the outcome variable is ""Green"", which I obtained by adding ""Yellow"" and ""Blue"". I know that ""Blue"" is influenced by ""Black"". Shall I add this third variable (e.g. ""Black"") as a fixed effect? It is not included in my hypothesis nor of interest to me, yet I feel I should add it or the estimates of the other fixed effects would be inflated (idk if this makes any sense).
I've checked the literature and I haven't found anything that could address this problem, so any advice/suggestion is appreciated.
Cheers!",Stats,2018-05-09 05:12:10,3
https://www.icpsr.umich.edu/icpsrweb/ go here for public datasets,1,8d4g7t,"Hi, does anyone have any suggestions on interesting multivariate datasets to analyse? ",Stats,2018-04-18 02:54:32,1
"Their acceptance to either college should be independent from each other, and when Independence holds, P(AnB) = P(A) *P(B)

Edit: this is also in line with your way of putting it where P(AnB) = P(A|B) *P(B) because when outcomes are independent for A and B, P(A|B) = P(A)",1,8d2n3z," Suppose that for a particular student at a high school,
they have applied to two colleges I and II.  The probability they are accepted by college I is 0.70, the probability they are rejected by college
II is 0.30, and the probability of being rejected by at least one is 0.35.

Find the probability they are accepted by both colleges.


I am getting stuck because I am using the formula P(AUB) - P(AnB). But I dont know where to go when solving for P(AnB) = P(A|B)*P(B)",Stats,2018-04-17 20:30:52,1
"It looks like your map function would do:
`tibble(‘df_a’)`
`tibble(‘df_b’)`
`...`

I don’t think that’s what you want to do. 

You could do something like:
`tibbles <- lapply(1:4, function(x) tibble())`
`names(tibbles) <- c(...)`

But then your tibbles are living in a list and not just floating in your environment. I actually can’t think of, off the top of my head, how one would do the latter programmatically. 
",1,8bqvtn,"Hey rstats,

I was wondering if it's possible to use purrr's map functions to create a series of new tibbles.

For instance, right now I have 4 lines where I initialize 4 tibbles:

    df_a <- tibble()
    df_b <- tibble()
    df_c <- tibble()
    df_d <- tibble()

Can I do something like:

    x <- c('df_a', 'df_b', 'df_c', 'df_d')
    map(x, tibble)
    
And then have 4 new tibbles in my environment. 

Being able to do this would help clean up my code and make editting it easier.

Thanks.
",Stats,2018-04-12 08:09:35,1
Use mutate and a rolling function like zoo::rollapply to count the number of NAs in whatever window you like. You can then filter on that new column. ,1,81otl8,"I want to chart the timing of flowering and fruiting for 50+ different plant species, and I found this chart from Evans, Smith & Gendron 1989 that would be really neat.

https://imgur.com/a/Oilm7

The only obstacle that remains is that I have some orphaned values in my data that I'd like to discard. Here's a simulated dataset:

    library(tidyverse)
    
    df <- data.frame(stringsAsFactors=TRUE,
         flower = c(""dandelion"", ""dandelion"", ""dandelion"", ""dandelion"",
                    ""dandelion"", ""dandelion"", ""dandelion"", ""dandelion"",
                    ""dandelion"", ""dandelion"", ""dandelion"", ""dandelion"", ""thistle"", ""thistle"",
                    ""thistle"", ""thistle"", ""thistle"", ""thistle"", ""thistle"",
                    ""thistle"", ""thistle"", ""thistle"", ""thistle"", ""thistle""),
         month = c(1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 1L, 2L, 3L,
                   4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L),
         flowerPct = c(90L, NA, NA, NA, NA, 70L, 90L, 100L, 40L, NA, NA, NA, NA, 60L,
                       50L, NA, NA, 70L, 90L, 100L, 40L, 30L, 20L, NA)
        ) %>% 
        group_by(flower)
    
    df
    
    #  A tibble: 24 x 3
    #  Groups:   flower [2]
    #    flower    month flowerPct
    #   <fct>     <int>     <int>
    #  1 dandelion     1        90
    #  2 dandelion     2        NA
    #  3 dandelion     3        NA
    #  4 dandelion     4        NA
    #  5 dandelion     5        NA
    #  6 dandelion     6        70
    #  7 dandelion     7        90
    #  8 dandelion     8       100
    #  9 dandelion     9        40
    # 10 dandelion    10        NA
    #  ... with 14 more rows

Notice that the first row of `dandelion` has a single survey where`90`is recorded, but the real flowering period only begins in month 6. **Does anyone know a way to omit a record based on how many NAs it's flanked by?**

",Stats,2018-03-03 01:55:52,3
NMDS is a pretty good way of doing that I think.,1,7j31jw,"I have this type of data:



    Statement1
    
                  Rank1(1)   Rank2(2)   Rank3(3)   Total  WeightedMean StandDev
       Groupe1       2          1         3          6         2.17        0.9
    
       Groupe2       1          1         2
    
       Groupe3       1          2         1
                                               Mean by group    -


Each time the statement is given and a person associates it a rank (1-4). Naturally, each individual belongs to a group (department where he works).

I can then make a resume by the chart giving mean by group and +- StandDev for each question. 

    +StandDev by group
    Mean by group           
    -StandDev by group
                          Statement1     Statement2    Statement3 

So we can see the most important statements.


My questions are:

1. What is the best way to find and visualize the correlation between the Groupes (i want to find closest and most distant in terms of rank they are giving for a group of Statements) ?

2. What is the best way to make (visualize ) a summary by group and not the Statement ?

Thanks in advance.

",Stats,2017-12-11 08:55:59,1
YOU CAN SEE IN THIS BLOG TOP TIPS AND TRICKS,1,7c8zn2,,Stats,2017-11-11 08:32:53,1
Are you trying to get us to do your homework? I'm going to up vote but only cause you got brassballs.,2,75sh02,"Male turtles have a mean shell length of 105mm and standard deviation 5mm. Female have a mean shell length of 110mm and standard deviation 10mm.

a) Calculate the probability that the shell length of a randomly chosen female is less than the shell length of a randomly chosen male.

b) Use the answer from above to calculate the ODDS that the shell length of a randomly chosen female is less than a randomly chosen male",Stats,2017-10-11 14:56:39,2
[deleted],1,70dwjn,"Have a set of data with some noise and read errors.

replacing the NaNs with mean values was easy enough.

Deciding how to treat value spikes not so much. Initially std dev was 2 orders of magnitude larger than the mean, plotting showed a very small number of value spikes well outside the range of the majority.

I calculated the percentiles and watching for a rapid change in the percentile value, then clipping the max|min range to remove the extreme values and return the std dev to a value within one order of magnitude of the (max-min) for the 90th percentile. Then plotting to see if there were obvious clipping affecting the majority of data.

Not a great method, and other than being told the data is sensor input from process plant (with no input on the normal max/min range) I have no knowledge of what the data represents.

I feel I've forgotten some data cleaning process I should be working through. Thoughts/references welcome.",Stats,2017-09-15 17:31:50,1
OpenNLP,2,6dx839,"I'm working on a sentiment analysis in R and want to be able to easily classify words of interest into adjectives, nouns, etc. The idea is to be able to create an automated email that plugs words into the appropriate places based on the word classification. Is there an R package that can do that? If not, any ideas on any alternate ways to do this?",Stats,2017-05-28 17:39:47,2
"Hi, I think you may be having trouble because it really doesn't make sense to find a ""correlation"" between two categorical variables, especially with a linear model. The reason for this is that there isn't any order between the categorical variables. Instead, I would suggest looking at a contingency table and using Bayes' to find conditional probabilities. Here is a helpful link: https://onlinecourses.science.psu.edu/stat200/node/71",1,5tsfe5,"Hi guys, I'm trying to find a strength of correlation between blood type and personality.  I have a data set for the five factor model of personality that contains information on respondents' blood type, but I can't figure out how to find the strength of correlation between these two pieces of data.


At first I thought of using Pearson Coefficient, but that won't work without numerical variables.  I'm currently trying to figure out the Goodman-Kruskal tau, but not having a lot of luck.  Any suggestions?",Stats,2017-02-13 04:43:48,2
did you ever find an answer?,1,5qitz1,"I'm familiar with the rule of 3 to find defect rates, like 3,000 samples without a defect has a confidence of 95% that the rate is no higher than 1 in 1000.

I'm wondering how does the formula work when there is a few defects in the sample? 

Say I test 3,000 samples and it has 1 defect or 2 defects? What's my new 95% confident rate? ",Stats,2017-01-27 11:41:30,2
Have no knowledge of baseball or what is required for your thesis but could something like taking the 'Moneyball' approach and finding flaws/improving upon it work - or developing something similar to that yourself?,1,5ppt5g,"Looking for a topic to pursue, and and my chairman gave me until the end of the week to come up with an idea. Not a great programmer by any means, but am using RStudio. My buddies directed me to this sub. Any suggestions for interesting/feasible topics using open-source data?",Stats,2017-01-23 09:32:35,1
"Hi, I don't have any information on formal analysis, but I imagine your answer will depend on what genre of music you are listening to. For example, most pop songs will have a different structure (intro - verse - chorus - verse - chorus - ...), while something like psychedelic rock will be more unstructured. I imagine for pop songs, you could find the average time of each segment and for the total song, and create tentative ranges for where a chorus will be (I assume this is the part of the song that is most sonically attractive). Then, attempt to skip into a chorus. I have to use lots of assumptions here because what is ""good"" music is purely subjective. ",1,5gm2xy,"Couldn't think of a better place to go to than this subreddit. 

All of us have been in search of good music and have randomly skipped around in a song to see if we'll like it or not. Sometimes, though, we don't click on the right parts of the song and assume we don't like it. Rather, it might have been our perfect jam. Is there any analysis on at which part (or time), on average, a song sounds the best?

",Stats,2016-12-05 07:11:53,1
"everton stats
",1,596j4i,,Stats,2016-10-24 12:30:38,1
You can get data from the FBI Gov Database.  https://ucr.fbi.gov,1,55h3d7,Just want to know what the actual numbers are. BLM says that cops killing black people need to stop but what are the actual statistics? cop killing black/cop killing white/black killing black/ white killing white. Want to get the bottom of this- every subreddit is pointing me to another place,Stats,2016-10-01 22:51:25,2
"It looks like [this page](https://en.wikipedia.org/wiki/Ising_model#Monte_Carlo_methods_for_numerical_simulation) may be useful to you in figuring out how to get the Metropolis Hastings algorithm set up for the first problem.

On the second problem, I'd encourage you to think about the geometry. If you take a point in 3-space and normalize it according to it's distance from (0, 0, 0), you get a unit vector, right? One that lies on the sphere around (0, 0, 0). So that would be a uniform distribution on the surface S. From there, you just have to understand how to use simulated values in calculation.

I'm not going to work out the details of either problem, because this isn't /r/homeworkhelp, but if you have an R-specific question, I may be able to help you out once you've worked through the actual meat of the statistical problem.",2,4n22d9,"This is the last assignment of the quarter and I am honestly extremely lost on how to do it. If someone could help me out or direct me on how to go about the problem that would be much appreciated. I understand that these two problems are about Monte Carlo Methods, but I don't understand much else.Any help is appreciated, thank you!

Link to problems: http://imgur.com/a/FKhT1",Stats,2016-06-07 16:37:14,1
"I'll give you some ideas/comments to think about.

Plot 1:
* talk about the spread of the residuals (constant/non-constant, patternless/pattern, trending/non-trending, etc)
* talk about the points that R has pointed out (do they look like points of possible concern?)
* that red line is a smoother that R draws through the residuals, it rarely helps in my opinion as a trend or shape in the residuals is often seen more clearly in other diagnostics

Plot 2:
* talk about whether or not the residuals seem to fit the hypothesized distribution (in this case Normal), it's okay to have some deviance from the straight line
* talk about the tails of the plot if they look cause for concern
* talk about any noticable shapes if they are aparent

Plot 3:
* this plot is very similar to Plot 1, but has the advantage that it is comparing the studentized residuals

Plot 4:
* this plot is for detecting high leverage point outliers points that are inside of the drawn red contours are the points of interest (do you have any?)",1,4ge05w,"Did a multiple linear regression with stats from MLB baseball teams with runs scored as response variable and OBP and SLG as my predictors. How can I explain each diagnostic plot in relation to my model.

Here are the plots http://imgur.com/a/sF55N",Stats,2016-04-25 09:09:02,2
"You could plot the correlation (plot(x,y, data)), state the peason correlation coefficient and the p-value of this correlation. I wouldnt bother giving the CI. The CI tells us its non-sig by overlapping 0.

You could also give the intercept and slope",1,4g0yb0,"This is the output I got

>Pearson's product-moment correlation  
data:  length and eaten  
t = 0.45595, df = 73, p-value = 0.6498  
alternative hypothesis: true correlation is not equal to 0  
95 percent confidence interval:   
 -0.1757993  0.2769016  
sample estimates:  
       cor   
0.05328878   

Apologies, my statistics knowledge is **very** limited. I understand that there is no strong correlation but i'm unsure as to figures I should use when including this in a report. Would I state the confidence intervals or the cor value?",Stats,2016-04-22 15:53:59,1
"Do you only have amphiban movement data for march or for the entire year? You could use a t-test to test for the mean difference in number of amphibans moving in March vs non-March then (use Welch's since you will have an unequal sample size). This is bad practice as you are sort of forcing the conclusion to be the one you want. ANOVA across all months would be a better use of your data in my opinion if you want one ""test"". You could use a Poisson regression model which would probably be the best regression model (yes, I know ANOVA is a regression model) to fit since you have a count response.",1,4cplr9,"I have two variables. One is number of amphibians using a tunnel over the course of a year.

The second variable is temperature.

I want to see if temperature is related to the initial movement of amphibians. Amphibian movement is a bell curve over March whereas temp is obviously bell curve over course of a year.

Looking at the graphs it looks clear that a sudden spike in temp resulted in a large movement of amphibians. 

Would the T-test be the statistical test i should use to prove this?

Many thanks.",Stats,2016-03-31 04:29:44,1
"You need to be more specific about your model, but I can tell you that log(x) = -.75 indicates that x is a number between 0 and 1.

Perhaps if you solved your output, those would be the percent chance of something happening.  Or maybe the odds of that thing happening are lower than 1 (indicating a better than 50-50 likelihood).  I don't know what base log you're  using, or what ratio your model is really predicting.  So I don't know how to help you any more.",1,4c4q4y,"Hello,
I am using R for log odds ratio. It keeps computing the log odds ratio at -0.75. Can someone explain to me in very basic terms what that means and how it can be a '-' number? I am calculating the probability of someone getting into a particular school with their data against an existing admissions data set. 

Thank you! ",Stats,2016-03-26 23:35:59,1
"The mastitis incidences would be considered discrete numerical data rather than continuous data, but still numerical rather than categorical.",2,49ziwn,"I am writing a research project to determine how mastitis incidences have increased over a period of twelve months, but I'm not sure of it is categorical and categorical data or categorical and continuous.

Obviously the months are categorical but would I be correct in assuming that the number of mastitis incidences each month would also be categorical, as there cannot be values in between each number (i.e. 22.4), or would it be considered continuous?

Thank you!",Stats,2016-03-11 09:35:01,6
"You could maybe try a [truncated normal](https://en.wikipedia.org/wiki/Truncated_normal_distribution), with which you can ensure that a <= X <= b, where a and b are between 0 and 100. Of course, you would want to argue that your random variable adheres well to this distribution, but it's a possible candidate. ",2,435zpn,"I'm having a hard time formulating this question but...

Let's say we believe some RV X represents the % of some quantity. We think X has a mean m and a standard deviation s (or some other representation of our confidence in m). We want to sample some values of X. If we assume X is normally distributed then we might end up with negative values of X which isn't possible. My intuition is to assume that X is lognormally distributed but I'm not sure what to do with that. How do I practically say: ""I believe X is approximately m with some confidence"" and get values distributed from that confidence?",Stats,2016-01-28 16:26:05,1
"If you are trying to find the probability of the 2 Individuals marking the same answer for each question then your calculations would look like (1/5) (1/5) (1/5) (1/5) (1/5) (1/5) (1/5) or (1/5)^7=.0000128 or
=.00128%
If you wanted to find the prob of 3 same answers and 4 different it would look like (1/5)^3+(4/5)^4=.417 or 41.7%
Hope this was at least a little helpful ",1,433oo3,"Can anyone help me out with a stats question.  I am trying to find out what the probabilityare for 2 different individuals taking a test/survey with with seven different questions.  5 Possible answers for each question
 
Any possible help or direction is welcome",Stats,2016-01-28 08:14:55,1
Would also try in /r/theydidthemath,2,42ospl,"What is the strategy you employ to maximize profits without triggering your risk limit?

Got this question on a test and am having difficulty solving it. Interested to see what the Reddit community thinks.

I posted this in /r/gametheory too but that sub is less active than this one so I thought I'd try here as well.",Stats,2016-01-25 17:35:32,4
"For linear models like lm() or aov()
use lm(temp ~ time * time^2 * time^3 * date, data = data) instead of 
lm(temp ~ time + time^2 + time^3 + date, data = data) if you want the summary to give you all the interaction effects. If there are too many effects you can always do variable selection.
Good luck! :)


",1,40ivds,"Hey rstats!

I'm trying to fit a linear mixed model in which a few of my fixed effects are not linearly related to the response variable 

Thus, I have included the squares/cubes of the fixed effects (e.g. temp~time+time^2  +time^3  +date)

I also want to investigate interactions between the fixed effects, do I use the squared/cubed term or the normal one (e.g. temp~time+time^2+ time^3+  date+time^3 :date OR time:date)?

Thanks!",Stats,2016-01-11 14:02:28,1
http://i.imgur.com/rLDP2W3.png,1,3zx6ht,"so i want to run a linear regression on my data, but need to normalise it first. any idea on how to do this? image from the normality test on minitab in the comments",Stats,2016-01-07 12:56:01,1
"Event A and event B are mutually exclusive if they don't overlap (think Venn diagram).

For example, event A is that I flip a coin and get heads, and event B is that I flip a coin and get tails. There is no way that A and B can happen at the same time. The events ""mutually exclude"" each other because if I flip heads, I know I didn't also flip tails, and vice versa.",3,3w6esa,,Stats,2015-12-09 21:42:29,2
"In academia, from what I've heard, R is used most often. In the real world, the more popular ones are R and Python in the private sector. I believe the public sector is still using SAS.

P.S. You'll find that /r/statistics is much more active than this sub.",1,3sfus9,"Hi. 

Question about stats software. 

What are people using mostly?

What are people using in the .edu realm, and what are people using once they have jobs?",Stats,2015-11-11 12:17:23,2
"This is a convention in contexts like, say, survey data where you have an ordinal or categorical variable that you know will take a non-negative value. Another convention is to use 9 or 99 if you're in a world where you know the variable will take fewer than those values. It's about making sure that you don't inadvertently code missing data as the value of some actual data.

But if you're going to do anything with the data, remove missing values. Like, don't regress on the -1 observations as though they were predictor data.",1,3sdhlf,Some kaggle scripts I've seen replace missing values with -1. Why is that? Is it because they assume there is some pattern to missing values and why -1?,Stats,2015-11-10 22:27:13,1
"We use the binomial distribution to think about percentages like that. Out of ""n"" attempts, ""x"" were successful, yielding a percentage ""p=x/n"". Assuming ""n"" is big enough for an individual, you could assign a Z-score to him/her (also referred to as a standard normal distribution). Briefly, you standardize p by subtracting the mean and dividing by the square root of the variance. The variance will take into account ""n"".

Try reading about the normal approximation to the binomial. Good luck on your project.",1,3s64gy,"I'm doing a project on field goal percentage and free throw percentage in basketball.  

I want to take into account free throw and field goal attempts, so I don't want to find their z-score simply based on their percentage. Basically, I want to make sure that 8 out 10 shooting registers a higher z-score than 4 out of 5 shooting. ",Stats,2015-11-09 12:17:55,2
"You're going to need to be a lot more specific. 

* What type of nonparametric test? There are tons of different varieties. 
* What do you need to test? Are you trying to determine whether the mean is sig. different from X? Comparing the means of two groups? Looking at the variance of two groups?
* What does your data look like? How many variables are you comparing? You should probably post the first 6 rows or so of your data so that people can help you figure out how to work with it.

",1,3rp9pr,I have to perform a non parametric test in Rstudio and have no idea what to do! Can someone explain to me how to do this?,Stats,2015-11-05 17:41:55,2
"Sort of. Let's say you have 4 numbers: 4, 6, 8, 10.
Their mean is: 7.
The variance is calculated by: ((4-7)^2 + (6-7)^2 + (8-7)^2 + (10-7)^2 ) / 4 = 5.
Standard deviation is: 5^0.5 = 2.236. ",2,3rgoed,Is standard deviation is the the average distance to the mean?,Stats,2015-11-03 23:37:26,4
"Degrees of freedom in a t-test is equal to the sample size (N) minus 1. The significance of a t-test is a function of three factors: the true effect size, amount of noise, and sample size. Increasing your sample size will (generally) decrease the amount of noise, giving the t-test more power to detect a significant effect (provided that there is a true effect to detect). 

Thus, increasing sample size increases DOF which increases your statistical power to detect a true effect.",2,3qlct0,"Does increasing the DOF make a t-test more significant? I can't figure out the correlation between the two. 
",Stats,2015-10-28 11:21:08,2
"Short response here. You just have to do a multiple regression with your categorical variable as a contrast. Let's say I want to see if height and sex can predict income. 

I will code boy as -.5 and girl as .5. Height would be equal to height minus mean(height). Interaction would be sex*height. 

Then in a linear regression, just add sex, height and interaction as factor. 

Just be sure that you're not in an heteroscedasticity case. ",1,3qbdhr,"I have an example of how to do an ANCOVA, but the homework problem assigned has unequal sample sizes. How does this change the calculations? How would it change the programming in SAS? The class textbook does not go over ANCOVA, and the handouts we have say nothing about unequal sample sizes. I've looked online and haven't found a straightforward answer. The gist of the problem is comparing two teaching methods and the data are pretest scores and gain scores for each method. TIA!",Stats,2015-10-26 11:58:24,4
Proof: http://imgur.com/ReYRwKZ,2,3o1r30,Can someone explain why n-1 is used in Standard Deviation rather than n?,Stats,2015-10-08 18:42:05,3
"You should give /r/datasets a try!

Also there's a dedicated sub to R if you have any questions: /r/rstats.",1,3dip0f,"I was wondering whether the /r/Stats community has any resources for obtaining data for which I can apply statistical methods to. 

I am using the R package, and just getting my feet wet. 

Any and all help would be appreciated.  Thanks",Stats,2015-07-16 09:16:30,1
"it sounds like you're looking to determine which amino acid sequences are related to an event to start. if that's the case, i would start with unsupervised learning methods to see if there's any clustering of amino acid sequences. if so, you can then move into supervised learning methods to see if the aa sequence is correlated to an event.

for example, you could do cluster analysis (unsupervised) to see where your aa sequences fall. if there's clustering you'd want to then explore the factors that cause aa sequences to cluster together.

quick breakdown on unsupervised and supervised methods:

unsupervised = you don't know what your response variable is. instead you have data and are looking for patterns.

supervised = you've got a response variable (example: looking at the effect of sunlight on plant growth - plant growth being your response) and are looking for correlations.",1,3b7onu,"If it helps, this is all currently in excel, and I have access to SPSS. 

I currently have a list of patients, and each patient has several different amino acid sequences associated with them. Some of those amino acid sequences are in common with other patients, and some are unique. I have eliminated any possible redundancy from each individual patient, so a sequence can only appear once for each individual patient, but like I said, it can be in common with OTHER patients.

What I want to do is find if any of those amino acid sequences are associated with the occurrence of a particular event. I also have the information of whether this event did or did not occur for each patient. 

Any ideas how I could go about this? A paper that looked at similar things suggested they used logistic regression, but I don't see how I would make that work. ",Stats,2015-06-26 10:10:25,5
"1. Yes

2. Yes

3. I wouldn't conclude that.  Concluding on null hypothesis isn't recommended. Maybe it would be better to conclude you obtain a little effect size than groups are similar. ",1,38e6p0,"Hi Stats Folks,
I did a T-Test in JMP recently and am in need of help deciphering the  results. I know they are terribly remedial, but here are questions I have:
1. Does JMP assume the null hypothesis is that the means of the two populations are equal?
2. Am I correct that if the 95% confidence limits contain 0, then the is not a significant difference between the population means?
3. Am I correct that if the p-value is greater than 0.05, we cannot reject the null hypothesis, and that then it is likely that the population means are quite similar?

Thanks for any and all help!!!",Stats,2015-06-03 10:02:50,1
"the number of variables correspond to the regression df (3).
residual df is 17, so regression df + residual df + 1 (intercept) = 21
n = 21 is the number of data I suppose.
 
MS = SS/df for each respective row

F = MSREG/MSRES the ratio of the mean square errors

F will have df 17 and 3

Check F table for p-value",1,341a36,"I'm stuck filling in an ANOVA.  I have my notes that say the Residual df is 3, is that because there are 3 variables (x1, x2, x3)?  Or how can I find out other information I need to finish filling out the missing data?  

Image: http://imgur.com/MselfNK",Stats,2015-04-27 07:52:30,1
What exactly are sigma^2 x-y and sigma^2 y-x? ,1,30w2yf,The x-y and y-x are subscript by the way. Can anyone help with this proof?,Stats,2015-03-30 21:16:14,2
[deleted],2,2y2epl,"Hey guys and girls,

I'm doing a very simple project that requires some statistical confirmation. I am comparing fishing yields in a lake between 2 different habitat types: habitats with aquatic vegetation and those without aquatic vegetation. I had 12 samples plots, 6 with no vegetation and 6 with. I recorded how many fish I caught at each site and want to see if there is a significant difference in fish catches between both types of habitat. It's been a while since I've done any stats and was wondering which test would best help me analyze my data? Everything is very much appreciated!

Cheers!",Stats,2015-03-05 14:31:44,3
"The link you gave is for a specific player. The code doesn't look at those pages, it only looks at the pages that list all the players for a given letter.

Look what's inside the `url` variable - it's a vector of 26 URLs, one URL per English letter, that points to the page that shows a list of all players corresponding to that letter.

For example, the first url in the list is [http://www.basketball-reference.com/players/a/](http://www.basketball-reference.com/players/a/). If you go there, you'll see the table that `readHTMLTable` is scraping.

Btw, that code is not very good R code... you should use `lapply` instead of doing a loop. I would change the code to something like this (not tested, but the right idea):

    library(dplyr)
    tbl <-
      lapply(url, function(x) readHTMLTable(x)[[1]]) %>%
      rbind_all


Although I'm pretty sure it'll break if any of the letters don't have any tables on their page, should probably add an error check. Aalso, I'd use `httr` package instead of `XML`",1,2szhbm,"Could someone explain how this this snippet is scraping the web? I got this piece of code and am having trouble understanding
how exactly the data is being read into `tbl`.  When I look at the page source of a page within the URL's subset ('http://www.basketball-reference.com/players/a/allenra02.html') for example,
I don't see anything referring to a table for the data that's imported into tbl. ' I think this is more of a HTML-related question, but if someone could explain the indexing with the readHTMLTable function, that would be great

Code 
    library(XML)
    
    ###### URLs
    url<-paste0(""http://www.basketball-reference.com/players/"",letters,""/"")
    len<-length(url)
    
    ###### Reading data
    tbl<-readHTMLTable(url[1])[[1]]
    
    for (i in 2:len)
      {tbl<-rbind(tbl,readHTMLTable(url[i])[[1]])}
    
    ###### Formatting data
    colnames(tbl)<-c(""Name"",""StartYear"",""EndYear"",""Position"",""Height"",""Weight"",""BirthDate"",""College"")
    tbl$BirthDate<-as.Date(tbl$BirthDate[1],format=""%B %d, %Y"")",Stats,2015-01-19 15:38:29,1
"It's not clear in your example what is your Y dependent variable and what are your independents. However, assuming 'concentration' is  your Y, and you are implying that data is skewed; have you tried simple transformations to attempt to make the histogram of the Y vars approx normal?  ie  SQT, LOG, neg reciprocal? If one of these vars achieves that then you can invoke linear regression.

Finally, your'e not really giving enough information for people to really help you. ",1,2rsvqe,"I have a question regarding a calibration model I am trying to build with a colleague and our current argument. At the moment, we are taking a sample with a known concentration and scanning every part of the surface area with a technique (I can't say what it is) and so we have a point value for concentration and then we have ~6,000 values with our technique. 
Next, we can take all these known values and the ~6,000 values and regress them on each other but the variability is a) huge and b) when looked at with a histogram, skewed. 
Now, I would say that due to the skew of the data, we cannot take the average value and just regress all of the known concentrations versus the average for two reasons:
1) it is the average of a sample with huge variability and therefore it isn't correct to just take that average and conveniently use that
2) the skewness of the data means we obviously can't use linear regression. 

Some further info: the skewness is primarily caused by bare areas where our sample hasn't deposited so we get a huge spike at ~0 and then we see our other data.

Now, the converse argument by my colleague is that as long as we always scan the same way and get the same area and amount of ""bare"" sample (which we should get nearly all of the time), then we can use this ""bad stats"" model okay. They don't understand statistics that well and to them this is ""fit for purpose"" and they are okay that predicting right is the key even though we are not doing things right. 
Their concern with letting me build a more complex model (perhaps a logit model) are that it is too complicated for a lay-man (an excuse that irks me).


So, the question: who's right? 

Follow-up: if it is me (and I know I may not be exactly right), how can i put this to them so that they understand the problem. I can see their point and where they are coming from but I just cannot accept it. Especially the avoidance of skewness and the large variability in the sample.

Thanks all!",Stats,2015-01-08 17:14:03,1
"Running into this now, did you ever fix it?",1,2lte90,"Hello. 

I attempted to conduct an analysis in SPSS and ran into an issue. After hovering over the Analyze drop down menu, my options had morphed to:
---------------------------
Reports
Descriptive Statistics
LogLinear
NeuralNetworks
Classify
Dimension Reduction
Scale
!analyze_mixed!
Nonparametric tests
Forecasting
!analyze_regression!
Survival
Multiple Response
Missing Value Analysis
Multiple Imputation
Complex Samples
Simulation
ROC Curve
-----------------
Has anyone run into this issue? Did you reinstall SPSS? Download a patch?

For clarification, I have SPSS 22.0.0. 
",Stats,2014-11-09 18:33:53,2
"Basically you read the plot columnwise to understand how different types of outliers can be found using cooks D or DFFITS. I really should have included residuals i guess but its old code.

Hope someone can use it.",1,2igrpi,,Stats,2014-10-06 10:29:07,1
"Tests, mostly. Assertions, validations, etc. You can write an assert to confirm every value is in some range, or only some percentage of values are NaN, or whether the average / max / min meet some other expectation.

Some of this is in unit tests (I rarely write any data function without thorough unit test coverage), some is inline or part of the code. You'll often want to disable this validation as it can be expensive, but it's good to leave the code and make it configurable (e.g. set an environment variable) to help with any live debugging.",2,xi8l0w,"I am a front end programmer who is used to looking at code *but not data.* What I mean is: when I am data wrangling with SQL results or data frames, I tend to *look* at small and large result sets which look more or less like each other. 

A lot of times I am looking at the \`head\` of a result frame and it looks alright but turns out the middle section of a 25000 row dataset was incorrect all along.

So how can I tell if the result sets I am wrangling with are correct?

&#x200B;

>Everything looks so similar and it's easy to make a mistake. Staring at grids of numbers can be confusing.  

&#x200B;

I guess [\#rstats](https://twitter.com/hashtag/rstats?src=hashtag_click) packages like visdat with functions like \`vis\_compare\` can help compare similar datasets with each other. 

Any other software that can help me track anomalies? 

Also, does one develop a certain instinct or level of confidence around judging the correctness of one's data as one works with it? Or do you need peers who cross validate your work for you?

Would appreciate some advice from wizened data wranglers.",data,2022-09-19 03:51:14,3
uhhh python,1,xg2ny1,"I am not sure if this is the appropriate place to be asking. But is there a program out there for Linux or even windows that will allow us to import data. Then will show like a profile of the company. 

We want to import data then use the domain name as the main key to match all records then have the ability to add to those company profiles with updated data (imported  data)?",data,2022-09-16 13:53:21,4
Probably need more details on the type of data you’re talking about.  Text files? Videos? Images? Spreadsheets? Databases?,5,xgaxed,Anyone know of local hosted software to manage large MB of data?,data,2022-09-16 20:04:23,1
[XKCD said it best](https://xkcd.com/927/),7,xf3a9l,"GON - General Object Notation
-----------------------------

One of the reasons we cannot create a simple browser is simple because of the unecesarily complex data formats used in web. (ex. xml,html,svg to name a few)

We already have seen lot of json alternatives, but I just couldn't find anything that can fit the problem scope of html, css and json while also being simple and readble.

This is the reason for creating GON. A simple and general data format. That is highly optimized for readability and is computationally efficient.

Imagine json but ' ' and '\n' instead of ','

No unwanted restriction on keys. For example div and ""div"" are different.

A possible index.gon file
```
  div: { class=""something"" id=""someid""
    val: [
      p: ""How are you""
      p: ""Second p tag""
      a: {href: ""cool.com"" val: ""This is cool""}
      text: 'Multiline strings are cool
and there is no reason not to allow them'
      n: 23342
      //: ""Free multiline comments
and until there is a double quote""
      n: 2342342.234234
    ]
   }
```

A possible styles.gon
```
h1: {
  color: ""f00""
}
```

An use case for config file
```
config: ""$HOME/.config/something""
```

//: is genius because it allows the data sender to allow whether sending the gon file with comments turned on or not for reduced size which can be done while streaming. //: easily fits the grammar

GON was inspired by the following projects,
* https://www.json.org/json-en.html - still verbose with commas and double quotes :(
* https://json5.org/ - doesn't remove the commas :(
* http://www.relaxedjson.org/ - too relaxed, especially with comments :(

https://zserge.com/jsmn/ is also an important reason which motivated me to come up with this format. This library is just perfect.

I don't have an ABNF for GON or implementation for that matter, but I can come with one when I have the free time.

Any feedback is valuable!",data,2022-09-15 10:56:49,11
"Define mismatched.

Are these two sets of data with the same exact timestamps but mutually exclusive columns? Do they share some columns? Do they share some columns but the timestamps don't match 1to1? It is difficult to make a recommendation for a better approach in Excel without really knowing the layout of the data.

30k rows is a lot by hand, but not really that much from a computing standpoint. Excel should be able to handle it unless you are really keen on learning to program with something like Python+Pandas.",2,xetrdm,"I have two timestamped but mismatched datasets, 20-30k rows and 10 columns

I'm currently using Xlookup to match all my columns using one timestamp as my lookup value.

This is understandably killing my PC. 

Someone enlighten me please, how should I be dealing with this data.

Its fundamentally basic, but large.

Date,time,X,Y,Z, output1,output2,output3,...etc etc



What software do I need to learn?",data,2022-09-15 03:56:53,2
"I’m sorry you are working on this “app” and are wondering if … … the app … … the underlying app functionality/use case… already exists ? Like you want an already trained model? 

Not for free no… Definitely not. You could probably parse public workouts for athletes or something and make the model yourself.",1,xetq1y,"Hi, 
I am working on an fitness/health recommender system  that takes in user’s fitness goals and output workout recommendations to users and need data to trains machine a learning model to do so.

I Was wondering if there are any machine learning dataset with

Inputs : user goals ( e.g to lose 2kg weight in 2 months)

Outputs : fitness Recommendations 

Thanks",data,2022-09-15 03:54:57,2
"For those that do not feel like typing a bitly link: 

https://www.cdc.gov/nceh/tracking/DeveloperSeries.htm",3,xe9ail,,data,2022-09-14 11:24:11,2
"Just add link to your data in some dropbox, so people can access, have in mind that every csv is additional storage which stack mat don't like.",2,xepbn4,"I posted this suggestion on stack and its getting downvoted by a bunch of programmers who don't work in data. Wouldn't it be nice to just upload a small snippet of the data you have for a stack question? Especially if you're making visualizations? Why can't they just have an upload csv function?

Anyway if you support this idea got upvote it, I don't accept that its a bad idea I think the wrong people are seeing it. In classic stack fashion they are just shitting on it because they can, and programmers generally are miserable anal people. One person said I was abusing the site in all caps and suggesting I format data by hand and put it in the question.

[https://meta.stackoverflow.com/questions/420383/feature-request-being-able-to-upload-small-csv-files](https://meta.stackoverflow.com/questions/420383/feature-request-being-able-to-upload-small-csv-files)",data,2022-09-14 23:30:53,1
would be cool to know more about your role and the experience you've got so far. whats your career aspiration?,1,xeafnb,"Hey all!

I’ve been working with data (first as a data support specialist and now an ETL engineering) for a little over two cumulative years now. I know I’m still rather green to the industry but I do enjoy it - really the flexibility it allows for, pretty good wages/benefits, and the work itself keeps my mind busy. That being said, I don’t find the work all that stimulating. I have a feeling it has to do with the entry level like aspects of my roles up to this point but I’d really like to take the next step(s) - ideally without sacrificing too much more of my time and freedom. Any tips, suggestions, or anecdotes of similar situations?

Thanks in advance!",data,2022-09-14 12:10:27,1
"We have had a lot of success with FME from [safe.com](https://safe.com). Covers all your reqs. You can get a decent free trail and have a cloud version also. Top of my head its around £800 per licence - alongside there ifra server. We are exploring FME cloud at the moment which will give us a PAYG option.

Their website appears to have fully transparent pricing.

They do regular meet ups and offer one to one support.",2,xdzio5,"Hi folks,

We are looking to change our data infra and wondering what are some good ELT tools. We are looking for a no-code ELT tool that has decent support. I would really like it if you can provide a range of tools (best, moderate, good)  
One-time load: 20GB & Daily batch loading of up to 200k rows per day.  
Sources: S3, MySQL, Mixpanel, Stripe, Google Analytics.  
Destination: BigQuery

Thanks!",data,2022-09-14 04:30:41,6
"9 times out of 10 in my experience, it's been poorly written queries or either not enough or improper use of indexing in the tables/database (over indexing can also be an issue).

On the querying front, I always recommend fellow analysts start by taking a look at how much data they're actually returning (row level on a million + rows can take a while). Analysts in our org tend to feel the need to query row-level data into Power BI, which they rarely need to do, offering flexible filtering options to stakeholders they tend to never use over data aggregation up front.

MySQL tends to be one of the SQL technologies I don't use as much these days, but I've noticed in PostgreSQL, MS SQL, and Redshift that in terms of performance, temp tables are better than CTE's are better than nested subqueries (they're also a bit more readable, imo). Doesn't mean this is always true, just sharing what I've noticed helping fellow analysts to refactor their queries from something that takes 5 minutes down to 5-10 seconds.

I'm not well versed in indexing, but many of the DB engineers I've worked with have been. When indexes have been properly implemented, I've noticed improvements. Would like to learn more here, just have never had to. That said, even great indexing doesn't mean a whole lot if the queries you're writing are poorly optimized.

As another poster said here, it's certainly a combination of these and other things you might want to look into. Your hardware specs look fine to me, so see what you can do with a little time investment first.

Best of luck!",4,xd58v8,"Hi folks,

We have a virtual machine (t3.medium, 2vcpu) with 4GB RAM on which we have installed the MYSQL server and have 2 DBs of size 2GB (1.5GB+1GB).

When we run bigger queries(>1 million rows) it takes more than 5-10 mins to run the queries. What are the ways we can improve this time? Should we change the memory/RAM size or CPU or something else? Any ideas are welcome.

Thanks!",data,2022-09-13 04:41:52,7
Someone enlightening me what latest marketing jargon is `data monetization`? Like scraping and/or capturing data and/or aggregating data with the intent to sell?,1,xd3abv," 

* To define, describe, and forecast the data monetization market based on components (tools and services), data types, business functions, deployment types, organization size, industry verticals, and regions",data,2022-09-13 02:51:34,6
"Anything like this I ask myself these questions:
1. What's the true end-to-end process. From data acquisition to how the output data looks. 
2. What features within the cleaned data can I extract? Can I automate the extraction? 
3. Are there cloud services that can make this easier? (I'm thinking can AWS read an uploaded document from S3, and read the document with its ML/NLP services).",3,xcaus2," I am going through a bunch of documents by hand in order to collect a dataset (I have already concluded that this will be faster than automating the process using for example OCR). Are there tools that can make this process easier, rather than manually entering the data into Excel? For instance, I was imagining having a set of checkboxes for each observation that I can click and thereby generate binary variables. I use Python. Any suggestion is greatly appreciated.",data,2022-09-12 04:57:16,4
"""Correlation is not causation"", springs to mind.",8,xcar4p," The Mars Hypothesis presents the idea that the Federal Reserve can set interest rates based on the movements of the planet Mars. In this book, data going back to 1896 shows that as of April 2020, percentage-wise, the Dow Jones rose 857%. When Mars was within 30 degrees of the lunar node since 1896, the Dow rose 136%. When Mars was not within 30 degrees of the lunar node, the Dow rose 721%. Mars retrograde phases during the time Mars was within 30 degrees of the lunar node was not counted in that data as Mars being within 30 degrees of the lunar node. The purpose of the book is to not only hypothesize that the Federal Reserve can set interest rates based on the movements of the planet Mars, but to also demonstrate exactly how and at the same time, formulate a system that would enable the Federal Reserve to carry out its application in real time. Using the observation of the planet Mars, the book contains a strategy for controlling inflation, interest rate setting recommendations and the predicted dates of future bear market time periods all way thru the year 2098. The book ""The Mars Hypothesis"" can be found on Amazon",data,2022-09-12 04:51:59,5
"https://stackoverflow.com/questions/4749706/lookup-city-and-state-by-zip-google-geocode-api

I've historically used Google maps API though there are some limitations on how many requests you can make. (Not sure if that's still the case tho)

Or geocoder was another tool I've used as well at previous jobs.

The task you're looking to accomplish is called ""geocoding an address"" essentially filling in missing or incorrect info and potentially extracting a lat/long coord.",1,xa5hzj,"If I have a 1,000 rows of data containing  just addresses and business names and I need city, state, zip, are there any free tools where I can batch map that info to pull in the missing info? Any help would be greatly appreciated!",data,2022-09-09 13:22:01,10
Don't you have access to a local MLS?,3,x95q7k,"I’m in the Residential Real Estate industry & I want to be able to differentiate myself in my marketplace.  Is there any resource, or inexpensive software that would give me micro economic data relating to housing.  Examples would be;
Permits pulled, Under Construction, # of Foreign transactions, Supply & demand, Mortgage applications & etc.",data,2022-09-08 09:59:31,2
"Check out Kaggle.com might help. This has tons of large public datasets ready for exactly this type of use case. May have to use more than one dataset as it will be hard pressed to find one that puts exactly what or who you need the data.

Cheers Good Luck my friend!
cheers",2,x8zvcw,"Hi all,

I'm looking to find something which i assume to be very basic but i haven't been able to find it.

I'm looking to find a public dataset with some generic information.

Role
Salary

And any further additional fields if possible.

I would prefer to have this by AUS, for example a list of 2500 roles and their average salaries.
Does anyone know a source?",data,2022-09-08 05:57:56,1
42,1,x8v7qw,"Recently, Company XYZ introduced a brand-new digital and online platform in connection with a regulated product it offers to the general public. Although the launch largely occurred as expected on the ""go-live"" day, there were a number of data quality difficulties, which present the company with a number of risks and issues.

You have been requested to do a quick review and update. Tell us how you would go about doing that, what tools you would use to analyse the data, how you would set priorities, and how you would handle stakeholders.",data,2022-09-08 01:55:18,1
Give me the data and I will judge you,6,x8fq2k,"In January 2018, I began keeping track of my happiness on a scale from zero to ten, every day of my life, for each fifteen minute interval I was awake. That was over four years ago and I have been recording my happiness ever since.

As I continued on with my happiness tracking—I began incorporating more factors into tracking as well. Including anxiety (also in fifteen-minute intervals), everything I eat as well as time I ate it/macros/nutritional contents, how energized I feel every day, and about sixty other factors.

I have all this data now, but because of the time commitment it takes to track it continuously, I haven't been able to do as much exploratory analysis on it as I wish I could have—if anyone would like me to share the dataset with them and would find that interesting let me know!

Attached is the example of the questionnaire I would ask myself (on top of food tracking and happiness tracking) I think a lot of cool analysis could be done with.

https://preview.redd.it/9zoyndcqyhm91.png?width=468&format=png&auto=webp&s=0c4902ff6d7fa2a8a64d6d64ed6275fcc1048d54

https://preview.redd.it/ooyuklzzyhm91.jpg?width=1990&format=pjpg&auto=webp&s=dfe921a18f6bd23042825c3ed3bad9f5f4398f9b

https://preview.redd.it/lzknpcx1zhm91.jpg?width=2342&format=pjpg&auto=webp&s=d02b5ae7d0d21fdf8c4ca875b380035365478937

&#x200B;",data,2022-09-07 13:34:11,3
"This can be done with low code, no code. Recently I used a low code platform DronaHQ to make a dashboard for my MySQL database.",2,x8ugmq,"I have my customer database present in MongoDB and let's say I want to get notified on mail  whenever a new customer is added to the database. 

Any ideas? How it can be build",data,2022-09-08 01:10:04,2
How familiar are you with window functions,1,x8ibjt,"**Background:** I'm working with a client who tags customer records with key words (similar to hashtags). A grouping of these key words tell a story about a specific customer interaction. There are 219 different possible key words they can use; however, each interaction record consists of about 5 key words (on average). All key words appear in an Excel file (same column, separated by commas).

The key words are added in no particular order (because the order doesn't matter). For example ""A, B, C, D, E, F, G, H, I"" = ""E, D, F, G, C, H, I, B, A"" (if each letter represents a word in this example, it wouldn't matter which order they appear in -- the meaning is the same).

**Side note:** *I would liken this to a language where the meaning of the speaker is not impacted by the order they use the words. And even if they add a word or replace a word, it will probably not have a huge impact on the meaning.*

**My question:** I need to figure out which combinations (regardless of order) of key words appear most frequently. I would like to query the three most common 3 key words, 4, key words, 5, key words …

My hope is I will be able to classify those groupings to help my client understand what sorts of customer interactions they receive the most.",data,2022-09-07 15:18:01,3
I have AT&T and I’ve been having issues the last few hours with getting service too.,1,x737ka,"Evening,

First time here, and I’m hoping to get some help! 

I’ve been scouring the interweave on my cell data phone because there’s no internet! I’ve had Verizon for years and it always seemed like the majority of the outages in my area Brooklyn NY, have come after periods of heavy rain.

I asked Verizon support if there was a way to request the data and they gave me some nonsense data privacy jargon. My next attempt is reaching out to corporate for the data in question. 

But I was hoping maybe one of you beautiful people might know a way I could find the data or might have it stowed away somewhere by some miracle. 

What I’m looking for is FIOS outage data for Verizon, within the last 3 years, specifically in the Brooklyn area, zip codes would be awesome but not necessary. 

Might be a pipe dream but indulge me, thanks in advance!",data,2022-09-05 23:32:54,1
Comtrade: https://comtrade.un.org/data/,1,x4neni,"From where I can get data for the period July 2021 to June 2022 related to export of sugar by the countries such as India, US and Thailand. Particulars required (but not limited to) are:

•Price
•Quantity
•Destination",data,2022-09-02 23:21:14,1
"Hi. 

Unfortunately I do not understand the response you got so I cannot say much on that. You’re right, the columns don’t make much sense. 

A quick idea could be using the city/country/region columns (the one’s that make sense) for your location purposes. If you need longitude/latitude for exact location purposes, it might be worth joining your data with a different set with the long/lat per city. Also, I haven’t done this but in the past I’ve read documentation for endpoints/columns that I did not understand and have often found much needed clarity. 


Good luck friend.",1,x3w6x5,"Hi everyone, I've been trying to extract location data from the [US DOE Geocube](https://edx.netl.doe.gov/geocube/#gogi) site but it seems that the API response has been encrypted (see Image 1). So I tried downloading the layer data (see Image 2) but I don't understand how to read it or if there is even any location data in it. The closest resemblance to any location data would be the `shape_length` and `shape_area` but even then I don't think that it has anything to do with the location (given that the column names have 'shape' in it) of the infrastructure layers in the map. Any ideas?

[Image 1](https://preview.redd.it/zlw8bms6pel91.png?width=573&format=png&auto=webp&s=75f8755c45070ace1e28e9a55ccabefee008a6cf)

https://preview.redd.it/um25rxjyoel91.png?width=1833&format=png&auto=webp&s=f5dad0ce6d4b870e3fb5c2434035aaefc044e62d",data,2022-09-02 01:33:48,2
"Please don’t take this the wrong way, but the way you’re framing your question makes it clear to me that you’ll need some coding lessons to get this done yourself. The script may or may not already exist, but it’s never as easy as hitting some button that says RUN and getting back what you wanted. You’ll need to tell it (program it) where to find your data (are the comments in a text file? Are they on Reddit? On facebook? Scrawled on pieces of scrap paper and photographed with a phone camera?), what exactly to expect the shape of the data to be (is it one song per line? Is that always the case? Is the song name identical each time it’s written? Is there a number before each song name? Is that the same each time? Etc), how to combine the votes from different voters, and where to put your results.

The good news— you can 100% learn how to do this yourself in a matter of 40-100 hours of studying, and then another 5-100 hours of work (exact # depends on the details I mentioned above). But more importantly— you’d gain the invaluable knowledge necessary to automate this kind of task for yourself in the future, and learn some really interesting stuff in the process.",5,x3sqes,I'm running a weekly top ten ranking poll and each comment will feature 10 songs ranked from 1 to 10 and each song will be assigned a point value based on its placement. is it possible that such a engine or script exists already that i could just plug in the data and get results? or would i need to create one myself?,data,2022-09-01 21:58:05,5
"Wow This is exactly what I do but for commercial real estate. Unfortunately we use in-house tools for this.

I would look up energy interval data analysis in google and see what comes up.",2,x3bbb5,"Not sure if this is the right place or not, but basically I have hourly data for my home power usage (kWh/Temp Indoors/Temp Outdoors/Cost) and I am trying to create an analysis with that data to show where power usage can be reduced and is increasing the overall bill each day/month. I want to take all this data and enter it into an analysis tool. Is anyone aware of anything that could possibly help me with this?",data,2022-09-01 09:00:19,2
"Use the pushshift API to get the total number of posts you've submitted, divide by the number of days since your first post.",2,x2o0az,"Not sure if right place to ask. If not then please redirect me.

It's a good question someone (a usernamed dumquestions) asked me [here](https://www.reddit.com/comments/wrkevd/comment/ikwkkgn/?context=999): How often do I post in a day? I tried redditmetis, but it didn't quite seem to help.

Is there any app or site or whatever that tells you how often a reddit user posts in a day or something?

Tried asking here but so far nothing:

[https://www.reddit.com/r/help/comments/x28lx7/how\_often\_does\_user\_x\_post\_in\_a\_day/](https://www.reddit.com/r/help/comments/x28lx7/how_often_does_user_x_post_in_a_day/)",data,2022-08-31 13:38:50,5
"It just means a window - a subset of data, likely a time period - that shifts aka slides. Like a rolling average, which takes the most recent N values. It's a general term that could refer to many specific things.",5,x0nana,"Hello guys,

 I am completely outside the data analysis community. I'm doing an assignment on a machine learning model for predicting fraud. 

the problem is that I can't understand the notion of **sliding window**. Any of you would have an easy definition ?

Thanks a lot!",data,2022-08-29 05:18:33,2
"Be methodical. Use phrase like ""As a ""type of user"" I must/should/could do xyz in ""reasonable business time frame"".

Test for hacking/breaking characters/{< etc, long text, foreign characters
Then tests for single and multiple word groups, hyphenated, enclosed words.
Test if you can for localisation/comprehension of words with double meaning.
Test for performance for loaded/high user volume",1,x0h0y6,"Hello people of reddit.  A friend was asked this question which I would like to know what would be an ideal response for. 

q1- Share samples of user-stories and acceptance test-cases via a Google sheet if you had been the product-owner for the Google search page. If you were asked to test the [Google search page](https://www.google.com/), what are the different types of testing you would perform? 

I'm studying what the terms like user stories and acceptance test cases mean but I have no idea what can samples of it be. :\\",data,2022-08-28 22:56:20,1
Republicans….. 🤦‍♂️,6,x00rf3,,data,2022-08-28 10:20:28,5
"If your current PC is windows based it still has DOS, you just can it cmd now. 

Source: old guy who started on a variety of DOSs starting in the 2.0 range.",4,wzv7ni,"Hi All-

I’m taking over a long-standing project that uses MS-DOS as the preliminary software. Within the app running on DOS, there’s tons of information- dating back almost 30-years.

I want to update everything into a modern tool to assist with evaluating historical records and identify areas of strength/ weaknesses. However, I’ve never even heard of DOS before last week. Does anyone know how I would transfer data from the old app into folders on my PC?

Not sure where to post this, didn’t see anything in the search. Thanks for your help.",data,2022-08-28 06:13:47,3
"It exists (see IRI, NielsenIQ) but probably not in the public domain. I'm sure there's a lot of time & effort that goes into maintaining such a list, so it won't be free.",5,wzcthg,,data,2022-08-27 13:34:57,2
"Power Bi desktop is free and can do all of that. The big plus is there is tons of good documentation and Microsoft Learn is pretty good. You can load up excel or csv files, which I am assuming is how your other apps will be exporting their data. Even better is you can form relationships between these datasets and start to get some cool insights. You can also add other data. Add some weather data to your model for instance and you can see things like, ""oh I eat more calories on rainy days"" or ""days that I exercise I read less"". There might be a learning curve if you are starting from scratch, but this is ultimately what I think you want.  


I've done this type of thing before and it is pretty fun and a good way to reinforce good habits.",3,wy88ic,"Hopefully this is a subreddit that can give me some guidance here.  


Problem:  
I have a whole bunch of different apps/data sources that stores some interesting data about me, my life, my habits etc.  


Solution:  
I want to create a ""Life Dashboard"" where I can create various visualisations of this data all in one place. Everything from Exercise, to cities visited, to books/pages read, completely customisable to whatever I'd like to track.  


Nice to haves:  
\- API or similar integration to avoid manual data entry when another app has already collected the data,  i.e. steps/exercise  
\- A range of visualisations and perhaps the capability to insert widgets such as Google Maps  
\- Flexibility in choosing how to display the data, rather than one single example  
\- Ability to enter any manual data that is required, via an App perhaps or something easier and more engaging than just updating a spreadsheet  


I don't mind having to build a fair amount of this myself, I consider it a fun side project, I don't expect there to be some magical tool out there ready to go, but I'm completely open to any suggestions at all.  


Some suggestions I've had so far include.....  
Airtable  
Notion  
AppSheet  
Bildr  


Would love to get any thoughts, feedback, general knowledge or otherwise on other suggested way to build this, or thoughts on the tools above that you may have?  


Thanks!",data,2022-08-26 05:43:44,3
"My company has a lot of data. But not clean data. Excel spreadsheets with color formatting which means different things. MS Project files with all sorts of random custom fields. Disconnected systems. Unstructured data. 
We have all the data related problems.",11,wx8gpx,I'm conducting some market research on the above question and would be really interested to know some of the pain points that you're facing. Thanks!,data,2022-08-25 01:20:25,16
"""Google Transparency Report""",2,wxeuu6,"Hi everybody!

I'm looking for a data set that would address the question of how strictly is the internet regulated in a country. The level of online censorship, how many regulatory laws are in place, things like that. Anything come to mind?

Thanks!",data,2022-08-25 06:52:47,1
It's better to compare decennial to decennial and ACS five-year to non-overlapping five-year. Or ACS one-year to one-year. Here’s guidance from the census on what is appropriate to compare. [Census Comparison guidance](https://www.census.gov/programs-surveys/acs/guidance/comparing-acs-data.html),3,wvxx97,"I'm an entry-level urban planner and part of my job is representing demographic changes to our clients. Considering the 2020 Census data isn't out yet, I started using the American Community Survey data from that year to compare to 2010 Census data. Is this okay or is it misrepresenting the changes over time since the Census is a count and the ACS is an estimate? Would it be more accurate to compare the 2010 ACS with the 2020 ACS? The problem then there isn't a 2000 ACS to compare to. Please advise.",data,2022-08-23 12:38:41,2
I would read the instructions for the camera in order to determine if formatting writes all 0 or 1s to the storage thus erasing the storage  bits or if it just releases the used space so it can be used by something else.,1,wuc0fb,"We recorded a fair amount of footage on a red Komodo to a 512 GB C-Fast Card and it was accidentally formatted by (through) the camera. The camera was shut off mid-format but our footage seems deleted.

I’ve tried Red-Undead and Test-Disk to try and recover the lost footage, but so far no luck. Anyone have any suggestions for last-ditch attempts to recover the footage?

Thank you so very very much

EDIT: It was human error, my wording was confusing. I apologize. It was formatted *by a person *through the camera.",data,2022-08-21 15:18:13,2
"Use Python & Pandas?

https://pandas.pydata.org/",1,wtzmt9,"Hello Guys,

I have excel sheet that contains sales transactions for the selling with discount transaction. I want to identify these transaction in the full transaction sheet but I cannot use filter because it is a big number of transactions.",data,2022-08-21 06:28:47,3
"Check out the FBI NIBRS dataset that is the source of a lot of the data in the crime data explorer.  You're not going to find a one stop cut and dry solution like you want, but you can construct it out of that data if you want to",2,wslveo,"Hi. I thought this would be an easy find but I'm not seeing anything very accessible. 

I'm looking for US crime data including violent crime and property theft, by zip code, for 2021 or the past several years. Population, city, and state would be useful but I could join on zip code if needed. 

Mostly for PA, CT, and MA, but other states could be interesting as well. 

The FBI's Crime Data Explorer seemed like a good choice but it's very granular, and only by city name. 

[https://crime-data-explorer.fr.cloud.gov/pages/home](https://crime-data-explorer.fr.cloud.gov/pages/home)

I suppose I could dive in there and find something useful but I just wanted to check if there's anything easier to access. 

I looked at Kaggle but didn't see anything recent there. 

Thanks.",data,2022-08-19 11:56:40,3
"(Total Male Full Time + Total Male Part time) / Total students = Male ratio.

1-male ratio = female ratio (Theoretically not including different gender preferences).

Otherwise just do the same thing as the males for the females.",3,wrrepa,,data,2022-08-18 11:47:48,9
"Yes, you can do it in Excel. This is just the first result, but you can search for ""excel workout tracker"" or something like that to see more templates: https://templates.office.com/en-us/exercise-planner-tm16410108",3,wrwt1v,"Hi there, I don’t know what to use to get this visually. I keep a progress journal but I’d like to try something else, like a graph. Can someone recommend me a program? I’m willing to self learn but I don’t know where to start. Am I able to do this on excel?",data,2022-08-18 15:26:42,1
"As someone who works for a company that provides that data to Wal-Mart, Applebee's, etc., you're largely correct about where house hold level data come from. The credit card company and Wal-Mart, etc. share this data via third parties.",4,wqqxyr,"Hi all,

I recently started getting my morning news from Morning Brew as it's a less serious take on the news that I get every morning. This morning there was an article explaining how companies such as Walmart, Applebee's, and IHOP have all increased sales by 6%-8% this quarter but mainly in customers that make a combined household income over $75,000+. 

This might be a long shot, but I was wondering if anyone knows where these companies get this information from. I was thinking it could be your credit card but other than that or user-provided data from things like surveys, I can't think of how they would get it.",data,2022-08-17 07:50:56,6
I always find it funny when digital marketing agencies can't digitally market themselves.,2,wqwgel,"I am not sure if this is the right place to be asking, BUT I am looking for intent data. We are looking for people that are interested in SEM/SEO services (in other words digital marketing services). 

I know there is several networks out there that provide this data, but they are unreasonable too expensive for us at this time. Does anyone have any input on how I could go about this?",data,2022-08-17 11:35:11,2
https://www.google.com/search?q=dol.gov+osha+data,2,wq48rr,"Does anyone know where to source OSHA data? Categorized by specific OSHA violations and the company's which violated them.

&#x200B;

Thank you!",data,2022-08-16 13:10:29,2
"**granular**: resembling or consisting of small grains or particles.

Age bands are not granular. I don't really see what this has to do with data quality though.",3,wpusyi,"Ms.Jones was tasked with collecting data for the organization via a survey. According to organizational requirements, the age options on the survey form were as follows:  
below 12  
12 to 30  
Above 30  
Identify the characteristic of data quality that this scenario best relates to.  
a.Valid  
b.Complete  
**c.Granular**  
d.Availability?  


I see it was marked incorrect.  


Data quality is a metric which does control the type of data in the network.  
Quality of the data is controlled by removing the duplicate values, removing the incorrect data, segregating the data in similar groups and making full use of graph and other interactive interfaces for data visualization.  
Based on the data provided, the correct option is option c.",data,2022-08-16 06:47:55,9
BeautifulSoup library on Python could scrape the data. There are sentiment analysis Machine Learning algorithms publicly available you could use to gauge satisfaction.,1,wpep6x,"Disclaimer: I am inherently out of my depth on this sub. If there is a better sub you know of that this post would fit on, please let me know.

Does anyone know of a way to pull/analyze data from 3rd party review sites? I'm vetting candidates for management positions in my restaurant and want to use data from sites like yelp, opentable, google, etc to use as benchmarks for past performance. 

For example, say a candidate worked in a management position at restaurant x from may 2019 - nov 2021. I would want to see what their average reviews were like during that period and compare them to the period before and after their tenure. 

This would also be helpful to use in house to gain insight into how certain actions (ie: change of operating hours, staffing, seasonality) affect our reviews

Second disclaimer: I am aware that plenty of people might have opinions on the value of 3rd party review sites, and may be opposed to my leveraging this data to make hiring decisions. Trust that I have my own misgivings about the value these sites bring to our industry. Nonetheless, they are a piece of the puzzle that I can't ignore. TLDR: This is not a solicitation of anyone's opinion on Yelp

&#x200B;

Thanks in advance!",data,2022-08-15 16:43:18,1
"Try Sankeymatic. Very easy to use and no coding required.
https://sankeymatic.com/

If you get stuck let me know and I'd be happy to help.",3,wowj5y,"Eyoo, I am currently writing my Master's thesis and I wanted to include a Sankey diagram to illustrate the response I got when contacting companies. However, it turns out it that requires some basic knowledge of coding, which I do not possess at all.

I identified 150 companies and decided to contact 50 of them. of the 50 I got 15 replies. of those 15 I managed to get 7 interviews (I know, it is a shitty sampling size). Are there any data nerds/legends in here that could help me create a Sankey diagram, or have any ideas on how to best illustrate the response?",data,2022-08-15 03:51:46,2
"Not a spreadsheet app, but I would recommend using pandas (a python library) on Jupiter notebooks. With it you can do anything that you would do in a spreadsheet, and much more, but it would require a bit of learning and googling if you're unfamiliar with it. 

Spreadsheets are not designed to work with large datasets. You would use a spreadsheet when data is smaller and you need to see the full table while you're working on it. With pandas you do not see the full table, but you can manipulate the data by using python and do it much faster than any spreadsheet.

It's also a great skill to have and a good avenue to explore if you're into data.",4,woriej,"For work, I have to analyze a spreadsheet. It's approximately 8K rows per tap, split over about 4 tabs.

Unfortunately the sheet seems to be too large to work with in Gsheets. I got it imported but whenever I try to load it the tab crashes.

I use Ubuntu Linux so have a preference for working in cloud software for better compatibility with other OSes.

Are there any other games in town (something cloud-hosted; spreadsheet analysis and perhaps basic visualisation and charting)?

Thanks!",data,2022-08-14 22:41:35,2
"Not sure if it's what you're looking for, but Lloyds of London publish a lot of that insurance information at the syndicate level:

https://www.lloyds.com/about-lloyds/investor-relations/financial-performance/syndicate-reports-and-accounts

Needs a chunk of downloading and extracting from the pdfs, but should be gwp/nep/nwp and claims ratios, probably at an insurance line level somewhere.

They used to do a more helpful summary document, but I couldn't find it. Might be worth emailing someone, they're often quite helpful.",2,wogcf4,Hi everyone. I wonder if anyone could help. I am a data analyst and working on my personal portfolio. I am looking for a dataset where I can visualise claims data/premiums written etc. Where can I obtain this? Any insight would be appreciated.,data,2022-08-14 13:48:37,1
"Capacity and availability planning. How much traffic needs to pass, how fast, how reliable? Once a system is designed and built -- measure it to make sure it is meeting the requiment of capacity and availability. If its not meeting those goals-- determine where the problem is and develop cost effective solution.

Key metrics: availability, load/utilization over time.",3,wn9ui8,"Basically, my university has a course on network analysis (within the data science major) and I am interested in taking it although I do have some questions. I wanted to know what some real life applications of network analysis have been (that you have worked on or heard of)? Also what sort of data is needed and is network analysis feasible in the corporate world (I’d imagine there would be a lot of use cases in marketing and logistics)? Or is it largely used for government projects like city and public transport planning?

TLDR: where is network analysis used, what are some examples of it (if you know any) and what sort of data is required to perform network analysis?",data,2022-08-13 01:56:46,3
"Not sure I understand what's going on here. When you say there is no overlap, what do you mean? It looks like quite a few people are overlapping buyers?

Are you trying to find out 'if someone buys product 1, how likely are they to buy product 2 later?'",3,wlus7s,"Can someone help me understand the correct methodology/logic I should be using here?

In my example, you’ll see 19 orders.  I am trying to determine, how many more people incrementally purchased product 1 (vs. product 2).  I know there is virtually NO overlap b/w both products so my calculation is simple (product 1/product 2).

My end goal is to have one overarching number for product 1 incremental %.  However, I am confused as to whether I should be averaging 19 orders or looking at them in aggregate.  By looking at them in aggregate, I am able to unduplicate all the people that purchased product 1 or 2.  

So my question is, what is the most sound logic to determine the product 1 incremental %.  As you can see, the difference in methodology leads to vastly different results (37% vs. 60%).

https://preview.redd.it/bvshqcg1z3h91.png?width=1874&format=png&auto=webp&s=3da90ff143dec51c059ed1a5edf05a51f8a4f3f3",data,2022-08-11 08:52:52,16
"I love that you posted it under r/data - you'll know why as you read further :)

# What is Privacy Center?
Privacy Center is a essentially a part of the product website where users are able to manage their privacy settings. It could even be a section within the Settings panel.

# What all does a Privacy Center include?
There really is no prescriptive list of what all it should include but here are the bare minimum:
- ability to opt-in and out of receiving marketing emails
- see the various marketing lists (these are various audience segments that the company maintains) that they are part of and remove/add themselves. Not every company maintains such lists publicly. Most companies actually use a blanket marketing opt-in. A fictional example: I may opt-in to receive marketing email from cnn.com and but I may specifically opt-out of receiving the daily digest. The vice versa does not apply though - i.e. a user who has opted-out of marketing communications can't be contacted about, quite literally, anything other than `essential` product notifications.

Optional, if GDPR applies to your company:
- a mechanism to request deletion of account
- a mechanism to request deletion of ALL data about that user
Please note that the actual deletion doesn't have to be real time or happen on the site at all. Legally, you are just required to give users an option to request. And, then you honor the request using some internal process that can last months. But keep in mind, if someone is requesting deletion, they likely want it gone in days vs months.

There may be more advanced options available and it depends on how much time money and effort a company has invested in managing their marketing communications. Some examples:
- Snooze marketing emails for N days
- Reduce frequency of emails
- Give feedback 


Having said all that, only media companies, who's bread and butter is newsletters, ads and marketing, do this well. All others, including even major tech companies just don't care enough to develop a privacy center. Eg. GitHub with >10million users worldwide doesn't have a privacy center. It never was a priority for them. And, this is why I love that you have asked this question in r/data 😀 . I hope this was helpful!",1,wlpkkb,"If you want to define a privacy center in clear and concise words to a less aware person, how would you do it?",data,2022-08-11 04:55:51,2
Unsure how you’d get this datapoint but perhaps somehow to monitor work from home workforce? Which would equate to less miles drove per year = longer time between new tire purchases.,2,wl7esv,,data,2022-08-10 13:09:18,3
"By EURO, do you mean the EU? If so, interest rates are set by the European Central Bank, and they definitely don't change every day.

Inflation is measured by aggregating the prices of basic goods and services and comparing the result to a previous point in time. As far as I'm aware, Eurostat does that once a month.

I don't know that it's any different in the US, so it would be great if you could post an example of a source for that, it might clear things up.",1,wlap7n,"Hello, first post here. I am a Data Scientist working on a personal project (quantitative finance bot). I am trying to find daily interest rates for EURO and daily inflation rates for EURO. All sources I can find post monthly, qtr, annual, but not daily. USA inflation and interest easy as pie, but EURO forget it.

Any government sites with API access for free? Any python packages that might link to this? 

Thanks for any and all help.

\_M",data,2022-08-10 15:25:00,7
Do you mean emissions data and so on? Then yes.,0,wks2yo,,data,2022-08-10 01:31:35,2
"Dataversity has loads of free webinars Twitter data peeps I follow: @datachick @RSeiner @SQLRockstar, @stdatawhisperer (Scott Taylor)",5,wkf7fw,"I am looking to start following either bloggers, instagramers, tiktoks, Facebook, subreddits, magizines, influencers, or books that discuss the latest trends in data analytics, data science, data sets, tips and tricks, tutorials, case studies, etc. This is in hopes to stay more in tune with the current landscape of data professionals. 

I currently work at a company that is a bit behind the curve when it comes the the way they handle data and reporting..but I don't want that to hold me back from my growth and exposure to different tactics, ideas, and ideals. 

Any recommendations is much appreciated! 
Tyia!",data,2022-08-09 14:28:00,5
The website told me I was pregnant. That is not only concerning but statistically unlikely as I have a penis. It made me consider which distribution would best describe my situation.,1,wklwns,Statistical Distributions are an important tool in data science. A distribution helps us to understand a variable by giving us an idea of the values that the variable is most likely to obtain.,data,2022-08-09 19:37:00,1
I am willing to do these kinds of projects,1,wi3dds,"I recently took on a consulting project to create a large number of data analysis and some machine learning projects for a consulting company.

There are so many that I can't possibly do it all on my own. I am struggling to figure out where to find someone who would have data science (python, analytics, and some ML although not required) skills AND would be willing to write projects (code, description) and an accompanying Medium post like blog post that describes the whole project.

I have tried fiverr and the results are pretty dim. Most folks there are writers first and analysts second (if at all). So the quality of the content is pretty low. 

Second thing I have tried is reaching out to those who publish DS related articles on LinkedIn - they rarely ever respond let alone show any interest :( 

After 2 months of searching and trying, I have hit a wall honestly. Are there any data practitioners who would like to work on projects like this and in return get some $$, publicity, shares on LinkedIn, reddit etc? Where could I find them? I would really appreciate some avenues & referrals.",data,2022-08-06 18:15:00,7
Try r/Sabermetrics!,1,whuazz,"Does anyone know of a service which provides live play-by-play data of MLB games?  Preferably has an API, but a scrapable website would work",data,2022-08-06 11:01:16,1
I got my Master’s in Business Analytics in 2019 and have a completely unrelated BS in Theatre. You might just have to search different universities. I’m pretty sure my GRE scores were a larger factor for me than my bachelor’s.,2,whgcpa,"So I have a BA in Marketing but I've been slowly transitioning into Data Analytics. I earned a couple certificates like the Google Data Analytics Specialization (where I learned R, Tableau, and SQL) and Python Programming Specialization, and currently taking a course for a Tableau Specialization certificate.

I noticed that finding a job for Data Analyst with a BA in marketing is much harder so thats why I've been practicing my technical data analysis skills every day. I've even created a very clean website portfolio with one case study I've done so far. But I've been thinking about pursuing a Masters in Business Analytics but just not sure If I can. I heard that to apply for a Masters in Business Analytics, I would need a BA related to that field.",data,2022-08-05 22:20:09,6
Are you looking for actuarial tables?,1,wh3x6x,,data,2022-08-05 12:20:45,2
What question are you trying to answer?,3,wgvooj,"So  I have school data which is very messy. I am wondering of someone can help me restructure it in a way it is easy to pivot/analyze. Below is snip of data.

https://preview.redd.it/zdhp8m88ewf91.png?width=1219&format=png&auto=webp&s=ec25f1045940081771d39765fbac1f3efef35350",data,2022-08-05 06:36:32,14
"Funnily enough I'm doing research on packaging waste and recycling myself. The EU publishes a wide variety of data regarding waste and its disposal on it's Eurostat site: [https://ec.europa.eu/eurostat/databrowser/explore/all/envir?lang=en&subtheme=env&display=list&sort=category&extractionId=ENV\_WASTRT](https://ec.europa.eu/eurostat/databrowser/explore/all/envir?lang=en&subtheme=env&display=list&sort=category&extractionId=ENV_WASTRT)

You may not find exactly what you are looking for, but it might be a useful place to start.",1,wgsnvd,"Data redditors, I need your wisdom (not sure if this is the right place to post, Im sorry if it aint).

For a journalism project, we are looking for data / site / dataset that has statistics / data on pollution in rivers in the EU. e.g. if I want to compare (plastic) pollution between Hungarian and Ukrainian rivers. Does anyone know any resources I could consult?

Thanks in advance!",data,2022-08-05 04:08:39,1
What issue are you having with Tabula? It works fine for me.,1,wfv1et,"Hi, 

Is there an open source alternative  to tabula? Since the devs are not maintaining it anymore. GUI is preferred

Must be offline as it will be used for handling confidential documents.

Able to highlight a particular area and repeat the selection over pages and export as csv. 

(You can try it yourself at [https://tabula.technology/](https://tabula.technology/))",data,2022-08-04 00:41:08,2
"Machine learning could do this on a local machine local vm with just semi decent CPU running a neural net could then pipe output data to whatever database platform needed this could be done pretty much all in python, else paid software solutions got to be floating out there, but check with your data governance team before passing data to outside software applications. Sound like a fun project, good luck!",2,wfltbv,"I am currently manually entering data from a handwritten paper document into a Microsoft Access database. While it doesn't have to be Access, I am looking for a way to scan my documents directly into a database or spreadsheet. I know the Microsoft Excel IOS app has a scanner feature, but I am not sure if it can handle handwritten notes. Does anybody know of any options for doing this?",data,2022-08-03 16:42:49,2
R,2,weytl5,"I'm talking about at least tables but, ideally, also graphs.

I want to embed those reports in my own webpages.",data,2022-08-02 23:15:52,5
"USGS Materials Commodity Survey might work for you.
    
https://www.usgs.gov/centers/national-minerals-information-center/historical-statistics-mineral-and-material-commodities
    
Also FRED is generally where a lot of people go as a start point for economic research. They essentially aggregate a shit ton of data into one simple platform. 
    
https://fred.stlouisfed.org/
    
Virginia Tech has a cool econ department source page as well.
    
https://guides.lib.vt.edu/subject-guides/econ/data-sources",1,wetzfl,"I need something like a CSV, excel, or JSON file I can download and work python magic with. Any ideas?",data,2022-08-02 19:07:12,2
"Ha, really? Jpeg to pdf then ocr then copy paste to excel maybe...",3,wekv41,"Dear Data Reddit, I have some datasets but it’s in JPEG is there a way to convert that to Excel? 
This is urgent please 🙏🏽😭",data,2022-08-02 12:35:29,5
"My sister and I had some amazing conflicts. That there were no emergency room visits or deaths is remarkable. I'll try to get you the dates, but they happened 40-50 years ago.",1,wcjfsb,"In particular, I'd like to know the following info:
1. Which year did the conflict start?
2. How long did the conflict last?
3. Which two countries started the conflict?
4. How many casualties did the first side suffer?
5. How many casualties did the second side suffer?

I have found some datasets, like one from the ucpd, however I don't think that it contained the casualties, unless ""gwno"" means casualties. However, such data exists. The Wikipedia page ""list of interstate wars since 1945"" contains many such conflicts, and links to the pages on the conflicts themselves, which contain the causality estimates. I wondered whether there existed a neat dataset of such data so that I don't have to manually take it from Wikipedia.

Thank you.",data,2022-07-31 00:55:58,3
Can you get it from EDGAR? https://www.sec.gov/edgar/searchedgar/companysearch.html,3,wbg9r5,"So basically I need to collect some financial data like (net income, EPS) from financial statements of around 30 different companies. This data needs to be for 5 years. Does anyone have any idea on how to do this quickly/ automate the process? I don’t want to Google it every single time obviously. I looked into making A web scraper but I’m not sure if this would be the best option.",data,2022-07-29 14:29:59,9
"I got news for you all, odds are (45%-ish) your health data (if you’ve been in a hospital) has already been sold, used, and planned for monetization. Companies like Premier collect patient, hospital operations, and hospital performance data for almost half the patients touched in the US on an annual basis: https://premierinc.com/solutions/data-analytics",3,wbbnd4,How would you feel about getting paid for your healthcare data if it was completely anonymized and you could direct what organizations could purchase it?,data,2022-07-29 11:07:50,6
"You are going to have to Google/check this.

Google AWS compute and AWS Storage.

The have Glacial storage which is dirt cheap (meant for large very infrequent access of data.  

Regards compute, they have Lambda functions (not sure if they support Python, but probably).which are meant for in frequent triggered compute.

As with all cloud solutions triple check the cost of data transfer in AND more importantly, out.

Glacial storage may not be appropriate as it seems that you need to access or query the data.  Here you need to consider the frequency and where it ends up and who uses it. When you take the data out of the AWS ecosystem you pay, so probably best to process/view/query it there. I think they do AWS Glue/Data Factory? for ETL and some log viewer tech , I think called CloudFront/CloudTrail. You could roll your on with an EC2 instance which you instantiate only when you need it and shutdown immediately afterwards or workspace/virtual machine.

Don't underestimate how people or apps will authenticate  and be authorised to access the data.  Essentially you can tie back to your organisations directory service and synch and then create roles.

Also depending on how reliable it needs to be and size of data you may need a dedicated pipe between your infrastructure and AWS.

Please bear in mind I have not checked all this and it's been a while and AWS changes/evolves quite a bit!",2,wbm5ad,"Hi, I don't really use reddit much but I'm doing an internship right now and wanted to ask for some advice as I know next to nothing about these things. Any help is greatly appreciated, sorry it's kind of long.

&#x200B;

The company has about 2 Gb  of data that is produced as logs from the robots each day. These logs are stored as JSON in .txt files and they want to automate the backup process. They don't want to back up the data with a server in house as the power's too unreliable in the area and they want remote access from anywhere in the world 24/7 (or as close to it as they can get). They also want to have editor privileges for some of the people who can access it and viewer privileges for everyone else who can access it to minimize the risk of accidental deletion. Lastly, they want to have the data that's backed up automatically processed by some scripts that are stored remotely (ideally python) that I'm going to build and then have the compressed, processed data stored in a separate folder in the backup. The regular logs need to be deleted after 30days but the processed ones held on to for at least 2-3months.

&#x200B;

The current approach is to use a 3rd party app to auto upload the txt files to drive then manually delete them after 30 days. The other parts have no solution and they'd rather not keep using a 3rd party app.

&#x200B;

My thoughts are either keep what they have but use Gdrive to handle the uploading of logs from linux command line to drive. Then use the sharing privileges of the parent folder to set viewer/editor privileges for people accessing it and somehow run python scripts from colab as new logs are uploaded. 

OR use the AWS CLI for uploading then use AWS's CDK or python SDK and EventBridge to auto run python scripts for processing the logs.

&#x200B;

I am in college with good coding experience but 0 knowledge of anything outside of what will be in the processing scripts themselves. This internship is a learning opportunity for me more than anything else so please let me know if I'm misunderstanding anything. Both these ideas have their own issues too so if there's a better solution please let me know.

&#x200B;

THANK YOU SO MUCH FOR THE HELP!!",data,2022-07-29 19:11:17,2
It's called a network graph (or diagram): https://www.data-to-viz.com/graph/network.html,2,w92908,"One of my biggest personal projects is ""mapping"" out the various connections between political pundits, think tanks, op-ed writers, etc. However, the problem is that while I have thousands of articles and hundreds of authors catalogued, I have no idea how to actually create a map for those kinds of relationships. Basically, I want a spiderweb, photos-connected-by-string-conspiracy-style thing where you have dots and lines showing connections.

I've seen actual researchers use this kind of thing when analyzing things like the relationships between YouTube channels or social media users:

&#x200B;

https://preview.redd.it/ioyg20llr0e91.png?width=1920&format=png&auto=webp&s=c5963fb1b64c9dd6463f9d149650e84c061b4ced

&#x200B;

https://preview.redd.it/5dptilqnr0e91.png?width=600&format=png&auto=webp&s=ed1d0a7196137c06b2363217857a72a6edcf6613

What programs would I use to create something like this? Preferably I'm looking for something that would be free, something I already have (i.e. Word, Excel), or something that is a one-time purchase.",data,2022-07-26 19:15:10,4
You need software that can do OCR (i.e. convert image to text). Lots of software available.  E.g. https://en.m.wikipedia.org/wiki/Optical_character_recognition,3,w8foew,"Hi,

Is there a way to extract tables from PDFs, preferably with the help of some AI? 

I am trying to extract tables from old census data (1881, 1901). The majority of the material is readable, some of it has slightly faded but can be recognized. I have attached a picture of reference as well.

&#x200B;

Any assistance will be greatly appreciated. 

https://preview.redd.it/oqeqex9txvd91.png?width=1920&format=png&auto=webp&s=5674fc97cb1cf155c603ea9f53500c2b26ce4840",data,2022-07-26 02:56:05,6
"If you want to dive into it, you can look at QGIS",13,w7y2r6,,data,2022-07-25 12:37:04,24
"The square root of the average square is not the same thing as the average. It's assigning a larger weight to larger deviations.

Simple example: Let's say you have -2, -1, 1, 2. The average is 0. The average absolute difference is 3/2 = 1.5, the standard deviation is sqrt(1/4 (4+1+1+4)) = sqrt(5/2) =~1.58

Let's look at a more extreme example: -1,-1,-1,-1,-1,5. The average is 0, the average absolute difference is 10/6 = 1.66, the standard deviation is sqrt(1/6 (5+25)) = sqrt(5) =~ 2.24",5,xigzda,"I distinctly remember my intro professor describing it as such, but when I asked my upper division professor, he said that wasn't true because you square the differences. 

But standard deviation requires you to take the square root of the differences at the end of the calculation, so doesn't it make it back to average again? Maybe the slight difference is from n-1 on the formula...",statistics,2022-09-19 09:19:44,12
"Jersey ""1"" and jersey no. ""4"" have no meaningful, quantifiable difference, nor zero point (0). So it's not like you can say ""the difference between jersey 1 and jersey 4 is 3"". 3 what? Similarly, you can't say jersey 4 is ""four times"" what? of jersey 1. In this case, the jersey numbers are just categoricals packaged in numbers.

What I meant with ""categoricals"" is best understood as nominal categoricals. It doesn't make sense to consider jersey 1 ""lower"" than jersey 2, although the numbers 2>1; the numbers here are just names expressed as numbers.",26,xiapig,"I am taking an entry statistics class in my senior year of high school and since my stat teacher cannot give me a straight answer I thought to ask here. When given a jersey number, a zip code, a phone number, are these all not examples of nominal data that is quantitative? Emphasis on the jersey number one because it was a test question I got wrong. How can a jersey number not be quantitative?",statistics,2022-09-19 05:31:46,36
"At least one block:

1-0.9995^1440

Exactly 1 block:

1440*0.0005^1 ×0.9995^1399",3,xigbsh,"I'm trying to calculate the probability of solving a Ravencoin block over a specific number of trials. The probability of solving a block does not change the probability of solving the next block. The outcomes are independent.

The probability of solving two blocks should be very low, so I'm looking for help on whatever is easier - solving ""exactly"" one block out of all the trials, or ""at least"" 1 block.

Things I know;  
n = the number of trials (will probably be 1440)  
p = the probability of solving any one block (currently around 0.0005)

Let me try and explain a different way. Given that I have a ""p"" chance of solving 1 block, and if I repeat the process for ""n"" blocks, what is the chance of solving 1 of those n blocks (and it can be exactly 1 or at least 1 - whichever is easier to calculate).",statistics,2022-09-19 08:57:58,6
"Unfortunately i don’t know spss, but principal components will be the eigenvectors of values in your covariance matrix. If there’s a covariance output it might be in there. Good luck",3,xhwyq2,I can't remember for the life of me which outputs from spss to use to form the PC1 and PC2 equations. I know that two have to be extracted. Is it from the correlation table by any chance?,statistics,2022-09-18 17:35:40,3
"If you're using SPSS, get the PROCESS dialog and use it to estimate simple slopes, which is what you want to look at to see what's going on in your interaction effect. PROCESS can do OLS or logistic regression, so it should work. The first few models (1-3) are interaction (moderation) effects. I think you want model 1, which is a simple 2-way interaction effect.

https://www.processmacro.org/index.html",2,xhubk5,"Let's say A is race (white/non-white), B is locality (urban/rural), and the outcome is love of dogs. A is not significant, B isn't, but A*B is. How do I tell if there is a stronger or weaker association between race and love of dogs in urban areas compared to rural ones? I'm looking at my SPSS output like 😱",statistics,2022-09-18 15:38:42,7
"It is actually the other way around. ""Regression"" as you usually think about it is a special case of the Generalized Linear model where the error distribution is gaussian and the link function is the identity link.",58,xhgykd,"I would like to double check that GLMs are a type of regression analysis please.  

Thanks!",statistics,2022-09-18 06:40:22,15
"With a t-test you are throwing away information (i.e., about potential interactions). I think ANOVA (or logistic regression) is the way to go. 

My understanding so far is that you've got these variables...

* DV: crying (does each participant have a probability, like a rating, or is it just ""cried vs didn't cry"" for each story?)
* IV 1: which pair (3 levels) - Repeated measures/within-subjects factor
* IV 2: which item in the pair (2 levels) - Between-subjects factor

If I got that right, then how you analyze this depends on your DV.

If ""probability of crying"" is a non-binary numerical variable--that is, each person, for each story, gets a *number* (like a rating of how likely they are to cry, or someone else's rating, or a ""probability of crying scale"" score), then do ANOVA^1.

 I've understood you correctly, you'd perform a 3 (within) x 2 (between) ANOVA.

If ""probability of crying"" is just yes/no for each story (e.g., Jenny cried for story 1a, didn't cry for story 2a, and cried for story 3a), then you should do logistic regression; ANOVA could probably be used if this isn't going to be published, but ANOVA has a not-insignificant chance of giving wrong answers every once in a while if used with binary outcomes.

Logistic regression is more involved than I think you're going to get, from what I've seen (though that's a judgment on my part). It will involve coding your IVs/levels, correctly specifying whether each is within or between subjects, and then knowing how to run and interpret the analysis.

^1 There's another issue here, which can be a really big one and I'm glossing over it; if you have a rating scale or similar as your DV, there's a good chance you should technically not use ANOVA, because your DV is ordinal, not numerical (i.e., not interval/ratio). However, there are a lot of caveats to that blanket statement. For this statement, I'm assuming it's either truly numerical or ""numerical enough.""",6,xhkxlp,"Hi, 

I have to compare if there is a difference in probability of crying depending on 3 independent variables. Each of them has 2 levels. 

The study was conducted in a way that the participants rated the probability of crying in different stories. 
So, there were 6 stories in 3 pairs. Each pair was about one independent variable. Pairs of stories were not related to eachother. Each participant rated the probability of crying for only 3 stories (only one level of each independent variable). So in that way, the samples are independent. 

I can analyse them using 3 t test - to compare the pair of storis 
Or i can use 3way anova to compare it simultaneously. 

Is there a way which is better? Also, my distribution is far from normal. Is that a good enough reason not to use these tests but rather nonparametric?",statistics,2022-09-18 09:20:00,10
"> I'm imagining a bivariate Guassian PDF. The probability of seeing x_1 AND x_2 is just the value of the PDF at the point (x_1, x_2)

This is the *density* at the point, but the density is not a probability. The probability of observing  the point (x_1, x_2) is exactly zero.

> It's the sum of ALL points consistent with x_1 + all points consistent with x_2 (so two, infinitely thin lines that intersect at (x_1, x_2). 

This is also zero.",3,xhsq0n,"Suppose I have some multivariate Gaussian N(μ, σ) where μ = [μ_1,...μ_k] and σ = [σ_1...σ_k]. Let's say that I draw a single sample x = [x_1...x_k]. 

I am interested in the probability of seeing x_1 OR x_2 OR x_3 ... OR x_k. In discrete, univariate probability distribution, the probabilities sum (what's the probability of rolling a 1 OR a 2 on a 6-sided die? It's the sum of both probabilities). Does the same logic work here?

I'm imagining a bivariate Guassian PDF. The probability of seeing x_1 AND x_2 is just the value of the PDF at the point (x_1, x_2), but the probability of seeing x_1 OR x_2 is hard to visualize. It's the sum of ALL points consistent with x_1 + all points consistent with x_2 (so two, infinitely thin lines that intersect at (x_1, x_2). 

EDIT: here's a worked, discrete example.

Here's what I'm trying to generalize: Suppose I have a simple, 2-element system of binary components ({X_1, X_2}) with the following PDF:

P(0,0) = P_1

P(0,1) = P_2

P(1,0) = P_3

P(1,1) = P_4

The probability of seeing X_1 = 0 OR X_2 = 0 is the total probability of all configurations consistent with the logical operation. So it is P(0,0) + P(0,1) + P(1,0). The states of X_1 and X_2 are not necessarily mutually exclusive (if X_1 is independent of X_2, for ex). 

The goal is the generalize this general idea to multivariate Gaussians.",statistics,2022-09-18 14:31:50,8
"Elementary explanation only coming up here, so apologies in advance for not addressing more advanced nuances.

ROC is just the total accuracy of your model as measured by true positive predictions and false positive predictions. It measures how accurate your model is at predicting a dependent variable on a scale between 0 and 1.

A roc with an area of 1 is perfect; it means your classifier predicted the right outcome 100% of the time.

A roc with an area of .5 is indeterminable from random chance, and means your model stinks.

Lower than .5 and random chance is actually better at making predictions than your model. Really not good.

Using something close to your example, a model that had an AUC of .95 and was used in medical imaging to identify whether a growth was cancerous or not could potentially be pretty good (depending on other factors). It means it was 95% accurate at predicting the right outcome.

It’s an evaluation of your models predictive accuracy; that’s it. It’s not perfect and doesn’t take into account a significant amount of important factors but on its face is a pretty reliable standard for assessing how good a ML model is.",3,xhwju5,"Hi all, I never covered ROCs at all in any of my uni stats classes and am having a lot of difficulty in interpreting the AUC stat of one I've run into in a research article. 

I think the best way to describe this difficulty is as one of scope,  the initial question I have found myself with rests on an illustration - If a given test has an AUC of 0.95 would that in clinical practice then be taken to mean something along the lines of - *""Hello Mr Smith, your scores on this test were above the threshold and that means there is a 95% chance that you have condition X.""* ?

But then more broadly and zooming out a bit, that question I think stems from the fact that I'm entirely unsure about whether the AUC stat tells me anything about a test's informativeness/utility for a given new case that does/doesn't meet identified thresholds in the first place.",statistics,2022-09-18 17:16:50,8
"So the coefficient being referred to is ""Pct temporary buildings"" in the model containing all schools, -0.020. The independent variable here is the percentage of temporary buildings out of the total building space, and the dependent is the percentage of student attendance. In other words, a 1 percent increase in percentage of temporary buildings leads to a 0.020 percent decrease in student attendance.

If there were a 5% increase in temporary buildings, we would expect to lose 0.020\*5 = 0.1% of the students, or 1 per thousand. They're just chosen to be round numbers to intuitively illustrate the magnitude of the effect.

Significance is not about the size of the coefficient, but about the size of the associated p-value. The p-values are not directly shown in this table hence the asterisk markings.",2,xhv999,"I was wondering if someone can ELI5 the following interpretation of a coefficient table that I found on a paper ([Table](https://ibb.co/Nxr4nX9)):

* “The coefficient, which reaches a significance level of 0.01 in the model containing all 226 schools, indicates that a school with 1,000 students that uses temporary buildings as 5 percent of its total building space can expect to lose about one student per day more in student attendance than a school of the same size without temporary buildings”.

1. Am I correct to assume that “1000 students” refers to 1 unit change in the slope? 

2. I’m not sure where “5 percent of its total building space” and “expect to lose about one student per day” comes from. 

3. I’m also confused about the significance levels. Some coefficients on the table are below 0.01 and 0.05, yet they are not marked as significant, and others are above 0.01/0.05 and they are marked as significant. Why?

[Table](https://ibb.co/Nxr4nX9)",statistics,2022-09-18 16:19:07,2
This is one a listen that could be helpful. https://podcasts.apple.com/us/podcast/customer-insight-leader-podcast/id1503058306,2,xhbym6,"Are there any podcasts/blogs that contain interviews of people working in different domains as data scientist?

I currently work in model risk validation(financial services) but would like to change my domain in the future. Hence, I am looking to understand the applications of data science in other fields. Thanks in advance!",statistics,2022-09-18 02:16:15,6
Yes! Look up 2 sample t-test.,4,xhp4lc,"Hello, I have two group of respondents who answered the same question. One group is n=283, the other one is n = 1825.

Is it okay to compare these two groups' answers to the same questions with independent t-Test despite the fact that the sample sizes differ largely? If not, what should I do to be able to compare them?",statistics,2022-09-18 12:06:15,4
Linear algebra always helps.,8,xh7dqr,"I am in a statistical learning undergrad course. I was curious and wanted to try reading ESL, alongside ISL to see if I can understand the things we learn at a deeper level. But I can’t help but notice how hard it is to read ESL compared to ISL. Does anyone know what math I would need between ISL and ESL?",statistics,2022-09-17 21:49:43,6
"e is too big for such a small sd.

For n=2, e=1.96×0.1/sqrt(2)=0.14

For e =0.05, n=(1.96×0.1/0.05)^2 =

15.4 or 16",2,xhp54w,Sample size is coming less than 1 using formula (zs/e)^2 where e is tolerance. Am I going any mistake. My sample has most values same 1 and e is also +/- 1. Standard deviation is very less around 0.1. What this means? Can anyone explain. I've to find sample size for 95% confidence and the given tolerance.,statistics,2022-09-18 12:06:51,2
"A correct answer depends on what your estimator for the mode is (not as simple as it sounds!) and what the distribution you're saming from might be. Oh and sample size will matter.

I believe I can probably guess what answer the person who wrote the question wants\* but I'd that's all the information they gave there's no single correct answer. I believe you can rule out each case for some circumstances by counter example but to demonstrate one you need a usable definition of sample mode that works in general. 


\* check your course materials for that, though. Who knows for sure what they might think?",5,xhnc6q,"I had this question on finals yesterday and didn’t know the answer.

Circle the correct answer:

Var(x bar) < Var(Mode)

Var(x bar) > Var(Mode)

Var(x bar) = Var(Mode)

Var(x bar) ≠ Var(Mode)",statistics,2022-09-18 10:55:20,21
"There are online power calculators 

https://www.stat.ubc.ca/~rollin/stats/ssize/n2",2,xhruge,"Hey guys, I have to calculate 4 power analysis for a thing in college. However, I have neither the understanding nor the program to calculate that.

Could someone help me out? From my estimate it can be done pretty fast if you know power analysis.

I would be eternally grateful",statistics,2022-09-18 13:56:36,5
Not sure but I'd definitely spend 4 years procrastinating.,141,xgt1az,"Given this time, what topics would you study, how would you research programs, what people would you reach out to, etc.?

**My background:**  just graduated with BS in statistics, math background is only calc  I-III and linear algebra, did well in my upper stat electives (bayesian  statistics, statistical learning, unsupervised learning, stochastic  processes), and now I work in big four

**My plan:**  self-study important topics (at least multivariable, linear algebra,  real analysis, ESL) and research many programs (so far like UWashington  and Colorado State) while working a few years for  industry experience  and pocketing as much cash as possible since I'd prefer studying  full-time instead of part-time

Any  help/critique is appreciated even if it doesn't directly fit the  background or timeline I'm working with. Also happy to explain more if  it helps. Thanks!",statistics,2022-09-17 10:52:25,56
"For treatments versus pre-post, you can make up two new variables: mean of treatments and mean of pre-post. Then in a Style x Computed variable ANOVA, look at the effect of the computed variable. You can do a Groups x Pre-post which will give you a test of pre versus post as well as the interaction. You can leave out pre-post data and I believe SPSS will by default do a trend analysis  on Trials including interactions of trend components with Style. In interpreting your results you should consider how many significance tests you have done.",1,xhizp7,"Hi, I have collected data from an experiment that uses a repeated measures design. It is something like 3 different styles of exercising, with 10 measurements (pre-, post-, and 8 trials) done across 2 different levels (e.g., upper and lower body). 

I understand that this would be a 3 X 10 X 2 design, which the repeated measures ANOVA conducted using SPSS returned a significant main effect of condition and measurement. I wanted to test my hypotheses using planned contrasts, but I am quite perplexed by how I should go about doing this. 

My hypotheses are that 1) the 8 treatment trials will differ from the pre- and post- measurements, 2) a linear trend across the 8 trials, and 3) a difference between pre- and post- measurements. 

I am quite out of my depth here, as I have not conducted an experiment like this before. What are my options for testing my hypotheses? I understand that custom contrasts would allow me to test hypotheses 1 and 3, but I am unsure as to how I would set up a polynomial contrast to only use the middle 8 measurements. 

A related question is how would setting up these contrasts be different from running each comparison separately? For example, removing the pre- and post- measures, or average the 8 trials and re-running the ANOVA for the contrast readouts.",statistics,2022-09-18 08:03:10,1
"The Galton Board does not simulate a normal distribution. It's nearer to a rough approximation of a binomial distribution.

Any statistics program and most spreadsheets can simulate values from a normal distribution. Some can be run online.",11,xginvc,,statistics,2022-09-17 03:11:40,8
">to make things a bit more complicated - during the test you can actually quit it without the score being recorded. So potentially you can avoid lower end of results by forfeiting the test if say you got 1/22 correct and you only have 3 more cards to go (therefore cannot beat the random chance).

>This will skew the statistics slightly but could this invalidate the 40% result?

Of course it invalidates the results. You're not measuring the underlying probability of guessing correctly, you're measuring what results people get when they are allowed to cheat.

If this test is set up properly, 20% of guesses will be correct and 80% incorrect. If you're allowing cheating then you're testing what impact cheating has on the results when you know for sure what the result is with no cheating.",5,xgwvyr,"A person guesses one card out of a 5 card deck, the deck is then shuffled to ensure randomness. This process is repeated 25 times and constitutes 1 test.

An average of % in correct guesses is carried out over 20 tests (that is 500 shuffles in total). The person obtains 40% of correct guesses. 

I presume this is higher than random chance which should be 20% over such a large sample size right? Or have i missed something?

Edit: to make things a bit more complicated - during the test you can actually quit it without the score being recorded. So potentially you can avoid lower end of results by forfeiting the test if say you got 1/22 correct and you only have 3 more cards to go (therefore cannot beat the random chance). 

This will skew the statistics slightly but could this invalidate the 40% result?",statistics,2022-09-17 13:33:32,13
Maybe try Generalized Linear Models with examples in R. It’s gentle and it includes examples in R which is nice,1,xgvgng,"I have only ever taken a basic introductory statistics/probability class and that was some time ago. But now I am taking a class on linear regression and we are using Kutner. The professor is a nice guy, but the class moves so quickly and he basically expects you to read and digest the textbook at a pace that is too fast for me. I feel like I need some notes that summarize the key ideas and results from each section. Does anyone know of some resources like this that they would be willing to share? I am having a bit of a tough time w/ the course, and the pace I read through the textbook is too slow to keep up with the pace of the class, so I'm falling behind. Any help would be much appreciated.",statistics,2022-09-17 12:33:59,1
"We don't know the duration of each job.  Assuming the resources are working at maximum utilisation to meet the stated completion dates, there's no spare capacity.  No matter when you move resources from area 2, the completion date for area 2 will be no earlier than Q1 2023.  Is this OK that the ""similar completion date"" is later that Q1 2023?


The statistical model I envision is queueing theory, though without the uncertainty of arrival time.  You could put all 23,000 jobs into a single queue and have all 29 resources work on them simultaneously.  Each resource takes the next job in the queue as soon as that resource is complete with a job.  The jobs could be ordered in the queue randomly, although there might be some theory that all the long jobs should be queued before the short jobs (or maybe vice versa), or maybe some other queueing priority.  


With this suggestion, I assume that there could be a new constraint.",2,xgmfp5,"Hi,

First time posting here so apologies in advance if this isn’t the correct place to post. 

Basically, I’m looking for what sort of model/equation I should use for a work related question. I am by no means a statistician.
The below figures aren’t 100% correct and are just an example. It assumes the resources have the same utilisation and productivity. 

In area 1, there are 8k jobs for 9 to complete with an estimated completion date of Q1 2024. 
In area 2, there are 15k jobs for 20 different resources to complete with an estimated completion date of Q1 2023.

I want to figure out at what point in time and how many resources I should move from area 2 to area 1 so that they both have a similar completion date?

I could do it manually through trial and error. But I have to do this for multiple regions which contain between 2 and 4 areas so would take me some time. 

Any help would be much appreciated.",statistics,2022-09-17 06:24:25,6
Sounds like a numerical methods course? It's perfectly important material but intro proof writing + real analysis is probably more important for MS program prep/admittance. Should really ask your profs or grad student coordinator in the department,7,xgeo28,"# 

I’m thinking about taking this upper division undergraduate course. Please let me know if this stuff is useful or not for a career in statistics, hopefully bio stats:

\-Solving Ax=b type problems with direct methods (triangular systems, Cholesky decomposition, banded/sparse systems, Gaussian elimination with and without pivoting, LU decomposition, QR decomposition) and iterative methods (Jacobi and Gauss-Seidel) 

\-Understanding the theory behind computation (rounding errors, sensitivity, condition numbers, backward error analysis, backward stability) 

\-Least squares problem (Gram-Schmidt, orthogonal matrices, QR decomposition)

 \-Basic iterative methods solving for eigenvalues (power method, QR iteration)

 \-Singular value decomposition

&#x200B;

&#x200B;

ALSO: (optional - feel to ignore); 

I’m wondering if I should take this course or a basic proof writing course? (I’ve never taken a proof based math course. I’m planning on applying for an MS in stats next year, I have a physics BS. The into proof writing is a prerequisite for real analysis, but I'm scared to take that).",statistics,2022-09-16 23:14:16,8
"Not necessarily.

Assume A and C are random variables that have no association, and B = A+C. This is could be the situation you described.",13,xgf9i8,"If a paper reports A is positively correlated with B and B is positively correlated with C, can you conclude that A and C are positively correlated without examining the data? Why or why not?",statistics,2022-09-16 23:48:43,10
"This is probably best characterized as a mixture distribution. In a mixture distribution you have a latent group, associated with some probability of group membership, and then individuals from each group follow their own characteristic distributions. In the geography quiz you probably have two groups, average people with baseline knowledge of geography, and people who have studied and are able to get all the answers correct with high probability.",70,xftocl,"I was doing an online geography quiz where you have to name all the countries in the world. In total the quiz has 196 countries. After finishing, you can also see stats of other users who did the quiz to see things like average score (I know this has a very strong sampling bias, as people who do online geography quizzes are not the average person when it comes to geography.)

However, one thing stood out to me. When I looked at how many people got each score, there was a kind of normal distribution centered around \~75 countries. So more people knew 80 countries than 90, 90 than 100, 100 than 110 etc. It slowly went down. But all the way at the end, there was a relatively high peak of people who knew 196. The reason being, that no one memorizes 195 countries. No one studies 190 countries and says ""I'm not doing the last 6"". So it kept getting less until it hit 196, when it suddenly spiked.

A similar thing can be observed in the pi digits world ranking list, where there is like 1 person to know 310 digits, 0 for 311, 0 for 312, 1 for 313, and then 10 for 314, because everyone wants to go for 314. And then after 314 it goes down to at most 1 entry per number. So there is a normal distribution centered at like 100 digits, but with a high spike outside of it at 314.

Is there a name for this type of thing?",statistics,2022-09-16 08:01:03,19
"I would not pay attention to most of these. Maybe double integrals and no more than that. A lot of these concepts are important for mathematical physics. For example, in fluid mechanics, you might encounter many of these concepts. 

&#x200B;

For the purposes of learning probabilistic programming, these are irrelevant. I'd focus on learning about probability distributions, statistical models such as Generalized Linear Models etc.",7,xg6t2s,"I'm trying to get good at probability theory and probabilistic programming ([WebPPL](http://webppl.org/)). How relevant are below calculus (calc 4) topics?

Double integrals, Triple integrals, Iterated integrals, Cylindrical coordinates, Spherical coordinates, Vector Fields Line Integrals, Green’s theorem, Curl and divergence, Parametric surfaces, Surface integrals, Stokes Theorem, Divergence Theorem, Complex Numbers and their properties, Functions of a complex variable, Complex Differentiation, Harmonic Functions, Cauchy Theorem

Approx what % of these topics are relevant?

If you could point out which ones and the brief explanation of the relevant would be much appreciated also!!",statistics,2022-09-16 16:47:39,15
"The  symmetry about the mean of the normal approximation to the binomial would to establish that (in the limit as the number of dice goes to infinity), P(*at least* one sixth of the dice roll a six) -> 1/2.",2,xg32ly,"So I've been reading about the Newton-Pepys problem, and most sites say that as the number of dice approach infinity, the probability that a sixth of them would roll a 6 approaches 1/2. What's the mathematical proof for this? Thanks!",statistics,2022-09-16 14:09:43,2
"You will have better luck in /r/datasets or in a domain-specific group. This group is more about the methods and tools statisticians use, than about locating particular pieces of data online.",7,xgcd2u,"Basically I want to know how many food insecure people live over any decently large distance (1mile, 10mile etc) away from a food bank. If anyone could find a statistic I’d appreciate it a LOT. Thanks in advance 🙏",statistics,2022-09-16 21:18:11,9
"Rewording the problem to make it more precise:
If you shuffle a deck, what is the probability that the top 26 cards are either all black or all red.

Does that sound right?",46,xf84zf,"I've somehow fallen into an argument with some magicians about the probability of someone taking a facedown shuffled deck of cards, intuitively sorting the cards into two piles, and the resulting piles being all black and all red. (This is basically what appears to happen in a particular card trick, sparking the question.)

Several folks were very insistent that the answer is 1 in 2\^52

It isn't, but I'm not confident in my own answer (It has been some years since university). I also haven't been successful in explaining why I think the above answer is wrong.

I'm hoping the folks here could help with the correct solution, or explain why I'm wrong and the answer is 1 in 2\^52.

Thanks in advance!

Edit: changed 2\^52 to 1/(2\^52). oops",statistics,2022-09-15 14:12:09,44
"I'd normally advise a mixed model where you are adding random effects (treat the data is hierarchical, task performance nested within person). The concern is, you only have 6 people which probably isn't enough for your level 2 variable.

Personally I don't think you have enough brain scan data to fairly analyze it beyond describing it.",11,xexp8j,"I have data from 6 participants about a psychology experiment they performed about 400 tasks of each. I also have a single measurement for each participant about brain activity in a certain region. If I want to correlate this brain activity with task performance, should I regress each task's performance (accuracy/reaction time) against the brain data (which only has 6 different measurements) or should I aggregate performance measures by participant (average reaction time, for example), so that I end up with 6 observations. This obviously has a big impact on standard errors, but I am not sure which makes more sense. Thanks!",statistics,2022-09-15 07:04:25,5
You can get a confidence interval for almost anything by bootstrapping.,15,xfhgly,"A lot of online resources mention the p values for non linear regression coefficients as wrong since it is impossible to correctly define a null hypothesis for these type of models. But I've seen multiple papers and statistical libraries report them along with non linear curve fit results. How is this done and what type of statistical test is it? My equation is exponential, for example: y = e^ax.",statistics,2022-09-15 21:27:05,13
"Usually mu is the assumed distribution mean. So for example if you're doing a t-test comparing the difference in two distributions, you'd most likely set mu to 0. Or testing to see of a coin is fair you'd make mu 0.5. You should know the mu used for the null hypothesis before performing an experiment",8,xfabfp,I'm a bit confused about setting the value of mu when running tests? How do I know whether it should be 0 or the mean or something?,statistics,2022-09-15 15:43:57,3
"Depends on whether you need to do inference in your future ""data science"" I guess. I'd think that would be fairly important. I can't imagine trying to do much of anything analysis wise without at least a reasonable grounding in inference.

It's possible to understand inference without specifically using that book in particular, but I think if you want to be able to do anything but the most boring bog-standard analysis (i.e. to not have to throw up your hands and say 'gee I have no idea' the moment something even a little out of the ordinary comes along), you will want to have a reasonable grasp on the theory of inference - point and interval estimation, hypothesis testing, prediction intervals etc.

If the book is giving you trouble, maybe start with a more basic book first. There's a ton of books on mathematical statistics that are relatively basic and with an eye toward being a bit more applied.",4,xf9h6x,I’m referring specifically to the Casella and Berger Statistical Inference book.,statistics,2022-09-15 15:07:30,3
"This is actually more closely related to the subfield we call comparative politics than it is international relations. There's an enormous amount of literature on exactly the question you're asking. 

There are many ways to model this question and the one you choose depends on the assumptions you make about the democratization process. Among your choices I would lean towards a hazard model as a logistic regression doesn't allow you to easily handle the right and left censoring problem you're going to encounter.",2,xf3oc0,"Hello,

I have questions on what kind of model to use for evaluating hypotheses about international relations. The data in question are datasets that describe the democratization/autocratization of political regimes. If countries are democratizing or autocratizing, I want to be able to look at the covariates that best predict these changes.

I was wondering what model might be best to do that. I was thinking of hazard analysis, the problem being that my countries in question don't really ""die"", they just change regime status for a time. Now I'm thinking of a logistic model to predict democratization or autocratizion--I didn't know if anyone had any thoughts? It's essentially fitting a model to data where most of a column is going to be 0 for no changes, punctuated by occasional strong changes.

Thank you!",statistics,2022-09-15 11:12:30,1
"First, you are missing 90% of the data for this assessment. So  any approach you take is going to be, at best, a bandaid. And I would encourage you to always interpret these results with several grains of salt. 

Second, you can \*assume\* your data is missing at MAR but you have no way of knowing this for certain. I would certainly run other sensitivity/imputation analyses that relax this assumption before presenting results confidently anywhere. Again at this level of missingness, making any good judgement about the structure of missingness will be very difficult.",2,xf2dqj,"I'm  working with data for a 30-item questionnaire that was administered at  24, 30, and 36 months of age. The data is largely complete, with the  exception of age 36 months; at this age, we have one item (let's say  item 11) that is missing like 90% of its data. This item is not missing  anywhere near as much data at other ages. The missing data mechanism is MAR.

In a  case like this, where 90% of the data is missing for the item, my  understanding is that one would drop the item. The problem, however, is  that this item is critical for producing an important summary score (90%  of our summary scores are missing at age 36 months due to this issue).  Because this item isn't missing at other ages, I do have an idea of what my expected distribution of values are. I've been imputing the  item-level data using MICE with some success, but I've run into an  issue.

**Among satisfying other  diagnostics, my understanding is the imputed data should have  correlations (between the predictor and criterion variable) that match  those of the original data.** However, given than 90% of the summary  scores (our predictor variable) are missing for age 36, I suspect that  the predictor-criterion correlation in the original data is artificially  high (especially compared to the other time points, where we have  complete data). Although it doesn't match the stupidly high correlation  from the original data, the correlation pooled from the imputed datasets  seems much more reasonable and is more in alignment with the other time  points, as well as the literature.

I'm  not sure where to go from here. My PI  tells me that the correlations  from the imputed data should match those of the original data. However,  this is going to be impossible, especially because the correlation we're  attempting to ""match"" (which is way too high) is based off a summary  score that is 90% missing. Would appreciate any advice on where to go  from here.",statistics,2022-09-15 10:18:57,4
"It isn't needed.

I love math and I strongly believe learning more math is always useful, but there are other things more relevant that you could focus on besides learning multivariable real analysis (which I take it to mean something like ""Calculus on Manifolds"" from Spivak).",8,xepjne,"Thoughts? I’m aware that single variable real analysis is important, but is multivariable real analysis needed? Or used anywhere? Will single variable real analysis be enough for say, measure theory?",statistics,2022-09-14 23:44:02,9
"You definitely do not want to average the averages

You want to take the weighted mean of the averages.",114,xe681i,"I have a data table I am trying to simplify; the population groups are made of girls and boys and those groups are further organized by age (1-6 months). The mean weight is given for each age group for boys and girls, but I want the average weight for ALL of the children for each age group. I don't have access to the individual data points. Could I just average the means for the boys and girls? If not, is there an alternative?",statistics,2022-09-14 09:21:22,33
"All the same parts apply. The main difference in my experience is that in an analysis report you can be a little informal with your language and use the active voice or the royal ""we"". It makes for an easier read.

The main goal is to explain what you did, why you did, what the results are, and what they tell you.",3,xep0wd,"Hello everyone,

I’m currently involved in a research project, and was asked by my professor to write a report and a poster presentation later on. I would like to ask what are the key terms to a project report. I know about the abstract, introduction, and the other necessary sections, but I’m rather confused since I’m not writing a research paper but a report. Any tips or advice needed. Thanks.",statistics,2022-09-14 23:13:41,13
"A logistic regression over Open Access publication sounds sensible. Pearson is appropriate to quantify the degree of linear relations between variables.

> b) reveal myself as a statistical idiot, was this test appropriate?

This is unnecessary. Asking for advice doesn't make you an idiot.",50,xe7y05,"Stats is an area of mine that's not as strong as I'd like it to be. I'm peer-reviewing a paper that tries to predict whether an article was published Open Access or not (so DV is a binary yes/no) based on a variety of characteristics about the authors - country income [4 levels], discipline [5 levels], academic age [continuous], gender [binary 2 levels], etc. 

They used Pearson correlation coefficient for all of these. Everything I've read says that Person requires both variables to be continuous AND normally distributed, neither of which apply to most (all?) of these pairings.

So before I potentially a) cause lots of revision work for the authors or b) reveal myself as a statistical idiot, was this test appropriate? If not, what test should they have used? I suspect the test changes based on which IV you're looking at.",statistics,2022-09-14 10:29:46,22
"The multiplication by √n stabilizes the variance, so it that it can converge in distribution. Without that, consistency would just leave  g(Xn)-g(θ) going to a spike at 0.",3,xejki4,"Hi all,

I had a very pleasant conversation last week with u/Kroutoner [in this thread](https://www.reddit.com/r/statistics/comments/x8cffk/q_error_propagation_what_does_it_perform_and/) (thanks again, you input was invaluable). I unfortunately got busy and did not have time to follow up, but I had a few more pointed questions that I thought were worthy of a separate follow up post particularly for newcomers like me.

Anyway, onto the questions.

1) This is a notational question. I've seen many  places write (in particular this [wiki page](https://en.m.wikipedia.org/wiki/Delta_method) that writes the results as sqrt(n)(g(Xn)-g(x)) converges in distribution to N(0,(g'(x))^(2)*s^(2)). However, I'm a bit confused since a similar statement of the central limit theorem states (see [here](https://en.m.wikipedia.org/wiki/Central_limit_theorem) ) that the variance decreases with more samples. Does this mean that my variance also decreases in the delta method as one increases sample number? This seems both odd to me, but also makes sense. However, I can't justify this to myself. Is there a reason why this might be true?
Edit: I think I understand my misconception here. While thinking about two, I was thinking of pulling from a static distribution, the problem is that I don't actually know what the distribution is in this case. Further, I think it's easy to understand that this sqrt(n) factors in here specifically because we also expect it in the case when the function of interest is the identity function. 

1a). In thinking about my edit, I realized that I was using an assumption that I should verify. The mean and the variance that I would use to compute in the delta method are in fact the sample mean and the sample variance. Correct? It seems like there should be no prior assumption of knowing the distribution at all. Everything is derived from the samples.

2) in my previous post, I assumed a form for g(x)=x^(2). It was pointed out to me that there are issues when the mean gets sufficiently small and the variance allows for points to start crossing over 0 (if one wanted to think of it as a sliding parameter, one could say that as the mean gets closer to zero the variance must get smaller much faster to maintain normality of the output). However, I am now curious about what happens if that isn't the case. Specifically, in this case, we know that the second order taylor expansion is more than enough to perfectly describe the output. So, we can write n*(g(Xn)-g(x))=sqrt(n)*g'(x)*(sqrt(n)*[Xn-x])+1/2*g""(x)*(sqrt(n)*[Xn-x])^(2) (all higher derivatives are 0). Further note that in our case, 1/2*g""(x)=1. This is also discussed in one line in the wiki page on the delta method. Anyway, we know that sqrt(n)*[Xn-x] converges to N(0,s^(2)) and it can be shown that n*[Xn-n]^2 converges to gamma(1/2,2*s^(2)) which is in the form gamma(k,theta) as defined on Wikipedia. From here the question seems to be is the mean still the same as x^2 or is it now modified by adding the mean of the new gamma function (s^(2)) to x^(2)? This seems to make sense since we can analytically square a normal distribution (by change of variables) to compute the mean to be exactly x^(2)+s^2. Further, then the variance should go as the sum of the variances of the normal distribution and the gamma distribution. One can calculate the variance by change of variables to be analytic, and find that one gets the same answer. Seems to be exactly right! So, I'm betting that this works out, but I'm too new to honestly say that this makes sense. I would appreciate if someone who knew more would chime in. 

3) One of the other questions I was left pondering was usually things work out because we know (sqrt(n))*[Xn-x] approaches a normal distribution. However, I was wondering what happens if Xn were to approach some other distribution. Usually this is hard to do given sums of random variables tend to normal via central limit theorem, but perhaps Xn is a product, which I've seen around going to a lognormal distribution (I can't prove it, but it makes a ton of sense). How does this affect the outcome lets just say for the first order? Is it similar to how I approached 2)? Ie the mean will be g(x)+the mean of the lognormal distribution? And the variance given by the variance of the lognormal distribution?

Thanks again for all of the patience and help! It's been vary interesting to have my eyes opened to this for the first time. 

QoO",statistics,2022-09-14 18:38:29,7
"> I've been told that if our data is not normally distributed, we can take a log function to normalize it. 

This is quite false in general

***If*** it has a lognormal distribution to begin with (the distribution you get when you exponentiate a normal random variable) then taking logs gets you a normal random variable.

On practical grounds, *if your variable is right skew* a log transform will often get you nearer to a symmetric distribution and somewhat closer to normal -- but it might also leave you with a distribution even further from normal (more left skew than it was right skew) or it might make no discernable difference at all.  There's also the problem that if your variable is not entirely on the positive half-line, log transformation is undefined.

Generally I tend to consider logs when percentage changes in the variable are particularly meaningful. I don't often transform data - there are typically better options - but when I do, I try to use transformations that make sense for the variable. Log transformation would be the transformation I use most often, but then I often deal with data   (a) which is very readily interpretable on a log-scale and (b) for which the transformation is often highly effective at producing variables sufficiently close to normal, but more importantly (i) with more nearly homogeneous variances and (ii) more nearly linear relationships.

While occasionally helpful, log transforms are not a panacaea - not remotely. Whoever told you otherwise is badly misleading you[.](https://www.youtube.com/watch?v=KS_f6O8mWsk)

In any case you should not expect to actually have normality (before or after any transformation) with real world population distributions. In large samples consistent goodness of fit tests will nearly always tell you that, even when the population distribution is very very close to normal (i.e. even when so close it's likely of no consequence for whatever you're trying to do). 

Indeed you can typically tell from the support of the variable (what values it can take) that it simply can't be normal, so a test of that hypothesis is pointless (you already know the correct answer). 

What's important is whether the non normality you have in the population is of practical consequence for the properties of your analysis (e.g is your type I error rate going to be close to what you want, is the power relatively good). Goodness of fit tests don't answer these questions.

> Is this something that I should expect?

Unless you're lucky enough to be in a situation where log transforms would work well on nearly all your variables, pretty much always, if you're trying to use it that way.

What was your variable measuring, and why did you need to transform it?",21,xeg4ka,"Hi all, first of all, I'm new to statistics, so apologies if what I'm about to say is wrong or very basic. I've been told that if our data is not normally distributed, we can take a log function to normalize it. So, I transformed my skewed data with natural log and rerun my analysis. The result is, yes, it looks more normally distributed, but what I don't understand is when I check my Kolmogorov-Smirnov and Shapiro-Wilk, they are still violating the normality (still significant). Is this something that I should expect? Or did I do anything wrong in the process? Because I thought if it's been log normalized, I would expect the Kolmogorov-Smirnov and Shapiro-Wilk to not be violating the assumption. Is it right for me to make this assumption? Thanks.

P.S. to moderator I cannot use the flair as it is greyed out for me.",statistics,2022-09-14 16:02:08,11
"I don't know about salaries in Switzerland and I presume they are commiserate to the high cost of living, but I do know something about starting a career in pharma.  

You may be overly optimistic about your pay given your experience and I wonder (no offense intended) if your training matches their requirements (most have a PhD at minimum). Do you have an opportunity in hand?",4,xe0vsu,"Hi folks, I hope you guys are all well. I did my BSc in Applied Statistics & Engineering, and my MSc starts next week in Biostatistics. I have relevant work experience in big banks and hedge funds. Tbh, I am not sure if I want to stay in this field or switch to big pharma. Honestly, I don't like the work culture in finance, even as a quant. I would prefer to work in Biotech, but I am used to having a nice salary. I heard my starting salary after my MSc would be $120-$140k, depending on my performance during my MSc, my master thesis, luck, etc.

Next to my MSc, I am working 60% at an Institute of Data Analysis in AI, etc. What do you guys think about the switch vom finance to pharma? How is the salary?",statistics,2022-09-14 05:37:12,9
"> Randomly either two or three filled chests.

What's the probability to have three filled chests? Are the rooms correlated in that way (e.g. always x rooms with three filled chests)?

If you don't find the gold chest with the first attempt and...

* you are in 2-chest room: 50% chance to find it with the next attempt, with 100 coins your expectation value is the same in both cases
* you are in a 3-chest room: 75% chance to find it with the next attempt, the expectation value is the same if you start with 150 coins and positive below.

If you don't know the state of your room you should take a second chance at 100 coins but not at 150 coins.

Strategy to maximize the expectation value in each room: Always take the second chance until you win once (trivial). If your next first pick wins, never take a second chance from that point on (also trivial). If your next first pick loses take a second chance (50 coins left). If the second pick in that room or the first pick in the following room wins then never take a second chance again. If both fail take a second chance again (25 coins left). If that wins then we need information about the relative frequency of 2- and 3-chest rooms. If you keep losing always take the second chance.",4,xe17dh,"I play this game and was wondering how you'd calculate the odds. 

You have 5 chests before you. Two or three contain a bag with one hundred gold coins. You get two shots to win one prize. If you win in one go you leave that room. How do you calculate the odds?

Now here is the kicker. There are ten rooms with the same set up. Randomly either two or three filled chests. Your first pick is free. But if you fail you need to forfeit half your winnings to get that second shot.

How much are you statistically going to win? Of course you are not investing in a second try if you have two hundred coins or more. 10 In a row is max 1000 coins. A very unlucky 20 misses is almost astronomical.
 So basically once you hit 200 the rest is only the free first guesses. So max 8 single tries. I guestimate about 6 wins. So 800?

I thought up a variation where between 20 and 30 chest contain bags with 10, 20, 30 ...100 coins.  A room has 2 or 3 filled boxes with never the same amount of coins. At least two of every amount of coins are in those boxes but never more than three. The second guess per room costs you half your winnings up to that point.

What would be the best strategy? Max winning is 3x100 + 3x90 + 3x80 + 70 = 880.  

Can anyone break this down?",statistics,2022-09-14 05:51:59,1
Probably the CDC site will have a data set,1,xecxia,"I work in web content creation for a media group that works with multiple health-related clients, and sometimes I need to get data on drug and overdose cases at different levels (national, state, town/city).

Is there some website where I can parse through this sort data with filters (nonfatal overdoses, New Haven, Connecticut, for example).

Recently I needed data from Seymour, CT (16.000 people) and googled until I found a downloadable spreadsheet from the CT examiner's office, then manually counted the number of overdoses in 2021. Not ideal.

Any help with this?",statistics,2022-09-14 13:51:01,2
"I'd talk to your adviser more, but I don't think they want a statistical calculation of variance. It sounds more like they want you examine and analyze why the process steps vary from person to person (or across roles or whatever), and across the projects. A numeric variance requires quantitative data and usually needs at least 40 or so different data points to give a result that is meaningful. 

Not having much expertise in this sort of qualitative research, I'd guess the variance model that you would want is something like: Project 1 does steps in order A, B, C because they have needs X,Y, and Z. But Project 2 does them in B, C, A because they have needs X and Y but have no expertise in Z. That is try to not just describe how the processes are different but link those differences to how other factors vary across the projects.",2,xe4jnb,"Hi everyone! First time to post something here, and I have little knowledge on statistics. I'm currently in the middle of my graduation research and I need help. I'm executing a qualitative research on process steps between two different projects. To gain insights on the process steps executed, I executed interviews with involved projectmembers (no quantitative data, only input wherein experiences were shared). My professor advised me to include a variance model for the comparison between the two projects. However, since my knowledge is (very) limited on statistics, I have no clue how to include or execute this. After some googling I think I found what the variance model means and how adding it would benefit to my research. Now I am stuck on how to determine the variance of both groups. Can someone help me with this? (sorry if I used some terminology wrong, I'm trying to use it in a correct way)",statistics,2022-09-14 08:14:30,4
"I hated that textbook. 

You could try using Penn State's online multivariate statistics course? They've got some of their notes online although it's not nearly as mathematical as that textbook, and I think they're using SAS.

Her videos might be informative as well:
https://youtube.com/playlist?list=PLwbyECqYiIcn_p_cWpBj4bUTrEEJvJml3

Also understanding properties of linear algebra was very useful as well e.g., Jordan forms, quadratic forms, positive definite, etc.",9,xdrf8o," Hello,  so I'm taking an intro to multivariate statistics for my degree. And  well my professor isn't the greatest. So the only thing I really have to  work with and try to understand these concepts is the text we are using  ""Applied Multivariate Statistical Analysis"", by Wichern & Johnson. I  don't doubt this isn't a great textbook. But personally just reading  from there isn't enough for me to really quite grasp the concepts and  apply them to homework and projects. I feel like I'm just googleing  answers then actually learning to turn in my weekly assignments, and  many of my fellow classmates feel the same.

I was wondering what other resources are out there or did you use to help get a good foundation in these concepts?",statistics,2022-09-13 20:39:05,8
"Could be inbreeding, check if it is doing .5+F (inbreeding coefficient)",1,xe6ect,"I used kinship() from the 'kinship2' package in R to calculate the relatedness (from pedigree data) between a group of individuals that has multiple families. The resulting relatedness matrix looks good, it is symmetric and the results make sense. 

Except for the diagonal, in which some individuals have r>0.5 to themselves. In the diagonal, r is never smaller than 0.5. It is either 0.5 or larger, so I am guessing that kinship() takes into account inbreeding even when calculating the relatedness of individuals to themselves?

I would be very grateful if someone could help me figure out what is going on!

Many thanks in advance!",statistics,2022-09-14 09:28:03,3
Something like this? https://subredditstats.com/subreddit-user-overlaps,24,xdionr,"For example, subreddit X has an amount of users that have joined it, I'd like to know the top 10 subs with the same users as sub X.",statistics,2022-09-13 14:07:41,2
"Well, these may not be exactly what you are looking for, but:

[Here's one](https://people.eecs.berkeley.edu/~jordan/sail/readings/andrieu-thoms.pdf) of the best review papers on adaptive metropolis methods (which I use all the time - they are very easy to implement, work well for many problems). IME, adaptive metropolis doesn't get covered in textbooks very often for whatever reason. If you are going to learn vanilla MH, there's no reason you shouldn't also learn about the adaptive versions, imho.

There's also this paper on [stochastic gradient MCMC](https://proceedings.neurips.cc/paper/2015/file/9a4400501febb2a95e79248486a5f6d3-Paper.pdf) methods (which I find a very elegant approach, but I'm less clear on their current status/acceptance as a scalable Bayes approach). Again, not an approach that you see covered too often in MCMC textbooks, but there has been a lot of research into it.",3,xd5vxj,"For a self-study course, I'm looking for bibliography that describes current MCMC algorithms. I'd prefer a book or a review paper.

My background knowledge is at the level of [Gamerman-Lopes](https://www.routledge.com/Markov-Chain-Monte-Carlo-Stochastic-Simulation-for-Bayesian-Inference/Gamerman-Lopes/p/book/9781584885870) or [Gillks-Richardson-Spiegelhalter](https://www.routledge.com/Markov-Chain-Monte-Carlo-in-Practice/Gilks-Richardson-Spiegelhalter/p/book/9780412055515) and I will read [Liu](https://link.springer.com/book/10.1007/978-0-387-76371-2) soon, but I'd like to find other good updated material that will help me understand the current directions of research.",statistics,2022-09-13 05:13:08,6
"You could perform a t test at the batch level. In order to do you this you would have to aggregate fish data within each batch. Thus, your group variable is the batch and the aggregates are the outcome.

If you have more than 2 batches you need ANOVA.",3,xdqp43,"I’m trying to analyse behavioural data from fish. They’re put into a tracking machine 24 at a time (batches), and the machine spits out some stats on their distance travelled, speed, etc. After this, I genotype them (mixture of wildtype, hets and homs) to see whether there are differences based on genotype.

My knowledge of stats is very basic (I’m taking a first year uni course), so all I’m confident doing at the moment is t-tests to see (for each behaviour variable) whether either group (het or hom) has a different mean to the wildtype fish. The problem is, I’ve been told that this can’t be done because the “batch” will be affecting their results too much. I’ve been told that a chi square test is better instead, but this doesn’t make sense to me. I’ve also collected data for multiple fish families (hatched different times, potentially different parents, analysed on different days) - am I right in assuming these different datasets aren’t comparable other than to try and replicate the results?

The fish genotypes are random across batches (I don’t know their genotype until after the data is collected), so I don’t see why I’d need to take this variable into account in the statistical tests. I’d really appreciate any help I can get on this, and would love any suggestions as to other things I could/should be doing with the data (I have 8 different behaviour variables). Thanks!",statistics,2022-09-13 20:03:07,3
I recommend reading chapter 5 of pratt and gibbons 1985. It explains the intuition and procedure clearly,3,xdeyyy,"Hello!

My fiancée is conducting a biomedicine research and she's having troubles with how to handle the statistics of comparing the two groups; a colleague told her to use the Wilcoxon signed rank test, but we haven't been able to figure out how to do that yet

She has data for the progression of a certain metric after each treatment step, one for a treatment group, for which the treatment was applied and one for a control group, with placebo treatment

So, basically, she has:
T0 (before any treatment):
One metric for each individual of group T
One metric for each individual or group C

T1 (after first session):
One metric for each individual of group T
One metric for each individual of group C

And so on and so forth for each of the times

We tried using an Excel extension to do this, but it seems that it got the median for each group in each time, which doesn't make sense to me - comparing data for individual A in Time 0 against individual B in Time 1, since they're completely unrelated

How to proceed?",statistics,2022-09-13 11:38:48,20
"I teach basic stats and made this video showing the step-by-step procedure for doing the paired t-test:

[https://youtu.be/LJzguaEYiHk](https://youtu.be/LJzguaEYiHk)

Hopefully it can help answer your question. Spoiler alert: I do calculate the difference in means first as part of the test.",2,xdofct,"Sorry if this is a super basic question, but if I am looking at whether a treatment has an an effect by comparing the measurements before treatment and after treatment (using the same specimens/people in both groups), do I need to use the residuals (difference between before and after treatment) in my paired t-test?",statistics,2022-09-13 18:16:43,4
"There are a couple of different ""camps"" when it comes to how to build PS. Here's my take: the purpose of propensity score methods is to remove confounding bias. So, you want to include all measured risk factors for the outcome in your model. Including variables that are \*only\* related to the treatment is not advised because it can decrease precision and increase bias.",3,xdhb82,"I am a little bit confused in what covariates I should include in my PSM model/testing. Is the covariates should be the one that affecting the probability of the sample getting treatment, or should the covariates both affect the treatment and the outcome?   
Also, on using PSM, you relies on one assumption, which is conditional independence assumption, I am trying so hard to wrap my head on it, can you please ELI5?",statistics,2022-09-13 13:12:19,11
"But why would you want to use these two? Both the board pay ratio and profitability sound like continuous variables, so not immediately suited for the two tests you mentioned. Of course, you can turn board pay ratio into a categorical variable, but why would you want to lose data fidelity?",2,xdhitv,I don’t want to use linear regression.,statistics,2022-09-13 13:21:02,2
You should probably just do them in separate steps rather than trying to come up with a single number that represents a composite weight.,6,xd7cx2,,statistics,2022-09-13 06:18:27,2
"In my experience as an applied researcher, the problem with bayesian methods is that (at least for regression stuff) you get results that are nearly identical to the ones obtained by frequentist methods, specially if you have a lot of data, but the former are computationally much more intensive and more complex programming wise.

So even if they are theoretically more sound, I've never really had a good reason to switch to bayesian. Sometimes I try it, but I get similar results with a higher cost",72,xcf7qk,"From the people interested in statistics that you know, what is the split between the Bayesian and Frequentist point of views?

Also in terms lf new reaserch being produced? Is it a generational thing?",statistics,2022-09-12 08:06:06,70
"> you can observe the meta distribution of p-values follow a power law distribution where roughly half are below p-value=0.05.

The general behavior of this sampling distribution of p under Ho and at various effect sizes under H1 (which does sort of look like a power function in many instances) is an important part of comprehending and interpreting p values, in my mind. Many people wrongly imagine there's some typical p value that new p value results will cluster around (you see it a lot when people just fail to reject for example) and they incorrectly imagine a repeat of the experiment at a slightly larger sample size would lead to a similar but slightly smaller p value - which is clearly not likely to be the case under either hypothesis.",2,xcofof,"I wrote an Observable notebook: [Is a coin unfair?](https://observablehq.com/@felipec/is-a-coin-unfair) in order to explore the true meaning of p-values in the simplest of examples.

I also show the distribution and the threshold where a p-value for 1000 coin tosses and an alpha of 0.05 would be considered statistically significant in order to accept the alternative hypothesis, which for this case it's above 531 heads or below 469.

I also show the likelihood function, since a lot of people seem to ignore that unlikely events do happen, and for example even if 60% of coin tosses land heads, the coin could still be fair (depending the number of tosses).

Finally I do what is not easily done in reality: do the experiment multiple times. By doing the ""study"" 1000 times you can see 5% of the time a study accepts the alternative hypothesis, even though it isn't true.

But you can see other interesting stuff, for example if you select `p=0.53` (the p-value threshold for success at 1000 trials), you can observe the meta distribution of p-values follow a power law distribution where roughly half are below `p-value=0.05`.",statistics,2022-09-12 14:15:36,3
"Yes, -.17 would usually be described as weak. 

A weak correlation can still be statistically significant, even if its practical significance is small-to-none (here, religious coping explaining only 5% of the depressive symptoms.)

As a rule of thumb, if |r| * sqrt(n) > 2, the result is statistically significant. So you may well see r=0.1 significant if n>400 and r=.01 significant if n>40000. (The exact test is comparing r/(1-r^(2)) * sqrt(n-2) to a t-distribution with n-2 degrees of freedom.)",13,xcskk2,"Hi, having a mental lapse right now. Stats isn't my forte, but I do have to read a lot of psychological research. 

&#x200B;

""The data confirm Hypothesis 1. Results of Table 3 showed a statistically significant negative correlation between depressive symptoms and positive religious coping (r = - .17, p < .05), and the regression analysis for predicting depressive symptoms (Table 4) found that positive religious coping was contributed in a way that was statistically significant toward explaining variance in depressive symptoms (B = - .21, SE = .05, ß = - .18).""

from:  [The Association Between Positive Religious Coping, Perceived Stress, and Depressive Symptoms During the Spread of Coronavirus (COVID-19) Among a Sample of Adults in Palestine: Across Sectional Study - PubMed (nih.gov)](https://pubmed.ncbi.nlm.nih.gov/33389439/) 

&#x200B;

Let's say I use the correlation coefficient formula to calculate a significant negative correlation and I end up with r = - .99. If - .99 is strong  negative correlation, how come r = - .17 (from the above) is significant? Wouldn't - .17 be a weak correlation? Thank you for your help.",statistics,2022-09-12 17:13:11,29
It’s a common design and Dunnett’s test is the significance test of choice. The omnibus ANOVA should not be done and Dunnett’s test should be considered a priori and not post hoc.,1,xcwoqg," An interesting question came up in a study design meeting that I don't know the answer to.

The study(ies) is aimed at testing the efficacy of three separate interventions, but not comparing the interventions against one another. A question of it one control group could be used across testing these three interventions (test each intervention separately against the single control group).

Aside from the possibility of Type I error, this idea feels wrong to me, but I can't explain why.

Any ideas about if this idea is feasible? And if so, what one would even call this design?",statistics,2022-09-12 20:25:25,10
"If you don't have normality, the t-statistic won't have a t-distribution *and* the z-statistic won't have a normal distribution. 

Under a few conditions, however, the usual t-statistic will converge to the normal distribution as n becomes large.

Whether you use the z or the t-tables usually makes little difference; the error in the normal approximation is often several times larger than the discrepancy between the two (i.e. the choice of which to use is usually the lesser of your worries.

---

Not directly related to what you'd be tutoring, but:

1. You could overcome the issue of the test statistic being neither distributed as t or z (and so, perhaps not conducting the test at or below the desired significance level, alpha) by using a permutation test.

2. However, if you know enough about the distribution to be able to figure out whether n is large enough to use t or z or whether it might not be, you might then also know enough to be able to do something better than any of these (e.g. use a statistic likely to give more power against the specific alternative of interest, under some suitable model, or a permutation test if you want to allow for the fact that the model is not going to be quite correct and still want strict control of alpha)",6,xce26n,"Which  test should I use? Should I use the z test or the t test? Which  distribution can I use the z distribution or the t distribution? As a  statistics tutor, I encounter these questions frequently!

Use the Z test if you know the population standard deviation

You  should use the Z test and z distribution if you know the population  standard deviation. The answer to the question when can I use the t-test  or z test should be as clear as this simple statement.

Use the t-test if you do not know the population standard deviation

A  corollary to the above rule: Use the t-test if you do not know the  population standard deviation and have to estimate it using the sample  standard deviation.

Will this not resolve most situations!?

Exceptions:  maybe you know the underlying population is normal or you have a large  sample size. But why bother with these exceptions? Technically it is not  the z test even with the larger sample size!

What am I missing?",statistics,2022-09-12 07:19:19,30
"You'd have to ask a genetic counselor & get your genome checked to answer that question unfortunately. Cancer is common enough that it could be by chance, but without knowing your genome, it's harder to answer.",11,xcyuec,"For specifics: Maternal Grandfather died from Diabetes; age: 60’s so it’s unknown if he had it. Maternal Grandmother had breast cancer; age: 70’s. Mother had ovarian cancer; age: mid 50’s. Paternal Grandfather died of natural causes; age: 90’s (assumed cancer free). Paternal Grandmother died of lung cancer; age: late 80’s. Father had prostate cancer; age: mid 70’s. 

I know genetics play a large part and I could be cancer free like my grandfather, but if anyone is interested I love to see the math and conclusion.",statistics,2022-09-12 22:17:23,1
"You arguably have four variables: time of day, day of week, busyness, and which restaurant.

I suggest you do it separately by day and then do a line graph where color is which restaurant, x-axis is time of day, and y-axis is how busy 

Most likely you'll find all the weekdays are similar. If so you can plot an average (as in the mean) weekday.",1,xctr2o,"I’m a college student and most of the time the restaurants on campus have a line out the door; If i want starbucks I have to go at 7am. I want to record how busy each restaurant is based on day of the week and time of day (the google stats are usually wrong). I can’t figure out how to visually represent the data with all three variables (how busy, time of day, day of the week) comprehensively. I know I’m going to have separate graphs for each restaurant but I don’t know how to plot the data. 

TL;DR: How to make a graph/plot/chart of data based three variables (time of day, day of the week, and the busyness(?) of the restaurants

Edit: Also i’m using excel",statistics,2022-09-12 18:06:33,1
If the two terms were in the same model and controlling for each other than the 13.39 does not account for ages under 12.,1,xcry5y,"[This is the article](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6367933/)

""In the 211 participants with CTE, every one year younger participants began to play tackle football predicted earlier reported cognitive symptom onset by 2.44 years (p<0.0001) and behavioral/mood symptoms by 2.50 years (p<0.0001). Age of exposure before 12 predicted earlier cognitive (p<0.0001) and behavioral/mood (p<0.0001) symptom onset by 13.39 and 13.28 years, respectively""

I'm having trouble understanding the relationship between the earlier onset by 13.39/13.28 years with the 2.44 / 2.50 per year. For example, if a player with an age of first exposure(AFE) of seven who somehow knew he was to develop CTE was trying to calculate how much earlier they're cognitive symptom onset would be to someone with an AFE ≥12, would you simply subtract 13.39 from the mean age of  cognitive onset of AFE ≥12, or would you also have to subtract 2.44 for every year below 12 years old? Basically, does the 13.39 account for all ages under 12? If you need clarification I can give it this might be confusing. 

Mean Age of cognitive symptom onset AFE<12: 45.65 years old
Mean Age of cognitive symptom onset AFE≥12: 58.11 years old",statistics,2022-09-12 16:44:48,2
"There's nothing wrong with analyzing data both within and between. Just as long as you're honest about what you find in both analyses.

Personally, I think it's a good thing because the analyses will tell you slightly different things.",1,xcpjg0,my advisor initially wanted me to do within group analyses on an rct. findings were null after controlling for baseline characteristics. he wants me to present this data in the paper but also look at within participant changes within the treatment group alone and then the control group alone. is it frowned upon to do this? or is it okay as long as all results are presented? i’ve been in grad school for ~3 months so still trying to learn best practices for stats. thank you.,statistics,2022-09-12 15:00:49,1
You'd be looking for differences across gender. Can you explain why your t test  did not make sense?,3,xciuns,"hey, we really need some help on a research we’re trying to run for class. using spss, both of us are garbage at statistics.

we have the results from a few different questionnaires with each have a single numerical result (for example participant 1 got a score of 37 on an empathy test) and we want to compare these results to gender to see if there’s a general relationship between them 

what test do we need to run? we’ve tried a chi, anova, t-test.. but none of them seem to make sense, results wise? are we doing something wrong?

our professor has been a great help up till now but he’s kind of ghosting us and we’re running out of time..",statistics,2022-09-12 10:29:27,13
"* Small sample size (10 old men)
* It can't really be helped, but the men were likely asked to participate at a hospital, so they may have had some illness that impacted their digestive system and the study didn't account for it. Who knows...
* One exclusion criterion is `participation in any regular exercise program`. Maybe it is clear to researchers in that field, but I (not a specialist at all) think it could be argued that this criteria is unnecessary. I don't see why (I could definitely be wrong on this) digestion and absorption speeds are related to exercise

That's all I can think of. Maybe a medical doctor with a specialty in food/digestive system can see issues with the choices of food quantities?",2,xcehoz," Hey guys - apologies if this isn’t the right forum, but I have some friends who insist that ground beef is more easily digestible than steak, regardless of time spent chewing. I realize that [this study](https://pubmed.ncbi.nlm.nih.gov/23636241/) says that older men generally chew food less, but are there any other holes I could poke in this argument from a fitness perspective? Sample size small? Too much error? I really don’t know statistics. Thanks for any help you can offer!",statistics,2022-09-12 07:37:08,6
"What exactly is the problem? X and Z1 are together informative about e1, so it makes sense to use them both to predict Y.

That is, with e1 unobserved you have Y ~ g(X,Z1) + Z2.",2,xbwiiw,"I am playing with a toy example to see how modeling could be potentially different for causal inference and predictive exercise. But I found myself ran into an identification problem in the causal case. I wonder if I missed something. 

&#x200B;

Suppose the true relationship among random variables y, e1, z1, z2 are as the following:

y \~ e1 + z2

x \~ e1 + z1 

Except e1, all other variables in the above are observable. 

e1, z1 and z2 are assumed to be jointly independent. 

&#x200B;

For the predictive modeling case, we want to predict y. For causal inference case, we want to understand the effect of z2 on y. 

Right off the bat, we know that z1 is independent of y. Interestingly, for predictive modeling, it is better to add this seemingly irrelevant variable z1 to the model y \~ f(x, z1, z2).

&#x200B;

But for causal case, it appears that x is a collider. It might not be wise to open up a backdoor between e1 and z1 by including x in the model. But we know the true model requires us to remove the effect of z1 from x to precisely recover e1 if we add both x and z1 to the model, hence to better capture the effect of z2 on y. 

Perhaps my understanding of DAG is wrong. Or in this case, do we actually have an identification problem?

DAG is here: [https://i.redd.it/99bw21ab8bn91.png](https://i.redd.it/99bw21ab8bn91.png)



Edit: replace e2 with z1.",statistics,2022-09-11 16:01:37,12
"Yes, it can be difficult to rectify the many-repeats idea of probability when discussing the 'probability' of one-off events going this way or that. Note that this holds in all manner of problems that either will not (the superbowl) or likely can not (very large clinical trial) be repeated.

First let's just stipulate that - usually\*\* -  this probability that is discussed is not really forecasting the actual election - not because it is frequentist (or not) but because it (usually) does not attempt to think about all the ways things could change in the next 8 weeks, as much as represent the uncertainty inherent in our understanding of the electorate at the current moment, 8 weeks or so before the next election.  In other words they typically apply, to the degree they apply to anything, to the question of what would happen if the election were held today, rather than what WILL happen when the election is run in 8 weeks.

When thinking about the first question - what would happen if the election were held today - rather than thinking of running the election over and over, I find it easier to think about all the the possible realities we might currently be inhabiting.  At this moment, there are infinitely many possible true states of the electorate that could have given rise to the current polling. In what proportion of these would, if expected voting patterns hold, the Democrats win the senate if the election was held today? This is a frequentist probability concept most consistent with what is actually being produced in these quotes, I think.

\*\* Some of the election data folks do try to do actual forecasting by taking current polling plus various other factors and indicators ('fundamentals') and mixing them into a true predictive model that takes into account the way things might change over the near future, and then simulating the election millions of times and counting up the results. While the data churn in these models can lend them an air of rigor and sophistication, I am personally not convinced they are better at picking winners than the qualitative-type models that look at a handful of items like ""is the unemployment rate rising"" and ""which party currently holds the white house"" to make a forecast in a very blunt way. Also, even the most seemingly rigorous attempts at this type of future-modeling carry an common caveat/asterisk, typically termed a 'black swan,' that more or less says: this is our prediction unless something crazy happens.",8,xbt7rs,"If i see polling statistics like ""Democrats have a 60% chance of holding the Senate"" or ""Biden has a 50% chance of winning re-election in 2024"" (I made these examples up), does a frequentist view of statistics mean literally if we go back in time, and have the midterm or presidential election a large number of times (near infinity), the proportion of times the Democrats in the set is 60%/Biden wins re-election is 50% of the time?

I am just trying to make sure I understand what this means for events that are not like coin tosses where its much more theoretical to conceive of repeating the experiment again and again.",statistics,2022-09-11 13:45:41,6
"That might be.a lot, but for real modeling roles without a PhD I have noticed that CS knowledge is paramount ironically even though stats maiors learn more about it. I think its the production skills that are sought after, so if you get those as a stats major that is more important. But CS seems to have less stereotypes to recruiters and “looks better” for the heavy modeling.

Stats major with CS minor or vice versa seems like it would be more doable",14,xbr5wj,,statistics,2022-09-11 12:22:48,7
"Without any indication as to your career aspirations, one cannot begin to make heads or tales of the kind of advice you wish to receive",44,xbj1m8,"I am currently enrolled in a BS of Computer Science, after having completed 4 engineering subjects last semester.

I have three options to consider:

1. Pick up a double degree and major in Mathematics/Statistics (4 years)
2. Revoke credit for my previous 4 engineering subjects, and major in Mathematics/Statistics with my 8 free electives (3.5 years)
3. Minor in Mathematics/Statistics with my 4 free electives (3 years)

Will employers care whether I have a double degree, major, or minor? What is my best option?

Please help me guys I have no idea how to decide...",statistics,2022-09-11 06:48:37,19
"IMO, You need to present your data so that there is a ""light"" column and a ""moisture"" column and a value for each sample.

Then you can do a fit with R's lm function, like [this](http://courses.atlas.illinois.edu/spring2016/STAT/STAT200/RProgramming/RegressionFactors.html). (Just a random first hit from Google)

For ANOVA and Tukey, you just use the aov() and TukeyHSD() functions on the same model.",2,xc5tcv,"I'm working with quite a small data and I previously used ANOVA and Tukey as post-hoc test when I had more data years before. Now, I actually forgot how to use SPSS but I have to use it for an old paper (but I can also use R). My data have 4 Categories with 5 levels each except for the last category with just 2 levels. For each level, I have one numerical value (average of 3 raw data).

Sample data:

&#x200B;

|Categories|Levels|Value|
|:-|:-|:-|
|Moisture|10|0.012|
|Moisture|15|0.123|
|Moisture|20|0.145|
|Moisture|25|0.000|
|Moisture|30|0.254|
|Light|With|0.512|
|Light|Without|0.012|

These are just sample data (I have 2 more categories). Are ANOVA and Tukey still applicable to my data? And how should I do this via SPSS or R?

I would appreciate any help with this! Sorry for the format, I'm using mobile.",statistics,2022-09-11 23:56:58,6
Try to take advice from people who actually work in finance.,12,xbep3g,"My likely plan is to do a Masters of Statistics (at the University of Melbourne), then go into the Finance industry.

I ask this because it seems that the [""Statistics and Stochastic Processes"" major](https://study.unimelb.edu.au/find/courses/major/mathematics-and-statistics/what-will-i-study/) is more Maths heavy than other statistics degrees (at the very least, I've probably made it more maths heavy than it needs to be).

I'm in first year, and my course plan for the Statistics and Stochastic Processses major is:

Year 1:

* Linear Algebra (Core)
* Calculus 2 + Real Analysis (Core)
* Group Theory
* *Foundations of Computing*
* *Foundation of Algorithms*
* *Principles of Finance*

Year 2:

* Vector Calculus
* Probability (core)
* Differential Equations
* Statistics (core)
* Stochastic Modelling (core)
* *Design of Algorithms*
* *Corporate Finance Decision Making*
* \+ other elective

Year 3:

* Linear Statistical Models (core)
* Probability for Inference (apparently this is the ""continuation"" of the Probability subject and goes over probability spaces, multivariate distributions etc.) - (core)
* Modern Applied Statistics (core)
* Applied Data Science (core)
* Complex Analysis
* *Investments*
* *Derivative Securities*

Is this a good plan? Or would it perhaps be more rigorous to take the Pure path?

**EDIT: Listed the CS and finance electives I have in my plan (in italics)**",statistics,2022-09-11 03:03:31,14
Very very small. Negligible. Pretty much zero.,2,xbtvhw," From the internet, I know that:

* A value of **0.2** represents a small effect size.
* A value of **0.5** represents a medium effect size.
* A value of **0.8** represents a large effect size.

However, how would I interpret a Cohen's D value of less than 0.2 (specfically, 0.014)? Would it be considered a very small effect size? Thanks",statistics,2022-09-11 14:11:40,8
Unconditional mean and variance is the worst possible forecast available. Conditional forecasts do better. Compare mse of conditional and unconditional forecasts.,3,xbyx2e,"Fairly new to forecasting and had a question. If stationary data means that the mean and variance is constant over time, what is the purpose of using an ARMA or any other model that requires stationarity. Wouldn’t taking the average of all the data give you a better prediction for your next point in time? Thanks in advance for any help!",statistics,2022-09-11 17:53:26,5
Yes,3,xbo5hp,"Hi,

I am looking at association of multiple genes with cholesterol markers where genes are independent variables and cholesterol markers are the dependent variable.

I have done simple linear regression to find the association in R but since I have many genes that I am running each of these linear regressions for, I would need to correct for multiple testing. Am I right in this assumption?",statistics,2022-09-11 10:20:20,3
Your question lacks so much context that it is utterly meaningless.,15,xbz8dq,I have 4 results that I obtained in a lab and I am asked to determine if it is statistically  significant. All the videos I see on student T tests have 2 columns of data that is used so I’m not exactly sure on how to do this.,statistics,2022-09-11 18:08:16,8
"A graph is supposed to answer a question.  What are your questions?  


Here are my questions about your situation, in no particular order:  Is the MPG constant over time?  Does the MPG you get in the most recent fill up depend on the MPG you had in the previous fill up?  Are the gallons consumed since last fill up a function of how many miles were driven since last fill up?


Usually, once your question is clear, the idea for the graph will be clear.  In any case, I suspect you would be using a scatter chart.",3,xbkanj,"I’ve been collecting data on my car’s mpg every fill up. I got a new car, so i just wanted to put all of this in one spreadsheet (which i did). 

I have three columns: Miles, Gallons, and MPG. How should I organize it to create a graph? What should i put on the X/Y axes? should i even include the MPG? 

Should i put one of these in ascending/descending to make it more meaningful (like maybe miles driven and the fuel it consumed.. idk!)

I can’t find any other articles about this type of graph.

TIA! :)",statistics,2022-09-11 07:41:42,3
"
> The direct and indirect effect are also of opposing signs, so I guess that is the reason

Yep. It's not hard to imagine a case where the total effect is zero while the direct and indirect effects are significant when they have opposite signs.",14,xavqf4,"Hi.

1. Is it possible to have an indirect effect more than the total effect? I carried out mediation using Hayes Process and for one of my hypotheses, the total effect is less than the significant indirect effect. The direct and indirect effect are also of opposing signs, so I guess that is the reason
2. However, I am also looking into other resources online and several people talk about a possible suppression effect. Any helpful and frankly, easy resources on suppression in mediation? I am only finding out more about it from web pages and I am thoroughly confused. How does one check for a suppressor effect in a mediational model?",statistics,2022-09-10 11:00:53,10
"[StatQuest](https://youtube.com/c/joshstarmer) is a favorite YouTube channel of mine. He has great illustrations and works through details in a clear way. 

The ML playlist has been particularly helpful for me. I think he does a good job of demystifying some of the heavier topics into their simple components.",94,xa8wi6,All i know in relation to statistics is Wassily Leontief's work with input output analysis model but i want to broaden my knowledge on statistics as a complete beginner.,statistics,2022-09-09 15:46:23,21
Can you show your code or some representative code? It will be easier to understand the problem and help you,2,xav1s7,"I am currently working on a problem set for R stats. The question asks me to find the probability of each instance in the sample set  [0, 1, 2, 3, 4, 5, 6].

   Serial Num -----  Num Defects

1.           1                          4

.

.

.

499.         499                     7


500.         500                     0

In several data points, see data point 499, the number of defects are shown as 7 which is outside of the questions listed sample space. 
 
Do I include this in my probability calculations?   Count of 0s / n.

 Or would I eliminate these values? Count of 0s / n - count 7

Or would I add the instances to the ones that equal 6? Count of 6 + count 7 / n",statistics,2022-09-10 10:32:25,2
"> In general, is it always the case that the lower the power the wider the interval and vice versa?

Beware; *power* is a 'population' level calculation, while a CI is a sample level calculation. Consequently a sample-based CI could by chance be relatively 'wide' when the true power is not low. Further, the CI might be wide but the power may be high if the effect size is large.  As a result, the answer to an 'always' question must be 'no', even though it would often be the case that a wide interval and low power are both related to the same issue (typically because of low sample size)",15,xabrz6,"Let's say we conduct a study and we know that it is underpowered. The result show a wide CI like \[0.02, 0.45\] with B=0.23, can we argue that the wide CI is because of low power and with enough power the interval will be narrower without crossing zero? 

In general, is it always the case that the lower the power the wider the interval and vice versa?",statistics,2022-09-09 17:59:25,10
"n is the sample size. N is the population size

Standard error of the mean =

Population sd/sqrt(n)",3,xaeaji,"This really confuses me a lot, would really love some help. Thanks a lot!",statistics,2022-09-09 20:02:04,5
"1, 4 and 5",5,xa1g75,"1. Time series and random processes in linear systems

2. Random processes in the sciences and engineering 

3. Financial mathematics

4. Linear algebra with applications

5. Statistics of stochastic processes 

6. Numerical methods for partial differential equations


Just looking for the most useful 3 of the 6. You can refer to them as their numbers. 

Cheers guys, really need your help",statistics,2022-09-09 10:31:11,16
"U set up some kind of confidence level. Standard school stuff is 95%. So the p value has to be less than 5% to reject the null or that it happened by chance. 

Binomial formula

Let 

P(s,f) = probability given some successes and failed predictions. 

s = number of successful predictions 

f = number of failed predictions 

n = total number of predictions (s + f)

P(s,f) = nCs(0.5)^n (0.5)^f

So if P(s,f) is lower than 5% then you could reject the idea that it happened by chance.

But it's completely up to you what confidence level u wanna set up.

Edit: To give u an idea. if they can predict 4 times in a row without fail then that only has a 6.25% chance of happening by randomly guessing. Obviously, by the confidence level i set up that's still not enough to reject the null.",9,x9kc0m,"I am going to ask a group of individuals to predict whether a specific event will happen (yes or no) in the future.  How many events do I need them to predict so that I can demonstrate a statistically significant result? 

If I only test their ability to correctly predict 3 or 4 events, they could just get lucky and correctly predict them.  

What statistical test would I use? Of note, the group of individuals will differ for each event prediction.  

Thanks!",statistics,2022-09-08 20:19:06,16
"There isn't really anything to remember; it's just a matter of understanding notation. If you're having difficulty distinguishing between them, the problem is almost certainly a more general difficulty with mathematical notation, and the only solution is to work on your algebra.",24,xa4ryo,"I'm doing statistics homework and these formulas are killing me so if anyone has a tip for helping me remember the difference?

Like I'm pretty sure (Σx)\^2 is the sum of all x values squared while Σx\^2 is all x values squared then summed up but I always need to make sure because they always feel so similar.

Edit: Thanks for the help everyone. I know this was a dumb question but you guys/girls really helped",statistics,2022-09-09 12:51:23,11
"There's no rule. You have to make the decision based on your project, your data, your analysis plan, etc. As long as you're thoughtful and informed about why you're including/not including and the potential advantages and limitations of either path, you'll be fine.",7,x9qxcz,[Question] I am conducting a hierarchical multiple linear regression to assess interaction effect/moderators. I plan to control the analysis with relevant covariates. Should I include all the confounders/covariates I used in the adjusted model? Or should I only include the variables that were significant in the adjusted model?,statistics,2022-09-09 02:30:20,8
It depends on the data and what question your answering.,6,x9loue,Which approach is mostly used when converting hourly data to daily data in time series?,statistics,2022-09-08 21:25:05,15
Thou shall not commit stepwise regression,30,x98ewm,"I want to do a step-wise regression on my data: basically to show how the R^2 alters as I add in several dependent variables. However, all my data is not normally distributed, so instead of lm() modelling, I’m doing glm() modelling, with various non-parametric tests (Kruskal-Wallis, Spearman-rank etc). I know that step-wise linear regression is a thing, but I’ve never seen it for non-parametric data. Can it be done on this type of data? 

Theoretically, I could transform my dataset to a step-wise linear regression, but I don’t know the difference between log, ln and sqrt transformations and which is best.",statistics,2022-09-08 11:46:27,27
"The best I'd recommend is using R for *all* of your stats learning/testing/stimulating. All means all btw. You need the mean of a vector? Use R. Need the covariance matrix of some time series? Use R. Some MC simulation might come in handy? Guess what, there's R. I would have never been able to grasp the meaning of statistics without R. In fact, I was pretty lost with the mathematics, it was R which showed me how it works.Then the mathematics became more obvious.

As for the market, strong IT skills are in high demand and this will only increase in the near future. I mean full stack skills which might or not go way beyond Python, depending on your employer. Good luck!",4,x9h3hi,I’m a freshman at a university studying to get my undergraduate degree in statistics. I plan on trying to land an internship in some sort of data analytics field next summer. Does anyone have any suggestions as to what I can do to strengthen my skills to prepare myself for statistics/data science internships and future career (i plan on working with data analytics and statistics)? I currently have very minimal coding experience with Python currently.,statistics,2022-09-08 17:50:09,9
"From what I understand, they essentially calculated mean values for each of the 7 subscales, resulting in 7 separate scores for each individual, then performed factor analysis on these 7 subscales, extracting a single factor, and using the resulting factor scores as a composite indicator of job satisfaction.",1,x9r7on,"Hi all / Moin, 

It's my first time posting to this sub. I am looking for guidance on my master's thesis project. I used the german adaptation of the JDI (Job Descriptive Index), called the ""Arbeitsbeschreibungbogen"" (ABB), developed by Neuberger. I am having trouble with the final steps, mostly because I have a small sample (15 good responses) and I haven't used this particular survey before.

For those who aren't familiar with the survey, just a quick overview:

It is a 79 Item questionnaire looking as job satisfaction, with 7 subscales respresenting different aspects of the job, composed of about 10 (+/-) Likert scale items. Similar to the JDI. At the end of each sub scale is a Kunin smiley scale for respondents to rate their the overall satisfaction for this section. 

Now, I have already completed basic descriptive analysis. I have looked at the frequencies, means, ranges, etc. of the Kunin items (overall satisfaction), and each of the subscales. 

In addition to the 79  item questionnaire, I included a few sociodemographic questions (gender, age, job title, work experience) to better understand the sample. 

So my question(s) more specifically:

(1) I figure with such a small sample I can't do major correlational anaylses. So my plan was to do T-tests where possible to see how job satisfaction is related to the sociodemographic factors. 

I could use tips here as to wether t-tests is the best way to go? Or if there is anything I should be mindful of when trying to draw conclusions from these tests.

(2) I could also use help with the ABB survey itself. 

I have calculated the subscale scores as outlined by Neuberger and Allerbeck (see below). I am just not sure how to get from there to a general overall job satisfaction score for each individual. 

I have been looking online and either people have gone far and above what I am doing so I am a bit lost, or others stopped at descriptive statistical analysis.

Here is the section pertaining to analysis (translated from German). The last paragraph is where I have been having the most trouble:

""Response specifications

The scale of items consists of four-point Likert scales with the response specifications: 1 = ""yes,"" 2 = ""rather yes,"" 3 = ""rather no,"" and 4 = ""no."" At the end of each individual scale, a summary satisfaction question is asked (items 9, 22, 35, 47, 61, 71, 79). Here, the response consists of Kunin faces with the response scale ranging from 1 = lowest satisfaction (face with downturned corners of mouth) to 7 = highest satisfaction (laughing face), but without prescribing verbal anchors

Evaluation notes

To calculate the satisfaction value, the following items must be reversed: 2, 4, 6, 8, 11, 12, 14, 16, 18, 20, 23, 28, 31, 32, 33, 34, 36, 38, 39, 43, 45, 49, 51, 55, 56, 57, 60, 62, 63, 64, 68, 70, 73, 74, 77, 78. The sum values of the seven scales are calculated by simply adding up the (reversed) items (without the respective Kunin items). The scale mean is calculated as follows: the scale sum value is divided by the number of ABB items in the scale.

The final question about personal weighting of each factor should be calculated using the constant point sum method. In a factor analysis including all seven scales, one factor could be extracted according to the size of the eigenvalues and the eigenvalue progression, the general factor ""job satisfaction"". The presence of this factor justifies a summation of the scale values, which is intended to supplement, not replace, a differentiated recording of job satisfaction.""

&#x200B;

Here is a link to the info page for the survey (German) for those who are interested:

[https://zis.gesis.org/skala/Neuberger-Allerbeck-Arbeitszufriedenheit](https://zis.gesis.org/skala/Neuberger-Allerbeck-Arbeitszufriedenheit)

&#x200B;

Anyways, any tips, advice, whatever would be gratly appreciated. Also, I am happy for help in English or German, though English is my native language. :)

Thanks in advance! Dankeschön!",statistics,2022-09-09 02:47:41,1
I don't think you have nearly enough information in the set up to apply Bayes rule,19,xa7q6o,"Hello, 
How would you restate the probability z=0 and y=0 according to Baye’s Rule? 
Thanks!",statistics,2022-09-09 14:56:04,5
[This](https://www.ethanrosenthal.com/2022/04/15/bayesian-rock-climbing/) might interest you,8,x8zold,"[https://www.alessandromasullo.com/blog/analysis-of-4-million-climbing-ascents/](https://www.alessandromasullo.com/blog/analysis-of-4-million-climbing-ascents/)

I'd love to see how I can improve this with better techniques and methods.",statistics,2022-09-08 05:49:52,4
"Numerically, regression and correlation coefficients won’t be affected by a constant offset (i.e. +1).

However there may be bigger concerns about whether it’s reasonable to assume a linear relationship between Likert items.",42,x8ylle,"I'm conducting a survey study and am using established measures. I always hesitate to change anything about a measure from it's original (even something as small as how items are coded numerically if that was visible to participants in the original study on the measure). Some of my measures are explicit to the respondent as being coded as 1-5 while others are 0-4. 

As my title states, does it matter to my regression analysis if I compare these? Is the range of 5 responses identical between a coding of 0-4 and 1-5, as far as a regression or correlation is concerned? Or do they need to be the same to avoid any issues?",statistics,2022-09-08 04:58:44,10
Try calculating Pearson's correlation coefficient which will tell you if there is a positive or negative correlation on a 1 to -1 scale. That's easy to do with python or R libraries.,1,x931ri,"Hi all, I'm new here and know very little about statistics. I was hoping some of you smart cookies can help me. 

I have built a free open source app for tracking data. You can use it to for example track when you drink alcohol and how much, or when you exercise, or your mood. It is very un-opinionated about what it should be used for so people use it to track all kinds of data. I would like to find a way to provide users with information about which data sets might be correlated or related in some way. For example say you track the weather and your mood every day it would be nice if the app could tell you if there's a strong relationship between those two data sets. 

Under the hood each data point has the following data structure:

    abstract class IDataPoint {
        abstract val timestamp: OffsetDateTime //The time the data point was tracked
        abstract val value: Double //A decimal number giving it a value
        abstract val label: String //An optional label for categorisation
    }

A few important things to point out:

* The dates/times can be completely irregular, some things may be tracked much more regularly than others. 
* Data could go up and down over the course of years and I need to know to what extent the data sets are synchronised in their movement (so i'm not sure if ""correlation"" is technically the correct word for what i'm after). 
* Two data sets may have completely different scales, one may be on a scale of 0 to 1000 and another 0 to 1, but maybe this can always be solved with normalisation anyway?
* A data point can have a label. This is often used if a user sets up a multiple choice tracker so that each time they track they select an option like ""Bad"", ""Normal"" or ""Good"". Users set up these labels themselves and a data set can contain any number of different labels. Labels are optional, but each data point will have a numerical value

What I am looking for is really any starting point on algorithms or statistical methods that can be used to tell if two (or more) irregular time series go up and down in sync or in opposition to each other. I'm also interested more broadly if anyone has any ideas for other interesting pieces of information you could extract from data sets like these. The addition of labels I expect lends it well to machine learning of some form. I'm not averse to digging into that and I do have some experience with it, but again I just don't know which methods would be best. Bear in mind this is going to be running on a mobile device not a server farm and I'm not really interested in uploading user data to servers for processing at this time. 

I'm certain this must be a very common problem but as I say I don't know much about statistics, data science or ML. Any direction would be appreciated. Thanks in advance.",statistics,2022-09-08 08:13:28,8
"Essentially the same as you might choose to do it for any other general parametric distribution. How would you typically 'validate' say an F distribution or a Burr, or a triangular distribution? What would count as validation for you?

Is the truncation point known or is it a parameter?

Do you have a particular distribution in mind?",3,x9c7b4,,statistics,2022-09-08 14:22:34,1
"Not really sure about any Excel shortcuts. However, performing most of the tests mentioned by you is relatively easy in Excel. All you need to do is :

1. Enable Data analysis toolpak in Excel. That'll take care of most of your descriptive and hypothesis testing. Plenty of tutorials available on YouTube. 
2. Excel formulas are also available to quickly out out values or to verify info. Data analysis toolpak is still the king. 
3. Refer statisticshowto.com by Staphanie Glen for revision on most topics and how to do them on Excel. Excellent presentation by her. 
4. For more complex analysis such as Logistic regression and other hypothesis testing, please visit www.real-statistics.com. A statistician named Charles manges it and his work is pure awesomeness! You can also download the toolpak and install as a third party add-in in Excel. 
5. Forgot to add. The Office support or Excel Support page is also very informative. Please do give it a try. 

Hope this helps. Wish you all the best.",1,x960h8,"\[Question\]Hi all, I am in an MBA program that requires that I take statistics. It's an introductory level course but my brain does not grasp any of the concepts. My two saving graces (I hope) are that I can use excel/TI-84 and I can have one page of notes front and back.

So here's where I need help. I figure my best bet is to have that one page of notes just be the excel shortcuts/formulas. But here's the problem-I don't know them and our textbook doesnt have them. Our book explains how to do the stats by hand but our professor encourages we use excel.

[Here are summaries of chapters 1-4](https://imgur.com/a/RdOvyVV), anyone have any one-pagers of how to do this stuff in excel so I can practice? Any/all help would be great and I am happy to share any class information that shows this is not against class policy.

Edit to add:

Frequency Distribution, Cumulative Frequency, Stem and Leaf Displays, Measures of Central Tendency, Weighted Mean, Measure of Variability, Variance, Coefficient of Variation, Z Score, Chebyshev's Theory, Measure of Relative Position, Quartiles, Sample Covariance, Correlation Coefficient, Empirical/Marginal/Joint/Empirical Probaility, Multiplication Rule, Bayes Theorem",statistics,2022-09-08 10:10:15,2
"Why would you need to do a *test* in order to cluster? You can measure the KS distance (it wouldn't be my first choice, but let's put that aside) and use those distances in your clustering algorithm of choice, no testing involved.",4,x8ruoc,"The best analogy to my data set is the income distribution of towns. Each town has an empirical income distribution and we are comparing a large number of “towns”. 

The goal is to be able to classify the towns with an unsupervised model. Unlike the usual cluster analysis problem where many dimensions are used to group observations, we want to use one variable (income) from different groups (towns) to classify the similarity of “towns”. This seems like a problem which has already been solved, but I am having trouble finding the answer.

My first thought was to do a pairwise two-sample KS test for every pair of “towns”. My team thought this was a bad idea because doing so many pairwise tests greatly increases type I error. KS tests and similar goodness-of-fit tests were made for the purpose of “did F(x) comes from the same distribution as G(x)?” Which is not the the same problem as “I don’t care if they are the same or not the same, I want to know how far F_1(x) is from F_n(x) for all n to cluster F into a dendrogram”. I read about a few methods like EMD, that supposedly deal with this but I had trouble understanding the topology and n-dimensional geometry.

Would someone be kind enough to jump start my understanding? Hopefully with a few different methods to check for robustness, thanks! :)",statistics,2022-09-07 22:41:59,3
"Yes, moving range to moving average to create a tracking signal. If you are worried about trend or level changes then a cusum is quite effective",2,x8k7nh," 

I have data like below. The top picture is 14 days of data points for every 100 sec.

There are some spikes that like the 2nd picture. Those are system noise/bad data.

The amplitudes and the durations are unknown, the range of the speed of the spike is very large. Most of the spikes have the patterns like the 2nd picture, but some are different. It may have multiple spikes with a couple data points of time gap. Some spike durations of a few data points (5-10 points). But some may have over 30 points. I have hundreds of the dataset each could have 100+ spike.

My job is to find a way to identify the spikes LIVE. I will have 5 data points of processing before making decision.

I was thinking to use machine learning to do classification. However, there is not much that I can extract from the time-series signal. Also I cannot put a pattern into machine learning.

Any advise?

[https://ibb.co/yYtmbz5](https://ibb.co/yYtmbz5)

[https://ibb.co/GPMV7pd](https://ibb.co/GPMV7pd)",statistics,2022-09-07 16:38:42,5
"Propagation of error as commonly used, generally speaking, is essentially an application of the delta method in order to obtain asymptotically normal approximations of the distribution of your measurements. In order to make this usable in practice, we usually use the asymptotic normal approximation as our model of what the uncertainty looks like. This is usually justified since sample means will usually converge to a normal distribution per the central limit theorem.   
Noting that we are using the normal approximation for the uncertainty, how we go about doing the error propagation depends highly on the characteristics of the measurements and how many samples we take.  For example suppose that we expect the the p(x)\_i are themselves relatively normally distributed (this can be justified by the resulting measurement error being the result of many sources of additive measurement error). If the measurement errors are themselves normally distributed, the sampling distribution of the mean value <p(x)\_i> will in fact be exactly normally distributed and your uncertainty measurement will usually be quite good on this scale. At the very least you can often expect the error to be highly symmetric and the resulting convergence to the normal approximation to be quite fast. 

On the other hand, the squares of these measurements will often be a rather skewed distribution and the resulting convergence to the asymptotic normal approximation can be quite slow. The resulting uncertainty you obtain for <p(x)\^2\_i> may be rather poor unless you have a large number of measurements. 

An additional advantage to the first approach by considering measurement error on the unsquared measurements is that you can then provide uncertainty measurements of the squared quantity using a non-normal measurement model. Instead of using the propagation of uncertainty to estimate the standard deviation of the squared quantity, you can simply square the lower and upper bounds of a confidence interval on the unsquared quantity to get valid but asymmetric uncertainty measurements. 

&#x200B;

&#x200B;

As for that paper, I don't really see the point in anything they do there. If you need to estimate the covariances between non-linearly transformed measurements, you can simply directly estimate the covariances between the transformed quantities. There's no need to estimate covariance on the untransformed scale and then propagate via taylor expansion.",6,x8cffk,"Hi all,

I find myself confused over the purpose of error propagation, when it proper to use and extending to higher order terms (when possible). To illustrate my confusion, I will use an example and continue to use it for the remainder of the post. 

Example: I make a measurement of a property p, which gives me samples [p_i] from an uknown distribution. I can also change some property of my apparatus and measure more samples to see how my property changes, I will label samples from this as p(x)_i where x is this other parameter I can change. Generally, I know that the physical property I'm actually interested in has a form of q= sum_x a(x)*p(x)^(2).

It seems like I have two options, one is take the measurements of a single x [again written as p(x)_i], square each measurement [(p(x)_i)^(2)] and then find the mean and uncertainty of this new distribution to record for my physically interesting parameter q. Or, I can find the mean of p(x)_i [due to a lack of symbols I will overload the expectation value, <p(x)_i>, square this <p(x)_i>^2 and propagate the uncertainty. Once I do one or the other, I would then need to propagate the error to the sum to see if it is a statistically interesting change in the property (maybe there's another property that I want to compare ie q(y) where y measures some other change to the system. However, I'm not sure that doing one over the other should make a difference. To me, they should be estimating the same thing (however it is known that squaring the distribution will bias the outcome). Is there a significant difference in what one might expect from the two methods?

Finally, in terms of error propagation this works well for places where the relative error is small since this makes use of a Taylor expansion. However, in this case, I was wondering of we could make this exact using the higher order terms of the Taylor expansion (assuming that it can be shown to be exact). I found a paper (https://journals.sagepub.com/doi/10.1177/0020294021989740), but I'm not sure if I trust it. The math does seem to check out particularly for the lowest order case. In my example, we have a nice case where our Taylor series is exact after three derivatives since it is polynomial. So, it seems that this should work (assuming we blindly follow the paper) by writing out cov(f+f'*x+1/2f''*x^(2),f+f'*x+1/2f''*x^(2)). Here, the f, and f' and f'' terms are the function and its derivatives centered around the mean; and x with its powers are random variables. Finally, we should get terms such as cov(x,x^(2))=<x^(3)>-<x><x^(2)> since x and x^2 should be treated as separate random variables, or cov(x^(2),x^(2)). It's easy to show that one can estimate these higher order covariances by looking at the higher central moments (third <(x-<x>)^(3)> and fourth <(x-<x>)^(4)>) while also using the results from the variance. This is done by expansion and very easy to see in the case of the third order. Further, all of these can be estimated from the data then as long as enough samples were taken at the beginning. Is this possible as long as one can estimate these properties or are there hidden assumptions, or am I completely missing the point?

In anycase, I wanted to give some thanks in advance for answering or giving discussion. I tried to look for some resources, but I could not find a good treatment of the underlying theory that I needed to better understand error propagation in a deep way. Most texts seem to be for first year undergrads, but just say this is how it is and not really drill down into the meaning.

Edit: I fixed some formatting to make it easier to read.",statistics,2022-09-07 11:22:38,8
"At the end of the day a lot of what gets you a job is going to be your network, I would choose the University of Melbourne and go for a Statistics master if they have it if you're concerned that you wont learn enough with a DS degree. If that doesn't exist then you should just go for the Data Science course and develop a professional network..",13,x82r9p,"Hi I just graduated from my Bachelor degree and planning to pursue a graduate degree for better career progression and etc. I'm planning to work in data related careers (data analyst, data scientist, statistician, and etc)

I'm currently trying to choose between three potential masters degree :
- Master of Science in Statistics from University of Queensland

 This one seems to be mathematically rigorous and I believe it would give me deep theoretical understanding. However it's located in less strategic location compared to Melbourne where there will be many internships and jobs opportunities. In addition to that University of Melbourne has significantly higher rank compared to University of Queensland.

- Master of Data Science from Univeristy of Melbourne

 After thorough research many mentioned that it's better to specialise in either Computer Science or Statistics and avoid DS degree. This is because many are cash cow degree and will not lead to mastery in either field.

- Master of Biostatistics from University of Melbourne

Working as biostatistician is also appealing for me. But I'm also open to work in other field and worry that I might not be able to move to another position because compared to other degree this looks less rigorous and more specialised.
Any advice and recommendation would be appreciated!",statistics,2022-09-07 04:32:58,8
"Backblaze [publishes survival data for their hard drives](https://www.backblaze.com/b2/hard-drive-test-data.html). Though, the dataset is massive and needs preprocessing.",5,x86khr,"I need a survival dataset for my dissertation, but I’m having a really hard time finding one and would prefer to work on real survival data rather than simulating some. Could you provide some resources?",statistics,2022-09-07 07:27:53,5
"You should not change the model (collapsing the factor) due to a test just because something wasn’t significant. Thats essentially stepwise regression and its bad because of inference after model selection.

You should run the 3 factor contrasts you intended to anyways regardless of whatever ANOVA says. ANOVA F test is actually largely irrelevant, and you can always go to the contrasts directly and treat it like a regression (forgetting about ANOVA F test itself). 

The modern causal modeling view doesn’t use ANOVA and just does the contrasts that were intended to begin with",3,x8g0ru," Hi  there!  I've got some research data that I just want to be sure that  I'm analyzing correctly.  I'm running the analysis in Prism 9, if ya'll  are familiar with it.

Study  design: Males and Females with and without a certain cardiac defect, who  exercise at a variety of exercise loads.  I collect data at each  exercise Load.

My understanding is  that I have 3 factors (Sex X Cardiac Defect X Exercise Workload, and  since I'm measuring multiple exercise workloads in each participant, I  have repeated measures.  As such, I would need to Run a 3-Way RMANOVA.   Because Prism recommends it, I'm running Sidak's Post-hoc tests.

If  I have a significant 3-way interaction (Sex x Cardiac Defect x Exercise  Workload), I report the F-value for that, as well as any specific  comparisons and their associated P-Value (IE at 70W exercise, Males with  the cardiac defect had greater breathing than females without the  cardiac defect, p = whatever).

If there is no significant 3-way interaction, but there *is*  a significant 2-way interaction, I can collapse the non-significant  factor and then perform a 2-way RMANOVA on the consolidated data.  So,  if there is a significant 2-way interaction (Sex x Exercise Workload), I  would collapse the Cardiac Defect factor, run my 2-way Anova (Sex x  Exercise Workload), report the F-value for that then any pairwise  differences.

If there is no  significant 3-way interaction, and no significant 2-way interaction,  then I can collapse the two non-significant factors and just report the  simple main effect of the significant Factor (Sex), report the F-value,  and differences  (Females breathed 70L/min vs Males breathed 50L/min, p =  whatever).

Do I have that all right, or am I missing something?",statistics,2022-09-07 13:46:17,16
"What does your weighting accomplish? Can you quantify that? You can now optimize your weight selection against some error.

Side note: Are your classifications ordinal? If not you might want to make sure you are drawing inference correctly from these statistics you are creating.",5,x8d7s0,"I have 5 classifications for 1200 rows - Dark red, Red, Dark Amber, Amber and Green.

I have converted them to a numeric scale - 1,2,3,4,5
Dark Red - 5
Green - 1

I have 2 set of scores (ML model score and external score) between 0 and 1.
Green might be closer to 0 and Red might be closer to 1.

I want to merge the ML score and External score by giving them some kind of weightage (maybe 50-50)

How do I decide what weightage to be given?",statistics,2022-09-07 11:54:55,5
"You could use a hierarchical model to estimate effects globally, and then at the individual survey level. That would probably be the most robust way to use both surveys without assuming the distributions in each survey are identical.",3,x7z9uu,"Suppose I have two surveys data sets. Both ask similar questions mainly around consumer behaviour. Survey A, is a random sample of the entire population. Survey B, is a convenience sample of mainly affluent segment of the population (25% of the richest of the entire population) . Survey A is conducted with F2F interviews and leave behind questionnaires. Survey B is filled online. Survey A, takes more time and people are paid to take part in the survey. Because of this, survey A, has sparse data on the higher income end. People with high income do not have the time and the money incentive does not mean much to them. Survey B is more convenient and offers a draw to a bigger prize so they complete that one.

For completeness, we want to merge the two so that we have a representation in all income areas. Question is, is this statistically allowed? If so, how does one fuse the two sets? Is there some sort of weighting that needs to be done?",statistics,2022-09-07 01:04:17,2
The plot is done on the response scale after backtransforming while those parameters are on the link scale. Logistic reg is nonlinear on the response scale so the intercept term itself wont match the “actual” intercept in a plot on the response scale.,2,x856iz,"Hello,

I'm using R to run a logistic regression and then plot the same using ggplot. I was curious, however, why the results seem to be so different? For example, the logistic regression output says my intercepts is -3.722, while the intercept estimate from the graph is around .05. Here is my code, I'm wondering what I flubbed up.

&#x200B;

    #Create my own logistic regression
    df <- tribble(
      ~x, ~y,
      1, 1,
      .9, 1,
      .8, 0,
      .7, 1,
      .6, 1,
      .5, 1,
      .4, 0,
      .3, 0,
      .2, 0,
      .1, 0
    )
    
    
    x_and_y_regression <- glm(y ~ x, data = df, family = binomial(link = 'logit'))
    summary(x_and_y_regression)
    
    ggplot(data = df, aes(x = x, y = y)) +
      geom_point() +
      geom_smooth(method = ""glm"", se = TRUE, method.args = list(family = binomial))",statistics,2022-09-07 06:29:23,1
"You're clearly well qualified. 

Leave your job and apply and accept a new 
Job with the title data scientist. If someone offers you a job with the title !(data scientist) decline.",7,x83m2c,"Hi all,

For a bit of context, I did my undergrad degree in Physics/Astrophysics, then back in March of this year I completed my PhD in Space Physics. My PhD was basically working with spacecraft data in a programming language called IDL. There was (as you’d expect) a lot of data processing, analysis and visualisation involved - time series, linear regression, manipulating and tidying data, histograms, t-tests, all that. Because of that, a job working with data seemed like a good place to begin (didn’t want to stay in academia).

So, after my PhD, I got an industry job as a Data Analyst (completely different field), where I’ve been working primarily in R but also a bit of Excel and SQL. I’d say I’m quite competent at R now - I have no issues with having to join datasets, tidy them up, use packages like dplyr, ggplot, and also using functions like lm/glm for modelling, chi squared tests, etc. I’ve also dabbled a hand in using tidymodels to train a classification decision tree, but not to any great success.

My question is, where do I go from here? Do I keep building my experience as a data analyst for now, since I’ve only been in the role for 6 months? Is a data scientist seen as a “better” or “more intelligent” role than a data analyst?",statistics,2022-09-07 05:16:39,6
"Here’s how I see it. Non-7 or 8 rolls are irrelevant and completely ignored. If you roll a 7 you win, if you roll an 8 you lose, anything else reroll. There are 6 permutations to make a 7 and 5 to make an 8 therefore there are 11 equally probable ways to end the game. 6 of them are a winner.",26,x7fllp,"I am working on a sample problem that asks that, when we roll a pair of dice, we stop when the sum of seven or eight appears; and we need to show that the probability of getting the sum of 7 before getting the sum of 8 is 6/11.

From my understanding, it's basically saying that you must obtain a sum of 7 before a sum of 8, and then you stop rolling.

I am seeing 2 solutions, both of which I am having a hard time understanding.

***Solution 1***: This is from my instructor. He first sets P(C1) as the probability of getting a 7 and P(C2) as the probability of getting an 8. He says that the probability of getting a 7 before an 8 is the same as the conditional probability P(C1 | C1 U C2). This is where I am stuck. I see that C1 and C2 are two independent events, so obviously P(C1 | C2) won't make sense, but I don't get the (C1 U C2) part.

***Edit***: Nevermind about (C1 U C2), I seem to get it now. That's because we are only looking at cases where the sum is either 7 before 8 or 8 before 7, hence (C1 U C2).

&#x200B;

***Solution 2***: This is a solution posted on [Stack Exchange](https://math.stackexchange.com/questions/890844/how-to-compute-these-probabilities).  The chance for a seven is 6/36 =1/6 and the chance for an eight is 5/36. The chance for neither is hence 1 - 1/6 - 5/36 = 25/36. Great, so far so good.

The answer proceeds to mentioned that we want to compute the chance of getting neither k times and then getting a seven, where k goes from 0 up. This resonates with my understanding of obtaining a sum of 7 before a sum of 8, then you stop. What I don't understand is the formula afterwards:

1/6 + 25/36 \* 1/6 + (25/36)\^2 \* 1/6 + ...

Any explanation/pointers would be greatly appreciated!",statistics,2022-09-06 09:55:30,8
"Yes, you can compare proportions but you need large (*large*) sample sizes. [You should already know a rule that relates n and p for using proportion tests - there are several around. They tend to lowball the sort of sample size you need but they at least give you a good bound you should beware of going close to, let alone below.]


There are several issues related to this if sample sizes are not quite large:


1. Difficulty with approximations (normal, chi-squared, t etc) -- if the expected counts are low, you won't have accurate significance levels with any of these common approximations.

2. Lack of available significance levels. Even if you have a test that gives you 'exact' significance levels\*, it's not much help if there's no significance level near what you want. I've multiple times seen people carry out tests that cannot attain the p-value threshold they're using (e.g. if the lowest available significance level is 5.2% there's no point looking for a p-value below 0.05; you'll *never* see one -- the power of such a test is literally 0 at every effect size).

3. The information in your data will closely relate to the expected number of successes in each group, rather than the total number of observations. This impacts power. You will simply have very little power if the expected success *count* in either group is low (NB expected under some specific alternative, rather than observed). You cannot overcome inherently low-information by choosing a different test; the best you can do is limited by the information that's there.

It's important to compute the significance level (actual attained level under your chosen rejection rule) and power of whatever test you're using at whatever sample size at some plausible effect size. Otherwise you seriously risk doing pointless things and never realizing it was pointless.

---

\* In particular, `exact` tests will allow you to compute exact significance levels for your test, *as long as you don't just blindly compare p-values with some nominal alpha* as if that alpha was available to choose (in other words, ***don't*** just do alpha=0.05). The Fisher exact test is often used but there's not typically a particular benefit\*\* over doing an exact version of any other test on proportions (e.g. an exact test based on say the Pearson chi-squared statistic, the two-sample z-statistic, the G-test statistic, the Freeman-Tukey statistic etc etc). They all *only address problem 1*; problems 2 and 3 remain.

\*\* usually for 2x2 tests the exact versions of all of them tend to correspond extremely closely. In larger tables, there are difference but there's something you can do that will be better than choosing any one of them.",5,x7kkku,Is it possible to run a 2 proportion test with such small proportions? Are there alternatives that correct for this?,statistics,2022-09-06 13:14:55,2
"One shouldn’t generally interpret every single term in a regression because of associations between them and Table 2 fallacy. In logistic reg there is also the non-collapsibility complication. 

When there are nonlinear/interaction terms then marginal effects (G computation) is the way to assess significance of the main effect exposure alone. Its also typically done by default on the probability scale and not log odds. R marginaleffects is a good package for this. Essentially this method is taking an average derivative E(dY/dX), and will not correspond to a model parameter directly.",12,x79ysm,"Hello stats community!

For my research report/paper, I'm looking at the association between the timing of an event (0 - early, 1 - late) and a categorical, explanatory variable (1 - intended, 2 - unintended, 3 - ambivalent). Intention (the explanatory variable) is known to be associated with the timing of the event (i.e. unintended leads to late timing). I'm currently building the multivariate logistic regression model. Once I adjusted for significant variables found at the univariate stage and confounders, this association is significant in the multivariate model (p<0.05). However when I add an interaction term between education (categorical) and intention, intention is no longer significant (p>0.05) while the interaction term is significant.

Is this a case of collinearity between intention and education or something else?

Thank you in advance! :)

P.S. Happy to provide additional information if needed, just don't want to overload the post.",statistics,2022-09-06 05:57:48,19
"Going from memory (I'll try to come back to this when I have had a chance to check the paper):

If I recall correctly W is based on the ratio of two different estimates of variance; one based on the usual estimator and one based on a linear combination of order statistics. It takes values between 0 and 1 but it's generally quite close to 1.

There's a large sample approximation to it that is based on the correlation in the Q-Q plot (the Shapiro-Francia test\*). The Shapiro-Wilk uses information about the covariances of the order statistics and is typically more efficient in all but large samples (though not against all possible alternatives) but - even though it's not exactly that correlation itself - I suggest interpreting it as a quantity that's at least akin to the correlation in the Q-Q plot.


---

\* there's another test that's almost the same thing - the Ryan-Joiner test",3,x7dcie,"More specifically, how do I interpret the W statistic, and what bearing does it have on normality?

Everywhere I look mentions the p-value as the determining factor for interpretation and that it is important to provide W, but nowhere can I find a clear definition of what W means.",statistics,2022-09-06 08:23:24,4
"The conversion between the currencies is a linear transformation. This should only affect the coefficient of the feature, but the credit score itself is not expected to change.",7,x7cg63,"I have trouble understanding my model.

In a  logistic regression model explaining calculating the creditscore, my estimate when the income is in NOK is 0.6 . 

Can I determine how much this estimate will be if the income is calculated in EUR with the exchange rate EUR/NOK = 9.9 NOK) ? Or maybe i have to retrain my model for each currency ?

&#x200B;

thanks",statistics,2022-09-06 07:46:18,2
"Two things here. One, echoing the others, is that a given hypothesis is never ""proven."" Rather evidence is built to refute or support a given claim. Subtle, but important.

Having said that there is a field of statistics called sequential analysis where the sample size is no longer considered fixed and the decision to reject is more fluid being assessed at each stage of sampling, rather than making a decision on one sample.",48,x6xq6g,In my college sophomore statistics class they taught good values in an experiment can fail to reject the null hypothesis but not prove it is true. Is there any standard in statistics for how many confirming experiments with different samples need to be conducted before the null hypothesis is considered proven?,statistics,2022-09-05 18:49:34,24
"So you will want to decide what exactly you want to calculate.  What stands out to me is that he picks 1st or 2nd on 7 out of 10 drafts.  Given any player has a 20% chance of getting one of the top 2 spots.

If you define success as getting one of the top 2 spots. You can use the binomial distribution to calculate the probability of 7 successes in 10 trials.

https://stattrek.com/online-calculator/binomial

I get .00087 probability that someone randomly gets one of the top two spots in 7 (or more) out of 10 trials",25,x6twi1,"Alright, so I did take a stats class in college, but that was a while ago and I'm having trouble with a problem. I've been in the same fantasy football league for 11 years, and the league manager says the draft order is randomized, but it appears that he (the league manager) often has an advantageous draft position. I'd like to calculate the odds of his incredible ""luck"". Disclaimer: this is all in good fun, I'd just like to give my friend a hard time using Stats.

Background: There are 10 people in the league. Draft order is supposed to be randomly assigned at the beginning of each draft.

These are his draft positions each year: 2012 - 2nd, 2013 - 1s, 2014 - 1st, 2015 - 2nd, 2016 - 2nd, 2017 - 6th, 2018 - 4th, 2019 - 3rd, 2020 - 1st, 2021 - 2nd, 2022 - 1st

This is my first post in this community, please let me know if this isn't the appropriate place for this! And thanks in advance!

Edit: Spelling",statistics,2022-09-05 15:54:03,14
You might like this! https://statproofbook.github.io/,32,x6ip4l,"Is there a reference statistics book with no explanations? What I am thinking of is a book or lecture note that covers undergraduate statistics and only contains definitions, theorems (and possibly proofs) without explanations, examples, or exercises. Thank you!",statistics,2022-09-05 08:18:13,10
"there is no clear cut recipe to mitigate endogeneity. you first need to understand the potential source of your endogeneity  - is it omitted variables? then find information to control for it. is it simultaneity? maybe you could find instruments to run e.g. 2sls. in any case, you will need to think carefully about your model and the information you have at your disposal. without providing further information it will be difficult for people to help you.",10,x6dj90,"I am planning to use Linear Regression for inference purposes and I am aware of the endogeneity problem. The Endogeneity problem will be problematic as my inferences could be biased.

So my question is : What are the steps to mitigate endogeneity ?",statistics,2022-09-05 04:09:33,3
"Good question, I can't help with a calculation, but with a simulation

    import random
    
    cards = ['A', '2', '3', '4', '5', '6', '7', '8', '9', '10', 'J', 'Q', 'K']
    deck = cards + cards + cards + cards
    
    counter = 0
    total = 10000000
    
    for r in range(total):
        random.shuffle(deck)
        for i, j in enumerate(deck):
            if i < len(deck) - 1:
                if deck[i+1] == j:
                    counter += 1
                    break
    
    print(1 - (counter / total))

`0.0454453`",69,x5s40s,"To clarify, to shuffle a deck such that a 7 does not neighbor a 7, Q with another Q, etc., for all 52 cards in the deck.  
  
Is there a way in which to even calculate the total number of shuffled decks that have this property, which, divided into 52!, would presumably give the probability?",statistics,2022-09-04 10:09:51,20
"Make your own list of topics you would cover. Write them down..
Then Google and look for other people's syllabi on a similar level. It's great to fill in the gaps and give you ideas on exercises.

I always give ""software output"" for them to interpret. This year i might also use different statistical programs so they know how to interpret it no matter where they're doing it in.

Edit: good luck? Stats is a very rewarding class if you're a good teacher! I'd stay away from the formulas unless you give a formula sheet. Understanding >>>>>> learning by heart",10,x5xka1,"Hi! I will be the instructor for a year-long AP Statistics course (at a private education institution).

This will be my first experience as a teacher*. 

I am responsible for designing/delivering weekly lectures & assignments, as well as unit tests.

Any advice is appreciated!

*Although, I have many years of Tutoring and TA experience (not in stats though - random math/physics courses)",statistics,2022-09-04 13:58:24,10
"The pareto should be conjugate for that parameter; it's a couple of lines of algebra. You might well not find a reference for a standard undergrad exercise. Maybe it's done as an example in a textbook somewhere instead, perhaps.

edit: The result is here: https://en.wikipedia.org/wiki/Conjugate_prior#When_likelihood_function_is_a_continuous_distribution

Reference 3 (see the list at the end of the page) states it; you could use that in a pinch.

Another possible reference is the Probability and Statistics Cookbook, Version 0.2.6 (2017) by Matthias Valle
 here: http://statistics.zone/ 


However, the derivation is straightforward. If you need it for a paper, you should just be able to state it.",7,x67bin,I'm struggling to find a good reference for the derivation of the posterior predictive distribution in this case. Anyone able to point me in a good direction? Thanks.,statistics,2022-09-04 21:51:58,7
"You can't reliably compare these two models unless you can compare them on the *same* data set

Here's some of the problems....

1) Suppose for simplicity that we're trying to predict a binary variable (e.g. is patient still alive after 5 years.). In A's sample, 95 percent survive, while in B's the rate is 50 percent. The first case puts a much higher premium on avoiding false predictions of death than false predictions of survival. The best model in one sample may perform poorly in the other.

2) Suppose model A depends critically on the variable X1. The R² for model A therefore depends in part on the variance of X1. The same logic may apply to variables X2 for model B.Their relative performance will therefore depend in part on the relative volatility of the independent variables.",3,x5xqbz,"Suppose we have two models, model A and model B, for the prognosis of a sample of 500 patients.

At 1 year follow-up, model A's R-squared was 30% and model B's R squared was 20%. Thus, model A's prognosis was more accurate.

At 2 year follow-up, the situation was reversed: model A's R squared was 10% and model B's R squared was 15%. Still, in this period, several patients left the trial, resulting in a sample of 200 people.

How could we weight time, R squared size and sample size to judge which model is best?",statistics,2022-09-04 14:04:44,1
"A general comment, without reading the linked paper: This is a common design. In psychology it’s called a within-subjects design and in medicine it’s called a cross-over design. In such a design the order of the placebo (or control) and active treatment should be randomised between patients. In medicine we usually worry about what’s called the “wash-out period”: will having received the first intervention result in a different effect from the second intervention. Where this design is practical, it is usually more powerful than a between-subjects design, and the analysis can be as simple as a paired t-test. See various books/articles/blog posts by Stephen Senn on crossover trials in medicine.

If there is no control, just pre-intervention and post-intervention measurements, this is generally a bad design.

P-values of 0.000 are not usually possible, although it is possible and common to get p-values which round to 0.000 to three decimal places. These are conventionally reported as p < 0.001.

I’m not sure what your reference to “multicollinearity” relates to, although I will say that in general multicollinearity is much less of a problem than psychology students tend to assume it is. (Linear models behave exactly as the ought to in the presence of multicollinearity, although it is often a symptom of something else having gone wrong in the statistical thinking that let to the choice of model.)",19,x5d1ym,"Econ undergrad with stats minor here - so a wee babe of knowledge. 

I am baffled by a study I am doing a report on.  It took me ages to translate but I finally worked out that the Control group was \*the same groups as the ones taking the test\*.  They take the test and then do a control KNOWING ITS A CONTROL.  It's about updating posterior beliefs so it makes sense in this case to tell someone they are the control.... but after doing one of the non-control tests?

I feel some kind of way about the way this study was formatted and it bothers me.  As I'm doing a report on it, I am trying to isolate that ""this doesn't feel quite right"" feeling I am having about it so I can discuss it in my paper.  

the paper is here for anyone who has academic access:

 [The Good News-Bad News Effect: Asymmetric Processing of Objective Information about Yourself on JSTOR](https://www.jstor.org/stable/41237187) 

I have found papers discussing it but literally nobody I can find discusses the Experimental Design portion, just the intro and results.  I, with my oh-so-high-brained stats minor, feel like something is amiss.  Also, one of the P-vals is 0.000?  Is...that even possible in psych? No test for multi-collinearity...am I losing my mind or is my tiny pea-brain just too stupid to pick up what's being dropped here?",statistics,2022-09-03 20:44:13,22
No. Use an online calculator.,1,x5swnf,Im dealing with tests atm. Given an alpha = 0.05 we know that the quantiles for each side are -1.96 and 1.96. So their areas added together are 0.05. For other alpha values we use tables most of the time but i'll have an exam soon and we had exercises with different alphas which i couldnt find on the table they've given me like 0.001. Ofc i could just look them up now on the internet. but Is there any easy method to calculate the x  for the quantiles (maybe approximately)  so i get the area alpha ?,statistics,2022-09-04 10:43:36,13
R by far because even for jobs that is more desired. It also teaches you computational thinking which is important while SPSS doesn’t. R will take longer though to learn if you don’t have a programming background,63,x4wg3h,"I am in a quantitative methods class for my masters in urban planning program, and the course is set up such that you can use SPSS or R for the problem sets. I have access to both for now. 

On the one hand, my book implies that SPSS is the dominant software in planning. At the same time, my searching here seems to imply that SPSS sees little use in the real world. I will also not necessarily have access to SPSS when I graduate whereas R will always be available. I was tempted to do the problem sets in both software packages so I could get used to both but I don’t think I have the time. 

What would be the recommended software to focus my time on?",statistics,2022-09-03 08:00:12,21
"Odds ratios for logistic regression

Edit: the p values for each independent variable also will tell you the significance of that variables impact on the response",5,x5awtx,"Example:

Let's say you wanted to test whether ""Difficulty"" and ""Map"" had an effect on a player's likelihood of completing a game, ""Completion"".

Trial| Difficulty | Map | Completion
|:--:|:--:|:--:|:--:|
1 | Easy | Fire | Yes
2 | Easy | Air | Yes
3 | Medium | Water | Yes
4 | Medium | Fire | No
5 | Medium | Earth | Yes
6 | Hard | Air | Yes
7 | Hard | Water | Yes
8 | Hard | Fire | No
9 |  Hard | Earth | Yes

In this example, I'd imagine the end result showing that the Difficulty setting does have an effect on completion, and that the Map setting, specially ""Fire"" might as well.

What hypothesis test would accomplish this?",statistics,2022-09-03 18:54:11,3
"Idea sounds right, but you will need some extra letters to account for punctuation. Also, you would assume the monkey has access to a regular typewriter which has way more than 27 characters.  A standard typewriter has 44 keys so I would use that 44 number.",17,x4sqwp,"Is my math heading in the right direction?

The Infinite Monkey Theorem is a proposition that an unlimited number of monkeys, given typewriters and sufficient time, will eventually produce a particular text, such as Hamlet or even the complete works of Shakespeare.

Given that Shakespears Hamlet used only letters (for simplicity sake), the probability of a monkey randomly typing the correct letter would be 1/26 (uniform dist.) for each letter in the book. 
As words are separated by spaces, and we can consider spaces to be a character just like letters, this would take the probability to 1/27 for each character in Hamlet to be correct (you could refine this by considering that some letters will definitely not occur twice in a word but for simplicity sake I am leaving this out).
Hamlet has a total of 130k letters hence, we can conclude that the probability of randomly typing hamlet is 1/(27)^130000 (given the simplified premisses)?

Is this going in the right direction?",statistics,2022-09-03 05:01:26,24
"The p value here is the p value associated with the I^2 statistic, so in this case it indicates a high degree of confidence that I^2 is big.

The large I^2 indicates that a lot of the uncertainty surrounding the intubation rates is a result of inconsistency/heterogeneity between studies as opposed to intra-study variance.",1,x5cuvz,"**this is not homework, I’m just trying to understand an article. 

I’m doing a literature review on a systematic review & meta-analysis on intubation rates following chemical restraint for my study and I’m examining the results they found, however I’m finding it difficult to interpret. 

The study found that the overall intubation rates was 16% (95% CI = 8%-26%). There was significant heterogeneity between the included studies (I2 =97%, p<0.001) 

What does this mean in terms of confidence interval, the I squared thing and p value? 
I’ve gathered that heterogeneity isn’t a good thing in this case as this means the studies they examined don’t overlap and have the same conclusion so they would have wanted more homogeneity, is that correct? But then what does the p value determine for this? 

If you can help, thanks :)",statistics,2022-09-03 20:33:50,3
"Since no one is answering, I'll say my inclination is that there may be a big disadvantage to using a moving average over the actual values measured in each wave. The main drawback of FE models is that you are using much less variation in your data to get your coefficient estimates (since you're subtracting from each measure the average across all waves), which increases your standard errors. Using the moving average would reduce the variation in your model even further so I would imagine you would be estimating an even less efficient model and potentially washing out the associations between variables.  

It is possible that using a moving average would be advantageous for mitigating measurement error, if there is any. But that may not be worth the efficiency trade-off.  

I would probably try out both approaches on some real (or simulated) data and compare the results. Either way, I would lean more toward not using moving averages because, from what I've seen, it's much more common to use the direct measures in each wave in FE models.",2,x53fut,"I don't know if my terminology in the title is correct so please accept my apologies if I'm describing things incorrectly.

My question is as follows. Say you have a classroom of 30 students and every day they take a survey from 1-10 of how happy they are for 10 days. So, you have a nice panel data set up, 30 responses per day for 300 datapoints

I would think this would be a good opportunity for fixed effects regression. [https://www.kellogg.northwestern.edu/faculty/dranove/htm/dranove/coursepages/Mgmt%20469/Fixed%20Effects%20Models.pdf](https://www.kellogg.northwestern.edu/faculty/dranove/htm/dranove/coursepages/Mgmt%20469/Fixed%20Effects%20Models.pdf).

Now my question is as follows: You could run the regression with the target variable as the daily score, or as the moving average of the past 5 days score (the average of what the student said the past 5 days).

Beyond perhaps reducing the size of my dataset (omitting the first 4 days before the average is valid), how would changing the target in this way change the meaning of the coefficients / the assumptions of the model? If it all?",statistics,2022-09-03 13:03:53,2
"First two formulas are definitely the same. 
You can get first one from the second using the definition of std(x) and std(y).",4,x4t9hy,"In our lecture for data mining, they use this formula:

R = **(Σ (x- x̄)(y - Ȳ))** **/** **(** √ (Σ (x- x̄)²Σ(y - Ȳ)²)**)**

In one of the recommended course books, they use:

r = Σ(x - x̄)(y - Ȳ) **/** (n 𝜎x 𝜎y)

And in an online example, they use:

r = **(n (Σxy) - (Σx)(Σy))** **/** **(√ (n Σy² - (Σy)²)).**

I don't understand the difference. In the first one they don't use the sample size, and in the second one they use the standard deviation.",statistics,2022-09-03 05:29:10,15
"For every parameter in your model, you have a prior. In a regression context, this would usually involve a prior for the intercept and one for the influence of each predictor in the regression, plus, assuming the outcome is Gaussian, a prior on the scale of measurement noise. Possibly you are confusing the prior, of which there is one per parameter, with the likelihood, which in the case of univariate outcomes consists of the specification of a single distribution class (ex Gaussian).",7,x4yym8,"Hello,

I have a question that, despite the amount of reading I've done recently on Bayesian statistics, I realized I didn't know the answer to. I was wondering about the use of prior distributions, specifically what their application to the predicted Y value is.

Is the prior used to model *just* the outcome variable? By that I mean, is the prior distribution going to concern *only* the outcome variable we're looking for or do I need a number of prior distributions to be able to model the priors of each input?

I know that brms or rstanarm packages seem to have space for only one prior distribution--is that the distribution of the expected results?

Thank you very much for looking at my question, I really appreciate it. I hope you have a good day!",statistics,2022-09-03 09:47:13,13
"The first sock is not relevant. The second sock has 1/9 chance of matching. Given the first socks are a pair, the third sock is irrelevant and the fourth sock has 1/7 chance of matching. Given the second pair, the fifth sock is irrelevant and the sixth sock has a 1/5 chance of matching, etc. So the answer is:

1/9 \* 1/7 \* 1/5 \* 1/3 = 1/945

This is for 5 pairs of socks. Fairly easy to generalize for any arbitrary number of pairs.",2,x4y1m2,"Hello, all - I knew there would be a subreddit where I could ask this question.

Background: I have a large number of identical black socks, and identical black compression stockings (yay for getting old...) that always tend to come out of the clothes dryer inside-out.  They all have subtle markings so I can match them up and ensure even wear and even stretching as the age.  (Yes, I have OCD tendencies - deal with it.)  It's also been \~35yrs since I last took a statistics class so I'm more than a little rusty.

(Stockings==socks, and socks==stockings, at least in this context.)

Today, I pulled five pairs (10 stockings) out of the dryer, and as I inverted every !@#$ one of them to get them right-side-out, I found to my considerable surprise that I was getting the pairs in perfect sequence.

i.e. for stockings A1,A2,B1,B2,C1,C2,D1,D2,E1,E2, they came out in that exact order.  Or at least AABBCCDDEE, I don't know which is which within a pair - maybe it was A2,A1,B2,B1,etc.

Now I'm wondering - what were the odds of that happening?  I sort of remember how to setup the problem if (any) strict (arbitrary) order were involved, but only care about pairwise ordering *within* the larger ordering and I cannot remember how to model that as a simple probability equation.

I don't care about the ordering between or within pairs, only that they come out sequentially in pairs - this would have been an equally unusual moment if they'd come out of the dryer as E2,E1,A1,A2,C2,C1,B1,B2 or any other pair-preserving order.

Help?  I know this is fairly basic, but I can't quite figure out how to set it up.

Formally, I guess this might be phrased - awkwardly - as: 

* Given N pairs of stockings, producing M=N\*2 individual stockings, where the M individual stockings are arranged in a random order, what are the odds that the random stocking ordering consists only of matched (adjacent) paired stockings {i.e. members of N}, regardless of intra-pair ordering?

Advice welcomed.  Stats advice, that is, not life advice on how to manage my socks, thanks very much...",statistics,2022-09-03 09:08:17,6
"The best is to just ditch Biostat and go for DS or ML roles within biotech/pharma. They involve more creativity, actual statistics and modeling, and you get to use R/Python. 

In my opinion, Biostat as a field is dying in the modeling/stats realm outside of regulated clinical trials in the industry which is the only place that uses outdated tools like SAS and 70 year old methods. You can see this in that more math/stats is actually used in positions that are not titled as such.

You will limit you ability to go into R/Python and modeling roles if you use SAS. Biostat = rote regulated stuff+SAS+ writing. I very rarely see a single Biostat titled position that isn’t this. 

I firmly believe Biostat programs need to have a software engineering track to prepare people who want to avoid the rote stuff and get into modeling roles as nowadays modeling is becoming engineering. Knowing how to put stuff into production seems to be the key to get more interesting work",4,x4jss2,"**Reaching out to any Biostatisticians or Stat Programmers:** 

1. How often do you use R or Python for non-client facing duties? (i.e. simulations, bioinformatics work, etc.). 
2. While most Stat Programmer jobs list SAS as being a requirement, can I pivot into ""R""-related duties while on the job? 
3. What types of biostats roles involve more advanced stat modeling and not routine analysis, protocol generation, etc.? 

The reason I ask about R and Python so much, is that SAS is horribly limiting as a software. Outside of Pharma and finance, it has little to no utility and as a software as well, I find it to be clunky, unintuitive, and incredibly tedious to work with when compared to R. 

So, I was wondering how much of an opportunity I'd have to strengthen my R-programming and Python skills as a junior level biostatistician or statistical programmer.",statistics,2022-09-02 19:57:06,17
"I think you may be more confused than you realize. Or perhaps I'm really just not following.

A simple 1-dimensional probability distribution describes the probability that a random variable takes on certain values. The probability of heads in a coin toss. The distribution of lifetimes of lightbulbs. The height of a human. A lognormal is one of these kinds of distributions. The density curve describes (more or less) the relative probability of finding an observation in the neighborhood of one value compared to another. The lognormal distribution's pdf is written out many places, for example [on Wikipedia](https://en.wikipedia.org/wiki/Log-normal_distribution).

The data you're talking about sound to me like they're meaningfully *structured*, which is not a time for using a simple 1-D probability distribution, but something more complex (like a stochastic process or some model where the distribution gets to vary as a function of covariates in some way).  When you fit a distribution like a lognormal, you need *independent and identically distributed* (IID) random variables. You need to observe the *same* process over and over again. The sort of thing you'd put in a histogram, or a boxplot. What are do you have IID observations of that you're fitting a lognormal to?

And if that curve plotted in the spreadsheet is what you're talking about looking like a lognormal, that looks nothing like a lognormal. A lognormal distribution has density 0 at 0. That curve does not. And the data that's being generated from is entirely the wrong kind of data for 1-D probabilty distribution fitting. That's paired data, Y with X. That's not what a lognormal describes.",1,x4lrgb,"Hi stat enthusiasts,
I have run into a very peculiar problem. It might seem silly to a lot of you. But I am in dire need of a way out. I am analyzing sets of high-speed images with MATLAB. The image of interest (https://www.dropbox.com/s/h4h26y3mvpao8m6/sample.png?dl=0) is an average of 3000 images (background subtracted). As shown in the picture, I am reading the pixel intensities/values along columns. As this is a laser beam, the shape or beam profile away from the wall has the shape of a Gaussian distribution. As I approach to the wall (the brightest part at the right of the image) because of some effect the shape is turning into one like a log-normal distribution. In this spreadsheet (https://www.dropbox.com/s/yeim06a5cq3iqg8/sample.xlsx?dl=0) I have pasted the raw intensities as I read thru from point A to point B. The column D has the raw intensities and the column E has the values achieved with a 'sgolay' fit of the column D values. If I plot these it pretty much has the shape of a lognormal distribution. I can get the mu and sigma with the 'lognfit' or 'fitdist' functions. Now the question is what is the equation [expressed as a function of pixel location (x) or the pixel intensity (y)] of the fitted 'lognormal curve' that could be used to recreate the fitted curve? Your help is highly appreciated.",statistics,2022-09-02 21:44:25,1
I did Colorado State’s program in person.,1,x4iozo,"I'd love to start an in-person program as soon as I can. I found University of Wisconsin's program but no others so far. I prefer locations in the US West, but I'm open to other US or international programs too!

My career goal is to work in computational social science, likely in health or economics.",statistics,2022-09-02 18:58:34,1
"You almost asked ""How do I statistics?"".",4,x48qqy,"And how can I do that in R?

Basically, I have a set of complex numbers and I want to determine whether their means and covariance matrix are significantly not different compared to a reference mean and covariance matrix, e.g. the origin and unit variance.

I don't have a statistics background so please guide me on this one.",statistics,2022-09-02 11:25:52,2
"> How is the the distribution in plot three calculated? I assumed that the calculation was simply a replace if(smooth_rv < min_obs) then{min_obs} else{smooth_rv}.Is this incorrect? When I ran the simulation I just wanted another set of eyes on this.

I would say that it is hard to know exactly what they did given the information (what are they modelling, assumptions, etc), but your guess is a reasonable attempt that will give you good intuition anyway. And your R code plot seems okay at reproducing it too (just smoothed - this is not necessarily good because it hides a bit what is the cutoff point that happened).

If you want to read more about why this type of thing may happen, the more formal name of this type of phenomena is censoring. This specific case is left censoring, when you can't measure for values below a certain threshold.

Some reading topics on censoring: https://blog.stata.com/2016/12/13/understanding-truncation-and-censoring/

https://stats.stackexchange.com/questions/144037/right-censoring-and-left-censoring",1,x3yyh0,"I was watching a [video][1] on generating forecasts for lead time. In this video there was three charts. 

1. Lead time observations. 

2. The ""smooth"" distribution is a mixture of Poissons where the lambda is the observation in observed lead times. 

3. The smooth distribution conditioned on the minimum of the observed lead times.

My question is this:

How is the the distribution in plot three calculated? I assumed that the calculation was simply a replace `if(smooth_rv < min_obs) then{min_obs} else{smooth_rv}`.Is this incorrect? When I ran the simulation I just wanted another set of eyes on this.

[![enter image description here][2]][2]

# My Attempt

Here is some R code for what I tried. I hope this is a good enough reproducible example.

```r
library(purrr)
library(ggplot2)
obs <- c(11,10,13,19)
rpoiss_1000 <- function(lambda){
  rpois(n = 10000, lambda = lambda)
}

vec_smooth <- map(obs, rpoiss_1000) %>% unlist()
vec_conditioned <- ifelse(vec_smooth < min(obs), min(obs), vec_smooth)

data.frame(vec_smooth) %>% 
  ggplot(aes(x = vec_smooth)) +
  geom_density(fill = ""tomato"") +
  scale_x_continuous(limits = c(0,30))+
  theme_classic()

data.frame(vec_conditioned) %>% 
  ggplot(aes(x = vec_conditioned)) +
  geom_density(fill = ""tomato"") +
  scale_x_continuous(limits = c(0,30))+
  theme_classic()

```

[![enter image description here][3]][3]

[![enter image description here][4]][4]


  [1]: https://youtu.be/nqQ5z4j6mHg?t=4089
  [2]: https://i.stack.imgur.com/1V1MR.png
  [3]: https://i.stack.imgur.com/k7es0.png
  [4]: https://i.stack.imgur.com/2d0du.png",statistics,2022-09-02 04:21:16,7
They don't seem related. Can you give us some context?,3,x4i2sg,I have that doubt cause' both seem similar to me.,statistics,2022-09-02 18:26:41,2
"You can still consistently estimate the average effect of the treatment, even without baseline data. Because you have randomized, you guarantee that baseline measurements are independent of treatment assignment between the two groups. 

&#x200B;

Baseline data is helpful to improve the precision at which you can estimate the average treatment effect, but is not necessary in this case.",11,x44ili," 

Suppose I have a group of subjects, and I've randomly assigned them to two groups: control and treatment.

I  don't have baseline data, but decide to manipulate the treatment group.  What, if any, conclusions can be made about the manipulation's effect  in the absence of baseline data?",statistics,2022-09-02 08:33:16,4
"Are you super sure that a linear regression is going to be useful in the first place?

If you're in the business of predicting proportions, wouldn't a logistic model be better suited to the task?

Each dollar will end up in one of three places at the end of the month: spent in region 1, spent in region 2 or saved.

A multinomial logistic regression could give you the odds ratio for each predictor variable for each outcome.",4,x3sz47,"I posted not too long ago but I was purposefully vague as the details of my work are sensitive. I will try and use an example to mimic the circumstances:

Let's say we're all parents and want to give our kids some **pocket money** every so often. We give them a **total amount** for the month **T**. The issue we have is that we don't want them spending all of their pocket money at once because we want to teach them to be responsible. We don't mind if they spend it all, we just care about how much they spend all at once.

Let's construct a timeline from the beginning of the month to the end of the month. We split this timeline up into two regions. Let's say **region 1** is the first 75% of the month, and **region 2** is the remaining 25%. The amount of money your kids spend in region 1 is called **q**. The amount they spend in region 2 is called **p**.

**It's very often the case that they save some money and don't spend it all.** 

The total amount of money they spend in a month is **p+q**. We are interested in the proportion of money they spend at the end of the month (region 2) vs the whole month, since spending it all near the end we wouldn't consider responsible behavior. This proportion is defined as D = p/(p+q). 

We haven't told our kids anything, however, and they are free to spend as they wish. The model we want to construct is going to try and predict D based on how much money our kids have spent so far in **region 1** which we called **q**. So we are 75% through the month when applying this model.

If we assume that the total amount we gave them is **T = p + q**, this means we assume they **spent all their money**. We can deduce the following given this assumption:

* D = p/(p+q)
* D = 1 - q/(p+q)
* D = 1 - q/T

The issue we have when predicting D like this is that the kids save some money 99.99% of the time and so this assumption is almost never valid. We have heaps of data from other kids and their behavior across thousands of months so we look into the distribution of their savings for each month. We can call the savings **S = T - p - q**. Initially we assumed T - p - q = 0. 

We find that S has a heavily right-skewed distribution, implying that kids typically only save a small proportion of their pocket money T each month. We also find that log(S) is *approximately normal* with a non-zero positive mean.

The model we want to construct is D = 1 - q/T. If we are to apply a simple linear regression model to attempt to predict D, we are doing this under the assumption the kids spend all of their money. So the fact that they don't introduces error into our model and makes the equality untrue. 

**My question:** We are working with an assumption that introduces error from a known distribution. I don't know how this error should be handled in the model. We know log(S) is considered roughly normal. How do we construct a model to predict D? We would set up the SLR as: D \~ 1 - q/T and pass 1 - q/T as a single predictor variable since all the variables there are known to us.

Thanks guys :)",statistics,2022-09-01 22:11:19,7
"I would argue that factor scores aren’t necessarily hard to interpret. Most statistical software fixes the mean of latent variable to 0 and standard deviation to 1, so factor scores can be interpreted as z scores. For example, imagine your latent factor is anxiety. A factor score of 0 would mean the person has an average level of anxiety, compared to the rest of the sample. A factor score of 1 would mean the person in question is one standard deviation more anxious compared to average, etc.


As for the methods:

Method 1 has some merit if you are working with small samples. In such case, the factor loadings/weights may be estimated imprecisely and may vary wildly across samples. The it may be better to assign the weights ""manually"" (which doesn’t necessarily mean every item has to have the same weight).


Method 2 is what I usually do. As mentioned above, I don’t think it’s that difficult to interpret, you can even linearly rescale the scores if you want.


I haven’t encounter method 3 before, but it seems like just a basic rescaling. So it’s essentially method two, but you put the scores on different scale. I see neither advantage nor disadvantage to this.",2,x4253t,"I have spent the last hour going through recent posts on a reply by one of the r/statistics readers on the process of doing factor analysis and generating variables that can be used in a regression analysis. [Edit: found the post here today](https://old.reddit.com/r/statistics/comments/wo5015/q_factor_analysis_multiple_regression_looking_for/ik8zx8l/) My question follows the following three procedures to produce variables that result from a factor analysis, either as scores produced directly by the FA, or where the FA is used as a tool to discriminate what variables should be clustered. The goal is to use said variable in a regression analysis.

method 1:

When I was in school I was taught to treat scales in the following manner. Do an EFA first, accept factor loadings of >.4, put the variables that belong to the resulting factor in a reliability test such as Cronbach's alpha and then compute a variable that is the mean score of the variables that belong together according to the FA and acceptable C's alpha score. This method, however implies  that the variables that contribute to the final factor variable would not be weighted, which is a downside. The advantage of this method is that you can interpret the results in a meaningful way.

Method 2:

I today wonder why my lecturer did not simply say: When you are happy with the result, save resulting factors as variables (which is often possible in software such as SPSS). Hair (2019) Multivariate data analysis, even states this as a good practise as the contribution of each variable included in the FA to the final FACTOR variable is weighted. A downside is that the resulting scores are more difficult to interpret.

Method 3:

In the post I was unable to find, the reader suggested that you could use both the original variable scores as well as the factor loadings to create something that is a mix of both the ways described above. He/she did not say it in these exact words, but the result would be a weighted contribution of each variable to the final factor variable. I am unsure the resulting score of ((X1 * Factor Loading X1) + (X2 * FL2) + (Xn * FLn))/n, would be as interpretable, or could be made interpretable if that were desirable.


Question:

Let's say that I have done a PFA, I have used oblique rotation and all resulting factor loading are higher than .4. Which of the above three methods would be preferred if you want the resulting factor variables to be able to correlate with one another and it is not necessary that the final factor scores can be meaningfully interpreted? I would personally dismiss the first option because the resulting factor variable scores are not weighted, but I cannot discriminate well enough between the latter two methods.

I am also curious about studies that have used above mentioned method 3. As the reader said that there were peer reviewers that had to be convinced with the legitimacy of this method.",statistics,2022-09-02 06:55:13,1
"> So every time I try to find the Q1 and Q3 pieces of data I always get it wrong

Note that there's at least 9 different definitions used in stats packages, and a few more across the range of available textbooks.

That's not even counting the definition of hinges (which are used in Tukey's definition of box plots).

> Is there a specific formula I am meant to use


No doubt there is, but we can't guess which one it might be - 
it depends on who is deciding what the required answer should be. If you have notes or a textbook, you should probably be using its definition.

If you can find the IQR correctly, then the two numbers you subtracted to get it should be the first and third quartiles.",9,x3di83,"So every time I try to find the Q1 and Q3 pieces of data I always get it wrong and I don't know what I am doing (at least when it comes to large data sets).

Is there a specific formula I am meant to use or prerequisites to finding those pieces of information? I know how to find the range/IQR (The largest data point - the smallest) but I just don't get how that correlates to the quartiles themselves.

&#x200B;

Any help is appreciated because right now I'm stumped.",statistics,2022-09-01 10:31:41,29
"Hello! All great questions here, and I’ll try to help the best I can. Most of this is from my own personal experience from being in your exact position (currently in my fifth year of PhD, after 4 year undergrad and 2 year MS in Stat): 

1. DO IT! As for asking about research experience in point (3), a Master’s Thesis is the best option. It’s perfectly okay if the project is not fully complete by time of application. For reference, I applied after starting my project (so I knew what the topic was and could at least mention it briefly in my application); and I did not finish and defend it until the summer between my Master’s program and Doctoral program. 

2. Statistics is one STEM field where it is not necessary to have an advisor chosen, in order to be accepted to the program. Most doctoral Statistics programs will require 2 years of coursework at the start of the program, in order to prepare you for qualifying exams. But, if you are more intrigued for your own interests, and to narrow down school lists, then try looking at each school’s list of faculty. More or less every Tenured-track Professor (the ones who can be advisors) will have their own personal website with research interests and recent publications. This is perhaps the best place to start for finding research you are interested in. 

3. An internship is an excellent addition, if you are able to make it happen. For me, I was lucky enough to find an internship between my first and second years. The project I worked on during the internship also became my Master’s thesis (with continued, unpaid work after my internship). 

4. Taking this course would only hurt your chances if you performed extremely poor. You will most definitely see this course again in a doctoral program, so it will be helpful for you to have seen the material once before. Additionally, schools will find you more educationally ready in the field of Statistics, with this course on your application. 

5. Unless you greatly desire to be in a program that heavily influences Statistical Theory, do *not* take the Math Subject GRE. That is a brutal exam that even my doctoral Mathematician friends couldn’t even score high on. It is also not required by most doctoral programs. IIRC, University of Washington and Stanford are two of the only schools that require Math Subject. For the regular GRE, check out the ETS GRE website. It has several free materials for prep, along with practice tests. For me, I prepare for regularized tests the best, by completing as many practice questions/tests as possible. On the English side, download an app today that allows you to practice vocabulary. But also remember that your English score does not have to be nearly as great as Math and Writing scores. Lastly, I really enjoyed Kaplan prep services for GRE. Their information and tips for writing high-scoring essays was extremely helpful. I truly was only hoping for a score of 4 on the writing, but with Kaplan’s help was able to get a 5 (out of 6). And their breakdown of topics needed to know for the Math part was helpful too. 


Some important things missing: letters of recommendation can “break you” if you don’t prepare for them in advance. In other words, don’t just pick the 3 friendliest people you know. Try to pick three people that each know an individual strength of yours personally. For example, my letters came from my work advisor, school advisor, and another professor that I was close with. 

In a similar manner, your personal statement can be the same way. For tips, I focused on my reasoning for wanting a PhD, while also personalizing each written letter as to why I was interested in that specific program. 

Finally, as a timeline of events, I would do the following: search through the lists of schools and make a top 5 choices list, while also adding “safe” schools to the list. Determine the application needs of each program. Most of them will require the same things, but specific programs may ask you to provide extra materials. Once you have a list of schools decided, study for and take the GRE before the start of your second year. The GRE will allow you to send your score to 4(?) schools for free, which is why you want to have an idea about programs before taking the test. Start thinking about who you would like to write your rec letters, by getting to know the professors in your program well. Try to have all materials collected by the start of your second year. Stats programs often work on rolling admission, so the sooner you can apply, the better your chances (i.e. even though some programs will have a deadline of December 1st, you do NOT want to wait till the last week, or even the last month, to submit your application. As an example, I submitted my application in early October. 

Hope this helps!",7,x3fb42,"I’ve recently started a terminal MS in Statistics this fall after completing a BS in Statistics. My goal is to get into a PhD program. What items (and their timeline) should I do to better my chances into getting into a program?
1) How much does doing a Thesis for Masters help? Since it most likely won’t be complete when applying. 
2) How does one find an advisor for a thesis? How do I get to know professors who’s research I may be interested in but are not teaching classes I can take?
3) How do you get research experience as a master’s student? During orientation, the chair said it’s unlikely master students can be a RA. Or would getting an internship also look good for PhD admissions?
3) Would taking Statistical Inference (with textbook Casella and Berger) my second year hurt my chances? I’m not taking it my first year because my school requires real analysis through measure theory to take the course. 
4) are there any online prep courses that can help on the GRE/Math subject GRE? I heard there is at your own pace courses. Which one is the best?

If there’s any important points I’m missing, let me know!",statistics,2022-09-01 11:48:08,5
"To me it sounds like a typical binomial distribution question - ""type 1 or not type 1"".  So you can deduce the confidence interval of the count of type 1 problems in the population, given confidence level.  I don't have the text in front of me, but I use the betainv function for this in the context of product defectivity in a manufacturing lot.",11,x38zd7,"I have 300 ""problems"". 

All 300 problems are independent of the others. There are only 3 types of problems that exist in this population. I used a random number generator to obtain 30 unique problems to examine, and all 30 problems were problem #1. Is there a degrees of freedom test I can use to estimate the proportion of the prevalence of problem #1 in this population? There is no known evidence that problem #2 or #3 definitely exist in the population, they are just known problems. 

Any advice helps, thank you.",statistics,2022-09-01 07:23:59,8
"If I'm getting the question right, you should always use the sample version of the sd, since you don't know the true distribution function of this random variable: you are assuming that each price is a random draw from an underlying stochastic process that is not known, so you should consider historical prices as a sample.",1,x3qhaj,"When you analyze a data set in a specific category, do you still need to use a sample standard deviation? I'm looking at single company's stock returns for 2016-current and I never know if I can use the population or the sampled values. On one hand it includes what feels to me like a true representation for that stock over the timeframe, but then I'm also not including every data point because I'm slicing it monthly. Could someone help me understand this and know how to differentiate? I feel like I can convince myself both ways.",statistics,2022-09-01 20:02:11,1
">I have experience in statistical analyses such as t-test, ANOVA, regression plus testing and assessment frameworks (CTT and IRT). I think I am quite capable of using SPSS, JASP and MPlus, and to some degree R.

.

>I know playing with available datasets is an option but without an authentic purpose I do not find the motivation. Developing your own research questions and conducting research is another (which I do too) but doing it requires much time and effort for actual research design, collecting data and writing-up.

I got hired by an education research/consulting firm just after finishing a masters in stats. It's hard to say anything without knowing what kind of work you want to pursue, but of everything you've listed IRT/CTT are your best bet in getting freelance work within education. I was one of two people (out of 30 education researchers) who knew enough about IRT to actually use it.

That being said, I don't know how far you'll get until you have a PhD under your belt. My personal experience in the education space is that nobody will take you seriously unless you have one or someone with a PhD endorses your work, even if that person has no basis with which to evaluate it. 

The big exception is things that outside of traditional research methods: forecasting/prediction, machine learning, data warehousing, ETL, dashboards, etc. The education consulting firm I was working for would contract that work out (before hiring me) because nobody could do it.",2,x3fxxu,"Hey folks! I am a Learning Sciences PhD student. I have taken a bunch of statistics courses so far, with more or less focused on educational sciences and psychology. I have experience in statistical analyses such as t-test, ANOVA, regression plus testing and assessment frameworks (CTT and IRT). I think I am quite capable of using SPSS, JASP and MPlus, and to some degree R.

So my question is how can I improve my statistical analysis skills, challenge myself, while earning some amount of money at the same time? I know playing with available datasets is an option but without an authentic purpose I do not find the motivation. Developing your own research questions and conducting research is another (which I do too) but doing it requires much time and effort for actual research design, collecting data and writing-up.

Honestly, my experience with statistics is only academic therefore unpaid. I do not know where to start to look for such freelance/ low-payment jobs for one who has basically no experience other than publishing a few papers, and I even dont know if I am adequate for the ""generic"" requirements in such jobs/tasks, whatever they are.

I would appreciate any advice!",statistics,2022-09-01 12:14:40,1
"I'm not 100% sure what you mean by residual centering (I can think of several possibilities in different contexts).

Can you clarify with some context - what it's being used in (e.g. regression, weighted regression, generalized linear models, bootstrapping ...) and a definition/explanation of what you specifically intend by the term?

> I think the more I read

what are you reading?",1,x3mu4t,Can someone please explain residual centering to me like I'm 5? I think the more I read the more confused I get and need it simplified. TIA!,statistics,2022-09-01 17:09:35,4
">Rather than apply for the PhD I wanted to do a Master first to get an idea if the PhD is for me. 

Why? It's much more common to start a PhD program and then dropout with a MS if you don't like it. If you can get in to a good doctoral program this is a much more affordable route. Plus, the best way to know if a doctoral program is for you is to try it.

On the other hand, if you want an affordable MS with a thesis, you could checkout some European universities. I think ETH might have something like that.",21,x2ze78,I am looking for the best masters program that would be a good pre-cursor to a PhD. Rather than apply for the PhD I wanted to do a Master first to get an idea if the PhD is for me. Initially I thought Berkeley would be a good choice. But after more research I see it’s overpriced and focused more on industry placement. It doesn’t even allow for a thesis option barring special circumstances. Stanford appears the same. I’m looking for a program that will be more so of a pre-PhD path.,statistics,2022-08-31 22:35:43,13
"What are you trying to do? What do you want the distribution to do?

You certainly could make a distribution that has the pdf proportional to lambda^-x / gamma(x+1). I don’t know what properties it’s going to have, but plotting it in R it certainly *looks* like a continuous generalization of the Poisson. I don’t think the normalizing constant will still be e^(lambda) and WolframAlpha doesn’t want to figure out the normalizing constant for me, so it might be gnarly.",2,x3d0lt,"basically the title. a poison likeihood has a factorial in the denominator, which generalises to a gamma function for positive non-integers. the whole thing still integrates to unity, so I was wondering if it's valid to use a poison likeihood with a gamma function denominator instead of the factorial.",statistics,2022-09-01 10:10:38,4
I think matching is independent of the outcome. It is based on the characteristics of the observed sample or on metrics performed for matching purposes.,2,x3h8ba,"Hi all,

I am looking into using PSM to help determine the effects that an academic intervention has on student dropout rates, and I have a question about how to use it. 

Currently, I have information on whether a student was in the treatment, and some demographic information that I want to use as covariates. I do not yet have information on whether or not students dropped out. 

I am trying to do as much of my analyses upfront as I can. Would it be possible for me to use PSM using the data that I currently have to create a control group that I can use in the future once I have the dropout information? Or do I need to wait to get the dropout data before I can do PSM?",statistics,2022-09-01 13:08:53,5
There are some free statistics books on [this list](https://aimath.org/textbooks/approved-textbooks/) of books recommended by the American Institute of Mathematics. I am an author of one of them.,9,x2ufhv,"Hey all, I'm a biology grad student with little to no stats background. I am getting to the point where I need to stop asking my colleagues to give advice/help me every time I need to analyze my data. What resources would you recommend for learning basics of statistics to help me choose the appropriate tests when analyzing my data?

Free, online materials would be best. I'd even consider paying for an online course as long as I can complete it at my own pace!

If it matters, I mostly just use the built-in functions in Graphpad Prism 9. The types of tests we typically use are things like t tests, Mann-Whitney, and ANOVA. I have no idea if any of this is even relevant to what you'll suggest but TIA anyhow",statistics,2022-08-31 18:21:13,7
"Don’t be afraid to apply what you learn to simple python and R code today. Not sure if the younger generation is still as apprehensive to jump into coding as my generation was when we started college … 11 years ago. 

Other than that, just the general advice for college kids. Stay mostly sober, don’t let girls get in the way of your studies, stay social and visible with your faculty, do what you’re interested in, and have fun.",31,x2e1qw,"Any tips as in skills to learn, most important courses to focus on, best way to study, etc.

Anything really if it's useful",statistics,2022-08-31 06:45:54,47
"Introduction to Mathematical Statistics, Hogg",8,x2mhhp,"I am not asking for answers or help here with these specific problems. I do not know how to exactly rephrase what I need to learn here in a way to get someone to recommend the optimal book, so I am showing what I need to accomplish after having read the hopefully forth-coming recommended book.

There are probably 100 stats books out there so I am looking for the most relevant one, as I do not have time to go through 100 books. The idea is that I can read/study the book and then understand on my own how to answer these.

[The questions have to do with covariance and providing proofs of variance from the expected value of different distributions](https://i.ibb.co/fDDhc1w/ME591-ML-FA22-HW-01.png) (I am probably butchering that.)

What book would be optimal for me to read/study having only applied statistical and ""100-level"" stats background, and need to learn more quickly? Which book do you find most clear to non-STAT majors?",statistics,2022-08-31 12:35:26,9
"Social networks analysis encompasses a broad set of methods, so it's unclear what sort of solution you might be looking for. Nonetheless, one good place to start might be to look at the publication history from Jon Kleinberg, he's been working on basically this particular problem since the early 2000s, and still his most recent paper was on the problems with engagement optimization: [https://arxiv.org/abs/2202.11776](https://arxiv.org/abs/2202.11776).  


Just searching 'social network' on his [homepage](https://www.cs.cornell.edu/home/kleinber/) surfaces 13 results, and if these aren't exactly what you're looking for you can dig into the references.   


Also LPT if you're really looking for applied works, it's sometimes fruitful to dig through [paperswithcode.com](https://paperswithcode.com), e.g. [https://paperswithcode.com/search?q\_meta=&q\_type=&q=social+network+analysis](https://paperswithcode.com/search?q_meta=&q_type=&q=social+network+analysis)",3,x2ppwt,I’m interested in learning more about Social Network Analysis as a possible solution or feature to a network optimization project I’m working on.  Does anyone have any recommendations on good entry level/applied research papers or white papers on the subject?,statistics,2022-08-31 14:49:57,5
"> ranges between 9-18% each month. I want to provide a Margin of Error for this error rate.


Other things being equal, p(1-p) is larger the closer p is to 1/2, so if n is the same, the Margin of Error will always be higher when p is closer to 1/2. So if you don't know what p you'll have but you can put a range on it, just use the value from the end of the range of proportions that's nearest to 1/2. In this case, 18% would be safe, as long as it's quite unlikely to go higher.

9% of 150 is only 13.5; if you're getting expected counts down that far you should be a little wary of the implied coverage of your margin of error (you choose some 1-alpha value like 0.95 but you might not actually attain as  close to it as you might like at such a small value of n^(.)p). It might be worth investigating some of the smaller-sample alternatives here: https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval",1,x2susi,"I’m using this [Margin of Error calculator](https://select-statistics.co.uk/calculators/sample-size-calculator-population-proportion/) and am trying to better understand this concept of “likely sample proportion,” which is describes as:

“The sample proportion is what you expect the results to be. This can often be determined by using the results from a previous survey, or by running a small pilot study. If you are unsure, use 50%, which is conservative and gives the largest sample size.  Note that this sample size calculation uses the Normal approximation to the Binomial distribution.  If, the sample proportion is close to 0 or 1 then this approximation is not valid and you need to consider an alternative sample size calculation method.”

Here’s my use case: I take a sample of work products and run QA checks on them. Every month the total population is around 1500, and we test a sample of about 150 of them. The error rate (% of tested work products with some sort of error) ranges between 9-18% each month. I want to provide a Margin of Error for this error rate.

According to this calculator, I should provide an expected result %. Historically, we have just fed whatever that month’s measured error % into this calculator. That doesn’t feel right — it feels too circular. What are my alternatives? Should I get an average/aggregated historical error rate and plug that in instead? Something else? I hesitate to plug the default 50% into this, as it would lead to margins of error that are quite a bit higher than what we’ve previously reported.

Thanks for any help!",statistics,2022-08-31 17:07:25,2
"Statistical tests commonly relate to test whether one can reject a null hypothesis which is oftentimes meaningless. An example is correlation analyses, where the null is that there is absolute no correlation, and hence increasing sample size will in near all cases indicate correlation (no correlation is just one of infinite possible outcomes).

I think you did right in mentioning cross-validation. Bootstrapping the data is another method. What you generally want is to partition the data with or without replacement. For CV, for example, when you have ""outliers"" or particularly important observations explain a large part of the estimated effect in your sample, and a fold does not contain those samples, the procedure will reveal this instability.

Another useful tool is internal and external calibration. There is the Hosmer-Lemeshow test, which is commonly used, but again the null for this test is bogus. Visual presentation, evaluation at relevant thresholds is far more informative and useful. Hope this helps!",3,x2geg7,"Hi all,

I was recently asked in an interview how I would go about testing for overfitting and understanding whether new samples have changing conditions compared to the test set. 

I know some methods of evaluation to avoid over-fitting (cross-validation and margin-of-error for example), but I failed to know what statistical tests can be applied to evaluate the extent of overfitting.

Likewise I understand monitoring of performance metrics and distribution curves (i.e. tail risk) for understanding if a statistical model is under performing, but is there any tests one could do to see if the new sample sets are an outlier - Would I, for example, have to build a classifier to do this, run a regression analysis, etc. 

Thanks!",statistics,2022-08-31 08:24:21,4
"Just from briefly glancing at it, what they are claiming is that Asians are more likely to be the victims of hate crimes by other minorities than are black or Latino victims. 

In other words, all minorities in this study were overwhelmingly more likely to be victims of white violence, but Asians also get more minority on minority hate crime than other minorities do. A double burden, if you will. 

There's some interesting subtext to the geographical results too - hate crimes against Asians are higher in places that are more affluent and where they make up more of the population, but they're less likely to be attacked at home. That suggests that such crimes are motivated, in part, by perceived economic threat... Asians are making up a significant proportion of middle class earners in affluent areas, and are getting attacked for it from both 'above' and 'below'. That dovetails with other research showing that right wing political violence is actually more common in majority white areas, suggesting that it's partly a response to perceived political threats.",9,x2d0h9,"Warning: Sensitive Topic

I'd like to know how the authors made the inference indicated in their [study](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7790522/) by using HLM logistic regression models. It just doesn't make sense to me since the raw data clearly shows that 245 of 329 (74.5%) of anti-asian hate crimes were committed by White offenders making them the majority offenders (see Variables and Measures or Table 1). When they got to the HLM logistic regression model part, the values they got somehow indicated that non-white offenders are more likely to commit hate crimes against asians than white offenders (see Results Section or Table 3). When comparing the descriptive and inferential analysis of the study, to me it seems that they contradict each other. Please ELI5, how the authors led to the conclusion that it is more likely that anti-asian hate crimes are to be committed by non-white offenders than white offenders. Thank you.",statistics,2022-08-31 05:59:43,6
"Get R, use the RStudio IDE, learn some tidyverse packages like dplyr and ggplot. Write Rmarkdown or Quarto docs for reproducible analysis and get comfortable writing your own functions. All free, tons of free recources to help you learn.",96,x218sa,"Hello everyone!
I'm studying international economics and I'll be learning statistics starting this semester. I would like to gain experience in a few statistical tools but currently I can't afford to purchase them. I've never used one before and I could only find 30-day trial versions of each. Obviously, I'm looking for long-term usage that also helps me learn the basics of such programs. Are there any free (not demo version) softwares you'd recommend that could also potentially be a nice addition to my CV in the future?",statistics,2022-08-30 19:02:21,26
"The sample size also depends on the phase of the trial. Phase 1 is usually not powered while phase 2 may be powered up to 50% especially when the result of the study is planned to be usedd for the evaluation of the probability of success of a phase 3 i.e., de-risking a future larger study.
For a phase 3, in my experience, there's no limit to the sample size. I have worked in vaccine efficacy studies where the sample size can be over 50,000 subjects with only a few hundred cases expected. Even for non vaccine studies where the sample size are typically much smaller, no study can be smaller than a few hundred participants considering the fact that health authorities often expect a certain number of subjects exposed for safety evaluation unless several studies are planned to be used for this purpose. In essence, unless you're talking about a phase 1 study, the number 30 doesn't even come up in my experience unless perhaps in a pilot study.",3,x2gz92,"I watched a coursera course from Duke University mentioned that better for a sample size to start from 30 observations. I don't know if this is oversimplification or wrong or true. Since I work in clinical trials, I get asked sample size questions frequently, I didn't take clinical research class in my biostatistics masters and never calculated sample size. But I learned from readings. The main 4 points I look at when I calculate sample size is 1. Study designs and research hypothesis if the subjects are paired or two groups. 2. I look at the main outcome and what test I will be using based on their research questions. 3 the effect size based on the Cohen's d large effect size 0.8 or larger medium is 0.5... Etc. 4. I explain the power to the researchers and I recommend they shouldn't get lower than 80%


Most of the time researchers prefer smaller samples because of funding issues. I feel uncomfortable that I should say ok for samples smaller than 30 (including the drop outs) . My question is, is there any limit for sample size, when do you say NO I can't do analysis with this sample size?",statistics,2022-08-31 08:48:38,3
Turn it into a Bayesian hierarchical model,1,x29vog,"Hi there,

I have a pretty tricky multi-level modelling problem at work, and could do with some hive-mind, internet galaxy brain advice as to how best model the large volume of data I have. 

The research question is to assess whether MRI scored knee cartilage in 4 knee compartments predicts self-reported knee pain amongst individuals with or at risk of osteoarthritis of the knee. 

The issue I have is that I want to use a level 1 predictor (compartment-level cartilage) to predict a level 2 outcome (knee-level pain) (see below). I obviously also want to account for the fact that knees are nested in patients, and compartments nested in knees, but for both - I am not too sure whether 2 and 4 levels is too few to treat them as random effects.

The theory we are assessing (lower cartilage = more pain) is supported when completely ignoring dependency and modelling a compartment\*cartilage interaction, i.e.

lm(vas\_pain \~ 1 + cart\_z\*compartment, data = d\_long)

I think one way of doing it would simply be:

lmer(vas\_pain \~ 1 + cart\_z\*compartment + (1|knee) + (1|id), data = d\_long)

But I'm not sure that adequately/efficiently captures 1. the fact that the outcome is on a different level to the predictor, and 2. between-compartment variation. 

Some more details:

**\*\*Structure\*\***

Level 1: compartment (4 per knee, n = 36000)

Level 2: knee (2 per patient, n = 9000)

Level 3: patient (n = 4500)

**\*\*Primary predictor\*\*** 

Cartilage Z scores \\\[note the scores were generated within compartments\\\] (Level 1: compartment, n = 36000)

**\*\*Outcome\*\***

Vas pain (Level 2: knee, n = 9000)

The data is laid out, at present, as follows (fake data with 2 patients):

   

|id|knee|compartment|vas\_pain|cart\_z|
|:-|:-|:-|:-|:-|
|1|Left|LateralA|8|\-2.9|
|1|Left|LateralB|8|\-8.2|
|1|Left|MedialA|8|\-1.2|
|1|Left|MedialB|8|8.2|
|1|Right|LateralA|5|2.7|
|1|Right|LateralB|5|\-1.0|
|1|Right|MedialA|5|\-2.4|
|1|Right|MedialB|5|0.2|
|2|Left|LateralA|7|0.1|
|2|Left|LateralB|7|6.1|
|2|Left|MedialA|7|2.2|
|2|Left|MedialB|7|0.3|
|2|Right|LateralA|3|1.2|
|2|Right|LateralB|3|0.5|
|2|Right|MedialA|3|\-0.4|
|2|Right|MedialB|3|\-0.2|

Thanks again everyone.",statistics,2022-08-31 03:17:23,2
"Roc-auc is more than accuracy

Overfitting is why performance went down",5,x2boq8,"Hi all,

I’m working in R and I’ve been using a boosted tree model (xgboost) to predict an binary outcome based on around 10 predictors.

Initially, when fitting the base model to the training data, I got a ROC-AUC score of around 0.7. Am I right in saying that this means the model can be expected to predict the correct outcome 70% of the time?

I then tuned the model to find the optimal hyperparameters and used cross-validation. After doing that, and running predictions/fitting to the training data again, the ROC-AUC score actually went down, to around 0.69.

I’m a bit confused as to why the model is performing worse (or would successfully classify a lower % of the time), after tuning. I’ve been following an example (using a different dataset albeit), and their score went up after tuning, but mine has gone down.

I’m new to using boosted trees (I’m working as a data analyst and haven’t really used machine learning models much yet), so any insight would be helpful.",statistics,2022-08-31 04:55:53,3
"28.9%.
I'll tell you how i came to this answer if you tell me what you've tried so far to solve this question.",3,x2kx8c,"You are interested in moving into the ground-floor of Rondebosch Flats. The
building has 5 floors (including the ground floor), and 7 units on each floor. You
read an advert which says that there are 2 vacant apartments available. What is the
probability (assuming all apartments are equally likely to become available) that
there is a vacant apartment on the ground-floor?",statistics,2022-08-31 11:30:41,18
Can you clarify what you intend by the word 'variance' in this situation?,3,x242vq,"Let's say I have 4 groups (red, yellow, orange, green) and these groups were asked about what is their favorite fruit among these 3 choices (apple, orange, banana)

The first group (red) has 30 members and their choices distribution was (1 likes banana, 4 apple, 25 oranges)

The second group (yellow) answers were (10 banana, 10 apple, 10 orange)

Etc...

So what mathematical operations do you recommend to calculate the internal variance regarding choices for each group so I can compare the group choices variance?",statistics,2022-08-30 21:22:11,4
Thanks for the announcement. Looking forward to it,2,x1fibo,"We will cover various topics such as stochastic modeling, Brownian motion, and gaussian processes to describe rich autocorrelation structures, persistence, and roughness, with some of the top researchers in the field.

The first day will be focused on the mathematics behind complex and rough regimes, the second day will be more focused on what this modeling means for economics.

You will find the [abstracts here](https://cournotiennes.blogspot.com/)

**WEDNESDAY, 31 AUGUST 2022**  
8:00 AM–11:30 PM (LA) / 5:00–8:30 PM (PARIS)

**Part 1 - 5:00–6:30 PM (Paris)**  (Chair: David Mordecai, Courant Institute, NYU)  
Elisa Alòs (Universitat Pompeu Fabra, Barcelona): The fractional Brownian motion in volatility modeling  
Rama Cont (University of Oxford): title forthcoming  
Mikko Pakkanen (Imperial College London): A GMM approach to estimating the roughness of stochastic volatility

**Part 2 - 7:00–8:30 PM (Paris)** (Chair: Knut Sølna, University of California at Irvine)  
Eduardo Abi Jaber (Université Paris I): Quadratic Gaussian models: Analytic expressions for pricing in rough volatility models  
Alexandre Brouste (Université du Maine): Efficient inference for large and high-frequency data  
Archil Gulisashvili (Ohio University): Time-inhomogeneous Gaussian stochastic volatility models: Large deviations and super roughness

**THURSDAY, 1 SEPTEMBER 2022**  
9:00–11:00 AM (LA) / 6:00–8:00 PM (PARIS)

**Part 3 - 6:00–7:30 PM (Paris)** (Chair: Jean-Philippe Touffut, Fondation Cournot)  
Jean-Bernard Chatelain (Université Paris I): Rough volatility and regime shifts: Views from a macroeconomist  
Xavier Timbeau (OFCE): Rough volatility: Challenges for Macroeconomists

VIDEOCONFERENCE IN ENGLISH

\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*

To register, simply reply to this message or click on the following link: [**REGISTRATION**](https://docs.google.com/forms/d/e/1FAIpQLSetubKuPgVZsb_9DeU5HRv4l-XsO4ipHiIkPSBjJC7BwbXKEQ/viewform?vc=0&c=0&w=1&flr=0)  
If you don't have any affiliation, don't worry, put anything that comes to mind.

This should be recorded and uploaded [here](https://www.centre-cournot.org/conferences_fr.html#conference28) soon after the conference if you can't attend",statistics,2022-08-30 03:29:36,1
"> I have already loged/ sqaured variables wherever possible. Now the question is, how should I proceed?

Why? Why do you care if your variables are non-normal? Are you fitting a model that assumes that they should be? They why not build a different model? We can't give any advice without knowing what you're trying to do.",11,x1q6ws,"Hello, 

I did a shapiro-wilk test as a robustness check and discovered W = 0.97969, p-value = 9.295e-10 -> non-normal distribution. A qq-plot confirms this as both ends of the distribution stray from the fitted line. I have already loged/ sqaured variables wherever possible. Now the question is, how should I proceed? I have seen different suggestions on the internet, some say it depends and sometimes the distribution is just not normal and not sweat it. Others think it needs to be addressed as it violates assumptions. 

What ways are there to create a normal-distrubtionish model? Removing outliers?",statistics,2022-08-30 11:13:47,10
"As far as difficulty level, they require a similar amount of background knowledge; if anything Wasserman might assume a little bit more familiarity with set theory and real analysis than C&B does. They are both harder than the usual 300-level math stat textbook like Freund's *Mathematical Statistics*, and easier than a full-on graduate level book based on measure theory. 

The two big differences:

Wasserman states his theorems without proof. The proofs are either exercises or omitted entirely. Because Wasserman doesn't ""waste"" any space on explanations, he has room to cover twice as many topics. 

Maybe this is fine if you want to use Wasserman as sort of a reference after you've graduated, or as a review text before starting a PhD. It summarizes the material from 2 or 3 semesters worth of normal classes. 

But I can't imagine using Wasserman as a textbook; either the professor or the student is going to be spending a ton of time writing out explanations (or just buying a copy of another mathstat textbook that includes them.)

Wasserman also tends to be a bit too concise at times. To give one example, he simply says a t-distribution is the name of the distribution with such-and-such ugly pdf. In my view that's a serious deficiency. A t-distribution is a normal distribution divided by (the square root of) a chi-squared distribution. You don't need to see the pdf; it's impossible to evaluate by hand anyway. You *do* need to know why we use t-distributions when we estimate the standard deviation; it's because you standardize a variable by dividing by its standard deviation, and the ratio between a sample standard deviation and a population standard deviation is described by a chi-squared distribution.",8,x1nwx5,"Can anyone give some context as to what audience each book tends to? From my understanding, the first book is a less theory heavy version of the other, but provides a more modern overview for those in fields like CS. However, the second book seems like something that is theory heavy and for those wanting to study classical stats.

Can anyone gives some insight into what advantage one book has over the other?",statistics,2022-08-30 09:43:39,4
"First test for equality of variances.

If the sample variances are equal, then you can do 2-sample t-test.

If not, then use the Mann Whitney test.",1,x1qqgx,"Im not sure if I have come to the right place but here goes nothing:

How it possible to determine if the delta between two data sets are significant? Basically:

The growth of multiple apples in Spain was measured over 12 weeks.

The growth of multiple apples in Sweden was measured over 12 weeks.

Did the apples in Spain grow significantly more? Which test would you use in this case?

Thanks a lot!!!",statistics,2022-08-30 11:35:33,3
"If you’re in the US, USAJOBS.gov could help with public sector jobs. The Environmental Protection Agency was hiring Statisticians a few weeks ago. 

ClimateBase.org has only climate-related jobs and many of these climate jobs are something you might have a knack for with a computer-science background.",18,x17778,"Hi everyone, I’m a junior in statistics this year, and I have honestly been wondering how to get a job that both pays well and benefits the environment. I don’t want my line of work to be analyzing data so some rich douchebag gets richer, I’d like my work to have a tangible real world impact that helps the environment. I’m primarily looking into biostats/informatics, as I’m most interested in forestry and botany. I’m also getting a minor in computer science, if that’s relevant, as well. I was wondering if anyone had gone into a line of work similar to what I’m eyeing, and what that’s been like for them. Also, which types of organizations should I try interning at to break into these fields? I’m also open to the idea of starting my own company/organization in these fields after I’ve made a decent amount of money post graduation, which would obviously be a much steeper curve to follow.",statistics,2022-08-29 19:27:58,26
"PhD statistician here.

No. Assuming the numbers in the bucket are randomly sorted and you’re not allowed to know what numbers have already been drawn at any point in time, it does not matter.",3,x1jhhb,"My buddy and I participate in Fishing tournaments every week and have to draw a launch number from a bucket (lower is better). Without knowing what was already drawn and numbers are not replaced is there an optimal time to draw a number? ( for example try to draw first, in the middle, or last) there are 50 numbers and usually 25 participants.",statistics,2022-08-30 06:42:34,4
"Does it have to be a book?  Michael Sullivan's Fundamentals of Statistics is popular as a very general intro, but I can't say that I think any book on introductory stats is going to be better than the free stuff online.

If you're starting from the very beginning, there are any number of good online resources that won't cost you anything, like [Khan Academy](https://www.khanacademy.org/math/cc-seventh-grade-math/cc-7th-probability-statistics#cc-7th-basic-prob) and [Coursera](https://www.coursera.org/learn/introductiontoprobability).  The additional interactivity and opportunities for feedback, limited as they are, still make these a great way to learn stats as compared to a book, imo.  Online tools won't get you super deep into the subject, but they're great for developing the basics, and they're vetted by the public in a way that no textbook ever could be, so you get a lot less of questionable terminology or inconsistent reasons for choosing a particular approach.",3,x1ap7b,"I've always found statistics interesting. I graduated with a BA in English and regret not double majoring in Statistics. As much as I'd love to go back school to learn - cuz I do miss school - I lack the money and time. 

The highest math class I've taken was half a pre-calc class years ago. I have no problem literally buying a text book if it written well enough to help me learn.   


What books do you recommend to learn statistics? Be it theory or pure math equations, I'm not sure where to begin. Thank you in advance.",statistics,2022-08-29 22:28:25,7
"Jobs with the title ""analyst"" would be good to look at. Not all of them would be what you want, but a lot would.",2,x1iqjk,"Ok, so I started college and chose statistics as my major. Going in, I was just hoping to improve in math and programming skills, and that is my plan, but then I was researching careers and something called like ""operations research"" caught my eye, which I think is related to decision science. 

While it's nice to just analyze data, I would definitely enjoy acting upon it, being able to optimize and tinker with systems/processes, and then evaluating and reflecting on the outcomes. That, I feel, would be a good career for me.

I know I'm getting ahead of myself, but I like knowing and planning. So what I am basically asking is what sort of jobs utilize the decision aspect of statistics, how plentiful are those jobs, and what credentials do I need (and I am fine with getting masters). Also, what classes would you recommend taking that has emphasis on ""decision science."" 

Thanks.",statistics,2022-08-30 06:09:30,4
"If you have only taken bio classes, you can't expect stats and math to come really easy. They involve different kinds of thinking.",21,x16req,"For background, I'm 34, and I have an associates degree in health care. I graduated in fall 2019. 

I just started this fall at a 4-year school, entering as a junior and majoring in Biology. It's the 2nd week of school and my Biology classes are so extremely hard. I'm particularly struggling with biostatistics. I go to the resource center for help every day and it helps me be able to at least finish my homework (at a pace much slower than my classmates) but I know that if I were to take an exam tomorrow on what we've covered, I'd be getting a zero. 

The classes I took for my associates degree (Anatomy and Physiology, microbiology, etc.) were not nearly this difficult for me to grasp. I don't know why this is so hard for me! It's making me feel really defeated, like I should rethink my major altogether. 

Any other science majors really struggle with this class? Any words of advice to survive it? Or in general?",statistics,2022-08-29 19:07:50,16
"Sounds like it would be a job for a prior on the final coefficients, no? Truncated above or below zero as necessary?",1,x104fe,"My question is related to the Bayesian Averaging of Classical Estimates (BACE) method, introduced in [Xavier Sala-i-Martin, Doppelhofer, G., & Miller, R. I. (2004)](http://www.jstor.org/stable/3592794).

I am working in a field where some explanatory variables have strict sign limitations from the underlying domain theory (i.e., some variables always need to have a negative sign and vice versa). Can this (and if, how) be taken into account in the BACE method without losing any of the statistical properties of the model?

The gist of the method is that it uses Bayesian model averaging to return a final model, for which the coefficients are the weighted coefficients of individual models, where the weights (model probabilities) are related to the performance of each individual linear model. A property of this method is that it has some nice explanatory statistical properties, namely, it returns the probability of each submodel and consequently the probability of inclusion, and expected value, of individual coefficients, the probability of the sign, posterior model size, etc.

My idea was that if any of the coefficients in the submodel have a wrong sign, the posterior probability of the model (P(M\_j|y) in the article) is set to 0. Would this preserve the statistical properties of the BACE method?",statistics,2022-08-29 14:15:07,4
"im happy to give some advice, this post will probably be a bit jumbled so apologies. I think as a statistician your core math skillset is probably up to scratch. I think focus on programming with python at first, its generally wider used across the private sector. I started with python and I find it easier for engineering and data manipulation work ( i do prefer R for data viz and modelling/stats, rmarkdown is great to know too). I think most places will use python as the core programming language in data science though, happy to concede that this isnt always the case. For python i would encourage you to make code modular, using functions, writing scripts to both process data or as a module to house functions. I would say to start with jupyter (i prefer lab over notebook, see what you prefer) as its used a lot in data science but it’s important to know when a notebook is hitting its limitation or its not suitable and instead write a python script you execute on the command line.

 I’m sure you can already and im sure you do this already but explaining technical concepts to no technical people ie explain all regression coefficients and what impact they have. Communication is amongst the best skill a data scientist can have, understanding a problem deriving a solution and delivering what is possible and not overpromising. Understanding business needs and what someone is trying to ask for is key and you can explain all of yr work in lay person terms.

Im sure your knowledge on algorithms is fine but to be sure cover stuff like: linear regression, logistic regression, regularization for both, decision tree (random forest), k nearest neighbour, k means, generative additive models, dbscan, should give youa  good base. I have found being creative in data science is also really important, having a good understanding of how things can be combined to give solutions.

Take time to learn GIT, read up on good practices, using branches, pull requests - knowing the basics of GIT is going to be really important.

Basic understanding of BASH or any commands line functions is probably useful too.

Learn SQL, the basica at least are a must have for any data scientist - understanding the structure and not just doing select *, not good practice. Learn how to aggregate in SQL, learn CASE statements, data processing whcih you can do in SQL ideally should be done there. I have got a small beginner pdf i could share if you would like.

It is up to you but it may be worth starting up a github/kaggle where you show off yr work, if you want to stay in pharma try find pharma datasets. I an not sure what your company is like but maybe you can tell your manager you are interested i  this change and they can start giving you work that is relevant.

At the end of the day remember statisticians are the precursor to data scientists. Im sure most of your skillset is nearly there, you just need to refine or maybe pick up some wider programming knowledge. Additionally knowing that you will never know everything but you are keen to go away and learn it to the point you can explain / implement it is extremely important. For instance you get assigned a task where you analyse text and have never done it before so you have to go away and learn how to use some level of natural language processing to achieve yr analysis but being thorough and learning correctly is very key. A large portion of my job is going away and learning then implementing what i learn, being effective at googling and reading the correct resources can save you so much time.

I am happy to answer questions or talk more, i have taught data science and given a lot of career advice and mentor at work frequently.

apologies for formatting im on mobile

maybe a bit about me too, im a data scientist in public health - sorry to answer since i was not previously a statistician but have worked with many who became data scientists. I think area you work in is an important factor in how your job plays out and what your day to day is and then what you want to do. For instance im in the public sector where my role tends to be a lot more policy focused and slightly more research based, i have to come up with a lot of creative solutions such as using markov chain simulations or looking at effects at certain areas where knn is used to get the counterfactual then analysis can be undertaken to compare using different methods. A lot of what I do does boil down to what impact has this had or what impact will this have but i enjoy it because its very creative in how we approach things. I am sure you will find what you enjoy, data science is extremely wide and I don’t believe a lot of the jobs are very similar.",40,x0kig0,"As a statistician who has successfully switched to the role of a data scientist, what was your motivation and how easy was the transition? Are you happy with the decision?

I am an experienced biostatistician in big pharma (about 7 years altogether in the industry) currently considering a possible switch and would like to hear others' opinions. I also have a master degree in business management so I'm thinking of either switching to DS in pharma or perhaps making a move to the business sector.
My main programming languages are SAS and R but I'm sure I can improve on my python skills quickly.
What possible factors do you think should be addressed to make such a transition easier?",statistics,2022-08-29 02:40:14,29
"I have three comments.

One, make sure that your pool of players are all playing by the same rules. I play purely by the published rules - guess any acceptable word at each turn, and try to finish in as few turns as I can. Many of my friends play ""in hard mode,"" i.e., they require every guess to be consistent with all the previous guesses and have a chance of winning, even though the rules do not require this (and if not playing in hard mode, your 2nd guess is very often a word with no chance of winning.) 

People playing in hard mode, even if they play very well, will need many more guesses, than people making merely adequately according to the published rules.

Two, I think that the full 7-dimensional multinomial model is rather wasteful. Your chance of winning in one is bounded above by 1/(number of possible words) - so no matter how skillful you are this is going to happen very rarely (indeed if someone has done this twice already, I'd strongly suspect them of cheating.) The chance of winning in two similarly bounded above by the number of possible sequences and is quite small.

With near-optimal play, you win in 3 and and in 4 with similar frequency, and very rarely need 5 (but it's not possible to guarantee winning in 4.) You never need 6. There are many almost-equally-good strategies that give you a long-run average number of guesses between 3.4 and 3.5. 

Three: that, in turn, suggests some possible metrics to examine.

You can calculate each person's mean number of guesses, and see how much worse than optimal it is, with appropriate error bounds. Nice simple way to make a ranking list.

You can count how many times a person has gotten 6es or Xes, as a measure of frequency of serious mistakes (if you get an X after your first week playing the game, either you played very badly indeed, or the answer was a word you'd never heard of and you resorted to typing in random pronounceable strings hoping they were words after exhausting all the words you could think of.)

Counting how many times you get 5s is probably a similarly good measure of merely being lazy. Often you'll have 3 or 4 candidate words after two guesses, and if you guess them in random order you'll finish in 5 or 6. In the large majority of cases, there was a 3rd guess available that could have distinguished among the remaining words and let you win in 4.",2,x1c0br,"Hello! I have a group of friends that play wordle on a daily basis, and we have a record of our results from several months ago. That is, for reach day and each person, the outcome (1, 2, 3, 4, 5, 6, X). Some people are leading the global score by a certain margin. Now, a natural question that arised is whether this could be mere by chance or if they are systematically better than the rest.

I would like to know how to approach this problem from a frequentist and from a bayesian point of view. I will provide my initial ideas, and any criticism will be more than welcome.

From a frequentist point of view, I would try a sort of bootstrap hypotesis testing, that is, I would assume as a null hypothesis that we are all equally good, so that we share the same set of probabilities, and then:

* Estimate p\_1, p\_2, ..., p\_x as the global proportion of each punctuation. 
* Assume that each player has probability p\_i of scoring i. 
* Simulate, say, 1000 different draws of N succesive plays (N=100, which is the number of days we have been playing, more or less). 
* Compute in how many of those outcomes there is a difference between the best player and the rest that is as big as the observed difference. 
* This will produce a p-value to assess whether to reject the hypothesis. 

From a bayesian point of view, my idea is much less developed. I think I would need to compare between two models, one with 6 parameters (a dirichlet distribution with 7 possible outcomes has 6 parameters) and other with 6\*M parameters (where M is the number of players). And I will start with prior probabilities for each model favouring the simpler or ""null"" model. Then, I will do bayesian inference and see what is the posterior probability of each model. Is this procedure sound? I have some questions, like how to stablish the prior probability of each model, and how to take into account that, of course, the model with more parameters will fit the data better. Should I use something like BIC or AIC to chose?",statistics,2022-08-29 23:47:03,2
"As long as the assumption of *independence* holds, your reasoning and calculations would be correct.

If instead the 100 people are chosen from some finite pool of potential occupants, or the people in the room can be connected (e.g. friend groups, or siblings, say, or, married couples being together) then you won't have the necessary elements in place for a binomial.",5,x10jq9,"Hi, I had a quick question I was hoping I could confirm, hopefully this is allowed!

I want to find out the probability of the following situation:

If there are 100 people in the room, and there is a 50% chance a person is from country A and a 50% chance the person is from country B, what is the most likely distribution of the people in the room, and what is the probability of this distribution of people?

I think this can be translated into a binomial problem, as there is only 2 possibilities, and an equal chance of each possibility. Plugging that into the binomial distribution formula I think the answer is 50/50 (from country A/B), with a probability of approximately 8%.

Have I read this situation correctly, is it a binomial distribution or am I missing something!",statistics,2022-08-29 14:32:31,8
"> I am not influenced by the other genes' readouts when computing any gene's P-value.



They would share common influences that are not in your model, and by not conditioning on them (or even knowing to condition on them, in many cases) there could easily be dependence between outcomes. The combined impact of all such things might be pretty weak or it might be strong enough to have a substantive effect but there's no clear basis to assert its absence.

That is you're thinking about this sort of direct physical dependence:

       [ A ]  ---->  [ B ]

which is unlikely to be present in any substantive way, but ignoring this sort of thing:

         [ H ]
         /   \
        /     \        (where here arrows are coming from H to both A and B)
     [ A ]   [ B ]   

Think about a twin study where they're  both answering a questionnaire on attitudes to social situations or being tested for sensitivity to some allergen. It's not like one twin's response can impact the other twin's response. But their H is things like shared genetic factors and environmental factors; their responses will tend to be more alike than if we randomly paired answers across twins, due to such factors taken together.",2,x126io,"I'd like to know if the following generated P-values are independent or dependent.

Let's say one experiment generated a 5,000 x 10,000,000 data frame where the 5,000 values are genes and the 10,000,000 values are gene readouts. If I compute a P-value for every gene (i.e., for every row), would these 5,000 P-values be dependent or independent of one another?

My guess for why it's dependent: Gene readouts for every gene came from the same experiment. Also, genes are part of pathways and biological functions that may influence the gene readouts of other genes.

My guess for why it's independent: Mathematically, I am not influenced by the other genes' readouts when computing any gene's P-value.",statistics,2022-08-29 15:41:07,1
"The main table is that of the coefficients which should have a row row for each variable. The values will then be the coefficient itself (I.e. in the equation for the fitted line we multiply the variable itself by this)z the standard error of the coefficient which is a measure to compute variation/confidence around the coefficient, and usually both a t statistic and p value associated with the coefficient (for null: coeff = 0, alternative: coeff != 0)

Then you usually get various measures like multi-variable R-squared and an adjusted R-squared value. There are other statistics too, but I forget what they are.


Can you write out the metrics that you are/aren't familiar with?",1,x0y193,"Interpreting result of GLM/OLS/STATSMODELS

Whenever I run Linear Regression via R or Python 's STATSMODELS and look at the model summary, a huge table with a lot of different metrics are displayed.

Are you aware of any particular Book/Video which one by one explains all of these metrics? What do they mean, how to interpret them, what range is good-bad, etc",statistics,2022-08-29 12:51:07,5
So the union minus the intersection?,2,x0w3yo,"Hello everyone, currently I am trying to write a restriction based on a set that I have, and for some reason I have been struggling a lot.

So I have a set L= Ls U Lio U Lpp U Lpd 

And the restriction I want to write is that a location lets say Y can be either on Lio or Lpp . (not both at the same time)

How I can write that?

Thank you in advance!",statistics,2022-08-29 11:31:55,4
"I would say take Time Series Analysis. If you have a foundational knowledge of stats (linear regression/ logistic regression), multivariate analysis will be easy to learn on your own.",19,x03qx5,"If you had to choose 1 elective, which one would you choose? Both seem great to me but taking Time Series Analysis will fit my schedule better.

The description of the courses from my University:

Time Series Analysis: Time series relationships; univariate time series models: trend, seasonality, correlated errors; regression with correlated errors; autoregressive models; autoregressive moving average models; spectral analysis: cyclical behavior and periodicity, measures of periodicity, periodogram; linear filtering; prediction of time series; transfer function models.

Multivariate Data Analysis: Multivariate normal distribution; Mahalanobis distance; sampling distributions of the mean vector and covariance matrix; Hotellings T2; simultaneous inference; one-way MANOVA; discriminant analysis; principal components; canonical correlation; factor analysis. Intensive use of computer analyses and real data sets.

Thanks!",statistics,2022-08-28 12:26:55,7
The more data you have on a player the lower variance of your estimate. The simple solution is to use confidence intervals rather than a point estimate.,9,x00jiu,"Am an American football coach. Need to grade players, but each player plays a different amount of snaps, and thus players who play more snaps have more opportunities to make positive/negative plays. How do I adjust my score system to account for the difference in snap count and make it more relative?",statistics,2022-08-28 10:10:59,5
"Can you summarize the article in PICO format? This will help visualize the design more clear. 

Normally cohort is longitudinal study (can be either prospective or retrospective). The outcomes can be anything according to the clinical question (or hypothesis setting).

Case-Control is like you have case (which is patients) and you have control (which is normal population) and then do inferential analysis to see whether your interesting factors actually have an impact or not.",5,wzr47u,"Recently, I have been handed a research project mid-way. The data was already collected and they have already got an ethical approval (Study Containing Human Subjects).

However, I have noticed that they named the study (Cohort). The question rose to me as I didn’t feel that the study is a Cohort, rather it was a Case-Control.

The study is about:

Certain patients taking medication (X). Small percentage of those patients experienced side-effects. Through the study we want to study the percentage of the patients who took medication (X). Also, we want to study the predictors that might have helped in the increase of those side-effects such as height, weight, blood sugar, haemoglobin etc.

I read online, but it didn’t really help me and I couldn’t find an example that would give me the definite answer.",statistics,2022-08-28 02:15:41,9
"What is your exact model, and what *exact* test are you trying to calculate power for?",1,x0aljn,"Hi!

So I’ve read online how gpower isn’t the best for calculating power for mediation analyses, however for the purposes of my thesis my prof is recommending that I use it anyways. I have one mediator in my model, and I’m not sure which multiple regression test I’m supposed to use on the gpower app, because there are 3, and I’m really at the ends of my wits with trying to understand stats. 

There’s single regression coefficient, r^2 deviation from zero, and r^2 increase. I assume the later two are SMR and HMR. My best guess is it’s HMR, but a very brief tutorial on YouTube used the single regression coefficient and I’m not sure why because there are two independent variables in a mediation model(?). 

Then, if it is HMR, why is there “number of tested predictors” and “total number of predictors”? What goes where? 

Thank you so much :”",statistics,2022-08-28 17:27:48,1
"Apply and emphasize your real world experience. Hard work, willingness to put In the effort. Half your competition is going to be unproven BS grads with better GRE scores and no experience with the real world. Some programs might not care about the score if there are other things to offset it. 

159 looks like 81st percentile. That’s not horrible. Not sure what the MS wants, but if you’re charming and friendly, the department secretary can tell you what last year’s cohort earned…",3,x03s0q,"I am planning to apply to grad school for Terminal MS in Statistics. The thing is I've really fudged my GRE . My score was (Q159V157) . I know its not really a great showing for someone who wants to pursue stats. 

I also know I cant really get my score up even if I tried the second time. I just don't do well with numbers under pressure.

I've got around 6 years of work ex as an electrical engineer and a STEM undergrad degree. I am not really going for top schools here as you can imagine why. 

What should be my recourse be in this case ? How do I decide which colleges to apply ? Not a lot of colleges publish average or cut off scores.",statistics,2022-08-28 12:28:15,8
"Not really an answer to your question, but about the US News rankings in general:

They make these rankings by sending a survey to every university's Institutional Research department. It was, I kid you not, 700 questions long, and took about 80 man-hours to fill out, at our own expense. I've done IR at two institutions. At one of them, we sucked it up, wasted all that time, sent it back, and got our generic middle-of-the-pack ranking. At the second -- with a smaller IR staff -- we decided ""we aren't making the top 25, we don't have time for this."" There was no legal requirement to respond but we got a series of increasingly dire ""if you dont respond we are gonna say awful things about you in our report and scare all your students away"" emails and phone calls from US News. We told them to take a hike. They had many fewer details about us than about other schools, so they just published a list of programs, some basic publicly available enrollment statistics, and didn't highlight us in any category. Whatever.

I would recommend that any school without serious aspirations to the Top 25 tell US News to take a long walk off a short plank.

I think it's quite possible that the school you're looking at either didn't submit or only partially submitted what US News asked for. I wouldn't presume to speculate how US News would have ranked them had they replied.",7,x02mso,"BU for Boston University here…

It looks like before 2022 USNEWS didn’t give a ranking for its statistics program under Math&Stat department. This year they gave a ranking for statistics subject but do not indicate the name of department, which is what they do for every other programs on their graduate ranking, either. 
This is a little bit confusing. So could some one explain the reason, and, if fairly ranked, what position would BU stat be in?",statistics,2022-08-28 11:40:50,7
"It depends on what you're interested in. Each if those things seems like a good answer to a different question. If you try to optimize one you will likely do worse on the other.

It probably doesn't make sense to focus on a single measure, punctuality is multifaceted.

For example neither of your measures mention how late trains are. I care a lot more about a train being 25 minutes late than 6.5 minutes late.

20% of the trains I take being 6.5 minutes late would be barely a blip on my radar. 10% of trains being 25 minutes late would make taking the train bordeline useless to me.",5,wzo8bu,"I am looking at a rail network and wondering about how to calculate the punctuality of its trains. For each train and every day of the year, I have a list of stops and information on whether the train arrived on time or late at each of these stops.

The most common measure to calculate punctuality seems to be share of stops that are reached on time. If a train journey goes from Stop A to Stop F (via B, C, D, and E), it has five stops. If the train reaches the first two stops on time and the remaining three late, it has a punctuality of 40%.

What is confusing to me: wouldn’t it make more sense to look at the share of all possible passenger journeys that are on time?

In the example above, there are 15 possible passenger journeys. A to B, A to C, A to D, A to E, A to F, B to C, B to D, and so on. If two stops (B and C) are reached on time, that means only three of the 15 possible journeys are on time: A to B, A to C, and B to C. That, in turn, results in a punctuality of 20%.

Does one of these approaches make more sense than the other? Or are they equally valid and just exhibit different perspectives on the data?",statistics,2022-08-27 23:09:52,4
"I think in terms of identifying which statistical approach to use before even identifying the hypotheses is putting the apple cart before the horse. 

Generally likert data violates parametric assumptions but its sort of a big debate in social sciences as you can use parametric tests if the sample size is big enough. You can turn a likert scale into a linear number scale and use ""strongly disagree"" and ""strongly agree"" at the extremes and let people choose the numbers in between. There is some validity to treating these as continuous.

If the hypotheses allow it you can also ask yes/no questions and use a cultural consensus analysis to see if respondants agree on each question. 

I think you have to ask yourself ""what is the purpose of these questions"". Do you really need a statistical significance to see that employees generally liked or disliked a certain area of the company? Is your company going to base decisions on a p-value? You're not publishing these results to a peer-reviewed journal, why even do statistics in the first place? Present the data in graph form and make a strong case as to why you believe the employees agreed or disagreed on certain areas. That would be much more effective.",9,wzf5dh,"I'm a new Human Resources Analyst but my education is far removed from statistics, so I'm making up for lost time via self study. I've taken it upon myself to develop a new exit interview. It will use mostly LIkert Scales to measure employee satisfaction across a few constructs.

I will soon be meeting with leaders at our company to collect their interests and from there develop hypotheses to be measured.

After studying Likert Scales for a few days, I have questions:

1. How would you test an employee exit survey for statistical rigor? I've read about Cronbach's alpha and McDonalds omega, but I need data to calculate. There's not exactly a pilot group of terminated team members, and I don't know how appropriate it would be to ask current employees to take a practice exit survey.
2. When using multiple Likert Scales to measure a construct, does it make sense that you average or sum the individual scores to create a mean/total score of the construct, as opposed to taking the medians/modes from a Likert-Like Item?  Does this cause the scores to operate like a continuous scale? Would this then affect which models you use to analyse the results (parametric vs nonparametric)?
3. What are some helpful resources (any media, including textbooks) for learning this part of statistics? Or am I putting the proverbial cart before the horse of learning?",statistics,2022-08-27 15:20:34,17
"Welllll, I have good news and bad news for you.

The good news is that a lot of statistics programming is of the ""few lines for a single function"" type. R and SAS have single commands that will run an analysis for you. Python, Julia, etc, have libraries where other people have done the hard C++ coding for you, so that you can run the analysis quickly there too.

However, a single real-world project can involve loading data from several sources, merging and cleaning up the data sets, recoding and aggregating a few things, running an actual analysis, and then exporting it in a nice pretty format. Even a small project that's just an afternoon's work often runs to a few hundred lines.

I have never had a reason to count - but I'd guess I have written 10,000 lines a year every year of my professional life. (A software guy might prefer to say I've written the same 1,000 lines of code dozens of times per year. There ARE some familiar patterns, and lots of opportunities to recycle chunks of previous written code, if you organize it well.)",131,wywwex,I’m currently taking a probability class and we’re using R. I like that it is only a couple lines of code to find a certain probability and it reminds me of Matlab when I took numerical analysis. Should I expect to code how I did for my C++ course (many lines for a single function) or more like my Matlab course(few lines for a single function) if I pursue an undergraduate in Statistics?,statistics,2022-08-27 00:49:39,38
">Theoretically it should just be their share of the total ticket pool

Without knowing the algorithm, the simulations are just going to give you the theoretical distribution you have already identified.  

You may as well calculate ticket share / total ticket pool",2,wz86qv,"I have a raffle where multiple contestants gain tickets with equal chance of being drawn by doing some task. That is there are some contestants with 1 ticket, some with 100, etc. Given contestant ID and the number of tickets they have, I use some blackbox algorithm that selects a winner. 

I want to create a report that tells contestants their probability of winning if the raffle was conducted today based on their ticket count and relative to other participants. 

Theoretically it should just be their share of the total ticket pool, but because I don’t know the exact picking algorithm, I want to run ~100,000 simulated raffles and report their probability of winning (if conducted today) as the fraction of simulated raffles that they won. Is this accurate?",statistics,2022-08-27 10:16:20,7
"I’m sure there are a handful of logical fallacies or biases that manifest in some related way, but for the specific question of sports metrics, I’m not sure they really apply. Usually, when we talk about fallacies, the idea is that we’re trying to infer some piece of true information about the world from limited data, and the fallacy is some flaw in how we treat the process. Confirmation bias is where we fail to recognize the points that don’t support our existing hypothesis. Survivorship bias is where we think X is more common than it is because all the not-X examples didn’t last long enough for us to see them. There’s some factual statement that our logical failure is leading us to misunderstand. In the mathematical sense, ""statistics"" is the field of inferring knowledge of a population by looking at data. 

Sports metrics aren’t really the same kind of thing. In sports, what people mean by ""statistics"" **are** the data. The ""smell test"" is just a way of saying ""my opinion"". You can’t really have a logical fallacy about your opinion. There’s no ground truth that says Tony Gwynn was better than Wade Boggs that we’re trying to estimate. So while I might devise some metric, apply it, see that it has Boggs rated much higher than Gwynn, and say, ""that’s crazy, I don’t think this metric is good"", I haven’t uncovered a flaw anywhere. I can’t say, ""there’s a logical fallacy XYZ here and that’s why the metric is bad."" It’s all just opinion.",3,wzdtu4,"Amateur here who is wondering about how valid this line of thinking is, especially as it applies to the world of sports statistics. There are more and more all-in-one metrics that try to approximate player value, and one common criticism of an analytic is that ""player x(who is widely seen as a role player) is rated higher than player y(someone generally considered a star) which makes no sense."" If this is some sort of statistical fallacy is there a name for this? To me, it seems like a type of confirmation bias that has people only accept evidence that supports their priors. 

That being said, even some of the leading people in sports analytics have reworked their own formula's if the results seemed to not pass the smell test. BPM in basketball is one such example that saw it rework its coefficients due to some outlier seasons. Is there a fundamental difference between a formula's creator seeing outliers that may reveal issues with his value weights, and an outside observer who sees unconventional results and disregards them as nonsense? The outside observer also may not even have a chance to dive deeper into what flaw the formula may have as the exact weights are often proprietary.",statistics,2022-08-27 14:20:25,4
"Try something like 

Compute filter = nindex(email, 'gmx.com')>0.

With email being the variable containing the domain names.

What this does is: first, return the location in the email string that is 'gmx.com'. If it is not contained (because it is another domain) then it returns a 0. Then you can make a logic check if the location is 0 (different domain), or greater than 0. Cases for which this is true are the ones you want to filter out.

Technically its not looking for strings that _end_ in 'gmx.com', but all that contain it. I guess its close enough, but I cant remember the exact syntax from the top of my head.",1,wz95xz,"I have reason to suspect that a bunch of my data using a ""[gmx.com](https://gmx.com)"" email is bots- how can I make a filter variable to filter out any email that ends in [gmx.com](https://gmx.com)?  Thank you in advance!",statistics,2022-08-27 10:58:39,1
RA Fischer used n=8 in the [lady tasting tea test](https://en.wikipedia.org/wiki/Lady_tasting_tea).,68,wyht0o,,statistics,2022-08-26 12:21:23,20
"Assuming it takes 3 seconds to flip a coin, it would take about 10 trillion years to flip the coin a total of 10^20 times. So, no not practical.",131,wyd7o0,"Forewarning: I know nothing of the mathematics behind statistics. Is it impossible in a practical, real life situation for a coin to be flipped 10^20 times and always land on heads?

Settle a bet for some friends and I.  They are saying that mathematically it’s possible, but practically it’s impossible.

My argument is that since mathematics are practical, the opportunity exist for this to actually occur in real life.

We are not good at math haha.  Please help settle this",statistics,2022-08-26 09:12:48,71
Consider picking up a programming language like Python or R. Tons of online resources and you’ll be thanking yourself later,6,wyii42,"most likely going to major in business analytics/stats, what are some extracurriculars/project ideas i could do?",statistics,2022-08-26 12:50:58,3
[Survey Monkey](https://www.surveymonkey.com/) has a fairly good free plan.,5,wya2zd,"I'm just trying to make basic polls for an online game community I'm a part of. I've been using Google Forms as my means to make polls, but I got a lot of worried complaints about the anonymity of their responses; since I could collect all their email addresses. 

So I've been trying to look outward for a possibly better alternative but from what I've seen, everything is more business oriented and have massive restrictions unless I want to sign up and pay for their service; which I don't need their extra services besides the bare minimum and I'm sure as hell ain't paying.

From what I've seen, there clearly isn't. But I wanted to ask anyways if someone who knows more is aware of a viable alternative.

(Edit) I'm still checking out what's available. But in-between I found SurveyPlanet which fulfills my needs. Unlimited surveys, responses, no question restrictions for Free",statistics,2022-08-26 07:04:35,16
"What does eliminating studies have to do with summarising the numbers of patients in each node?

You've said that he wants to eliminate studies but not how or why he is trying to do that. Is this a misguided attempt to reduce the number of nodes or something else?

You should have a protocol. Did you get it reviewed? Is it registered anywhere? What does it say?

If this is what it sounds like (an attempt to simplify the analysis by reducing the number of nodes) he needs to review his understanding of what network meta-analysis is. You have a sparsely populated network, or one that is sparsely populated for at least some nodes. You can strengthen it by combining some nodes where appropriate (eg different doses of the same drug, different drugs which belong to the same class). Or you can split into two different networks if they don't really connect other than via placebo (eg systemic treatments vs topical treatments). You can't just arbitrarily throw evidence out and, as you say, if you do the nature of the remaining network changes.

There is an argument for excluding tiny studies because they're unreliable, more prone to publication bias, and they're disproportionately time-consuming for the amount of evidence they contribute. But this is something you should specify upfront and, if you choose to do a protocol amendment along those lines, it must be based on the study sample size, not the size of the node they contribute to (or their results, obv).",2,wygvfj,"I'm working mainly on data extraction from over 100 studies. There are more than 40 interventions. One of the study investigators asked me to verify which nodes have the maximum number of patients in them, which seemed easy enough. However, he is eliminating studies in a way that I am struggling to understand.

For instance, the placebo arm is naturally one of the largest study arms in the NMA. However, there is a study in which there is a placebo arm and ""Intervention B,"" each with only 10 patients each. Intervention B is not in the top 15. Do we eliminate that entire study, which then could affect the top 15 nodes once you subtract the patients from the placebo group; do you have to do this iteratively and then add studies back in? Is there a more efficient way to do this? Is there some resource I can use to help explain?

My study investigator is not adequately explaining this or at least--I'm not picking up what he's putting down.

Thanks for any help!!",statistics,2022-08-26 11:42:39,1
"> and contains a variable with the number of tests ordered and the number of tests that were reactive (positive). I compared the number of tests between groups using a Poisson model as I'm working with count data.

Umm, why? While the Poisson is *a* count model but I don't see that it would be relevant to this situation -- unless you're treating the number ordered as random and modelling *that*. You've got number positive out of number ordered; that would typically be a model for count-proportions, like a binomial or a beta-binomial or a Poisson-binomial  etc etc.  You don't supply the proportions themselves, it needs to know the denominators.

> I therefore created a variable that shows the proportions positive (#pos/#ordered).


I wouldn't usually do that either.

In large samples you could get a rough approximation to something sensible if you supply suitable observation weights but I'd probably just start with a suitable count glm.",5,wy2lyp,"Hi world! 

Can anyone advice me on a model to
compare proportions between groups?

I've been doing some analyses in STATA to compare the number of diagnostic tests ordered by doctors from 2 groups (attended education and didn't attend education). Every record in my dataset is a doctor in one quarter-vear period (ie one doctor will have multiple observations), and contains a variable
with the number of tests ordered and the number of tests that were reactive (positive). I compared the number of tests between groups using a Poisson
model as I'm working with count data. I've also compared the number of positive tests this way, but it would be more informative to compare proportions positive.

I therefore created a variable that shows the proportions positive (#pos/#ordered).

However, I'm unsure which model to use here. I think that a poisson model or a binomial model are inappropriate. However, it's also not quite a Gaussian model as most test outcomes were
negative (ie proportion positive is often zero).

Thanks in advance! I appreciate an hints toward the answer :)",statistics,2022-08-26 00:26:56,2
"You can estimate the sample size if you know the statistical power of the test you're aiming at, the confidence level and the likelihood of the event occuring.",3,wyg0dt,"Hi everyone. I’m a bit confused on what stats to do in order to find out how many observations/counts are needed per sample. I know we can do power analysis to determine sample size. Is there a way to determine the number of counts per n is needed? I am wondering, for example, if you are calculating an average muscle fiber size for a sample, how many fibers do you need to count, (as there are thousands of fibers and can’t possible count them all)? Thank you!",statistics,2022-08-26 11:06:49,5
*Any* NHST? So independent of my choice of test? What checks does it perform?,1,wydnpw,"I made an app that allows you to check the correctness of reported statistical tests.

[http://statcheck.steveharoz.com](http://statcheck.steveharoz.com/)

Just copy in some text from a article, and the app will extract any NHST statistical tests it finds to confirm if they are internally consistent.

I hope it's useful!",statistics,2022-08-26 09:30:56,7
Ever use LMMs or GLMs before?,1,wy68yk,"I making GLMMs for count data from natural observations and a simulation model that I made. I am new to mixed model, and want some advice on how to proceed. 
Would anyone be available to discuss? I can divulge more information then",statistics,2022-08-26 04:03:51,5
"If you look at the right things, and if you're doing anything inferentially that relies on them, pethaps.

Why are you using WLS?",1,wy96ki,"By assumptions I mean homoscedasticity, exogeneity, correlation etc. 

I know those assumptions are quite important in case of an OLS but is it the same for WLS?",statistics,2022-08-26 06:25:33,1
"There is an effect of race (white is different from black)

There is an effect of gender (male is different from female)

There is no interaction (the way that white is different from black does not change according to whether that person is male or female)

I recommend reading into interaction effects some more until it clicks (e.g. check out the Wikipedia page).",49,wxumtt,"Let's say we're looking at race, gender, and race*gender. This logically doesn't make sense to me. What am I missing?",statistics,2022-08-25 17:42:35,18
"What is the goal of this venture? If you're looking to reskill into maths for a job, I would suggest that self education and a phd in neuroscience is plenty. 

If you're looking to learn, you can evidently do that without going into immense debt. 

Ultimately I have found graduate stats education to be less educational than self guided learning. One thing I will suggest is trying to pick up some practical work on the topic. I'm sure you could leverage your phd domain to find some data work in biostatistics.",2,wxwopj,"Hi, I've been working on (re) building my math skills with the possible goal of going back to school to get a M.S. in statistics. Since there's a Springer sale, I was hoping for a recommendation or two (non-Springer recs are welcome too). I am well aware of certain sites that freely provide PDFs, but I prefer physical copies if I'd need to flip back and forth, and also to support good book writers.

Brief background, once upon a time (~20 years ago) I wanted to be a math major but when I got to Analysis w/ Baby Rudin, got my ass handed to me (combo of being lazy, dumb, not mathematically mature, etc.). I had done fine in Multivariable Calc and Diff. Eq. (both B+ I think), but not too well in Linear Algebra (C+; I think we used an ""older"" text by Bill Jacob). Anyway, in all of the above I hadn't put in much effort.

I have a PhD in neuroscience, but I never lost that ""math itch"" and have been drawn more and more to statistics over the years. I have an urge to get a M.S. to properly learn more advanced/theoretical statistics instead of reading a hodgepodge of books and articles. So I've been restudying the prerequisite math. Over the last few months I finished Morris Kline's Calculus text (Dover), and am working through 2 proof books (one by Solow and the other Velleman's).

The more recent books I've bought:

* Axler - Linear Algebra Done Right
* Abbott - Understanding Analysis
* Courant & John - Introduction to Calculus and Analysis I (I figured I'll buy Volume II on sale)

Others I have from awhile ago are:

* Both of my Multivariable Calc and Diff Eq books (both Edwards & Penney, which I think were OK)
* Shilov - Linear Algebra (never read all of it)
* Mike X Cohen - Linear Algebra (more recent but pretty basic)
* James Gentle - Matrix Algebra
* Strichartz - The Way of Analysis (since there's some highlighting, I apparently worked through some of it 20 yrs ago, but don't recall anything about it)

I think I might be good on Linear Algebra and Calculus/Analysis (at least once I finish all of them); my first struggles in math were in proof-based math, so if there are any stellar proof books I'd be willing to buy one or 2. I don't have a ""deadline"" along the lines of ""I want to apply to grad school by January 2023; I'm willing to take the time and do things properly.",statistics,2022-08-25 19:16:42,6
My department is SAS heavy as the last person did everything in SAS before she left. My bet is you can just find something since SAS is so powerful and you have a legit applied stats background…,3,wxmjiw,"Some background info about me: I graduated with an MPH degree in Biostatistics and Epidemiology in May 2019. Shortly after, in June 2019, I began my career in the pharmaceutical industry as a Statistical (SAS) Programmer at a Contract Research Organization (CRO). Due to unsustainable working conditions (i.e., frequent, uncompensated overtime hours), I resigned from this position in January 2020, which is the last position listed on my resume. Currently, I am and have been a stay-at-home-parent throughout the pandemic.

Now that my little one is starting part-time daycare soon, I am considering dipping my toe back in the field. However, my main concern is regarding the feasiblity of balancing my family life and work, in addition to the ways in which to proceed on this path after being out of the industry for almost 3 years (and my short career). I am also open to other career suggestions that may be a good fit for my background and experience. I would like to use my hard-earned degree, and I do enjoy statistical thinking and programming to a certain extent. However, my family is of utmost importance to me.

Summary of my questions: 1) Is the biostatistics/statistical programming field ""family friendly""? Are there opportunities in the field for 20-30 hour work weeks? 2) What would be some next steps for me to take now if I want to return to the industry?

Thank you for your constructive feedback!",statistics,2022-08-25 12:03:59,6
[Trustworthy Online Controlled Experiments](https://experimentguide.com),5,wxm6rd,"Hi I'm looking for any resources/books that I can read over the weekend to:

\- learn some advance A/B testing concepts or framework for my Google DS interview next week. I was told by the recruiter that Google is heavy on statistical analysis.

\- learn practical knowledge beyond the basics to apply to my day to day work (also DS)

Thanks in advance!",statistics,2022-08-25 11:49:53,3
There is no “best” ticket purchasing strategy. The odds remain the same regardless of how and what order you purchase the tickets.,16,wxtpjz,"\*\*Currently debating a friend on this and would like to see others thoughts\*\*

There is a raffle for $3M in your city.

There is a 90 day window to buy tickets, but there are only 550,000 tickets.

Each ticket will be sold by day 90.

Each ticket costs $15.

Your grandfather has given you $825 to buy tickets however you please.

You go and buy all the tickets at once and are given raffle tickets #100 through #154.

Was this the best ticket purchasing strategy or was there a better purchasing strategy to increase your odds of winning? If so, why?",statistics,2022-08-25 17:01:37,6
"There's really nothing better than R for publication figures, but I would strongly caution against any manual editing outside of the program. Any changes you need to make you can make with the code, it just takes some practice.

One of the, best things about R for publication is the image type options - there are functions for any of the odd types that journals might request. Off the top of my head I've used tiff, png, jpeg, pdf and eps functions for image submission.",18,wxh36c,"What data visualization do you use for publications? My team likes Excel it is straightforward and simple, I thought there might be better data viz tool. I used R in the past using ggplot and other tidyverse packages but editing them consumes lots of time when the PI doesn't like a small feature. I don't think Power BI and Tableu used for medical papers as well.

Also, I don't know if Python is better in data viz than R? I don't use Python but I would think about learning it if it is really better.",statistics,2022-08-25 08:23:33,28
R + RStudio.,96,wx4xpp,"Hi, I’ve using PSPP but it tends to crash my Mac every time and to be honest it doesn’t run as smoothly as I wish it could. 

Any suggestion would be lovely. Preferably free but paid are welcome too.",statistics,2022-08-24 21:50:02,17
"Coefficient of variation is not variance. So you cannot get to a conclusion like ""variance of a stock (g/m2) was higher than the flux (g/m2/year) contributing to it"". You can talk about the relative size of coefficient of variation (they're unitless, when comparing population values you can certainly say that 1.3 is bigger than 0.8), but the degree to which it's particularly meaningful to do so may be less clear (what does it mean to say that the CV is bigger when we're talking about different variables?).

If you are in such circumstances, you might consider whether it might make more sense for your case to work on the log scale (logs are also unitless; if you use the same base of logs, the size of the sd on the log scale should relate to the coefficient of variation on the original scale). It's still not necessarily very clear the extent to which such comparisons are very meaningful, though.",1,wxln5z,"Can CV be used to make a meaningful comparison between samples with different units?  
For example, variance of a stock (g/m2) was higher than the flux (g/m2/year) contributing to it.",statistics,2022-08-25 11:28:18,2
No conflict there -- if Ho is that the parameter = 0 then Ho is rejected at sig level alpha if 0 is not included in the 1-alpha confidence interval.,3,wxlkex,"Referring to the highlighted sentence: I thought that the null hypothesis is rejected if the test statistic is outside the 95% CI for the null hypothesis which is zero in this case. Is the highlighted formulation equivalent?

https://i.ibb.co/LvxFR9M/19-B11793-8-A62-4650-BD4-C-C73-FF7-F943-B9.jpg",statistics,2022-08-25 11:25:06,2
"I’m not sure what you mean. In ANOVA, the numeric variable is the response. What data are you working with?",2,wxdy7r,"Hello--thanks ahead of time!

I'm  trying to run a statistical analysis of multiple means using an ANOVA,  but this requires a factor, but I don't have a response variable--just  two independent variables. Any advice here?",statistics,2022-08-25 06:13:41,11
What is the point of such an analysis? One clearly has 100% of their appointments in Virginia.,1,wxhyir,"Hi everyone.  I'd like to compare two employees based on the number of appointments they have in Virginia.

My problem is that each employee can also have appointments in other states. 

Employee A: 5 Virginia appointments, 50 total appointments

Employee b: 15 Virginia appointments, 15 total appointments

Virginia total: 20

I'd like to find a way to compare the number of appointments each employee has in Virginia while also normalizing that ratio by accounting for their total appointments.

Thank you in advance for your help!",statistics,2022-08-25 08:59:35,2
"I commented in your other post about this, should be about 4.3% (see code [here](https://gist.github.com/jenxing/f5a0bc496bc84ff11bbee1d083c32bf2)).",2,wxggin,"

So recently we held a draft lottery for our fantasy football league. However, we wanted to know the odds that the person with the best odds to get the first pick in the draft fell all the way to the worst pick. He had his name in the hat 9 out of 34 times with a 26% chance of getting the first pick. So I’m just wondering what we’re the odds that his name ended up getting picked last?",statistics,2022-08-25 07:58:41,12
"They are both the same conceptually:

Hypothesis testing is fundamentally an extension of deductive logic to allow for randomness in the data collection process. Deductive logic is ""If A implies B, then not B implies not A"". Without randomness, the logic of a hypothesis test for a fair coin would be ""The coin is fair implies half the flips come up heads. I flipped a coin 100 times and it came up heads in 70, so the coin is not fair""

But we know the coin flipping is a random process, so instead of concluding that precisely, we design a test statistic, T, with a known distribution when the null hypothesis is true. Then we can say ""If A then T falls outside the rejection region of the test, T is in the rejection region then not A"". We are then only wrong when T falls in the rejection region when A is true, which only happens with the probability we choose (the alpha of the test). 

The above concept works the exact same way if your test statistic is a mean of sampled data where the distribution is assumed normal because of the central limit theorem, etc. The appeals to the CLT, etc are just to ensure that we know the distribution of the test statistic. In the binomial case, we can just already know the null distribution.",4,wxg7c9,"I am having trouble seeing the similarities between these two concepts. So for example, the classic way I learned hypothesis testing and p-values is an idea of I take one survy from the population, and calculate a statistic. I make some assumptions like normality of the data or rely on the central limit thereom, impose my null hypothesized value, and have a distribution of the test statistic under the null. Then i see how likely my test statistic or draw of my sample from the population is, given the null.

But then I see examples like [this](https://math.stackexchange.com/questions/1514231/p-value-of-a-test-of-binomial-success-probability) for a binomial experiment such as flipping a coin. here you plug in the null hypothesized value for the proportion, and then directly calculate a probability. 

&#x200B;

Are these in fact the same exact concept/thing? or is there something substantively different about these two different ways of hypothesis testing? I can see that (echoing language I have seen on this sub in previous posts), these are both creating a model of the world under a null hypothesis, with the former relying on many more assumptions, and the latter having a direct straightforward model of the probability. But they just seem to me as wildly different ways of doing things, and seem like they must then be separate concepts. So are these in essence the 'same thing', or is one example actually quite different in concept and principles then the other?

&#x200B;

Please let me know if I need to clarify my question, it is hard to explain what is confusing me.",statistics,2022-08-25 07:48:18,4
Are you familiar with the field of data science? Tons of jobs out there for exactly these problems statistics can help with,5,wxkcm7,"I am doing undergrad in statistics course and have been learning things like mgf, cgf, transformation of variables, random sums etc, but I can never see how they could be useful in real life. 

Can anyone explain? Thank you!",statistics,2022-08-25 10:35:53,3
"~~Assuming independence, the probability of passing all three tests is 0.3\*0.2\*0.1 = 0.006 (0.6%). The probability of not passing test C in 20 tries is (1 - 0.006)^(20) = 0.8866. So the probability of passing is 1 - 0.8866 = 0.1134 (11.34%).~~

See my answer below.",2,wxd632,"I trying to program a calculator for a game I am currently playing. 

Suppose a student starts with test A, and will proceed to test B, then test C if he passes.

The probability for the student to pass test A is 30%. The probability for the student to pass test B is 20%. The probability for the student to pass test C is 10%.

How do I calculate the probability of the student to pass test C, starting from test A, within 20 tries? My hunch is using geometric distribution but I am rather unsure since this problem involves multiple different ""stages"" with each stage having different probabilities.",statistics,2022-08-25 05:39:21,13
"If you want to perform any analyses for the individual time segments,  at 10 cases, you will not have enough power to perform meaningful analyses.  But if you are using the time increments as a starting point (where you ultimately collapse categories), then an n of 3000 is not bad.",2,wwy92i,"I work for a moving company as the sales manager. We have always charged for the time to get to the job before it starts, and then home after it is finished based on the cumulative drive time. Our software provider has launched an upgrade that takes away the systems ability to calculate this time automatically. I now have to manually set mileage ranges and equate that to the amount of time I want to charge for.

This presents a problem- We are based in a city and the first ~40 miles are all city driving. Beyond that depending on the direction you are heading you may be in the mountains, which slow down our trucks quite a bit, or may be flat. This means the further you go the less linear the data will look for the time needed to reach a specific destination.

I have exported our data and am manually inputting the drive length in miles to the (e.g. 33.58 miles) and the drive time that the system had approximated (49 minutes). I want to include enough data to be able to get an average drive time to mileage ratio that I can break into 15 minute increments into our system.

For an easy example 30 miles would round up to 45 minutes, 45 miles would roughly round to 60 minutes.

I need enough data to be able to get an average for each 15 minute increment with a maximum time of 5 hours.

what is the best way to get an idea of how many data points I would need? My guess is I would have a minimum number of data points/15 minute range from our historical data.

I have read you ideally want 10 times the amount of the the smallest independent variable (from one article I found on google).

I would take this to mean I would want at least 10 data points per 15 minute increment. That would be 150 points x 20 increments (5 hours x 4 15 minute increments per hour = 20 independent segments). This would be 3000 data points.

Is this even remotely accurate? I would love some insight and to learn from those who actually know what they are doing.

Thanks for the help!",statistics,2022-08-24 16:30:50,5
"> I'm curious under what model assumptions could a linear regression model have the same output as a quantile regression model. 

*Output* is a sample thing. Be careful not to conflate population distributions with samples.

> if the residuals from a linear regression model were perfectly normally distributed,

This is a perfect example. A normal distribution relates to an *uncountably* infinite population. No sample is ever ""perfectly normal"".

Nevertheless a sample median can equal a sample mean, even when the ecdf of the sample is not symmetric; similarly a 0.5 quantile regression line could coincide with a mean line without the residuals even being symmetric; a much weaker condition.
 

> Using the standard deviation of the residual distribution from linear regression, e.g., would the upper and lower bounds of a 95% prediction interval be the same as those of quantile regression predicting the 97.5% and 2.5% quantiles, respectively? 

Again, this might happen with just the right sample. Again, it wouldn't necessarily even require symmetry of residuals.",2,wx5gdx,"I'm curious under what model assumptions could a linear regression model have the same output as a quantile regression model. For instance, if the residuals from a linear regression model were perfectly normally distributed, then would a quantile regression model predicting the median be the same as a linear regression model? Using the standard deviation of the residual distribution from linear regression, e.g., would the upper and lower bounds of a 95% prediction interval be the same as those of quantile regression predicting the 97.5% and 2.5% quantiles, respectively? Thanks.

&#x200B;

Edit: Somewhat related question, but is there any reason why a larger input quantile for quantile regression would result in lower predictions i.e., a prediction from a .25 quantile model is lower than the prediction from a .20 quantile model with the same input values. I'm using statsmodels quantreg, it doesn't happen very often but I'm not sure what the issue is (perhaps there is some randomized component of the statsmodels implementation, sample size is small leading to varying estimates, or perhaps just a bug in my code).  ",statistics,2022-08-24 22:18:11,1
"Importance in the ML sense only tells you about what features are critical to the model’s performance. It says nothing about relevance of the feature in the population. 
Ultimately, theory (domain knowledge) and experiments are how we hypothesize and establish causality. You can use predictive modeling results to complement theory and justify hypotheses, but they aren’t meant to be used in a vacuum.",62,wwgsw8,,statistics,2022-08-24 04:16:02,12
"
Describing the variables doesn't determine the test. (You can have very different hypotheses - requiring entirely different tests - about the exact same variables.)

You haven't mentioned a specific hypothesis, without which it's impossible to discuss a test for it.",3,wx665e,"Hi, I have been analyzing survey data using Stata and have been struggling to pinpoint the right statistical test to use in this situation. The first variable is whether someone uses or doesn't use for example Chinese herbs (there is about 5-6 alternative medicines listed) so categorical yes or no. The second variable is how often they have used kind of alternative medicine (so like Chinese Herbs) on the same day as using a pharmaceutical, with the 5 levels from Never to Always. When tabulated, the variable for Chinese herbs and same day use, becomes a 2x5 table with one row being no not using Chinese herbs but yes they use alternative medicines on the same day.


I assumed the correct test would be to use Chi2 test but I was told by my supervisor that a more appropriate test would be a t-test to test the difference in means and that we only want to table the yes row. As i've done more research, i've been been confused as i've seen a mann-whitney U- test and a kruskal-wallis test recommended for ordinal data. My question is, what is the appropriate test to compare statistical significance of the distribution of a single variable like Chinese herbs, as well as comparing the statistical significance difference between groups such as Chinese Herbs and Aromatherapy oils. Apologies, I have limited statistical experience especially with this kind of dataset and i'm at a loss for how to proceed. Any advice or information would be helpful.",statistics,2022-08-24 22:58:17,5
"Nope, you want it to be as abstract as possible.",16,www915,"I enrolled in a second linear algebra class in order to be prepared for statistics phd programs. The second class is much more abstract, in the sense it focuses on abstract vector spaces first rather than focusing on matrix computations. I was going to take it to get a more rigorous and proof based understanding.


However, I’m wondering if this level of abstraction is needed for a phd? The topics are all relevant and seem interesting, but do I need more of a *numerical* linear algebra class for statistics?",statistics,2022-08-24 15:06:18,4
"The [worldfootballr](https://jaseziv.github.io/worldfootballR/) R package can help you download from some of the big ones.

[fplscrapr](https://wiscostret.github.io/fplscrapR/) can help you download fantasy football information. But the FPL API only supports the current season but, thanks the [this legend](https://github.com/vaastav/Fantasy-Premier-League), you can get data going back to 16/17. 

See, also, the packages recommended [here](https://cran.r-project.org/web/views/SportsAnalytics.html), which contain packages to help accessing some other sites as well as with plotting data (eg pitches etc).",3,wwizeb,"Have been looking for a while but no success. Am willing to pay but if I find something free, that would be most desirable",statistics,2022-08-24 06:01:44,3
"You can take the mean of the embeddings of the images. That gives you a 192 length vector that represents the set. Use a distance measure to compute the difference between the sets.

You might be able to use the set's full distances to come up with a distribution to try to get some interpretation of your difference between sets. I'm not sure how well it would hold up statistically, but it would be interesting to explore a bit.",1,wwiwlp,"Hi all,

Im currently building a pipeline to detect drifts in my data.

I'm   able to update my dataset (n=100k - 10m), in which every row has a few   categorical features and an embedding. The embedding vector (len=192,   normelized) is the output of some CNN and Im trying to use it to detect   drift in the data.

I was wondering: which statistical test you would use to detect drift in such a scenario?

I've  used evidently for Wasserstein but it doesn't seem to be appropriate  for this problem.  I've read somewhere tit's more appropriate for a  univariant case. I've  also seen a nannyML's  PCA solution which I  liked.",statistics,2022-08-24 05:58:41,1
"If there's no theory/domain knowledge, why do you need to have any specific function (rather than some smooth function such as a natural cubic spline, say -- it looks like about 3 knots should be plenty sufficient to describe that)?",4,wwaxn5,"I have some data I'm doing a regression on. Wondering if you guys have any ideas for what function to parameterize this as. (No relevant domain specific knowledge for the data, by the way). Looks like a power curve, but it's got an oblique linear asymptote. [https://imgur.com/a/mP6Kjnh](https://imgur.com/a/mP6Kjnh)",statistics,2022-08-23 22:28:20,13
"I get what you're trying to show, but economically that distinction between principal and interest is meaningless. Accrued interest is principal.",10,ww4s50,"With the student loan forgiveness debate sure to be re-ignited again tonight, I figured one of the key statistic that can be used to determine the level of necessity of borrowers as a whole is the percentage of student debt that is comprised of interest. I have been unable to find this type of breakdown anywhere and it's unclear how common the anecdotal stories of ""I borrowed $30k, paid $30k and still owe $30k"" are. Are these minority outliers or are these common cases?",statistics,2022-08-23 17:26:27,7
"You should not try to use linear regression. Logistic regression can also give you an estimate of a probability (in this case the win rate), so there is no advantage to using linear regression. Only downsides.",32,ww2l1h,"Hello! I currently work as a product manager aligned to my company's data science team and have a noob question around our approach for a problem we're trying to solve.

Let's say that you're participating in an auction and want to estimate your rate of winning based on the bid price:

|Price|Auctions Participated|Auctions Won|Win Rate|
|:-|:-|:-|:-|
|$1|100|10|10%|
|$2|120|25|20%|
|$3|115|30|26%|

Based on the reading I've done, the raw count data above follows a Bernoulli distribution with the outcome variable (Auctions Won) having a binary, dichotomous, mutually exclusive result (won vs not won). Therefore, using a logistic/sigmoid function would be most appropriate here. 

**Dilemma**

Instead of using a logistic function, and modeling the raw counts themselves, our team is proceeding forward with fitting linear model to the probability itself (Win Rate). My questions:

1. Is there anything wrong with this approach, aside from the fact that the output of the model will be greater than 1, which doesn't make sense for our data? 
2. Let's say that we fit a logistic function to the raw count data (Auctions Won) and a linear function to the proportion/probability data (Win Rate) - how would you evaluate the goodness of fit between the two models?
   1. My understanding is that you'd use log likelihood (product of the probabilities) for the logistic function and MSE  (squaring of the residuals) for the linear function - since you're calculating fundamentally different things in each model, is it fair to compare them using these method? if not, what would be a good comparison method that extends across distribution types? (I'm seeing KL-Divergence pop up on google for ex)

Apologies in advance if these questions are appropriate for this forum! Happy to post elsewhere, really just hoping to understand what the heck is going on at work haha.",statistics,2022-08-23 15:49:05,31
You might find the [GAISE guidelines](https://www.amstat.org/education/guidelines-for-assessment-and-instruction-in-statistics-education-(gaise)-reports) helpful.,2,wwceco,"I am teaching an introductory Statistics course (summarizing data,  probability, distributions of random variables, statistical inference,  linear regression). This is at a second tier university and involves students from non-technical majors (i.e., not engineering/math/etc.).

Ever since I was a graduate student I have viewed these courses as a major waste of time for everyone involved. The students are forced to take it and basically view it as the last ""math"" class they ever have to take.

I am interested in initiatives like ""Data 8"" at Berkeley which can be described as ""teaching statistics without statistics"" ([http://data8.org/](http://data8.org/)).  For example, they do not formally talk about probability distributions or different types of statistical tests. Basically, they skip details that students are unable to appreciate or understand, and instead make students work with data and see get a feeling for how things work.

However, Data 8 is a class run by an army of TAs and lab assistant, and the students at Berkeley are excellent. I only have a grader for my class and I cannot devote all my time to develop a new course.

I would love to hear any advice regarding how to make this class a more enjoyable experience:

\-  Are there textbooks or lecture material that you can recommend? Last year I used a textbook called OpenIntro Statistics because it had been used in previous years and is free of charge ([https://leanpub.com/os](https://leanpub.com/os)).

\- Are there online courses that can be used as a template for structure and ideas?

\- I am particularly interested in simple computer exercises that can help students get a better feeling for the subject.",statistics,2022-08-23 23:54:25,3
"> given this script runs once a minute.

If you're still operating with the 'script runs once per minute'  approach (rather than running it more frequently) then you just work with the probability it doesn't happen and then subtract from 1.

Define 'success' to be *the event happens* in one trial. Let p = P(success in one trial).

P(even does not happen in k independent trials) = (1-p)^(k)

P(event happens at least once in k such trials) = 1 - (1-p)^(k)

e.g. with k=5 and prob= 1/2 we have 

(1-p)^(5) = 0.5

1-p = 0.5^(1/5) ~= 0.87   (if you need more figures: 0.87055)

hence p ~= 1 - 0.87 ~= 0.13    (0.12945)

If you wanted it to happen at least once with 50% chance within k minutes instead of 5 minutes it would be

p = 1 - 0.5^(1/k)",13,ww1gmv,"Hi, I'm a hobbyist game developer trying to maintain old code in an ARMA 3 server. The comment for the code for structures to despawn in the game says:

(random 1 > 0.965) Aiming for a 50% chance to have vanished within 20 minutes after the base has despawned, given this script runs once a minute.

The random method generates a random floating point value.

I have been requested to make it so that structures despawn on average in 2-5 minutes. What do I change the threshold to in order to satisfy this requirement?

EDIT: Thank you for all of the feedback. I just submitted the fix for review, hopefully the players will be happy with the change.",statistics,2022-08-23 15:02:15,4
It depends! Have a read about the concept of power.,8,wwhlbe,,statistics,2022-08-24 04:56:00,3
Depends on the school. You'd probably need calculus I-III and linear algebra at the bare minimum. A more standard prep background often includes a semester of advanced calc / real analysis and a semester of probability.,7,ww7k2t,"If so, what classes do you think are necessary to have as a background for an undergrad student for a stats phd?",statistics,2022-08-23 19:35:00,16
"There is no answer to such a broad question. It depends on the domain, noise in the data, and on the method.",9,wwt1w6,,statistics,2022-08-24 12:55:40,10
Why do you have all this information except the sample size lol,4,ww85nu,"Im currently doing a data analysis internship which also involves using statistics. Entails two treatment groups with which i did the mean and standard deviation for both groups. Im also provided with the power and significance level. The task is to find the sample size but im kinda stuck!! 

The answer is supposed to be a value over 100 per group but when i use the formula consisting of z value, standard deviation and delta, i get a value like 6🙈🙈. 

2 * (Z I-alpha/2 + Z beta)2 * SD divided by delta 2

Here are my values for two independent groups. 

Power = 80%
P value = 0.05
Mean of group 1 = 18.71
Mean of group 2 = 9.25
Std of group 1 = 26.63
Std of group 2 = 25.25

Thats all the info i have. Kindly help me out! Thanks",statistics,2022-08-23 20:04:11,8
"One way to increase kurtosis would be to take the values outside (say) the quartiles (Q1,Q3) and spread them much further out, while shrinking the values inside them somewhat toward the median.

There's a few ways to approach that",2,wwaudy,"Need help:
I have some data that is pretty normally distributed. I want to create a synthetic data set that has the same median but higher/lower kurtosis.

For example for skewness, I’m just multiplying all values on one side of the median to increase the skew.

I was trying to be fancier with a sinh-arcsinh transformation, but that was a waste of time. Any ideas?",statistics,2022-08-23 22:23:04,9
"> I took undergraduate linear and abstract algebra, multivariable calc, a little ODE, and Analysis 1. 

okay, 

> Is it unwise that I take the class?

I don't see why it would be unwise based on the information you give. Preparation-wise you might want to work a bit with a more basic math-stats text if you haven't had a class in mathematical statistics but you might well have covered a lot of it already. Your university library should have dozens of options.


Have you had any probability in there?",1,ww4za5,"Hello, 

I am currently a 3rd-year undergraduate student in economics and math. I recently got special permission to take an MSc intro to mathematical statistics class but I am nervous about the math. I took undergraduate linear and abstract algebra, multivariable calc, a little ODE, and Analysis 1. 

Textbooks: 

* Casella and Berger's “Statistical Inference” (2003).
* Linton, Oliver (2017). “Probability, Statistics, and Econometrics”.

Is it unwise that I take the class? Should I prepare somehow before the semester?",statistics,2022-08-23 17:35:33,6
I'd compare average sales only for stores that changed business model pre- and post- change. You don't need the stores that didn't change operating methods. Any change in performance of those will be due to some other reasons that we don't care about just yet.,2,wvmwlx," 

Hey everyone,

I need to analyze the performance of stores that changed to a new business model (still selling the same products).. at first I thought about comparing the monthly average sales pre and post model change of the store in particular, and the same period of time of a nearby store (control group) that didn't suffer any changes as comparison. Is it ok to do something like this?

My problem comes when trying to understand the effects of this new model as a whole, considering all the stores that are using it currently. Is it ok to sum all the averages of the pre and post periods of all the new stores and the respective control groups to create a comparison?

The new business model started in 2020/21 (different months by store), and my data starts from 2017, so I think I got a pretty good sample size.

Can you guys give me and with this one?

(thanks in advance! and sorry for the bad english)",statistics,2022-08-23 04:58:52,5
"I think the general idea Harrell is getting at is that the more spread out your predictions are (higher variance) that implies your model has more information. Imagine having no independent variables, your model would just predict the mean which is the same across all observations. Once you add in variables the predictions deviate from the mean and therefore have higher variance.",6,wvo73q,"This is one of those things I understood *just a few weeks ago*, but that I now have a hard time to understand. Makes you look really silly when asked by colleagues what your results actually mean...

Say we are fitting two nested logistic regression models, where we want to evaluate the added value of a new prognostic factor. For simplicity, I will use an example where we are interested in predicting development of CVD, for which systolic blood pressure is an established prognostic factor. We have now identified a new biomarker which does not interact with blood pressure, and we wish to know whether this biomarker improves prediction of CVD.

Apparently, if the full model (systolic blood pressure + biomarker) adds clinically meaningful information, the variance in the predicted probabilities should be higher than that of the basic model (systolic blood pressure) ([https://www.fharrell.com/post/addvalue/](https://www.fharrell.com/post/addvalue/)).

How can this be explained (like I'm 5...)? In my head, more spread out predicted probabilities will not necessarily correspond to better concordance with the observed outcomes - or?

Edit: Does this by chance relate to the Brier score?",statistics,2022-08-23 05:59:55,9
"There's two likely causes and only one of them has anything to do with the variables being related. So it might say nothing at all about the relationships between b and c.

1. It may be - even with uncorrelated variables - that even though c has p>alpha, nevertheless the mean square error in the reduced model increases enough to make b have p>alpha in turn.

2. The second way is essentially a particular example of omitted variable bias.  https://en.wikipedia.org/wiki/Omitted-variable_bias

 See the last paragraph under ""Intuition"" in that article to work out the direction of the effect, which depends on whether the relationship between y and c is the same direction as that between b and c or whether they're in opposite directions.

It may also be that both explanations are involved.",4,ww16s9,"Let's say I have three independent continous variables a, b and c. Since c is insignificant, I remove it from the model. This however makes b insignificant as well. What does that say about relationship between b and c in regards to dependent variable? Does it mean that b simply does not explain dependent variable well enough without c? What should I investgate besides correlation to help me understand it better?

Thanks in advance!",statistics,2022-08-23 14:51:06,5
"In general the answer is no. Even if they are on the same continuum, eg. Strongly disagree to strongly agree, participants tend to treat them differently (especially with/without neutral). Rescaling does not ""fix"" this, so don't treat them as numbers and try to take means. The best you can do is maybe compare grouped category % (eg. General agreement %) although even this is less than ideal.",35,wvfedd,I want to compare the result of my survey which utilized a five-point likert scale to a secondary source (6-point likert scale).,statistics,2022-08-22 21:41:00,17
"St501 is not very hard if you have some basic familiarity with calculus. It is also very important to understand the fundamentals of probability and statistics before trying to apply harder concepts. It's not crazy maths, it's very reasonable, and if you have Dr. Post teach it, it's a lovely class that'll only make you feel more confident going forward. That being said I know absolutely nothing about RIT other than it was a school I considered for undergrad over a decade ago. I have done the NCSU masters and thought it was great.",5,wvtbw3,"Hi,

I am trying to decide between two online master's programs in (Applied) Statistics - North Carolina State University vs. Rochester Institute of Technology. Both programs' classes started this week, but I have a week or so to decide which one to stick with.

I was wondering if anyone would be willing to help me look over the curriculums and figure out how the two programs differ.  (I will also be trying out a class from each program before making my decision.)

Here is NCSU's curriculum

[https://statistics.sciences.ncsu.edu/graduate/online-programs/online-masters/coursework/](https://statistics.sciences.ncsu.edu/graduate/online-programs/online-masters/coursework/)[https://statistics.sciences.ncsu.edu/graduate/courses/](https://statistics.sciences.ncsu.edu/graduate/courses/)

Here is RIT's curriculum

[https://www.rit.edu/online/study/applied-statistics-ms#curriculum](https://www.rit.edu/online/study/applied-statistics-ms#curriculum)

One obvious difference is that NCSU's program has 7 required courses and only 3 electives, whereas RIT's program has 3 required courses and 7 electives.

One appeal of the NCSU program is that their statistics department is very highly ranked, according to US News ([https://www.usnews.com/best-graduate-schools/top-science-schools/statistics-rankings](https://www.usnews.com/best-graduate-schools/top-science-schools/statistics-rankings)). However, the first required course (ST 501) appears very hard, and I'm wondering if a more highly ranked program will just be harder.  The flip side of it is, will a more highly ranked program have *better* teaching and thus be better for learning?

On the other hand, one drawback about NCSU's program is that all exams are proctored and closed notes. Not that I have any intention of cheating, but having to go somewhere to take an exam, and not being able to reference notes, adds to my anxiety about how hard it is going to be.

Also, I was wondering if anyone here can speak to RIT's statistics department's quality or ranking, because I could not find it in the ranking I linked to above.

Lastly, I was wondering if it looks as if NCSU's curriculum has a lot more theory than RIT's?  If so, does it seem unnecessarily theory-heavy for if I'm looking to go an applied route professionally (i.e. I'm not interested in getting a PhD)?

Thank you for any insight you may have.

I appreciate it!",statistics,2022-08-23 09:32:01,10
"It's a straight up two sample proportions test; doable either as a z-test or as a 2x2 chi-squared test (the categories there being ""in score 3"" and ""not in score 3"" on both A and B).

p-values are interpreted the same as any other p-values.",4,wv6nkb,"Hi there,

I believe this is a fairly basic question, but my stats knowledge is lacking. Basically, I have 2 categorical variables, and I would like to understand if there is a significant difference in Group A and Group B within a single category.

As an example, here is a chart that I found online (source) which includes similar variables and categories to my data. Referring to this example, I am wondering how I might determine if there is a significant difference between the percentage of males who report an effectivity score of 3 and the percentage of females who also report a score of 3.

I imagine that I will be using a Chi-Square test of some sort, but I am not sure which test or how to make use of the resulting p-value.

What, if anything, would be an appropriate way to do this?

Thanks!",statistics,2022-08-22 15:04:19,6
">My confusion stems from the fact that when I read papers that report significant slopes but have very low R2s, I don't know what to think. On one hand, well, there's a relationship, on the other hand, I'm like ""well, it doesn't explain much of the variance"".

You'll see this kind of thing a lot in medical literature, or any situation where the outcome has many, many causes.

You will never see a useful\* statistical model on death with a high R\^2. People die from lots and lots of causes. You could take all the medical labs and examination results of a patient and decide they're perfectly happy, only for them to trip and fall down the stairs on the way out.

So in medical literature, it is not at all uncommon to conclude that treatment X had a highly statistically and clinically significant impact on patient survival, even when the R\^2 (or area under ROC) was low. All it means is that patients \*in general\* lived longer after receiving treatment X, but there are always people who die of other factors, including simple bad luck.

To sum up: Don't disregard medical research just because the factor(s) being examined did not explain all the variation in death rates. Eat healthy food and get exercise",18,wuul63,"I wonder in which problems the slope matters and in which R2, or is it the case that both of them always matter? My confusion stems from the fact that when I read papers that report significant slopes but have very low R2s, I don't know what to think. On one hand, well, there's a relationship, on the other hand, I'm like ""well, it doesn't explain much of the variance"". I guess it's a confusion about analysis of variance vs analysis of the mean.",statistics,2022-08-22 07:06:38,20
Is r/statistics the right forum to answer these questions?,1,wusne2,"The [ARPA-E PERFORM datasets produced by NREL](https://github.com/PERFORM-Forecasts/documentation#arpa-e-perform-datasets-produced-by-nrel) contain both historical and forecast data on load, wind power, and solar power. NREL offers the following piece of information about the day-ahead [forecast data](https://github.com/PERFORM-Forecasts/documentation#forecasts):

>Day-ahead forecasts are generated with a 11-hour-ahead lead time, a 48-hour horizon, an hourly resolution, and a daily update rate.

I downloaded the data for [wind power in ERCOT (2018)](https://data.openei.org/s3_viewer?bucket=arpa-e-perform&prefix=ERCOT%2F2018%2FWind%2F) and the two first columns with issue time and forecast time, respectively, are shown below with the first few rows of the dataset (Forecasts quantiles are not included).

&#x200B;

| Issue time |Forecast time|
|:-|:-|
|2017-12-30 18:00:00|2018-01-01 00:00:00 |
|2017-12-30 18:00:00|2018-01-01 01:00:00|
|2017-12-30 18:00:00 |2018-01-01 02:00:00|
|2017-12-30 18:00:00| 2018-01-01 03:00:00|
|...|...|
| 2017-12-31 18:00:00 |2018-01-01 06:00:00|

Given the American Meteorological Society's definition of [lead time](https://glossary.ametsoc.org/wiki/Forecast_lead_time) as ""*the length of time between the issuance of a forecast and the occurrence of the phenomena that were predicted*"", I assumed that the difference between the forecast time and the issue time had to be 11 hours. However, from row 1 in the table above, we could see that such difference is 30 hours. For the second row, it's 31 hours. In row seven, we have a difference of 12 hours.

When I saw the issue time was the same for various forecasts, I knew my understanding of forecasting and its nomenclature would be tested. I realized that if I have to choose a set of all forecasts for a fixed $n$ hours ahead, given any $n = 1, 2, ..., 48$, I wouldn't be sure of what forecasts (rows) to select.

**There is something I am missing and that's what I'd like understand. What is the lead time here? And what does the difference between the forecast time and the issue mean in this case?**

All input/feedback will be appreciated. Ty.",statistics,2022-08-22 05:43:35,2
"The other way around: If the p value is small (smaller than 0.05), then the variables can said to be linked",6,wv0gxx,"Hi all!

I'm working on a ML project at the moment for fun and am currently at feature selection. I have use the Chi2 test to compare my dataset to the target variable ie Gender to Purchase (with prediction of purchase being the outcome).

Am I right in thinking that if the p-value for the test is larger than the alpha value (usually p > 0.05 in academia), then one can conclude that the variables are linked? So far, all my models have had very poor accuracy so I've been trying to refine them by choosing only variables that actually contribute.

Equally, if anyone can suggest any other valid statistical tests, that would be great! The dataset is entirely categorical variables (Gender, Browser, etc).

Thanks so much!",statistics,2022-08-22 10:57:49,4
"Isn’t that what the trial is for?

Imo 199 (student price) is not very expensive for software, if it’s easy to use and gives you all the features you need.

There are also various R packages for variograms and spatial statistics.",2,wuqn9b,"I'm currently a graduate research assistant, and I'm using the GS+ Software for my research (wherein I'll be using spatial techniques to estimate some variables). Problem is it's only 10 day trial. Now, my professor is asking me whether to buy the software or not.

Has anyone used this before? If yes, what are the pros and cons of the software? I've never seen reviews for this software, and I don't think it's widely used at all. But I think it's doing its purpose for my research. I'm just not sure about its accuracy in producing results.",statistics,2022-08-22 04:05:55,3
"Yes, you would need to forecast the predictors.

Since the data are time series, the residuals will likely show evidence of autocorrelation, which violates the standard regression assumption of independent errors. So you’ll then have to correct for this by modeling the errors as an ARIMA process. These are called dynamic regression models.",3,wux0dk,"Hi,

I’ve been studying the use of regression models to predict values for the dependent variable. I understand that regression models can predict a value given the independent variables however what happens if we do not know what the independent variables are? Do we not then also have to predict the independent variables as well? 

For example if we consider a multiple linear regression and want to predict sales for a shop and let’s say the predictors are CPI and price of product. Then if we want to predict sales for the upcoming 5 years, would we not also have to estimate CPI and the price of the product for the next 5 years? And then this will eventually turn into an endless loop whereby to predict CPI we may build another regression model with CPI as the response variable with certain predictors which will then have to predicted again and so on. 

I’d love some clarification on this and whether regression models can really be useful in predicting the future values of a certain response variable.

Thanks!",statistics,2022-08-22 08:43:03,5
This is a pretty lame question,0,wuu3o5," 

Ive been self teaching some statistics and was wondering, if I was to mention this on a univeristy application, whiich of these topics link well in some way shape or form to econometrics and how/why/when? Thanks in advance!

This is the list:

1. Discrete random variables
2. Poisson distribution
3. Geometric and negative bionomial distributions
4. Hypothesis testing using poisson distribution
5. Chi - Squared tests
6. Probability generating functions",statistics,2022-08-22 06:46:31,1
"A Z score is just a linear transformation of your data that tells you how many standard deviations above/below the mean each original data point falls. 

A Z test is an inferential test that tells you whether the difference between your sample mean and the known population mean is statistically significant or not.",39,wu6511,"I am not quite sure if I understood the difference of the Z-Score and the Z-Test correctly.

So as I understood a Z-Score can be calculated for a fully measured population

And the Z-Test is used when we have a sample and want to say something of the population.

Is this correct? If not please help me understand it better. 

Thanks",statistics,2022-08-21 11:11:00,9
"You need to do ANOVA

The extra data will help, and there's no reason sample size needs to be the same, as you are comparing averages.

I would actually have 6 treatments.
The control group pre, during, and post promotion; The same for the test group. You need to show that the test group has the same pre promotion average spending. Then determine if the test group increased spending.

You can also compare the groups to themselves... But not as great methodically...",9,wu95ui,"I’m going to conduct  A/B analysis on a promotion campaign my company ran. They for sure defined a treatment group, but I’m not seeing anyone labeled as the control group. 

Would I just consider everyone who wasn’t offered the promotion the control group? I know it’s a dumb question. For a school assignment, we were required to randomly identify the control group prior to the test. 

At this company, we have a total of 100,000 members, 70,000 who are active. 

40,000 were included in the treatment group (these members were offered the promotion) , so that would leave 60,000 for control who weren’t offering anything.

I want to see if the ones who were offered the promotion increased their spending because of the promotion.

Should I include all 60,000 in the control group, of should I randomly select 40,000 members so that’ll it’ll be 50/50 split?

Lastly,  I have the data for the month prior of the promotion and the data for the month following the promotion. But the promotion ended 6 months ago.

So I was going to run two tests. 1 comparing before promotion and immediately after the promotion, and the other comparing before the promotion and the current activity (so 6 months after the promotion). Would this be the best way to do this or would you suggest another way?",statistics,2022-08-21 13:18:23,7
"Use the SE the same way in each case. In both, you're using multiple measured values and their variability to estimate an unknowable quantity (the actual population mean is generally unknowable just like the infinite precision length of something is unknowable).",3,wtxtl1," 

So I understand standard error in how it relates to the variation of sample means around a population mean. But I have been thinking about uncertainty in measurements, say you try to measure length or some other physical quantity with an instrument a few times, and want to calculate the standard error of the mean value you get, from the standard deviation of the sample.

In the first case you have a finite population and an actual population mean, but in the second case you can never really know the actual measurement value, with the best estimate being the mean of an infinite number of measurements?

Do you use standard error the same way in each case?",statistics,2022-08-21 04:55:00,27
There's no good basis to judge this on the available information. We can't conjure models and data out of thin air.,18,wucwxv,"So I have a Band and we play in pedestrian zones. There has never been a gig where we did not receive any bank notes (euros meaning >=5).

Now last gig we had two sets. First set was about 20 min and we received 65 € in mostly 5.

Second set was 20 minutes again and we received no bank notes whatsoever and that's kind of suspicious to me. I have no idea about statistics but is there any chance that somebody stole our money?",statistics,2022-08-21 15:58:01,10
"If this is academic research consult your organizations statistics department or any senior researcher with experience in causal inference.

You will be able to calculate something with a dummy variable but the caveates here are not on how to do the estimation but on what to include in the model.

Omitted variable bias, collider bias etc. can only be ruled out by statistical AND subject matter knowledge and a misspecified model may not have a useful interpretation. 

Also the interpretation itself may depend on a nuanced understanding of the models and methods.

If you done want to estimate the causal relation (impact of covid specifically; causation) but only how something changed during covid (but maybe due to other causes, trends, ...; correlation) you have somewhat more liberties in choosing your methods. For some applications this is enough.

In this case pick a model, keep an evaluation set to check for overfit and do some sensitivity analyses.",11,wtv0wq,"Hi friends, 

I am working on a research but I am not that skilled in data analysis. 
My aim is trying to use a regression (I will have to work on a panel data but I still have to check whether OLS, fixed effect, or random effect would be better for my model) to study the determinants of corporate executive remuneration (For instance, Salary = B1xSize + B2xTenure + B3xGeographical Diversification…).

Do you have any suggestion in how I could study the impact of covid in my model? I was thinking about creating a dummy variable 0 (Years before covid, in my sample 2013-2019) and 1 (Years after covid, in my sample 2020-2021).  
Do you have any other ideas?

Thank you in advance!",statistics,2022-08-21 02:02:02,5
Where is the screen shot?  Can you convert your output file to pdf and post that?,1,wuc4x0," He everyone,

Sorry if this is a dumb question as I'm pretty new to stats. I am doing a dissertation and was doing a regression analysis and my p value for coefficients and F statistic is >0.01 and there is a small b next to it. What does this mean? I know p of less than 0.05 is statistically significant but the > sign is throwing me off. Posted screenshot if that helps

Any help would be thoroughly appreciated

Thanks !!!",statistics,2022-08-21 15:23:27,11
">  so it it the case that the sample mean with say n = 2, has its own population distribution, and the n=3 is a new population distribution, and so on? so for each possible sample size, there is a separate population distribution? 

Yes.

Sample means of random samples are themselves random variables with their own population distribution

> and it is these population distributions that converge to the normal?

Almost. In the limit as n goes to infinity, the distribution of sample means will converge to a single spike of probability at the original population mean. 

Instead to get convergence to a single distribution we must 'undo' the effect of increasingly large samples on the variance of the sample mean (larger samples make the values cluster more tightly). Accordingly, if we standardize the sample means, the distribution of the sequence of those converges to a standard normal.

However, dealing with finite sample sizes (rather than thinking about convergence issues), if we just focus on the shape and ignore the shrinking scale, then you have the same ""shape"" with the sample mean as with the standardized sample mean, and so that shape, in a slightly looser sense, approaches the normal.",7,wthwd3,"when looking at the Central Limit Theorem, if i understand correctly, its a theorem about the population sampling distributions? so it it the case that the sample mean with say n = 2, has its own population distribution, and the n=3 is a new population distribution, and so on? so for each possible sample size, there is a separate population distribution? and it is these population distributions that converge  to the normal?",statistics,2022-08-20 14:24:06,11
"Looks like you want to look into discrete probability distributions (examples include binomial, negative binomial, poisson, etc.)",49,wszouk,"Hi I'm in my first year of undergraduate. I like discrete math, but so far I've only seen continuous math used in statistics. Stats is the easier path careerwise compared to maths. But I also don't want to give up studying discrete math(combinatorics, set theory, logic etc) that easily. So I'm hoping that discrete maths is indeed used in statistics in some significant way. Doesn't matter if it's theoretical. As long as I have an excuse to study DM and get a bsc in stats I'm happy.",statistics,2022-08-19 22:57:39,15
"$300 isn’t much to spend; I recommend using it to purchase some books.

What topics are you interested in learning about that relate to your job?",10,wso11r,,statistics,2022-08-19 13:27:36,22
"Reversion to the mean is not causal. 

If you play an infinite number of holes and each hole has that probability of success, you would see your success rate approach that value.",2,wtatyj,"
So a couple of months ago I got my first hole in one in golf. For the purpose of this example, let’s assume that every hole i have a 1/10000 chance of making a hole in one. Over a long period of time, wouldn’t regression to the mean imply that I would not make another for a long time, but because each hole is an independent event, my odds are the same for each hole? This is a simple question but i’m not sure how to phrase it in google lol.",statistics,2022-08-20 09:12:08,8
Strata perhaps?,17,wsg9jc,"Is there a latin-looking word that means variable used in market research / data analysis? I remember hearing this word and forgot about it. Similar synonyms include: subgroups, sub-category etc…",statistics,2022-08-19 08:04:02,14
"I don't think this is true actually. Imagine you have two centered distribution with sd 1 and 100 respectively, and you have samples 0 and 1. Then both 0 and 1 are more likely to come from the distribution with sd 1 than that with sd 100.",1,wso94e,"Obviously a mouthful, that is why I am trying to find a name for it.

&#x200B;

**Edit (Clarification by example):**

I meant to rank samples by value. So if we have distribution *A* with st. dev. 100, and distribution *B* with st. dev. 1; and sample *w* and *x* from *A* and sample *y* and *z* from *B*. Then there is a greater chance that the maximum of (*w*, *x*, *y*, *z*) will be either the *w* or the *x* that comes from *A*.",statistics,2022-08-19 13:36:57,8
"Why do you believe the sample size for the chi-squared should be multiplied, yet you are correctly stating that the sample size for the z-test is just right as it is?",4,wsephn,"Hello, everyone. I have been setting up a few AB tests at work and a specific scenario started a philosofical debate in my mind.

The question we want to answer is whether the ""proportion of conversions"" (pA) in group A is greater than such proportion in group B (pB).

I used [G\*Power](https://www.psychologie.hhu.de/fileadmin/redaktion/Fakultaeten/Mathematisch-Naturwissenschaftliche_Fakultaet/Psychologie/AAP/gpower/GPowerManual.pdf) to get my sample requirements. I simulated an instance where pA = 0.49 and pB = 0.51, using the alpha =0.05 and power = 0.95 on both types of tests below. Ffor the effect size calculation, I simply followed the software's option to provide the resulting h or w based on the inputs of pA and pB that I provide.

&#x200B;

||two-tailed *z test* (difference between two independent proportions)|*Chi-squared* test (Goodness-of-fit, contingency tables )|
|:-|:-|:-|
|Effect size |h = 0.04|w = 0.02|
|Total Sample Size Required|**32,482**|**32,487**|

&#x200B;

My confusion comes from the fact that I always used the **z-test sample size requirement as a ""final requirement""**, while the **chi-squared worked more as a ""required count of successes""**. 

This would mean that although the two sample sizes required for each test look similar as of now, for the chi-squared test I would still need to account for the fact that on average only 50% of the people who enter the test end up as a success, **so I would need to multiply those 32,487 by 2** to arrive at the real initial sample size needed.

Can someone please point out where the mistakes in my reasoning are? Thank you very much to whoever is able to help!",statistics,2022-08-19 06:58:10,11
"I'm not familiar with a special name, but I would call it ""the covariance matrix of β-hat,"" and then perhaps mention that the diagonal elements are the squared standard errors if I thought it would help the reader.",4,wse85l,"I am having a very basic question for which it is very complicated to find a response online : has the variance-covariance matrix that contains the square of standard errors (and related covariances) a name?

To be clear, I'm referring to the [matrix defined, e.g. in the answer to the Cross-Validated post](https://stats.stackexchange.com/questions/44838/how-are-the-standard-errors-of-coefficients-calculated-in-a-regression), named var(β) in the post.

For context: I need to use it for a calculation in a scientific article, and want to use its proper name if ever it has one. Otherwise, I'll just call it something like ""standard error variance-covariance matrix"" (obscure as it is for the reader...).

Thanks for any pointer!",statistics,2022-08-19 06:36:28,12
"Take classes and learn how to use R by doing projects either in class or by following some tutorials. Actually write code yourself and understand the analysis and visualize data. This will give you confidence, you’ll have something concrete to talk about in interviews, and you will likely not forget this way",5,ws3lzt,"Hi! I'm a third-year undergrad Econ and Business Analytics major without any statistics credentials beyond: Business Statistics, Finite Math, Calculus I.

I'm seeking a career in data science or some quantitative role consisting of statistical analysis. My econ curriculum has some econometrics, but not enough to really count stats and math as a strength.

What is the best way to develop stats/math skills and also show them to employers? Do I take extra classes? Coursera certificates? Projects?",statistics,2022-08-18 20:43:49,4
"This is hard because there is a lot of guesswork here. Even if you know the probability of someone having no siblings, there is still the issue of many people who don't get married at all, or people who get married more than once. Then there is the question of whether your lack of siblings and the lack of siblings of your spouse are independent or not, and even perhaps whether gender has anything to do with it.",5,wsdeq4,"I'm having trouble figuring this out - both finding data on how many adults have no siblings in the US, and how to do the math  of two of these people getting married - do you simply multiply the odds?",statistics,2022-08-19 05:59:13,5
The sample size was low therefore the observed result would not be unlikely if the true correlation were zero.,69,wrj7si,Can you help me internalize this?,statistics,2022-08-18 06:10:28,24
Essentials of stochastic processes (durrett),1,ws3xcs,"I'm an undergrad and I would lile to learn about markov chains, is there any recommended book on this? Thanks in advance!",statistics,2022-08-18 20:59:52,1
"K means distinct samples. Those are your groups. So if k-1 equals 3, then k equals 4. You have four different groups.

Once you have that, write out all the formulas for the info you have and rework the formulas for the above until you can pull out N. 
Just write them all out and then fill in the information you have and you'll see it!",3,ws7q5m,"Hi i am having issues with filling up my ANOVA Summary table. 
The only information given are -

SSBetween = 1253.68
SSWithin = 3762.7239
SStotal = 5016.4039

dfBetween = 3 

MSBetween = 417.8933

I’ll probably be able to fill up the rest of the table if i am able to find N. But i am stuck and have no idea how to find N with only the info given above. 

My professor gave a clue dfTotal = N-1 and dfBetween = k-1. 

Please help! 😭😭",statistics,2022-08-19 00:33:25,5
"These are 2 seperate questions: how many options should you provide and should that be an odd or even number (10 is not the closest even number to 7, but you probably already figured that out). 

Odd number of options offers the opportunity for a neutral answer, up to you to decide if you want that. Is being neutral a valid answer or do you want to force people to make a choice? 

Probably you want 5 or 6 options, see https://psycnet.apa.org/doiLanding?doi=10.1037%2Fpas0000648 for more information.",16,wrgqdc,I'm considering what's appropriate: 1to7 or 1to10 for a survey question.,statistics,2022-08-18 04:08:50,7
Probably the easiest way to figure this out is to look at the schools in the tier that you are targeting and look at what the CVs of recent hires look like. The bar is always fluctuating depending on what the job market looks like.,4,wrtm6z,"Hi all! I am currently a PhD Statistics student at a fairly low-ranked, small program (think of >75 in the US News Ranking). However, my program actually has a very good placement in academia; almost all went to R2 universities, and only 2 went to R1 universities. Given the modest prestige of my program and our norm so far, I think, realistically, I will also end up at an R2 university if I want to pursue an academic job. However, I am just curious to see if I would have a small chance at some R1 universities on the lower end or some upper, aspirational R2 universities since I think I am having some advantages compared to my peers. I was lucky to squeeze out two papers after my Master's since I had been working with my advisor since my senior year of undergrad; one was already published (impact factor 1.9) and one is about to submit (impact factor 1.3). Right now, I am working in the same research area and I think by the time I start the job search, which is 2 years from now, I may have a good chance to get 2-3 publishable projects done (aiming for journals with impact factors in the neighborhood of 1.5).

I know this is a hard question to ask (and hard to give a number) since it can be highly dependent on the schools I am applying to, the job market situation (which is always competitive), and the applicant pool (the journal prestige also matters a lot), but I really want to get some perspective to have a plan for the next few years. For instance, if I don't have a chance at some very high-research-activity universities, I will try to work on improving my chance at teaching-focused universities by applying for a part-time instructor job at my local community college (my program does not have many opportunities for students to teach their own class). I for sure won't apply to prestigious universities like UCs, Michigan State, or Colorado State (or maybe anything < 60 in the US News). Probably, I will aim for similar-ranked programs like mine such as the University of New Mexico, Louisiana State, or, Oklahoma State (sorry, I don't mean anything downgrading; just feel programs like these are more realistic to try in my case).

Please help me if you have some insights on this. Perhaps, it would be helpful if you could share your number of publications at the time you applied. A side question I have is how submitted papers are perceived compared to accepted/published ones. Thank you so much!",statistics,2022-08-18 13:17:30,7
X=1 in the top formula,7,wrxhsk,"So I’m working out the problem “sum greater than 1” where you have to find the tipping point where an amount of random numbers (0,1) sum to greater than one. 

In my journey to asking ‘why’, I’m curious, where does the 1/n! come from? It seems like it sort of pops out of no where and am curious how you might be able to prove how 1/n! works through integration/derivation. 

I think it has something to do with the law of iterated expectations, but feel like I’m missing something. 

Here are a few formulas I’m using as reference 

https://imgur.com/gallery/TI5taYI",statistics,2022-08-18 15:56:18,9
Do a combined scatterplot using a different symbol for the new data points. This will reveal what is going on with the correlations.,4,wrj3s3,"Hello everyone,

I am working on a panel data set, observations are a number of countries. Initiatlly I had around 40 countries, but increased that to around 70. Problem: With the smaller sample size, I get an r-squared of around 0.35 - but with the bigger panel, its only 0.044. The variables are the same, and the bigger dataset has the smaller dataset in them, so its only the roughly 30 other countries making the difference. Is that even possible or did I make a mistake somehwere?",statistics,2022-08-18 06:05:33,5
Have a read up on spurious correlation. You might then look into causal inference (and how tricky it is to establish).,5,wrs0j3,"It has been years since college and my last statistics course. I am now disabled and have time to answer some questions that don't have answers in the scientific literature. To do this I need to compare various unrelated data sets. For example, one project is in relation to a 1950s Russian scientist who thought that solar cycles were predictive of human events like war. He was put in the gulag and, though I did find one study finding a correlation between solar cycles and economic recessions, it doesn't appear that anyone has revisited his original claim about war. I got the data sets, but then I realized I'd forgotten how to actually check that data for statistical significance.

Would any of you be kind enough to tell me the names of the methods I might use for this purpose? Book recommendations are also welcome.

Thank you.

Edit: I thought it would be obvious but I only care about correlation here, not causation. I am not interested in proving that solar cycles cause wars, only in evaluating whether the claim that they are correlated holds up to statistical analysis. ",statistics,2022-08-18 12:12:21,16
A longitudinal mixed model will get it done. Just about any decent stats package should be able to run one.,1,wrimsh,"We have data from an online health intervention. Users can choose to measure their health state at any moment in time. We'd like to correlate the change in those heath states with feature usage (there are multiple features).
Obviously, there is a lot of literature on how to analyse a longitudinal study with fixed measurement times but what if users can freely choose to measure? So neither the intervals between nor the number of measurements is consistent between users.
So far, we just looked at the difference between the first and last measurement timepoint per user (luckily, most users access their health state at the beginning of the intervention) to make it simple. However, we'd like to include all measurement timepoints if possible. The prior heath state as well as feature usage so far will most likely be important covariates.
Do you have any literature suggestions how others have solved this issue? Of course, other hints are welcome too.",statistics,2022-08-18 05:44:04,1
"It depends what you mean by ""fixed"" covariate adjustment, I can think of two different approaches:

First, for a estimated (given the data) effect of a covariate without penalisation use the penalty.factor argument (a vector for columns in x which is set to 0 for the covariate you want to not be penalised and 1 otherwise).

Second, for a fixed/specified (i.e. not estimated) effect of a covariate use an offset. In short this will be the variable multiplied by the desired regression coefficient and this will not vary over the cross-validation. See `?offset` for general documentation.

Why you're not getting the desired results with `penalty.factor` I don't know. The simulation concept sounds right - could it be a problem with the setup? I haven't used penalty.factor for years, but I can't imagine it's not tested/functional in general.",2,wrcw2g,"Hi all fellow stats geeks,

Is there a reliable way to perform elastic net regression with cross validation with a ""fixed"" covariate adjustment? My task is to identify differentially regulated proteins between two classes while holding a set of key deomgraphic variables constant. Using glmnet::cv.glmet(), one can specify a penalty.factor, but I have a hard time understanding whether this will achieve my goal. I've tried simulating data whereby a fixed covariate (that is specifically not penalized) should explain a set of predictors, but not others. However, I haven't gotten meaningful results (it does not appear to really affect the beta coefs of the right predictors). Any insights on this would be very helpful!",statistics,2022-08-18 00:17:47,12
What does the data look like?,1,wr0dqh,"I am developing a model that predicts below-ground distributions of plant roots with depth. For context, this distribution reflects the amount of root biomass per increment of soil depth from the land surface down to 2 m (i.e., the distribution is bounded).

I would like to test which soil and climate variables in my model have the most influence on the shape of the predicted root distributions. I could do this empirically by evaluating relationships with the location/scale parameters, but I was hoping someone here might point me in the direction of a more formal analysis, preferably one that is non-parametric and takes into account the full ""shape"" of the distribution. Thanks!",statistics,2022-08-17 14:15:04,9
"The irreducible error tries to capture the error in capturing the label values(Random error term). Say for example, if you are measuring the length of a particular object. Even though you know that the object is `x`cms long, with multiple measurements, you can get `x+0.001`, `x+0.004`cms depending on the device you use to measure the length of the object.

The irreducible error in the equation accounts for this measurement errors (there may be more than the measurement error).

So by this definition, the minimum MSE that you can achieve cannot be lower than the irreducible error, because by definition this irreducible error is not a function of your input (explanatory) variables.

In this example, the irreducible error is assumed to be a zero mean gaussian unit variance process i.e.,`N(0, 1)`. If you look at equation 2.3 in ISL, the error of regression models contains the difference between predicted value and observed value and the variance of the error component. The dashed line in the diagram you shared indicates this variance component. Hope this helps!",9,wqqytj,"How is irreducible error estimated in Figure 2.9 of ISL (Introduction to Statistical Learning)?

I recently started reading ISL, I am a beginner. Is the irreducible error defined so it is a little bit lower than the lowest MSE because we know the lowest MSE can’t be lower than irreducible error? Like, its estimated AFTER the MSE is calculated rather than before right?

Link to the Figure as the sub doesn’t allow images: 

https://miro.medium.com/max/494/1*UemLg4lcXbqpe_DC97I3Jw.png",statistics,2022-08-17 07:51:56,5
This  [free online book](https://onlinestatbook.com/2/index.html) that I co-authored has interactive self-test questions at the end of each section,11,wqw29b,"Hi all! I’m a university student in a research methods class that essentially just covers the bare-bones of statistics. I have a fully multiple choice exam on Friday (19th Aug) and I wanted to know if anyone here knows of any good online resources for studying for stats. Possibly ones with quizzes, little tests, etc etc. I tend to work well with those when I can test myself. 
I’m talking mostly the basics of stats, ie calculating confidence intervals, calculating z, p, & t scores, etc etc. 
If this isn’t appropriate for this sub, I apologize! Please remove this if so and let me know a better sub to post this on. 

Thank you kindly :)",statistics,2022-08-17 11:18:59,3
"Assuming if a dead ticket wins there is a redraw, I think you'd need to know how many other tickets were given out per eligible person for the remaining 9,980 tickets to incorporate the likelihood of winning a redraw.  


Ignoring redraws, to answer your question, I think it would be something like:  
P(Bobby wins 10 of the 10 draws) \* $9000 (everything he is giving up because of the 1 win rule)  
\+ P(Bobby wins 9 of 10 draws) \* $8000   
\+ P(Bobby wins 8 of 10 draws) \* $7000  
...  
\+ P(Bobby wins 2 of 10 draws) \* $1000",3,wr1jtk,"Say there is a pool of 10,000 tickets I gave out for free. There is going to be a drawing, and each ticket drawn wins a $1,000 prize. There will be 10 winning tickets ($10,000 total). Now say I made a rule that each person can only win one time.

Bobby gets 10 tickets, so if one hits, the rest cannot win.

Billy gets 10 tickets but he has 9 kids with him so he could technically win on all 10.

&#x200B;

How do I calculate how much value Bobby is losing because only 1 of his tickets can win?",statistics,2022-08-17 15:02:39,2
I would try the mann-whitney u test and compare the means of different sample sizes. Would use a t-test for comparison of means with same sample sizes,1,wr0wuk,"100 people view treatment A and respond to questions 1,2,3 and 4. Half of those see treatment B1 and answers the same question 1,2,3,4 again. The other half see treatment B2 and answers the same questions 1,2,3,4.

Quests 1,2,3,4 are likert questions.

1. I would like to compare the means of the individual likert responses (questions 1,2,3,4) for the 50 people who saw treatment B1, to the likert responses means of treatment A which was seen by all 100. Same for B2 vs A.

2. I would like to compare the likert responses between the 50 who saw B1 and the other 50 who saw B2.

Any help would be greatly appreciated!",statistics,2022-08-17 14:36:39,2
"Yes it will.

In OLS the correlation coefficient between the residuals and your y variable  will be given by

r'=\[1-r\^2\]\^0.5  where r^(2) is the r^(2) for your regression.

Which is positive except in the trivial case of r\^2 =1",7,wqtcho,"It is clear to me that the estimated value of y (criterion) and the predictor never correlate with the residuals. Yet, I’ve been wondering if the actual value of y is correlated with the residuals since it is composed of the estimated value of y by the regression plus the residuals.
I hope I could make my question clear, English is not my mother tongue and statistical questions are quite hard to formulate :D",statistics,2022-08-17 09:26:39,10
What is your question?,2,wqbrip,"Never really handled this sort of data before and would love some help. I got daily covid numbers separated by zip codes for just over a years worth of time. As well as demographic data within each zip code i.e. racial composition, number of unemployed individuals, number of individuals below the poverty life, etc. Any suggestions would be greatly appreciated.",statistics,2022-08-16 18:30:28,9
"What is your research question, what are the competing hypotheses you are testing? 

Once you have clarified this, there are a few things you can do.
If your assumption is that not all conditions are equal, the most straightforward (I think) would be to fit a model with a 4-level condition factor as predictor and compare it to a model with just an intercept. That's a very weak assumption however, you may have more specific predictions to test.

Given that the data is ordinal (Likert), you need an ordinal model. Since each participant saw four items, you need a mixed effects model. So you should use function `clmm` from package ordinal. It would look something like this:

```
model1 <- clmm(Answer~Cond+(1|Subject), data=...)
model2 <- clmm(Answer~1+(1|Subject), data=...)
anova(model1,model2)
```

Edit: I assumed you are using R. The same idea would apply in a different language, but the code would look different of course.",4,wq4m4d,"I'm somewhat of a statistics noob, so apologies if this Q is very basic. I'm trying to compare multiple likert scales (each survey participant took 4 likert scales with the same 4 point ratings). Given the paired wilcoxon test is for 2 groups Is there another test to compare all 4 at the same time. Or is the practice to just run the paired test for all combinations (AB, AC, AD, BC, etc) and present the combinations where p<.0X.",statistics,2022-08-16 13:25:29,4
"Question 1, whether it's possible for a bird to stay 0 days since tagging, is more of a scientific question than a statistical one. If the ethology of the birds and the properties of radiotaggting suggests that the birds could have stayed 0 days, you can leave them as zeros. If the science strongly argues that it's impossible for the birds to have stayed 0 days, then you could handle the 0s as missing values instead.

For question 2, there is model for this particular situation: the [zero-inflated negative binomial](https://stats.oarc.ucla.edu/r/dae/zinb/).  This [link walks through various options](https://rpubs.com/kaz_yos/pscl-2) for count models and the differences in interpretation",9,wq14h4,"This is more a question about how would you deal with the following situation. I have data from 2 different years of the number of days 70 radiotagged migratory birds stayed on a site. I want to know which body traits (sex, age, weight, wing...) might be influencing the number of days they're staying since the tagging. So onto the problems and questions: 

1) I have 11 birds that were tagged and never detected the day after. I don't think it was a problem with the equipament but since they were all tagged at the end of the day and never detected again, I can't imagine considering this like 1 day. However, does 0 days makes sense in this situation? Is it possible to say the bird stayed 0 days since tagging? 

2) I'm using a GLM with negative binomial distribution because of overdispersion and because I think the response is count data. The histogram is fairly skewed to the left so not a normal distribution for sure. Is there a way to be sure of what distribution I should be using through more tests? Would you suggest a different distribution and approach?

Sorry if this is vague, feel free to ask me for more info, I appreciate any help.",statistics,2022-08-16 11:04:39,7
"I think you are looking for the extended McNemar test to compare paired performance of the entire set. Lookup the paper as its quite an elegant test and its implemented in SPSS (and R).

You will still need a subject matter expert for interpretation as the difference is quite small, but still very notable. Your test is slightly more sensitive but the reference test is slightly more specific. See if you can translate this clinical performance to a prevalence adjusted scenario which will be the case when you use this performance on the diagnostic population rather than on this selected population

Of note: I did my PhD in molecular diagnostics for cervical cancer so AMA",1,wpwmf2,"Hi,

I wanted to calculate the sensitivity, specificity, PPV and NPV for a test. The reference test is not 100% correct as is the test that I want to compare to the reference is not 100% correct.

Reference test: TP 82, FN 6, TN 138, FP 0

My test: TP 88, FN 0, TN 133, FP 5

But the Contingency table always wants the reference test to be 100% correct. Is there a way to get clinical sensitivity etc.

Thanks",statistics,2022-08-16 08:03:01,9
You’re in the statistics subreddit. The statistically significant correct answer is R. R will do basically everything you need within statistics.,62,wpiebv,"Hi! I'll be starting an online graduate program in statistics next week, and I'm trying to create a tentative plan of study, so I was wondering if I may ask for some advice here.

The program, consisting of 10 3-credit courses, only requires 1 programming intensive course: Python, SAS, or R.

If I take only 1 programming intensive course, which language would you recommend I learn?

Is it important enough to learn a 2nd programming language, even if it would mean I would have to sacrifice an elective? (I only have 3 electives to begin with.)

Also, in case this makes a difference, the two areas of statistics I'm most interested in right now are environmental statistics and survey statistics. I'm planning on taking survey statistics through the University of Michigan Summer program, separately from my graduate program. But I would like to use my graduate program's electives to take classes relevant to environmental statistics.

I'd also like to know what language(s) are most relevant for environmental statistics and/or survey statistics.  Thank you!

Edit: Thank you, everyone, for your thorough replies!",statistics,2022-08-15 19:32:56,44
"""Quant"" working in ALM.  You'll definitely have the resume for some form of a quantitative risk management role, but if you hated the finance culture before then you might not like it any better from a data science perspective.  My hours are reasonable, most weeks between 40 and 50, some between 30 and 40.  Towards year end when there is a big push to have model improvements implemented I've worked closer to 90.  The pay is lower than tech but I enjoy the domain so I'm happy where I'm at.  I also have the option to work from home and have only been back to the office once since the pandemic began, but this really depends on the institution you work at. Impromptu work on weekends is somewhat rare for me since my work is project based, but occasionally I'm asked for some ad hoc analysis that requires working on the weekend to finish on time.",4,wpdsb4,"I'm about to start my second semester of my M.S. Statistics program after switching careers from Finance and can't figure out if working in quant risk for a bank is up my alley after graduation. When I say quant risk, I mean working as a statistician in developing models for balance sheet, interest rate risk and credit risk management sort of thing.

Background:

\- 3 years FP&A financial analyst

\- 1 year private equity / real estate investing

\- CFA level 2

\- Currently enrolled in an M.S. Statistics program

I couldn't stand the culture in Finance at the end of the day and knew I wanted to work in statistics full-time instead for a variety of reasons. I love the quantitative depth it entailed versus most positions in finance and that a lot of positions seemed laid back, low-stress, and relatively well-paid. Data Science is really tempting from a culture/salary perspective, but seems like few positions feel like you're actually working as an applied statistician.

**Quant risk analysts, do you like your job? Can you share you salary and working hours and overall work/life balance? What's the stress like if any?** 

**What I'm afraid about is being tempted to use my finance background to leverage getting a higher title and position using stats after my program, but then working 60+ hour work weeks and having impromptu work on weekends and all that bullshit I left finance for. I also want to work 100% remote no strings attached.** 

**Am I a good fit and should explore this career path more?**

Any input is really appreciated. Thanks.",statistics,2022-08-15 16:02:43,3
"I would recommend Calc III, Linear Algebra and Real Analysis to be on ""equal footing"" with the undergrad math/physics majors.

More applied programs may look past having Real but would still probably want to see Calc I-III + LA.

I would also recommend doing very well on the Math section of the GRE.",23,wp21af,"I realize this is a question that needs a bit of context - I just finished a stats/programming-heavy data science masters program at a large, reputable university. 

I'll qualify that I am far from an expert statistician, but I would consider myself having a fairly well-rounded stats/research foundation - i.e., I took classes and completed projects related to general stats, simulation-based inference, regression (broadly - including the matrix algebra behind it), EM/likelihood, bioinformatics, and GLMs. I also consider myself to be fairly strong on the technical side - R / Python / SAS - and do have a peer-reviewed journal publication under my belt.

So that's the ""good""... The bad, however, is that my transcript is lacking a lot of the ""pure"" math concepts that are commonly cited on PhD admissions requirements. I've taken Calculus I and II... but that's about it. I did quite well in them, for what it is worth. And through necessity for my machine learning and regression-based classes, I've taught myself a decent foundation of linear algebra - though that sentiment is obviously not reflected on my official academic record. 

I have noticed, too, that stats programs rarely have a formal list of classes for admission requirements (as opposed to departments like CS or Economics). So I'm not sure if that works in my favor?

In any case, I really just need to decide if I should take mathematics courses to supplement my transcript over the next year or two. OR if I should just go ahead and apply to programs as soon as I am able to. 

Any insight on this would be much appreciated!",statistics,2022-08-15 08:08:51,17
I wrote a tutorial on this which you may find useful: https://colinquirk.com/simulated-power/,2,wph10r,"First time doing power analyses, and I’m trying to figure out how to do on for a multinomial logistic regression (3 categorical outcomes, 7 predictor variables). I’ve gathered there’s no existing package to do this, but the advice I have found so far (“run Monte Carlo simulations”) is too vague for someone with my skill set, I’m afraid. Anyone know of a tutorial or something like that that could walk a noob like me through solving this? Thanks!",statistics,2022-08-15 18:29:11,2
"I do work in a different field, but we just hired someone straight out of college and didn't look at her grades once. Experience was all that mattered.",11,wp7sac,"I know both can be very important, but which one would you put bigger weight on? Would you trade off a good GPA for a stronger work experience? (Please, put into account the possibility of someone thinking of going to grad school one day)

Note: By a ""good GPA"", I mean a good GPA (e.g. >= 3.6) in a quantitative degree (CS, Math, Statistics, Physics, Eng, etc...) but Of, course ppl career shifting or from other fields are more than welcome to share their opinion as well.

Would love to hear your take on this, thanks.",statistics,2022-08-15 12:00:20,11
First and foremost check if the populations have the same selection criteria. Then you can compare the performance of those studies without a problem. I recommend performing logistic regression (edit to add: or log linear regression) so you can add a factor for the population and include an interaction term for the test and population to see if the association changes between populations. Or have a look at mosaic plots to see Chi squared in action across populations.,3,wpdev1,"I want to compare the performance of 14 diagnostic tests. All tests have been conducted with different sample sizes ranging from 3000 participants to 90000. 

Furthermore, can I calculate the correlation of a performance metric for all of the studies (e.g. accuracy of each diagnostic test) with a variable (e.g. scan resolution) that is present in each study?",statistics,2022-08-15 15:47:16,2
"At what level? [Statistical Rethinking](https://xcelab.net/rm/statistical-rethinking/), [Doing Bayesian Data Analysis](https://sites.google.com/site/doingbayesiandataanalysis/) and [Bayesian Data Analysis](http://www.stat.columbia.edu/~gelman/book/) are all great books, but boy do they differ in terms of depth and difficulty.",39,worgqj,,statistics,2022-08-14 22:38:55,17
"You should get the same distribution using Gibbs sampling or the true posterior distribution (which we know because, as you said, we have a conjugated prior)

If you get different results, it may be because the posterior isn't quite what you wrote (see [NIW](https://en.m.wikipedia.org/wiki/Normal-inverse-Wishart_distribution#:~:text=In%20probability%20theory%20and%20statistics,family%20of%20continuous%20probability%20distributions.). The posterior on Sigma doesn't depend on mu, it only depends on the sample.",2,wpdnk5,"Picture is included for reference.:

[https://i.imgur.com/jDI1LME.png](https://i.imgur.com/jDI1LME.png)

Suppose  I have a model in which my observations, x\_i, are multivariate normally  distributed with mean mu and covariance Sigma. Furthermore, I treat mu  and Sigma as unknown, with priors as follows:

The prior distribution on mu is a multivariate normal distribution with mean mu\_0 and covariance Sigma

The prior distribution on Sigma is an Inverse Wishart with parameters nu and Psi.

Am  I correct in thinking that I could use Gibbs sampling to sample from  the conditional posterior distribution of mu and Sigma also using a  multivariate normal and Inverse Wishart distribution, respectively (of course with new parameters) since I believe we have conjugacy?

If  so, why might one use a normal-inverse-wishart distribution in this  scenario? I get different results when I use a normal-inverse-wishart in  this situation so I think something is going wrong. Are things  complicated by the fact that mu has a dependency on Sigma in its prior  distribution? I'm just a little confused here!",statistics,2022-08-15 15:57:12,1
That's right. You're missing a minus sign in the exponent though.,10,wp2psa,"Hello. In time to event analysis, the hazard function (h) can be derived from the survival function (S) by the equation h=-d/dt ln(S). Can that be re-written as S = exp( ∫ (h) dt) ?",statistics,2022-08-15 08:37:40,4
"I did my PhD in Marketing (Quantitative Marketing, Marketing Strategy, so a lot of coding, statistics, data analysis).

The curriculum itself wasn't really the important part of the PhD. Coursework could be very difficult and overwhelming, but I wouldn't recommend investing much time to prepare for that. 

Research is really the most important part of the PhD. And then it was nearly impossible to prepare in advance. What I used for my research was very specific, not covered by any of the courses I took, not even available yet in the software I was using to code (so I had to change to another software), and materials to learn that was very scarce. 

I wasted a lot of time studying things that I never used, and that I have now forgotten because I never used them. If you really want to prepare in advance, my recommendation is to check papers that are close to the research that you expect to do. And then see what kind of statistics they are doing. What are the models, what are the methods, what are the statistical issues they are dealing with, what type of data they are using, etc.",2,wp1ty6,"I will begin my PhD in Business Analytics next year, and in the meantime, I am looking for some resources to strengthen my statistical skills with, as the curriculum will be heavily stat-focused. I am not entirely new to the field, but my knowledge is limited to a few specific areas. Therefore, I think it lacks depth and variety.

My background:

Passed entry-level stat in high school

Engineering undergrad

Master's in Business (Passed one graduate level stat course)

So far, my primary source of knowledge has been the documentation for StatsModels and Scikit-learn and some blog posts on Medium.

TIA for your recommendations.",statistics,2022-08-15 08:00:13,4
"I did my bsc in stats and do full stack after graduation. I did two internships, first was an analytics /research / software development role. Second internship was purely data analytics. Towards graduation I moved towards software as that was more enjoyable for me.


All those positions would be open for you if you have 
experience, internship would be best -  otherwise projects to show your experience and interest in specific subjects.


Maybe the only roles a bit out of reach for bsc would be machine learning. It's more difficult to get a role related to machine learning with a bsc but not saying it is impossible.


Some courses that helped me transfer into software roles were data structure and algorithms courses, software development, networking, and to some extent machine learning courses (familiarize with Python/API's)",4,wohlgz,"Im a cogsci-psychology bsc student who finished 1 year right now, and during that one year I realized I love math and programming so I decided to switch cogsci to a more quantitative degree, I would pick comp sci but I don't get accepted at my uni, however I do get accepted to statistics. This is not my ideal option but it is okay, I did a couple statistics course and did enjoy them, not as much as the math and programming ones though. Anyways, by the 4th year I will be done with the psychology courses and have only statistics courses left, so I would have time to pursue more compsci related courses. What I wanna know is, if I have a statistics degree, and do the following courses: introduction to Cs, data structures and algorithms, algorithms, OOP and SNL. What job positions will be open for me? Will software development be one of them? I want to know if these are jobs that will suit me, or maybe i should switch a university, which will be kinda rough..",statistics,2022-08-14 14:43:27,10
"> Does anyone know how to “get up to speed” with programming for a statistics phd program?

Practice on the sort of thing that would be relevant to the stuff you'll be doing in that program. (Are you in a country with a lot of classwork in the first part of a PhD program?)

In many cases (some programs will be exceptions, depending on their focus), a lot of class-relevant work likely involves relatively short scripts or interactive work (load some packages, read in and organize data, run some analyses -- R can make these sort of tasks quite short, codewise - often only a handful of lines, unless you want to make it a more general function), though some can require more considerable levels of effort. Some research, however, may involve much more statistical programming, but you can continue to build the skills as you go.

The first task would be to work on the sorts of things you'll want to be doing in the first semester.",5,wonzkj,"Hello, I will be applying to MS/PhD programs in statistics this fall. I have had lots of exposure to mathematics and statistics in my undergrad. I have self learned coding languages like python, and have taken one or two cs courses, but I’m far from calling myself a good programmer. In fact, that’s probably my weakest skill. I can’t leetcode at all, and it takes me a very long time to think through such problems that require data structures and algorithms.
Does anyone know how to “get up to speed” with programming for a statistics phd program? I don’t want the coding part to slow me down since the math will already be very hard.",statistics,2022-08-14 19:38:47,6
"40 is a pretty small sample size. If you try a much larger number, does the test pass?",4,wor6yy,"Hello everyone, 

I am doing something regarding central limit theorum.

so I am trying to prove the the averages of the sample sized n from a expenotial distribution follow a normal distribution. 

so I simulated a 1000 observation from an exponential distrubtions setting the lambda = 0.2 for all simulations.

    sim <- rexp(1000, 0.2)
    # showing the first 10 observation of the vector
    sim[1:10]

then I sampled 40 observations from this population took its mean then did this process 1000 times.

    averages <- NULL
    for(i in 1:1000) averages <- c(averages, mean(sample(sim, 40)))
    # showing the first 10 observations
    averages[1:10]

so when I plot the histogram of the averages distribution and oveerly normal curve it seems to be normal

    histogramav <- hist(averages, breaks = 15)
    x_values <- seq(min(averages), max(averages), length = 100)
    y_values <- dnorm(x_values, mean = mean(averages), sd = sd(averages)) 
    y_values <- y_values * diff(histogramav$mids[1:2]) * length(averages) 
    lines(x_values, y_values, lwd = 2)
    abline(v=mean(averages),col=""blue"",lwd=2)
    legend(""topright"", ""Average of Sample Means"", col= ""blue"", lwd=10)

but I do Shapiro-Wilk p value is significant meaning i have to reject that the popultion is normally distributed, why this is happening to my simulation?

    shapiro.test(averages)
    
    	Shapiro-Wilk normality test
    
    data:  averages
    W = 0.98848, p-value = 4.551e-07",statistics,2022-08-14 22:23:20,15
"> Why does Minitab report the W-statistic instead of the U-statistic for the Mann-Whitney U Test? 

They're equivalent tests so it literally makes no difference whether you use U or W. There's also more than one form of each statistic, so you need to know which *exact* definition is being used if you want to convert between them. 

> is there any other way for me to get the U-statistic value?

  Sure, if you know exactly which W statistic Minitab is calculating. It should be explained in the documentation

  For example, if you follow the original paper by Wilcoxon (1945), I think it's the sum of the ranks in sample 1

   If you follow the tables from his 1947 paper, I think it's the sum minus the minimum value.

   If you follow some later papers, it may be something else again, like the smallest of the ""sum of ranks minus smallest value"" across both samples. 

   Whatever definition they use, there will be a simple conversion to a U statistic.

e.g. if it's the sum of ranks in sample 1, Wikipedia gives a direct conversion:

https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test",2,woqm6k,"If I'm using Minitab, is there any other way for me to get the U-statistic value?",statistics,2022-08-14 21:52:22,11
"Basically wherever you have that nested data and would end up normally using a linear reg/GLM you would use a mixed effects. Everything else is the same.

The exception is if you have observational longitudinal data and are going for some causal inference, then mixed effects models probably should not be used because they are biased for the causal effects in longi data outside an experimental setting. However I don’t know how much this issue impacts business applications, in some cases the fancier methods barely make a difference.",2,woi502,,statistics,2022-08-14 15:07:24,3
Spearman requires ordinal data. Pearson requires interval data.,4,wo7djz,"Hi there!

I want to calculate if the performance of a network is affected by different variables. I have 10 different networks and their performance metrics (ACC, SE , SP) and 4 different variables (sample size, image resolution etc.).

All studies use different variables (sample size, image resolution etc.). Is the Pearson\* rank order test  the right option?

&#x200B;

\*sorry, it seems I have messed up this post from title to context. I meant Pearson!",statistics,2022-08-14 07:14:44,11
"You say you did an exploratory factor analysis and a confirmatory one. I want to make sure we're both on the same page with your method.
I am assuming the following:
- Your questions were not a predefined scale used by others.
- You split up the dataset randomly in two sets: the one for exploratory, and the one for confirmatory
- You used a non-exploratory method to confirm the factors (e.g. SEM structure, or a program that has this feature built in under CFA)
- Your KMO was over .60 at least (EFA) but preferably .80. if not, some variables need to be taken out: check correlations for over .90
- You used an oblique rotation method of some kind (as long as it wasn't orthogonal, such as varimax)
- you used a combination of scree plot, eigenvalues (.90 or higher as a possible candidate), and logic
- you ended up with logical factors: you could see how most of the items could be interpreted in light of these factors. If not, you redid it with different results until the end result made sense.
- you ignored factor correlations below .30 or .40

Ok. So after those assumptions (which i hope you found online too), you must have decided on your dimensions/factors. Let's call them factors in general.

Say you have two factors, and the first is made up of 5 items and the other is 3 items.
There are multiple ways to calculate the actual score:
1. Use the mean function under calculate variables (if you don't use the mean function but divide by five, you might have a wrong result with missing values). First factor would take the mean of those five variables. Second factor would take the mean of the three items. This is the most common as you can still interpret the scores: they will be a score on your likert scale.
2. Use the factor loadings on your dimensions times the item's score and add up the result. So item1*itsfactorloadonfactor1+ item2+FLonF1 etc. You should have 5 for the first factor and three for the second factor (with my above assumptions of 2 factors of 5 and 3 to clarify the methods). This will basically weight your variables for each factor, so the most important variable will have the highest factor loading. This does result in weird scores, and not all peer reviewers understand that this method is completely valid. 

I hope that answered the first part.

Your question about ""picking seemingly related questions"" seems odd given the method. But I'm assuming you mean that the factor analysis said they're related and then the confirmatory one confirmed they're related as well. If you didn't use confirmatory on your second half, I'd suggest just using the ExFA (or principal components analysis) and skipping out on the ConFA.

I don't understand any of your other questions.

After you made your new scale variables according to either of two methods above, you use just those two for your linear regression model. 

If that wasn't your purpose, you don't need an EFA
If you wanted to show with a SEM how your variables are related, you'll need a theoretical model first (don't just do an EFA which is already subjective AF). Why are certain variables related to others?
Once you have that, draw your model with your laten variables being made up of the actual questions. Then check the fit. If the fit is fine (there should be documentation of what is considered good or not on the various fit indices), you're done. If it's okay but not good enough but close, you could ask to see some suggestions from the program to see if any of them make sense theoretically. Don't just use whatever the program gives you.

Honestly, i feel SEM might be beyond you right now if you're not grasping EFA for 90%. I'd focus on that first because that seems to be your main issue right now: the correlations are so high that your regression keeps telling you that there's multicollinearity.

So do the exploratory one first and make your factors. Then do a regression with those factors and input your control variables.

I'm not sure what you are predicting though. It's unclear from your question. You're seemingly just putting every question to predict every other question. I don't see the point in that, as an EFA basically looks at that.",12,wo5015,"I am analyzing the results of a survey, where the answer to each question is rated on a Lichert scale, and I am trying to understand the data set.  More specifically I would like to be able to pick one variable and examine how this variable depends on the remaining variables in the survey. 

I started out doing exploratory multiple linear regression. But I have trouble interpreting the results, because I have a lot of confounded variables. So each time I pick a new dependent variable I get a new set of independent variables. This is not very useful, because the answers are hard to interpret.

So I continued to do factor analysis (exploratory and confirmatory) to find 10 factors that can explain some of the answers to the questions in the survey. Now I hope that I can use these factors as an inspiration to define independent variables for my multiple linear regression, but I am not how to do this in practice, and I would be grateful for some advice. Here are some ideas that I have had so far. 

* One idea is to pick a set of questions that seem to be related and calculate the average Lichert score to define a new variable. Now I can use these new variables as independent variables in my multiple regression.
* Another idea is to use the expected values from the factor analysis as independent variables in my multiple regression. 
* A third idea is to use the factor analysis as a starting point to construct a structural equation model.
* A fourth idea is to use the  components from PCA as independent variables (But this is complicated because sometimes the components are had to interpret)

This is a hobby project, but I would like to use it as an excuse to learn some statistics. Therefore I would be grateful if someone could give me some advice or recommend a good reference that I can read to learn best practice. So far I have been using `factor_analyzer` and `semopy` to create models in python, but I think that I need some direction before I can succeed. 

I tried to ask this in [askstatistics](https://www.reddit.com/r/AskStatistics/comments/wg5cal/factor_analysis_multilinear_regression_is_this/), but nobody answered, so I hope that it is OK to ask here as well.",statistics,2022-08-14 05:14:50,6
That appears correct. The interaction means that the effect of treatment differed as a function of baseline,3,wo9qpq,"I need some help with understanding this table: 
https://link.springer.com/chapter/10.1007/7854_2010_45/tables/3

With ""fasting triglycerides"", it says a ""Data presented in separate columns due to significant baseline by treatment effect. Median WC = 39 in., median SBP = 122 mmHg, and median TG = 148 mg/dL""

Does this mean that the table is separated into two columns, one for people below the median triglyceride level (at first visit/baseline), and one for people over the median at that point in time? Or am I misunderstanding this.

Thank you kindly for your help!",statistics,2022-08-14 09:00:49,2
Look into opportunities for undergraduate research - ask your professors in math and stats about them.,11,wnvuht,"I'm a second year undergrad and I'm thinking of pursuing higher education in statistics (aiming for PhD most likely). I wanted to know what I can do to find that thing is right or not right for me, and I'm a bit at a loss as to what I can do to set myself up for success.

People talk about looking at research papers and what not but I don't have the analysis background required to understand probability theory and higher math background to understand these papers I find from professors' pages.

Is my lack of direction a bad sign that I don't have what it takes? I get A's in all my math and stats classes but I'm pretty aware that that's nowhere near enough rip.

Any advice?",statistics,2022-08-13 20:01:46,11
Fight approximately 400 people chosen from around the world at random and you’ll have an estimate of where you lie.,125,wnjqve,"I was hanging out with my dad the other day and during his rambling he claimed he could beat up 95% of the population. I didn’t think much of it and let him go on but now I’m thinking. How much of the population could I beat in a fight?  I boxed for 3 years and got in a couple fights in high school so I can handle myself at the least. I’m thinking something like [population subtract approximate number professional fighters/boxers, most body builders, most likely most professional athletes (male or female). I’m confident I could beat any child under 14. Anybody over the age of 55. Most women that aren’t professional fighters or body builders. The population of men would be near impossible to guess accurately but approximately what percentage do you guys think an average guy could win in a fight?",statistics,2022-08-13 10:21:40,40
"Three parameter count models can also be used for underdispersed data; eg Faddy-Smith, Waring, Famoye, Conway-Maxwell and other generalized count models. However, it would be advisable to first identify the cause of the underdispersion and pick a model accordingly",2,wnvqxs,"Research Help! Underdispersion Issue.

 Data Question - Underdispersion - Research

 

Hello everyone, I'm sorry if this is not the right place for this question. I have been looking for the best subreddit to post this under in hopes of some help, so please point me in the right direction if this isn't the appropriate place.

I'm a PhD candidate working on my dissertation. I've been stuck for about a month on a particular data problem, and I cannot figure it out on my own. I've been trying to contact the statistical support team available to graduate students at my university, and I keep getting ignored (currently CCing everyone in the department to see if it helps). I've found some answers through academic articles and some youtube videos, but it leads me to a second problem.

Part 1. I work with SPSS. I'm working with count data that is under-dispersed. The mean is larger than the variance (M 1.64 & V 1.46). Data do not follow a Poisson distribution (underdispersion). Poisson regression cannot be used as it requires equidispersion (mean and variance are equal), and negative binomial regression is only appropriate for overdispersion.

Part 2. Academic articles tell me that the Conway-Maxwell-Poisson distribution and Generalized Poisson regression can be used with under-dispersed count data, but I cannot find information anywhere about how to do this using SPSS.

Does anyone know (or can point me to any sources) how to address underdispersion using SPSS?

Is there a cutoff point for underdispersion? The difference between my mean and variance is very small (M 1.64 & V 1.46). Would it be appropriate to use Poisson regression with these data?

Thank you for taking the time to read this and for any information you can provide!",statistics,2022-08-13 19:56:36,2
Include the interaction term with all other variables present in the model. Subsequently check for its significance.,17,wnime2," Hey, dear fellow Redditors :)

I am currently running a logistic regression analysis with 2 categorical and 1 continuous variable. I was planning to implement an interaction term for 1 of the categorical variables.

The question is, do I put the interaction term into the ""final model"", with all variables, and see if it is significant or not, or do I test it on its own with no main effects present? Another possibility would be to just test it with the main effect that is also being tested for the interaction effect, meaning that I would run a logistic regression model with X1 and X2 and X1 \* X2 and leave X3 out to see if X1 \* X2 is significant.",statistics,2022-08-13 09:32:12,15
"You need to sample from a distribution with more density in the upper range. I recommend you research the beta distribution and then Google ""draw random sample from beta distribution in <your programming language of choice>"".",2,wnp5l0,"Hi,  
I'm trying to write an algorithm/formula to select a random number between 0 and 1 but I want there to be a higher probability of selecting a number towards the top of the range (or bottom in certain scenarios). I'm new to statistics and not really sure where to start. Can someone direct me towards a resource to understand how to set this up? Thanks!

I'm writing code in Swift and have access to Math.random functions but I don't want an even distribution.",statistics,2022-08-13 14:29:59,12
"A regression would be the best fit here (pun intended)

But on a more serious note, i would try all three (mean comparison ttest, independence chi and regression) just in case and then choose the most robust one",3,wntmux,"I’m attempting to determine the relationship between test score and gpa by sex. Do I perform a correlational analysis (to determine the P value) and then a T test to determine the difference by sex? 
Or maybe it’s a chi-square. I’m pretty sure it’s not a regression analysis because it’s not predictive. 
Any input helps. Thanks in advance",statistics,2022-08-13 18:07:27,4
"For me this sounds like a case for moderation analysis. You run the MLR on the full dataset and include any subpopulations in the model as dummies, then you interact the subpopulation with any variable of interest and then yes, you will obtain evidence for any effect differences among your subpopulations. This gives you a statistical comparison, together with a series of p-values, otherwise i am not sure what was your intution on how to compare them?",2,wnnzzn,"Hey folks,
When comparing MLRs having the same dependant and independants variables (two different populations within the same global dataset) but different coefficients.

Can we compare the two MLRs coefficients as one population being more or less sensitive to a variable  change (disregarding the p-valu",statistics,2022-08-13 13:36:23,4
"Low 200s. Faculty position in the SFBA, fresh out of PhD (no postdoc), half hard money, half run-of-the-mill biostats and ML project consulting. No teaching load and dank benefits/perks. I acknowledge I'm kind of an outlier, but I love my job. If I had to choose all over again, would have taken it over a tech or some other industry job without a second thought.",39,wmu56u,"So firstly here's my own salary after bonus each year:

1: 60k (extremely low CoL area)

2: 121k Bay area

3: 133k Bay area

4: 152k remote

5: 162k remote

currently being offered 190k total (after bonus and equity) to return to bay area

We need this thread cause ASA salaries come from a lot of data scientists. Are any biostatisticians here willing to share their salary or what they think salary should be after X YOE? I ask cause I was looking at this thread:

https://www.reddit.com/r/recruiting/comments/rq7zdh/curious_about_recruiter_salaries/

Some of these folks make over 150k with just a bachelors and live in remote places with cheap cost of living, better than when I was in the bay area with my MS, plus their job is chattin with people from the comfort of their home. Honestly seems more fun sometimes than writing code/documents by myself not talking to anyone.

Meanwhile glassdoor for ICON says 92k for statistical programmer and 115k for SAS programmer analyst. yikes",statistics,2022-08-12 12:47:04,99
"Hell yes. Images are data. Vision is statistics. Perception is inference.

Here is something I really like because it is an intersection of nature, vision, and statistics. Also the main author was my dissertation chair, all around favorite researcher and teacher ever.     Truly brilliant and kind human being. 

Chubb C, Lu ZL, Sperling G. Structure detection: a statistically certified unsupervised learning procedure. Vision Res. 1997 Dec;37(23):3343-65. doi: 10.1016/s0042-6989(97)00187-9. PMID: 9425549.

It got left behind in the neural network/deep learning/AI craze, but in 2018 we picked this idea back up and honestly applications of Gaussian replacement could easily be a dissertation topic, or at least a couple papers in a stats journal. It deserves much closer study.   I wish I was able to write for that audience.",8,wmxn1a,"Hi r/statistics,

I am an upcoming graduate of an MS in statistics. I have recently become very interested in image processing and computer vision, I have been learning openCV in my downtime.

Is it possible for a statistician to work in a role focused in image processing and computer vision? What texts/resources do you recommend for applications of statistical inference in signal/image processing?

It is my understanding that mostly CS and EEs work in this field, but it feels as if statistics and CV have some crossover. I would like to learn more on the intersection of the topics so I can work on some projects, and learn the skills. Currently, my school does not have any courses specifically on this topic.",statistics,2022-08-12 15:16:59,5
"So basically it’s like if you followed 1 person for 100 months (which’s 8.3 years), that person might experience 29 transmission events in that time.  Or, say if you followed 100 people in the same apartment building, you’ll only have to wait 1 month on average for 29 of those people to have experienced a transmission event. Also if you want rate per month, that’s 28.7/100=0.287 events per person-month. That can be handy because you can multiply 0.287 by any number of people and find out how many events might happen to those people in 1 month, or multiply 0.287 times any number of months to find out how many events might happen to 1 person in that time.",3,wn2wl8,"I am having trouble interpreting these research results and would love some help. I don't have a research background and knowing this answer could make a big difference in my life.

""The rate of transmission was calculated as the number of transmission events divided by the number of person-months of exposure x 100 and expressed as the rate per 100 person months""

I am trying to figure out a way to conceptualise ""28.7 per 100 person-months of exposure""

Is there any way to convert this into transmissions per month or percentage of transmitting per month? How?

Any help would be massively appreciated",statistics,2022-08-12 19:22:46,10
"I have done both and taught students doing both. If you can afford to do it and learning is a high priority for you, go to school full time, preferably after working for a couple of years. On the other hand, the financial picture can be way better working and going part-time, especially if the employer pays. But you may feel like you are being dragged through and lightly dusted with the material rather than being immersed in it. Consider also the extent to which you would be taught by different faculty and in classes with different people.",12,wmqxuu,"If you had the option, would you prefer to do both at the same time or just focus on grad school?",statistics,2022-08-12 10:27:19,12
"Bit too many to be nominal. They aren't infinite, but I would treat them as interval.",1,wn3xes,"Hi everyone! I have a question regarding the data types of these variables:

1. Are latitude and longitude coordinates nominal or interval?

Appreciations in advance.",statistics,2022-08-12 20:13:59,1
"Perhaps you're looking for the principle of maximum entropy, motivated by statistical physics. This asserts that after accounting for all of the information that you have about a random process, the best probabilistic model is the one which maximizes the entropy functional $H(p) = - \sum p \log p$.

- If you know nothing at all about the system, then the maximum entropy distribution is uniform.
- If you know just the mean of a discrete distribution, then the maximum entropy distribution is exponential.
- If you know just the mean and variance of a continuous distribution, then the maximum entropy distribution is normal.

Often it is possible to deduce nontrivial convergence theorems by studying how entropy interacts with limits; for example, you can prove the central limit theorem by showing that entropy monotonically increases to its maximum along any sequence of averages of iid random variables.",5,wmtzhf,"I’m specifically referring to physical interpretations and *not* philosophical interpretations. 

What I mean is can we explain physically why certain systems have certain probability distributions? The only distribution that could be explained with a physical description of whats going on seems to be uniform distribution. But why is something like the normal distribution so common in random events? Is there a reason so many systems converge to it? Is there some physics behind it? And what about the other probability distributions?

Intuitively, it feels like there should be a single probability distribution that occurs naturally, perhaps due to some property of the universe (much as physical constants like the speed of light are). And that all the other distributions are some transformations of this naturally occurring distribution which can also be rigorously mathematically defined. Instead, a lot of them seem to either be entirely empirical or just the application of some function/curve that fits best with the observed data .",statistics,2022-08-12 12:40:10,5
t test for 2 independent means. The rating scale can be considered a continuous variable. It's common practice.,3,wmta0i," Hi guys! I have two independent groups that I am trying to compare. Each group consists of people's ratings (1-9) for 20 videos based on how much the video is liked. The difference between the groups is that they have flipped/reversed scale labels. I would like to see whether there is a difference in mean ratings between the groups (both for all videos, and for each video separately). Which statistical test(s) should I use?

I was thinking doing an ANOVA-type test but I am not sure if this will work since the scale is made up of integers, so I'm not sure if it fulfills the continuous dependent variable requirement? Would the necessary test be parametric or non-parametric in this scenario?",statistics,2022-08-12 12:08:32,7
t test,0,wmy1m9,"I want to see if the mean between two different answers on a questionnaire is significantly different (same population). Answers range from 1-5, which test should I use?",statistics,2022-08-12 15:34:25,13
"Probability. Solid knowledge of basic statistical inference. 

 Depending on the mix of theory vs application,  linear algebra,  and probably calculus ... and possibly some familiarity with complex numbers (for roots in/on/outside unit circle stuff) sequences and series manipulations (for playing with ratios of polynomials in the backshift operator), good knowledge of regression would help as well.",2,wmtyze,"Hello,  
I'm starting a master's in financial engendering very soon. I have a class on time series econometrics, I've put the description below. What subjects or books could I read to prepare for this class?

Description:

The goal of the course is twofold: (1) develop a comprehensive set of tools and techniques for analyzing various forms of univariate and multivariate time series; (2) show how to use econometric software such as Matlab or RATS to estimate time series models. The topics include stationary univariate models (ARMA), non-stationary univariate models (ARIMA), regime-switching, multivariate time-series models (VAR and cointegration), and conditional heteroscedasticity.

Thanks,",statistics,2022-08-12 12:39:34,5
"If you want to do a PhD in stats, DS or CS are not the best preparation compared to stats.",5,wmeh7j," I feel like I am making a very risky pivot by applying for Statistics instead of MS DS or even MS CS. My profile is suited for them, but I think I should prefer a complete and applied/foundational Statistical background before transitioning to a PhD(most likely). I believe that Stats would be my best bet when I am planning to pursue interdisciplinary research.

Relevant Maths-courses :

1. Discrete Mathematics: Sets Relation and Logic, Combinatorics, Graph Theory and Algebraic Structures
2. Signals and Systems: Continuous and Discrete-time Signals and Systems, Analysis in time domain - Laplace and Z Domain, Frequency Domain Analysis
3. Mathematics I : Series (Finite and Infinite) and Convergence, Multivariate Calculus
4. Mathematics II : Complex Analysis, Ordinary Differential Equations, Partial Differential Equations
5. Mathematics III: Linear Equations, Vector spaces, Fourier Series, Partial Differential Equations and applications

I am currently a part of a study group and trying to self-study ""Probabilistic programming"". Is it just imposter syndrome to think that my chances of getting in with this background would be low? Considering other people would be from pure Math/Stats backgrounds? Not insinuating it to be a bad thing, it would be awesome to have such people around me, because they could guide me!

Also, Do you think the MS Stats/Applied Stats are more uniform when it comes to the curriculum across various universities? I am primarily looking at UC's. Are there any suggestions as to which of them has a better curriculum?

Thanks a ton in advance for your guidance!",statistics,2022-08-11 23:54:50,16
"Have a look at the Diebold-Mariano test - it's a test for comparing whether the predictive accuracy of forecasts is different.

A simpler but less rigorous method is to just take the mean Brier score of each forecast method at each lead time and then compare these averages to see which is better",2,wmm9p0," I've been tracking forecasted precipitation and actual precipitation to see the accuracy of different forecast services. I'm using Brier score (lower is better) grouped by hour ahead to see the forecast accuracy. Here is an example of the results for 3 different forecast services: [https://imgur.com/sCufK4h](https://imgur.com/sCufK4h)

 I have a hypothesis that the purple one is more accurate because it's more conservative in its forecasts and since it doesn't rain very often the Brier score is better for that service. For example, it may constantly show a 10% chance of precipitation, where the other service might show a 0%, then a 70%. Below is a sample of my data. How would I go about using statistics to try and prove/analyze this hypothesis? Any help is greatly appreciated. 

**Example Raw Data:**

&#x200B;

|Service|Hours Ahead|Forecast Precipitation (pct)|Actual Precipitation (amt)|Brier Score|
|:-|:-|:-|:-|:-|
|A|0|.10|.3|.81|
|A|1|.10|0|.01|
|A|2|.1|0|.01|
|A|3|.1|0|.01|
|A|0|.1|0|.01|
|A|1|0|0|0|
|A|2|.15|.1|.7225|
|B|0|0|.3|1|
|B|1|.7|0|.49|
|B|2|.6|0|.36|
|B|3|0|0|0|",statistics,2022-08-12 07:11:35,2
"I can't provide more exact guidance without knowing more about the model you are trying to estimate. I do can tell you that, generally speaking, if you are estimating a fixed effects model then the whole point is to render the effect of any time invariant covariates mute. So yes, the software will drop any variables which do not change over time. If what you want is to model the influence of such time invariant variables on your time-changing outcomes, then you are looking for a mixed model approach (a.k.a. hierarchical linear model, multilevel model). While a fixed effects model provides each unit with their own intercept, a mixed model is way more flexible and allows you to e.g., model an intercept and a slope for the level-2 covariates (e.g., time invariant) too. Take a look at this resource to learn the intution behind this:

[https://towardsdatascience.com/hierarchical-linear-modeling-a-step-by-step-guide-424b486ac6a3](https://towardsdatascience.com/hierarchical-linear-modeling-a-step-by-step-guide-424b486ac6a3)",2,wml1zb,"

Hello dear academicians and students,

I have been trying to estimate a gravity model of trade using the fixed effects method of panel data analysis. However, I can not perform the regression because of the distance variable (distance between countries in KMs).

in Eviews it gives a ""Near singular matrix error""

in Stata the distance variable is automatically omitted from the analysis.

When I look at the collinearity matrix non of the other variables seem to be fully corelated to distance. there is some level of correlation but not high enough to cause a problem.


I assume this problem is due to the fact that distance between countries doesn't change through time and always stays constant. Is there a way that I can make the distance variable dynamic through time without hurting my analysis. or can I bypass these mechanisms in the statistical programs I have mentioned.

Please help.

Thank you very much.",statistics,2022-08-12 06:16:52,5
"Usually, a bit of domain knowledge and the ability to use your head is enough. For example, is people's height normally distributed? Well, normal distribution has support from negative to positive infinity. Is it possible for a person to have negative height? Or be infinitely tall? Of course not, so height cannot possibly be normal.


However, that doesn’t mean the normal distribution can’t be useful for modeling heigh. We don’t use statistical models because they are exactly true, but because they are useful approximations. So the question you should be asking isn’t *""Is this assumption violated?""* but *""Is this assumption close enough for my purposes?""*. To answer the second question, a combination of domain knowledge and diagnostic plots/tools is used (e.g. plotting the distribution of residuals as a histogram).",10,wmclo1,"I’m a noob and struggling with figuring out when to conduct assumption tests (Shapiro, Wilcox, etc.) 
This confusion came to the surface while trying to come up with a phase/gate flow chart to guide my learning.",statistics,2022-08-11 22:04:36,3
"The most straightforward way is to extend binary logistic regression (which is typically used for two party systems) into [multinomial one](https://en.wikipedia.org/wiki/Multinomial_logistic_regression).

If you want to do it Bayesian style, through MCMC, you can use R package brms, which [provides](https://paul-buerkner.github.io/brms/reference/brmsfamily.html) multinomial/categorical family.",3,wm9qt9,"Let's say I have a dataset with multiple polls from a multi-party electoral system taken overtime with diferent sample sizes and I want to use them to forecast a future election (based solely on polls, no economic/prior elections data)

Although there are many resources online on how to model a 2 party system, I can't find anything related to a multi-party one

What should I do to make a probabilist model that returns me the probability of each party having a given %?

I'm aware that first I must build a distribution given the data (which one should I use? I was thinking about a dirichlet distribution, but I'm not sure about this choice)

Next I know I have to run a MCMC some thousand times, but this step gets me real lost on how I should proceed.

Can anyone help me?

Thanks in advance",statistics,2022-08-11 19:40:37,1
"I think your question stems from a sort of confusion because GP and normalizing flow are not really comparable. They're different things. A Gaussian process (in Bayesian statistics) is a prior on an unknown function space. Of course, by definition, a finite collection of indices will follow a multivariate normal distribution. On the other hand, normalizing flow is a technique used in variational Bayes (or variational inference) to relax the restriction on the family of distributions for your variational approximation. Typically, you'd go for mean-field (i.e., posterior independence) or structural VB. In either case, your approximating distribution is limited to something like a multivariate normal or a product of known distributions. Normalizing flows can help relax this.

You can use GP and normalizing flow together.

TL;DR;
GP is a prior distribution. Normalizing flow is an approximation technique.",7,wm058v,"Hi all,

For those who have dabbled with both, I was wondering if I could get a high-level take on the pros and cons of using normalizing flows vs. Gaussian processes for generative purposes in performing Bayesian inference of state space model parameters conditioned on time series data. In the past decade, there's been some decent demonstrations of both, such as [this case](http://mlg.eng.cam.ac.uk/pub/pdf/TurDeiRas10.pdf) for GPs and [this instance](https://arxiv.org/pdf/1910.00879.pdf) for normalizing flows.

At cursory glance, they seem similar in frequent employment of multivariate normal distributions. For the fixed base density that normalizing flows begin with to satisfy the reparameterization trick, one can use multivariate normal distributions that are then transformed layer by layer (or continuously in the case of neural ODEs/SDEs/dynamical systems etc.), whereas one sees multivariate normals parameterizated through kernel functions in GPs. My gut instinct is that GPs can be slightly simpler, more stable, and less memory intensive with fewer hyper parameters to tune, while normalizing flows (inverse autoregressive flows in particular) are more flexible and expressive while still having easy log density estimation.",statistics,2022-08-11 12:36:44,13
"I don't see any plot. But generally, no, the upper and lower whiskers are not supposed to be the same length.",3,wmeh32,,statistics,2022-08-11 23:54:37,5
"No Bayesian courses?

Regression and Categorical Data Analysis for starters anyway",1,wm9qfg,"(Repost but w more information) Title, but more specifically, what topics will give me the best all around knowledge of stats? My school unfortunately does not offer too many stats courses, but what would you convey are necessary courses to take (list below) Thank you! I am required to take 5 core courses: * **Real Analysis** * Introduction to measure theory and Lebesgue integration, and their application to probability theory. Monotone and dominated convergence theorems, Fubini’s theorem, Fourier analysis and Banach spaces. * **Topology** * Metric spaces, topological spaces, compactness, completeness and connectedness. Introduction to function spaces, with emphasis on the uniform topology * **Regression Analysis** * General linear model in matrix form, simple and multiple regression analysis, transformations, variable selection, multicollinearity, analysis of variance, robust regression, logistic regression, principal components and factor analysis. Statistical software utilized. * **Theoretical Statistical Analysis** * Hogg-Craig Theorem, Cochran Theorem, convergence in probability and distribution, Cramer-Wold Theorem, Continuous Mapping Theorem, Weak-Law-of-Large-Numbers, Lindberg-Feller Central Limit Theorem (CLT), Lyapunov CLT, Regular Exponential families, Neyman-Factorization Criterion, the substitution principle, asymptotic relative efficiency, the method of the moments, the MLE and its asymptotic efficiency, Uniformly Minimum Variance Unbiased Estimation, Rao-Blackwell Theorem, Lehmann-Scheffe Theorem, minimal complete sufficient statistics, ancillary statistics, Basu Theorem, confidence regions, likelihood ratio tests, Wilk’s Theorem, Neyman-Pearson Lemma, Uniformly Most Powerful test, Karlin-Rubin Theorem, other large-sample tests. * **Multivariate Statistics** * Multivariate normal distribution, multivariate data analysis, inference about a mean vector including Hotelling’s T2 and the likelihood ratio statistic, Wishart distribution, MANOVA, classification and discriminant analysis, principal components, factor analysis, canonical correlation, multidimensional scaling. Applications and use of statistical software And then I take 5 elective courses (here's a list of all of the electives offered, some are just rarely offered) * **Nonparametric stats (which I am enrolled in for next semester)** * One, two and K sample location methods, the histogram estimator, kernel density estimation, the choice of the smoothing parameter, other density estimators: orthogonal basis, penalized maximum likelihood, nonparametric regression: Nadaraya-Watson, choice of smoothing parameter, k-nn, splines, bootstrap. * **Stochastic Processes** * Markov chains, first step analysis, recurrent and transient states, stationary and limiting distributions, random walks, branching processes, Poisson and birth and death processes, renewal theory, martingales, introduction to Brownian motion and related Gaussian processes. * **Probability Theory** * Operations on sets and events, ‏‏‎ sigma algebras, probability measures, Lebesgue measure, measurable maps and random variables, independence, Borel-Cantelli lemmas, zero-one laws, integration with respect to a probability measure, convergence theorems for integral, product spaces, and Fubini’s theorem. Laws of large numbers, convergence in distribution, and the central limit theorem. * **Time Series Analysis** * Time series, stationary and nonstationary time series models, seasonal and nonseasonal time series models, trends, ARIMA (Box-Jenkins) models, smoothing methods, estimation, diagnostic checking, forecasting techniques, spectral domain, periodogram, filtering, spectral density. * **Categorical Data Analysis** * Inference and measures of association for categorical data, generalized linear model, logistic and Poisson regression, logit, probit and loglinear models, analysis of matched pairs. * **Statistical Computing** * Methods for generating random variables, Monte Carlo methods, Monte Carlo Integration and variance reduction, bootstrap and jackknife, optimization and solving nonlinear equations, EM algorithms, Fisher scoring method, and Markov Chain Monte Carlo methods. * **Topics Course** * it seems they rotate which course is offered: * Bayesian stats, linear models, functional data analysis, advanced probability",statistics,2022-08-11 19:40:09,1
"No, its more than likely your underlying code is slow. You should be vectorizing everything except the actual iterations. And especially so in R you should not use loops (but this is the case for Python too-you should be leveraging numpy arrays as much as you can). 

Hard to say much without seeing the code and equation together",56,wlnxfu,"Hello guys,

I am currently running an MCMC algorithm in R and it's taking way too long that I'm thinking about running it on Python instead. I just want to know if anyone actually noticed any time difference before I switch to Python.

Thanks!

&#x200B;

EDIT: 

Just to be clear, I'm not talking about packages. I implemented my own Gibbs sampling with M-H step.

I also wanted to ask how you deal with Pi product within the conditional distribution. I calculated it by using for-loop and I think it's one of the reasons that is slowing the whole process. Anyone knows how to effectively deal with it?",statistics,2022-08-11 03:19:26,25
"in my own self study + with recs from friends of mine in Applied Math and Physics PhD programs, it was harped on me that the best intuitions for statistics are taught in other areas of math, specifically calc 3, linear algebra, and real analysis. In my self-study of stats, I found working through Khan Academy’s Calc 3 course, Gilbert Strang’s ""Linear Algebra is for Everyone"" + his Lin Alg 1 MIT lectures on youtube, and Victor Bryant’s Yet Another Introduction to Analysis is especially useful. if you’re looking for ways of linking all of those to something like applied social science research, Nick Huntington-Klein’s The Effect was nothing short of amazing — it’s essentially a causal inference textbook that explains all the intuitions behind them and includes models in R and Python, geared specifically toward social science research design methodology! It was published last year so it’s comparatively very very modern.

I have 3/4 of an undergraduate degree in EE and physics, and degrees in anthropology and philosophy and I’m currently doing a MS in Econometrics. I def took calc 3 and linalg in undergrad but the intuitions weren’t taught really, so going back to review was especially useful.",10,wlq8dt," I am interested in self-studying statistics and have an engineering background, having taken classes in probability and random processes. Many of the books I have found are either recipe-oriented, geared to biologists, psychologists, etc. or are proof-oriented like Casella/Burger. I can handle the math but am missing the intuition behind it (which I believe to be the more subtle and challenging part of statistics). As an example of the types of questions I would like to answer, when should Fisher's exact test or the Chi-square test be used and _why_?

Do you have any suggestions of references for self-study? I found some of Fisher's original papers online (Design of Experiments) and found them readable but am looking for more modern treatments. Thank you!",statistics,2022-08-11 05:30:31,5
"I'm not a fan of the two answers you've already gotten here. T-testing is meant for continuous data, whereas neither the counts of responses nor your rating of 1 - 5 are ""continuous"". Counts are nominal and rating is ordinal. Expressing your results through a t-test is just an awkward way to make your point.

Absolutely, the chi-squared test of independence is specifically designed for a question like this. Keep in mind that the test is just giving you the binary result of ""yes there's a difference"" vs. ""no, there's no difference"". So if you had, say, 5 different age groups and you plugged all of that data into your statistical test, there could be a difference in how many rated 2 in one specific age group and that would result in a ""yes, there's a difference"" result, even if all the other comparisons showed no difference. If you're interested in more specific differences, like is there a difference between the 20-50 age group vs the 50+ age group in the scores of 1 and 2, you can definitely do that, but you'd just use the data for those groups only. Hopefully that makes sense.

You can also generate a mosaic plot from a chi-squared test which is a nice visual showing significance. If the tiles in the mosaic are clearly sized very haphazardly and differently, it's a nice visualization of a statistically significant difference.

Keep in mind also that it's best to have at least 5 data points for any cell if you're using a chi-squared test. You have N = 86 so you may need to watch out for this. Results are considered somewhat unstable if you have any counts fewer than 5, so if you barely declared significance, you may have to concede that your results are unreliable.",6,wlzld0,"I conducted a questionnaire and want to look at if there’s a significant difference in how to different age groups answered a question. The answer is ranged from 1 to 5 and 86 respondents answered. I’m not sure how to go about this, any help would be greatly appreciated. Thanks!",statistics,2022-08-11 12:13:40,17
"It's because the sampling distributions and defining effect size can be challenging. Simply put power analyses don't exist for complicated experimental designs in closed form (e.g. a formula). 

Instead, you might make some simplifying assumptions about what you'd expect to see and simulate it. Still if you're conducting work in a new area defining the simulation settings can be a circular exercise in itself.",5,wlva03,"Okay, I am easily locate a calculation for a simple t-test using alpha, power, and effect size to generate needed sample.  Can do the same for correlation.

What I don't understand is what when your study is much more complicated than a single comparison (which is pretty much every time).  Or when I'm using a much more complicated multivariate analysis (which is pretty much every time).

What am I missing?

EDIT:  Perhaps I should be more explicit.  I am running a pre-post analysis with two treatment groups and a control group.  I will be comparing them on three outcomes (knowledge, confidence, and skill).",statistics,2022-08-11 09:12:52,2
"In what context are you standardizing the distribution?

There's a *historical* reason for standardization: Probabilities were calculated just for the standard normal and printed in a big table. You could just use transformations to get probabilities or quantiles from any scaled or shifted normal. That's not really a factor today with the easy access to computers.",36,wlf8qk,I already have a normal distribution anyway?,statistics,2022-08-10 18:55:54,14
"For it to *actually* be normal, the population size must be (uncountably) infinite and there is no largest value in the population (the support is not bounded; there's always some proportion of it beyond any specific number).

Conversely if the population is finite it cannot actually be normally distributed. You might be able to treat a finite population as if it were like *a sample* from a normal distribution (and if the model were correct you could perhaps estimate its size that way) but the finite population distribution itself would be discrete.",4,wlvark,"I have all the points on the right side of a normal distribution, is there a way to estimate its size (population)? This is the data: [https://docs.google.com/spreadsheets/d/e/2PACX-1vQiAWVYP8frDCBBcmG868wDo7Vuga81xup2p0QBW1-opMNGnIquYsc5dsHPUuX1UyVo7-SFbPQuP5ZG/pubhtml?gid=0&single=true](https://docs.google.com/spreadsheets/d/e/2PACX-1vQiAWVYP8frDCBBcmG868wDo7Vuga81xup2p0QBW1-opMNGnIquYsc5dsHPUuX1UyVo7-SFbPQuP5ZG/pubhtml?gid=0&single=true)",statistics,2022-08-11 09:13:47,19
"Chi-square pdf and cdf are only defined on non-negative numbers.

The F distribution F(i, j) is the distribution of ratio of two independent, chi-square distributed random variables, each divided by their dof, i being the numerator and j the denominator.  To make the discussion simpler, let's assume that we realize this draw from the F distribution by drawing m,n from chi-square distributions with dof (i,j) and divide them out by i and j.

The cdf, F(a, i, j) is the cumulative probability that such a ratio m/n is less than or equal to a.

Let's consider n/m.  Well, this should be distributed with an F distribution with F(j,i) degrees of freedom.  Lets consider the cdf of m/n: for a particular m, n, the probability of m/n >= a is the same as the probability of n/m >= a. Or, 1-probability[n/m <= a].

The 1-a comes in because when you reverse the position of your two chi-square samples (which are both non-negative), a large F-value will become a small value and vice versa.  That is the reason for the 1-a.",2,wlv3av,"Can someone point me towards an explanation of why in the F-distribution, F(a,i,j) = 1/(F(1-a,j,i))?  
Also, I would intuitively think that this rule should to some extent be applicable to chi2 distributions. However, I can't figure out how. Does someone know?",statistics,2022-08-11 09:05:13,3
"For ML, you pretty much need it all.  For reinforcement learning techniques, at least, there's no known upper bound.  Every time they expand the training, results improve nontrivially, which is why we get enormous AI projects like GPT-3 and DALL-E.",5,wlikaj,,statistics,2022-08-10 21:42:51,9
"Not only will you need to (find someone to) do the stats, you'll likely also need to graph it and understand it and describe your methods and understand them to correctly draw your conclusions. This is no small favor you're asking and you're doing yourself a disservice by doing so",9,wm2sv4,"Basic statistics in program Statistica 13.0

I am writting my final thesis for University and the title is Comparision of time, accuracy and usability of measuring the range of thoracic spine movement with bubble goniometer and mobile application. I did not choose my title so I'm doing it with little less passion and I hate fact that statistics are involved.

I measured 10 students, 4 movements on each student with 2 different measuring devices. I think it's easy for someone but I don't know a thing about Statistics and if there is a way results and graphs and comparison can be done in half an hour to someone else for me it would take alot more first to understand how program works.


If there is a kind soul I can even paypal some money.

Thanks in advance.",statistics,2022-08-11 14:28:09,2
"The actual answer is contained in the trial’s raw data. If you could get access to this, it would be easy to compute.",2,wlpvun,"I think I have a pretty close guess, but not confident in the approach I took and hoping someone has a better approach.

Basically, I'm looking at some results from a clinical trial and trying to use what they reported in the paper to inform how I design mine. In this trial, 2499 participants were randomized to two experimental groups, and they basically planned to follow-up with participants every 12 weeks for as long as possible, up to 144 weeks. The final dataset they analyzed successfully followed all randomized participants for a total of 3324 person-years ""with a variable duration of observation."" But, the median participant was followed for 1.2 years, with a maximum of 2.8 years (\~144 weeks).

I'm trying to estimate **what percentage of participants in the trial were successfully followed for at least 1 year,** assuming the latest follow-up times for participants were roughly normally distributed. I just generated a small dataset with median=1.2 and max=2.8 and then found the percentile associated with 1.0, so \~60% made it to 1 year. Is that reasonably accurate?",statistics,2022-08-11 05:12:13,4
"I would check this out: https://arxiv.org/abs/2008.05756?context=stat

F1 score is a pretty easy metric to use. There's no simple version of AUC that works for multiclass problems.",1,wli8yt,"  I created two different types of models from the same dataset to see which one performed better. I’m just not sure if I can actually do an AUC for the ordinal logistic, or if that’s even the best way to compare them?",statistics,2022-08-10 21:25:14,1
"The industry is about 80% PhD and 20% MS. A PhD in statistics or biostats would be your best bet. Undergraduate, mathematics or analytical discipline.

Internships are very important, especially to know if you are interested in the job. However finding a relevant one in undergrad will be tricky, most of our interns are PhD candidates. There are other analytical internships and public health internships that could help get into that PhD program.",9,wl443k,"I’m curious as to what you would go to school for, as well as some extracurricular activities I could do, such as a project or internship as a high schooler. Thank you!",statistics,2022-08-10 10:59:04,7
The answer depends on how you collected your data! Did you use only multiple choice questions/ drop downs (which gives you categorical variables)? Or do you also have ones where they typed in a numerical answer(a quantitative variable)?,1,wlj7sa," 

Hi there. A complete statistical novice here, but with a huge desire to learn!

I have a data set gathered through Google forms. Basically it data from an online survey with close-ended questions, measuring if people from my country would consider emigrating abroad.  
The first couple of questions were to determine their gender, age, education level and place of residence. Then the questions were more migration-related, such as if they would consider emigrating, etc.

I want to find out if there is a relationship between different variables. For example I want to find out if more men or women answered ""yes"" to my question about considering emigrating. Or, if people with higher educational attainment more frequently answered ""yes"" or ""no"".

I am not asking for a tutorial how to do it. I would like to find out if this what I need is more correlation analysis or cross tabulation analysis, because I didn't really get the difference even though I have just spent 2 hours watching youtube tutorials.

Also, do you think that regression analysis would be of any significance here? Thanks a lot!",statistics,2022-08-10 22:19:58,3
"You would need transaction data for both loyalty members and non loyalty members, and the ability to link transactions within customer over time.  Credit card transaction data would do that.  

Randomly assign customers to different schemes of tiered rewards, like double points on Wednesdays, $0.03 cents off vs $0.10 off as a minimum discount, email/text notifications of flash sales.  If you can link those treatments to the specific customers and then vary the discounts over time, ANOVA and multiple regression can be used to derive lift metrics.  

You could also target encode the various schemes and fit it all with a GBM, and then do post hoc graphs to illustrate the effect sizes.",4,wlboyi,"Statistically, is there a way to attribute the sales increase from tiered rewards program?

First off, I feel like I know this answer to this (I need more data), but I’m new to stats and thought you all might have some thoughts; even if it’s “you can do it this way if you have this data.” 

Let’s say a company has a tiered rewards program that offers discounts on products based on how much a person spends in the store per month. For the sake of this, let’s say how much gas they buy is what is being tracked. So the more gas they buy, the better the rewards.

I’m trying to come up with a way to determine how many extra gallons of gas was purchased as a result of the tiered program.

Some of it seems easy. You simply track people who bought gas at your store before the program roll out and compare it to what they are buying now. But is there a way to account for the likelihood that some customers are traveling more? My assumption is no, unless we have that data.

Also, what about new customers? Maybe they just moved into the area and we’re going to buy gas with you anyway, we really can’t attribute every gallon To the tiered program.

Would love some ideas on statistical methods, if any, or just ideas of how I could address this problem  or even “you’re wasting your time.” Thank you.",statistics,2022-08-10 16:07:38,1
"The mean of the means is the mean. 

That is, the long-run average of sample means is the same as the population mean.

That is, the center of the bell curve that is generated from plotting observed sample means will be the overall mean or expected value from the population. If E[X] = 3.5, E[X_bar] = 3.5.

So if the average tiger weighs 200 kg, the average weight of a sample of 20 tigers should tend to 200 kg.",20,wkw05x,"In the example [here](https://www.youtube.com/watch?v=JNm3M9cqWyc) with the odd but symmetric discrete distribution, is the mean of the sample means also expected to end up as 3.5?",statistics,2022-08-10 05:20:12,19
"No, a VECM can have less than k-1 cointegration relationships, which probably still include all variables. So in your case, I would try a VECM with rank 2. You can also add exogenous variables in VECM estimations and even a MA dynamic, but the later is probably not implemented in your software. I would also try a VARMAX estimation and compare the result with the VECM. 

Just for completeness:
rank = 0 -> no cointegration, a VAR of the first differences is recommended 
0<rank<k ->(all) variables are cointegrated, use a VECM with lowest not rejected rank
rank = k -> no cointegration, a VAR with data in levels is recommended",3,wl5392,"I’m trying to forecast sales for a particular product. The product consists of four sub-products (Y1 Y2 Y3 Y4) that I would like to model together along with exogenous economic variables. I was hoping to use Varmax to do this but after running Johansen's cointegration test on the levels of Y1 Y2 Y3 Y4 I’m no longer sure if this is the best path.

In order to proceed to VECM does rank k-1 (rank=3 in this example) need to be the first HO to fail to reject? Or put dumbly, do all variables need to be cointegrated in order to proceed with a VECM? I’m less familiar with VECM and would prefer Varmax but not sure If that is permissible with the results below.

 
The results of the Johansen cointegration test are shown below:

H0: Rank=0: P=<0.0001: Rejected

H0: Rank=1: P=0.0072: Rejected

H0: Rank=2: P=.5238: Fail to Reject

HO: Rank=3: P=0.0347 Rejected

 ",statistics,2022-08-10 11:37:47,2
"> However, the two continuous variables can be put into bins and therefore could be categorical.

I don't have much feedback on the rest of it, but do not do this! Binning a continuous variable and treating it as categorical is just throwing out information--you are trashing the exact values of the continuous variable and replacing them with a less precise label (which is often also harder to analyze!). Unless you believe the underlying concept is *truly* categorical there is *no* reason to do this. I have seen so many crappy analyses where the researchers binned for no reason and lost a ton of descriptive power as a result. 

Easy example: Research paper investigating the effects of long work hours on mental health. The researchers binned everything under 45 hours/week into a ""low work hours"" category... thus lumping people who work full time in with people who are completely unemployed, and making it impossible to tell whether there is an amount of work hours under 40/week that is optimal for mental health. It's just silly.

I agree with the other commenter--start with some simple analyses and go from there. If you have covariates available, think about whether you would *expect* any of the covariates to have a relationship with your dependent variable, and come up with a few planned analyses to investigate these hypotheses and your main research questions. Then once you have the data, start with a broad exploratory data analysis and develop further from there in collaboration with your profs/co-researchers.",5,wkxx31,"I'm currently doing a project where there are many ways to approach it. At it's root we want to see the affect of a continuous variable on two different variables; one continuous and one categorical/binary.

However, the two continuous variables can be put into bins and therefore could be categorical. 

Then when I start digging deeper technically one of the 'dependent variables' could be considered an independent variable.

Then there are some other possible covariates where I'm wondering if I should include them in the analysis or simply filter out the ones I'm not really interested in. 

It's just so much to look at and so many ways the data can be analyzed.

How do y'all deal with this? Any advice. It's driving me insane and as someone who tries to be efficient I really can't see a solution other than doing every possible analysis and stressing over the results until I have a report that's as good as its gonna get withing the time I'm allotted.",statistics,2022-08-10 06:47:44,4
">Let’s say we test Drug A (Experiment) & Drug B (Control/Standard) with NHST in equality (comparative) design.  
>  
>If the results show that when comparing Drug A to Drug B the p-value is non-significant, can we conclude that Drug A is similar to Drug B?

You should be very careful of wording here...  A p-value >0.05 (or whatever the alpha level has been set at) does not mean that you can conclude that Drug A and Drug B are similar.  It means we don't have enough evidence to say that they are different.  This is a very important distinction.

There are several reasons why you may have a non-significant p-value.  Maybe the sample size was too small or there is too much variability between subjects - either of these can give you a  non-significant p-value even if there really is a difference between drugs A and B.

Not having evidence of a difference is not the same as having evidence of no difference.

I use the analogy of a criminal trial a lot for hypotheses and p-values.  Lets say you are a juror- you are supposed to assume that a subject is not guilty (our null hypothesis). If the prosecution provides enough evidence to the contrary then you reject 'not guilty' as a possibility and as a result find them guilty.  However, if you do not think enough evidence to the contrary has been provided then you have to find the subject not guilty - that doesn't mean that you are convinced that they are innocent, only that there is not enough evidence to be sure (beyond a reasonable doubt) that they are guilty.

Edit - formatted quoting incorrectly",4,wl5q6v,"I have a hard time trying to interpret p-value in biomedical field and need you guys to confirm my understanding.

Let’s say we test Drug A (Experiment) & Drug B (Control/Standard) with NHST in equality (comparative) design.

If the results show that when comparing Drug A to Drug B the p-value is non-significant, can we conclude that Drug A is similar to Drug B?

I happen to see this kind of interpretations all the time in medical journals. They conclude the similarity when p-value is non-significant. Also in the Table 1 of most papers often give p-value of >= 0.05 to show that the patient characteristic is similar.

My understanding is that when p-value is non-significant, it only means you cannot reject null hypothesis. Thus the result is inconclusive. 

So what do you suggest when interpreting non-significant p-value?",statistics,2022-08-10 12:02:50,7
"No, this rule applies to normal distributions and in the case of a t-distribution is only true in the limit (i.e. very large n)",16,wkw0u2,,statistics,2022-08-10 05:21:07,14
"Dummy coding, creating k-1 binary variables for each categorical variable with k levels.  Then stepwise the dummy codes into the model.

You can also target encode the categorical variables and then run a GBM using the h2o package in R.

Thank you for listening to my TED talk.",2,wl4a97,"Hello,

Im working with some ecological data. Im trying to create a linear model to describe forest gap sizes based on some categorical predictors. Im not the best with stats. 

I have three categorical predictors Soil drainage, Canopy Class, Species Mix.  Each of these has multiple levels. My response variable is a continuous variable called ""Gap Size"".  My issue is that whenever I model this with lm() in R, I find that it violates every regression assumption. Non-constant variance, normality of residuals, etc...

As i understand it, I can transform my response variable ""Gap Size"" to attempt to fix these issues. However, my advisor tells me this will destroy the relationship/order of magnitude. Im kind of at a loss at how to approach this issue. Are there other ways to model this that don't have these assumptions of linear models? 

Any help, guidance or links to literature to explore would be great! Thanks :-)",statistics,2022-08-10 11:05:35,6
"This is really a question about cognitive heuristics.  The assumption that humans (especially as todlers) make our decisions based on large amounts of data is not really accurate.  

We typically make our decisions based on a few use cases, and we make generalizations about the observed scenarios.  Sometimes they are accurate, sometimes they are not.  When they are not they lead to errors (cognitive biases).",72,wk6eao,"The Fisher information for the exponential family generally prescribes big amounts of data in order to achieve good estimation of the parameters. Generally we have that the variance of the estimator is something among the lines of 1/n, which translates into big amounts of data.

This means that if humans are statistical learners, the uncertainty will usually be very high whenever we learn something. But that is not the case since toddlers are making confident decisions.

&#x200B;

Hence, my question is: how are humans able to learn in a statistical manner if that is the case?",statistics,2022-08-09 08:35:19,20
If 200 people are being bit by 200 different pit bulls or something relatively close to that number (same for the goldens) then yeah. That’s possibly 20% of all pit bulls showing violence if we’re certain of their total population and the number of nonviolent ones. What would make this interesting would be if a small number of pit bulls (say 10) are racking up 200 attacks a year.,8,wkjq8p,"Say, for the sake of the question, there are 1000 pitbulls in the world and 1000 golden retrievers in the world. Also say, every year, 200 people get bit by a pitbull and 50 people get bit by a golden retriever. Would it be statistically accurate to say, in a given encounter with a dog, you are more likely to get bit if that dog is a pitbull (as opposed to a golden retriever)? In other words, would an objective statistician (who knows this data) be more afraid of petting a pitbull than a golden retriever?",statistics,2022-08-09 17:50:16,5
">We first did a univariable logistic regression with the biomarker which found no association, so I then tried doing the multivariable with other data we have like demographics and other markers. This is because I read sometimes a nonsignificant univariable variable can be significant with multivariable.

That's true. P(Y|X) = P(Y) does not imply P(Y|Z,X) = P(Y|Z).

Here's an example. Suppose smoking (X) causes cancer (Y) and being obese (Z) causes cancer. Also, smokers have far fewer obese people than non-smokers. (Perhaps because smokers know the extra cancer risk of both of these and want to offset the smoking effect).

Then it could be that smoking is not associated with cancer because while smokers have a greater risk of cancer from smoking, they are much less obese and the non obese have a lower chance. It could cancel out.

But if you add being obese to your multivariate regression, you would see that smoking now predicts having cancer.

Do you have such an explanation for why in an univariate regression you find no relationship but in a multivariate you do?  Does it make sense? If you don't, your audience is going to believe that you tried models until you found one set of variables within which your variable seems to have a relationship with the outcome, but this doesn't generalise to the population. It´'s Just p hacking. And they'd be right, because as the example suggests, it is rather unusual to have unconditional independence but conditional dependence.",31,wkb22k,"For context, I am a medical student doing a project with a physician and a statistician. 

We are attempting to understand if a biomarker can predict an outcome (based on prior research) using a multivariable logistic regression. We first did a univariable logistic regression with the biomarker which found no association, so I then tried doing the multivariable with other data we have like demographics and other markers, but that also was insignificant. I did this because I read sometimes a nonsignificant univariable variable can be significant with multivariable. Additionally, our theoretical basis for this project is based on using this variable, so I feel like that justifies its continued use.

Nevertheless, the statistician says I should not discuss this in the manuscript. But, again the variable is the whole reason for this project, and its nonsignificant results are really useful to know in this field of medicine.

I do not know what to say to the statistician anymore, I have expressed this three times to the statistician but it seems we just disagree. I would like him to be on the paper with us, but I also need us to move forward and I cannot if he does not approve.

I appreciate your thoughts!

P.S I want to thank everyone who has contributed to this post, it’s all been very helpful and I continue to appreciate new comments!",statistics,2022-08-09 11:39:10,35
"Without any context, you don't.

Try /r/HomeworkHelp?",5,wky04d,,statistics,2022-08-10 06:51:22,6
"Familywise corrections are something TO CONSIDER, not a rule or a ""must"". That said, in some cases, not using them would be pretty stupid. In some cases, using them would be stupid. You really need to think through them on a case-by-case basis, and even then, using them or not will be a decision with pros and cons. 

Personally, I'm sparse in the number of analyses I run, and I'm running them to test strong a priori hypotheses, so I rarely feel familywise corrections are warranted.",2,wkh7c4,"Do you need to do multiple comparisons correction (i.e. Bonferroni OR any other) in the following cases (the cases are not related):

- I am testing a few hypotheses grounded in theory only.  I heard multiple comparisons corrections are really relevant most for very large comparisons I.e. DNA. 

- I have 2 measures of symptom A and 1 measure of symptom B.  These conditions are not linked.  If I test 2 hypothesis of symptom A, and 1 hypothesis of symptom B do I need to divide alpha by 3?  Or can I divide alpha by 2 for symptom A and use 0.05 for symptom B?

- If I have two separate participant cohorts and I want to test them both using two hypothesis.  Do I divide alpha by 2 or 4?  

- I have one cohort but two treatment phases.  I want to test 2 hypothesis during each phase.  Do I divide alpha by 2 or 4? 

I know Bonferr. Is too conservative for most things, no need to mention that. 

Thank you!",statistics,2022-08-09 15:52:45,3
"Under suitable assumptions (which i font doubt you can come up with) you will be twice as likely to sample someone from a 2-person house as  a lone-person-house, 3 times for a 3 house and so on.

So the obvious estimator is to halve the 2-houses, divide 3-houses by 3 etc.

Sum N(I)/i

Now that's a good start, ... but


What you miss - of course - are the completely unoccupied houses. You would need some other way to estimate the proportion of those because they presumably contribute to the denominator of the average, unless you're actually trying to estimate the average occupancy not of houses in general but only of houses containing people",1,wkleva,"Got this question in an interview last year and royally bombed it - came back to me randomly in the shower and would like to discuss it here.

The question goes: Suppose you are trying to survey the average household size in a city. You randomly sample N people on the street, and they tell you the number of people living in their house. How do you construct your estimate?

This question is difficult because you are more likely to sample people from larger houses - there will have to be some weighting factor involved in the averaging. How do you quantify this weighting factor? Is there a Bayesian approach to this problem?",statistics,2022-08-09 19:12:15,2
Harmonic mean?,3,wk68b5,If I want to average 3 stats that are not on the same scale sometimes I use the inverse of each number to obtain an average. I forget where I learned to do that and I'd like to know more about it but I can't remember the name of the process.,statistics,2022-08-09 08:28:31,10
"It certainly can be defined that way. It’s an identity, it works using either definition. SSR+SSE=TSS",6,wkh9p3,"So I've been brushing back up on my stats for work and I wanted to get a really solid understanding of R-squared as it is used so often in the finance world.  I understand that it is calculated as 1 - SSR / TSS and that logic makes sense.

What I don't get is, if one is able to calculate each of SSE, SSR, and subsequently TSS, then why isn't the formula for R-squared simplified to R-squared = SSE / TSS?

I have googled this repeatedly, watched countless YouTube videos, and checked across all my stats books and while there's so much information explaining what R-Squared is and how it is calculated, there's no source of information that I've found that provides an answer to this question.  I am sure I am missing something basic here but after countless searches I can't plug this gap in my understanding.  I would really appreciate it if someone could shed some light on this!",statistics,2022-08-09 15:55:41,4
"The context of the study design matters here. For a randomized trial, this is ill advised because the groups are expected to be balanced by virtue of randomization, and any imbalances are due to chance. Testing for differences at baseline doesn't change this, but will erroneously lead to false positives (approx 5% of the time using the traditional alpha level). There is academic literature on this very topic available if you search for it.",12,wjzdaj,"E.g. to show that group 1 is comparable to group 2 regarding age, sex etc. at the start of the study. 

I’ve seen a lot of critique on this issue, but it’s so extremely common.",statistics,2022-08-09 03:02:16,9
"I'm not sure I get it. Are you saying that you as the GM might tell me, the player, ""you need X to beat this challenge"" and I only need one X+ result but I can choose how many dice to spend?

For instance if you say I need a 4, and I roll a single die, that's a coin flip since 3 out of 6 faces are bad and 3 out 6 are good. But if I decide to spend multiple dice and still only need a single 4+, then you model it as the 1 minus the probability of failing to rolling low on each die. So probability of getting at least one 4+ roll is 

1 - (probability\_of\_bad\_roll \^ ndice)   


In our example of roll 2 dice and need a 4 or higher, that's

1 - (.5\^2)  
1- .25  
.75

If you told me I need a 5 or higher and I thought ""oh, that's hard, better roll 3 dice"" it would be (with some rounding)

1 - (.667\^3)  
1 - .3  
.7

If it's not an issue of ""you need at least one high roll"" but ""you need *n* or more high rolls,"" the probability gets more complicated and at that point I'd nope out on the math and do a simulation like this R code:  
`dicepooltest <- function(ndice,threshold,diesize=6) {`  
  `rolls <- sample(1:diesize,size=ndice,replace=T)`  
  `beat <- rolls >= threshold`  
  `sum(beat)`  
`}`  
  
`results <- vector()`  
`for (i in 1:1000) {`  
  `results[i] <- dicepooltest(4,5)`  
`}`  
`summary(results)`",1,wk05bn,"So I've got a game where you have dice challenges. For example, you need to get a 4 (or higher) and a 5 (or higher) and then you wager how many 6-sided dice you'll throw to meet the challenge.

I know how to calculate chances for a single dice *[1 - (x/6)^y]* and I know how to calculate challenge for multiple dice if you throw exact same amount of dice *[(x/100)(y/100)...]*, but I don't know how to get the probability for multiple dice challenge if you throw let's say 4 dices, to get a 2 or higher and a 5 or higher.

Can anybody help?",statistics,2022-08-09 03:48:18,6
"On the first page: they are eliminating any data from the data set where data is missing from at least one column in a row of data. Using a ""complete case"" means only utilizing rows of data where there are no ""N/A""s, so to speak.

Training and test data sets typically refers to using a fraction of the data to create a model, then using the remaining data to test that model and calculate the amount of error associated with that model. Here I just see the person fitting one of each and not doing anything with that, unless I'm missing something in the code?? But typically, you fit a model, you do cross-validation such as this train vs test method, you calculate error from that method, and then you compare that error to other fitted models and see which model had the least error. That is generally considered the best fitting model.

Otherwise, looks like the general gist here is to create a logistic regression model which tells you the probability of an event occurring given certain levels of your input variables. This threshold that the coder is talking about is just the amount of probability at which you'd diagnose one way or the other. If this were, say, about cancer, and a threshold was set at 0.8, then if the model said that certain values of input variables like diet or exposure to chemicals or whatever led to 0.81 or higher in the probability model, they'd say, yes, you have cancer, you need to start treatment for cancer, and if you measured 0.79 or lower, they'd say, nope, no cancer here, move on with your life. The coder here likely has data on whether the person actually has whatever condition it is that you are predicting, and they seem to be trying a lot of different thresholds to see how accurate they really were in their predictions, looking at both the accuracy of correct diagnosis (sensitivity) and the accuracy of correct disease-free diagnosis (specificity).

Hope that helps.",2,wkbbdw,"This is kind of half a coding question and half a ""is this analysis useful?"" question.  So I'm working with someone who is a much stronger coder than I am, and they wrote up some code that is difficult for me to understand. Right now I understand...*some* of the basic syntax and I recognize some statistical terms in there, but that's about it.  https://imgur.com/a/1Nlt44R

I don't strictly need to understand every detail of the code, but I would like to be able to follow the logic of what's happening here, and why.  The code's output is annotated in a way that I can't interpret, and mostly I just need to be able to tell her what would make things more interpretable.  Is there anyone who can offer advice?",statistics,2022-08-09 11:49:20,3
"Certainly they coincide when the observed information doesn't depend on the data (so it is constant, and remains the same when you take its expectation).",3,wk1vrz,"I’m taking a class of likelihood inference. 

Our professor basically explained that, after you calculate the likelihood and loglikelihood function, the negative second derivative with respect of theta (unknown parameter) of the loglikelihood is the observed information, J(theta); while the fisher’s information I(theta) is the expected value of the observed information.

What I don’t understand is under which circumstances, J(theta) and I(theta) are the same.",statistics,2022-08-09 05:20:16,2
"It might just take several months. That's what happened to me and other people I know. It is super depressing. 

I got my first job 6 months after graduation. I had an internship during school though.",16,wjs09l,"Hey all. So, Im currently entering my final semester of my MS in Statistics but have been unable to secure any internship/relevant work experience thus far. It also doesn't look like Im going to get any offers for anything this semester either (I have absolutely no idea how I did not manage to snag anything out of the 200+ jobs I applied to, but that's the way it is I guess). As such, my question to you all is: how fucked am I? I've take CS courses, have programmed some projects, and have obviously dealt with data analysis and such. I'm not really sure what I want to do, but hopefully this semester gives me some direction (I'll be taking classes on Big Data Analysis/HPC, Natural language processing, and maybe quant finance). To those who are/were in a similar position, what did you do? It honestly just feels like I'll just never get a job tbh, despite working so fucking hard for this degree.",statistics,2022-08-08 19:51:06,9
Are there classes that you are required to take?,1,wk1bbh,"Hi, 

I'm a former pure math undergrad that will be enrolling in Masters in Probability and Statistics next fall.

I choose this path, instead of pure math, for healthier job prospects and more applicability to the real world. (And to get more involved in coding, since I didn't get many opportunities in undergrad)

I already have some courses in mind such as grad probability, intro to mathematical finance and stochastic processes, but I wanted to ask you for some areas of statistics that you find really interesting (take in mind I like theory).


I have a list of courses available in the section Probability and Statistics:

Linear Model Analysis	

Multivariate Analysis	

Biomedical Statistics	

Computational Statistics

Mathematical Statistics

Introduction to Mathematical Finance

Statistical Methods in Data Mining	

Probability Theory	

Biostatistics	

Applied Bayesian Statistics	

Reliability and Quality Control	

Introduction to Stochastic Processes	
Time Series Analysis

I can also choose some Phd courses: advanced versions of multivariate analysis, probability and stochastic processess, and advanced statistical inference. (those don't have a syllabus, It's more of an introduction to active topics in research)",statistics,2022-08-09 04:51:25,2
"Yes; this is in essence what dimensionality reduction using PCA does. Dimensionality reduction with this technique does a linear transformation of the data to preserve as much variance as possible, and then you remove the components associated with the smallest loss in variance.

The things we analyze in the case of images will be blocks of pixels, and the transformation here will be the linear mapping from the original data to the principle components back to the data (which has potentially lost information).

This is not the only example.",3,wjq0f1,"Is there a method of [dimensionality reduction](https://en.wikipedia.org/wiki/Dimensionality_reduction) that describes objects (things we analyze) as transformations? More specifically:

1. Splits the object into (2) parts.
2. Describes the object as a transformation of one part into the other. 

**Example 1**

[""Angelus Domini""](https://i.imgur.com/gzzcyY8.jpeg 
) by Jacek Yerka

*For simplicity: imagine that there's only 1 tree and only the stone floor.*

1. I split this place into 2 parts: the tree (1) and the stone floor (2).
2. Transformation: take the floor, split it into the stone plates, rearrange all the stone plates into a line, and blend the borders between the plates... so you get a ""tree"".

**Example 2**

[""Fisherman's Shack""](https://i.imgur.com/an7Jqft.jpeg) by Jacek Yerka

1. Those are more like ""perspectives"", not ""parts"": how we see the place from the side (1) and how we would see the place if we were under it (2).
2. Transformation: no transformation is needed, both perspectives are equivalent, the place always looks like a thin line that can be split into small parts.

**Example 3**

[""Painting 3""](https://i.imgur.com/OEBheaB.jpeg) by Jacek Yerka

1. I split this place into 2 parts: the ""net"" of balconies (1) and the wall with an empty space (2).
2. Transformation: take the balconies and ""holes"" between them, arrange the balconies very densely and put the holes out in the 3D space... so you get a thin ""wall"" and an empty space.

***

I'm also interested if there's a way to make an [ordering/ranking](https://en.wikipedia.org/wiki/Ranking) of such ""transformations"".

I heard about one method where transformations are used. But they're used to represent the space of objects, not parts of particular objects:

[Nonlinear dimensionality reduction, Applications](https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction#Applications_of_NLDR)

> *For example, consider a dataset that contains images of a letter 'A', which has been scaled and rotated by varying amounts. Each image has 32x32 pixels. Each image can be represented as a vector of 1024 pixel values. Each row is a sample on a two-dimensional manifold in 1024-dimensional space (a Hamming space). The intrinsic dimensionality is two, because two variables (rotation and scale) were varied in order to produce the data. Information about the shape or look of a letter 'A' is not part of the intrinsic variables because it is the same in every instance. Nonlinear dimensionality reduction will discard the correlated information (the letter 'A') and recover only the varying information (rotation and scale). The image to the right shows sample images from this dataset (to save space, not all input images are shown), and a plot of the two-dimensional points that results from using a NLDR algorithm (in this case, Manifold Sculpting was used) to reduce the data into just two dimensions.*

Edit: nonlinear dimensionality reduction.",statistics,2022-08-08 18:17:42,11
"Markov models like that are used to study protein folding.  This review might help:

[https://pubs.acs.org/doi/abs/10.1021/jacs.7b12191](https://pubs.acs.org/doi/abs/10.1021/jacs.7b12191)

But we never did anything fancy to generate the MSMs.  We clustered the trajectory frames based on similarities of the 3D structures (which was the challenging part), counted the number of transitions between each pair of states, and often ""symmetrized"" the matrix by averaging it with its transpose.",3,wjcwzh,"Hi all! I am trying to look for methods that handle large, sparse discrete first-order Markov chains. My data set has a large state space with 100+ states and some transitions either never occur or occur very rarely. I am specifically curious if there is a way to make transition probabilities corresponding to those rare transitions become 0 exactly. Mathematically, I can tweak the regular maximum likelihood problem to make some transition probabilities 0 exactly, but I then struggle to compute the log-likelihood (because of the log of 0s) and to compare different settings where transtion probabilities are made 0 to choose the optimal setting.

There is a thing called sparse Markov chain, but I think this one is for higher-order Markov chain, which does not seem like what I am looking for. If you are aware of any relevant papers, please let me know. I really appreciate your help. Thank you so much.",statistics,2022-08-08 09:16:08,6
"If you have enough data you can bin Age, and then stratify by Age and Sex together and compute the sensitivity and specificity in each category. This will also account for nonlinear effects of age. 

If you don’t have enough data then you will have to use a logistic reg with an interaction between Age and Sex and then plug in numbers or make plots to see the effect. This assumes Age has a linear effect on the OR in both Sex subgroups. Don’t rely on coefficients as it is confusing to interpret those on the original scale. Basically you would do 2 logistic regs—one in the true positives group and one in the true negatives and the 0/1 response would be whether it agrees or not.",1,wj88gx,"Hello, I am looking to do an analysis on a diagnostic test. I am trying to compare how specific variables (Ex. Sex, age, etc.) effect the sensitivity and specificity of the test. I am comparing it to a test that is to be considered a golden standard and will always assumed to be 100% accurate. How should I go about this?

Thanks in advance!",statistics,2022-08-08 05:58:26,2
"In multiple regression, the linear combination of the IV’s that results in the lowest variance of the residuals (the mean residual will always be 0) is used to compute the residuals.",8,wjaazx,"My impression was that residuals would be calculated separately for each IV, but this does not seem to be the case in SPSS. Are residuals for each IV calculated and then averaged or is there a different method to calculating a single 'residuals' parameter?",statistics,2022-08-08 07:30:44,6
"The reason for looking at the ZPRED*ZRESID plot is that it can reveal many problems with the model at a single glance. If the points are not seemingly random, but instead reveal a funnel or butterfly shape, it indicates heteroscedasticity which might necessitate the use of HC standard errors. If you see the residuals arranged in a curve, it means that you have a nonlinear relationship that you did not address by adding polynomials of the appropriate degree. You might also want to look at a simple histogram of the residuals, which can reveal if residuals are severely non-normal. From there you can start thinking about whether the model is ill-specified, for instance, if you would be better off using a Poisson or negative binomial model.",1,xiggla,,AskStatistics,2022-09-19 09:01:12,3
If they're truly indpendent that should be fine.,2,xicqii,"Hello all,

Let's say I have the joint PDF f(u,v) which is simply the product of two independent random variables f\_U(u) and f\_V(v). I have been struggling to find a good method to randomly sample from the joint distribution f(u,v).

What I'm wondering is this - since the distributions f\_U(u) and f\_V(v) are independent, could I sample say, 1000 values from f\_U(u) and 1000 values from f\_V(v), and then take the corresponding entries of each vector as a random sample of the joint distribution?

In other words, lets say we have the vector U\_sample containing 1000 randomly sampled u values, and the vector V\_sample containing 1000 randomly sampled v vectors. Could I then make a matrix (U,V)\_samples by taking the values of U\_sample and V\_sample corresponding to the same position in the respective vectors?",AskStatistics,2022-09-19 06:54:43,1
Those are both categorical variables. A Pearson correlation wouldn't be appropriate. Look into a [chi-squared test of independence](https://libguides.library.kent.edu/spss/chisquare).,3,xibt6m," 

I have a problem when using SPSS for doing analysis. Hope someone could help.

I am doing correlation analysis for my school project and I am not sure which analysis method should I apply.

E.g  IV=education level ( Primary school, secondary school and tertiary school), DV=having smoking habit( Yes or No)

Which method should I apply to find out these two variables correlation?",AskStatistics,2022-09-19 06:17:54,2
"If a is the probability of getting a heads on the first toss of a coin, and b is the probability of getting a tails, on the second, then P(a|b) =(p(a)p(b))/p(b) = p(a), but a = {H on toss 1} is trivially not a subset of b = {T on toss 2}",3,xi8f90,"If **P(a|b)  = P(a)**, then does it mean **a** is a subset of **b**.

If so, then doesnt that imply: If **a** and **b** are independent, then **a** is a subset of **b**.",AskStatistics,2022-09-19 03:42:52,5
"Please omit ""Dumb question"" from titles in the future.

https://www.reddit.com/r/AskStatistics/about/rules/
 
See rule 5:

> 5\. Use an informative title

> Use a title for your post that very briefly describes the statistical problem you need help with. It should not be about your emotional state (""Desperate""), personal circumstances (""Bad at stats"", ""I'm a beginner""), nor how urgent you think your problem is, nor your assessment of how easy you think it is. It should not consist largely of redundant information like ""Quick Question"" or ""Please Help"". If personal context is essential, put it in the body of the post.

---

Your problems run deeper than concluding causation. 

1. If you don't have a random sample, you don't even know whether any differences you find are due to bias rather than say a treatment effect. 

2. Besides bias from sampling, consider that there may be other sources of bias such as omitted variable bias. Group/treatments differences that you see may have nothing to do with the group itself, but simply with differences from unmodelled effects.",6,xhwj2h,"Let’s say I have 5 email marketing campaigns with 1,000 customers in each group. Each campaign receives some type of offer. 

I can’t check them for causation since they aren’t chosen at random (campaign they are placed in is based on previous months activity) and since I don’t have a control group (group that doesn’t receive an offer at all). Or am I wrong?

Boss wanted me to do this. He’s not dumb, but I also think he’s mistaken…but I could be wrong as well.

Thanks.",AskStatistics,2022-09-18 17:15:53,4
"In small sample sizes, estimates (like the mean) do not follow a normal distribution. Estimates tend to suffer from “fat tails” (your estimate may differ substantially from the true mean) when the sample size is small. The t-distribution has “fat tails” when the sample size is small. As the sample size gets larger, the t-distribution converges to the normal distribution.",7,xhsxfm,"Disclaimer: I'm an ELA teacher. I'm doing my PhD in Curriculum and Instruction and quantitative research is a required course. I promise I'm not stupid, I just cannot wrap my head around math sometimes. 

Can someone explain t distribution vs. z-score to me? I'm trying to figure out how standard deviation fits into each concept, and I'm a bit lost. Help would be very appreciated! Feel free to approach this ELI5 style, I won't be offended. :)",AskStatistics,2022-09-18 14:40:15,4
"""Factorial"" here means all the IVs are factors in the same sense as factorial ANOVA. It's still just logistic regression, just as ANOVA is regression.

Typically if someone emphasizes 'factorial' like that, they're indicating a designed experiment, often balanced.",3,xhzexs,"I have read the recommended guide in the sidebar but still a little confused.

I am looking at RCT data where people were randomised to a healthy diet or non-healthy diet.

They were followed to see the number of heart attacks in each group over time.

Whether they took daily aspirin was also recorded as part of the study. Aspirin use is here known to potentially decrease heart attacks.

What I want to do is check whether there was a difference in rates of heart attack among non-aspirin users vs aspirin users, by intervention or control group. If that makes sense?

So I am thinking:

1 dependent categorical variable (had heart attack y/n).

2 independent categorical variable (used aspirin y/n, intervention or control group).

On this basis the tool suggests factorial logistic regression - is this the right conclusion, and what is the difference between factorial and regular logistic regression? I can't find too much discussion of the 'factorial' bit online. 

Any help is very appreciated.",AskStatistics,2022-09-18 19:29:55,1
"Do you know Aesop's fable of the boy who cried wolf?

First, the villagers believed there was a wolf, but there was no wolf (a false positive, or type I error).

Then, the villagers believed there was no wolf, but there was (a false negative, or type II error).",34,xhfw9k,"I have been reading about hypothesis testing and every time I read, I think to myself ""now I understand it"" but the very next time I see myself solving a few questions which include hypothesis testing and I get confused. 

How do I *really* understand this? 

Please ignore the stupidity in this question, I need help. Thanks",AskStatistics,2022-09-18 05:53:16,16
"I’m a statistics major, pm me for anything you wanna know",1,xhp2jd,"Hi guys, I'm a hs senior applying for uni next year. I'm generally interested in applied math (not pure). After some time researching I find that Stats may be the right choice for me but I'm still not quite sure about that. Would be great if I can have some insights from you. What are some entry-level job roles after I graduate? Is it going to be in high demand after 10-15 years? What are the biggest challenges for you when breaking into the field? All comments are welcomed. Thanks!!",AskStatistics,2022-09-18 12:03:56,2
"You are calculating it incorrectly, or possibly you are using an extremely high margin of error. Show us your numbers, and we can probably point out where you are going wrong.",1,xhoxj8,My sample size is coming less than 1 for a given tolerance value. How is this possible?,AskStatistics,2022-09-18 11:58:46,3
"The diagram is just a pretty unclear version of a basic Bayesian inference diagram. A priori, they assign 50% probability to the coin being biased and 50% to it being unbiased (ignoring the egg, pink square vs white square each take up half the box). 

The egg is meant to represent the likelihood, the fact that two heads are more probable for a biased than unbiased coin. This should probably be a different color to avoid confusion, but the dark pink represents the fraction of the unbiased coin space corresponding to the two heads observation, while the pink on the right is part of the biased region that corresponds to two heads.

Even beyond the color issue I mentioned, it's still a pretty unintuitive image. But that's what they are showing.

They aren't really taking about an improper prior in the sense of a prior that doesn't integrate to 1, but they are discussing why the prior giving 1/2 probability to each of the biased/unbiased models is not a great prior. 

Honestly i think the image adds nothing but confusion. The text is clear enough.",1,xhkgtk,,AskStatistics,2022-09-18 09:02:06,3
"> There might be 4 categories of unusual event with expected counts of (0.0373, 0.6875, 1.1562, and 7.219), leaving an expect count of ""nothing happened"" at 99,990.9. If I observed event counts of (1,2,4,0), what is the probability that an event at least that far from expectation

So you wanna know the probability of their total being at least 2.1 below their expected combined total?

As you alluded to, ΣPois(λj) = Pois(Σλj)

> but instinctively that seems very wrong, as it would treat event counts of (10,0,0,0) and (0,0,0,10) the same when it seems pretty clear that the first is way more unlikely than the second.

We have 4 independent processes happening simultaneously and contributing to a common total. We're not saying that each path to 10 is equally likely, but that the chance of 10 is the same as it would be if the 4 processes were just 1 with a combined lambda. Being Poissons, they're the limit of Binomials, so see if it makes more sense when you consider, say, combining two Binomial RV's (each with large n and small p). I'll do the same myself some time to see if that leads to a satisfying explanation.

But if you wanna test whether that sorcery really works, a non-magical calculation is to add up each way of getting a total of 7 or less. You'd need to add each integer composition's probability. Yes, that's ugly as hell and requires code.",1,xh99iv,"I am trying to write a statistical test for a real-world problem where some process is observed many, many times, and the counts of certain ""unusual"" events are recorded. The model being used is that these events should each follow a Poisson distribution, and I would like to find the probability that a given set of observed values (or worse, by some reasonable metric) would have occurred if that were true.

Some complications here are that all the failure cases have very low probabilities and thus expected occurrences, and that *technically* the variables aren't independent, because they will have a fixed sum, but since the vast majority of observations are ""nothing happened"", the failures are very close to being independent, and since those are what matters that's an assumption I'm willing to make.

For example, let's say the process is observed 100,000 times (a fairly small amount for this). There might be 4 categories of unusual event with expected counts of (0.0373, 0.6875, 1.1562, and 7.219), leaving an expect count of ""nothing happened"" at 99,990.9. If I observed event counts of (1,2,4,0), what is the probability that an event at least that far from expectation would have occurred if the underlying process really was emitting them consistent with the Poisson distribution with those lambdas? How about (10,20,40,100)? Yes, the p-value is likely very high, but if it's above, say, 2^-40 then I need a result with good fidelity. I don't necessarily need a precise calculation; a conservative bound is fine too.

If this were a single value, then I could easily find P(X>=n) for some lambda, but I don't know how to do this for multiple independent values. I've tried lots of ways of doing this, and none of them produce a p-value that matches simulations. I sort-of vaguely understand why multiplying each P(X>=n) value doesn't work, but several attempts at corrections have also failed.

I could just sum the event counts and their expected rates, since Poisson distributions work that way, and calculate a p-value for that, but instinctively that seems *very* wrong, as it would treat event counts of (10,0,0,0) and (0,0,0,10) the same when it seems pretty clear that the first is **way** more unlikely than the second.

Ideally, the answer would take into account that that ""0"" result is far less than the expected 7.219, but I would be happy with a one-tailed result if that makes things easier. I also know that a there is no general definition for a two-tailed result for an asymmetric distribution, and I don't have a strong preference for how it is calculated, although the R method of taking the CDF of the other tail starting with the value whose PMF is less than that of the observed value or the method of taking the highest-possible CDF of the other tail that does not exceed the original would both be preferable to the method of doubling the single-tail result.

I have read the link for ""what statistical test should I use for this data/hypothesis"" and it was not very helpful for me. I'm not sure if these are dependent or independent under that scenario, and they are certainly not normally distributed. The result I got from the table was to use ""ordered logistic regression"", and several pages I found for that were... not illuminating as to how I might apply that to this problem.",AskStatistics,2022-09-17 23:36:26,2
Statistical Inference - George Casella,3,xhjcvr,"A while back I asked this subreddit about good books on probability theory. I was recommended an amazing book by Joseph K Blitzstein and Jessica Hwang ""Introduction to probability"". It helped me immensely throughout my course in University but alas, it's not enough now.  I hope I can find something akin to that about mathematical statistics.


(The way I distinguish fields of statistics may seem arbitrary and my lingo may be outright wrong, I study in Russia and thus everything I write here is a crude translation, sorry for that)


I'll outline general topics of my course. It's revolving around samples and different criteria for minimazing errors, effectiveness of estimates, confidence intervals, etc. Some of the topics are:

Cramer-Rao inequality,


Fisher information,


Method of moments,


Confidence intervals,


Hypothesis of unknown mathematical expectation for known variance in case of Normal distribution,


Pearson criteria,


Kolmogorov criteria,


Xi^2 criteria ,


Again, sorry if I botched some of the terminology and thanks again!",AskStatistics,2022-09-18 08:17:38,3
You are going to have to be far more explicit about what you’re trying to do for anyone to give you reasonable help. What are you trying to use tf-idf for? What are you trying to classify? Etc.,1,xhhvwt,"I have this general understanding that for simple text classification ML models, **frequency vectors** of tokens work well with **Multinomial NB** and **binary vectors** work well with **Bernoulli NB**. Notably these two vectors have only integer components, so there's some innate sense of count. But **tf-idf vectors** are not integer, they're continuous real numbers. Now does it make sense to use them with **Multinomial NB** or **Bernoulli NB**. Or should I go with **Gaussian NB**, please provide explanation :)",AskStatistics,2022-09-18 07:18:54,1
"I don't see what value there could be in descriptives for that task - you'd have to ask them what they think the point of it is. 

Your supervisor might perhaps be confusing randomness with representativeness, bit that won't be a helpful concept when randomly choosing 3 values.",3,xhhdae,"Disclaimer: doing undergrad thesis; not a stats student.

So I have this Excel sheet with around 3k items. I need to randomly choose 3 items out of it. My adviser said I need to conduct descriptive statistics. Can anyone lead me to the right approach? The way I see it is that I can simply run a formula in Excel to return me 3 items randomly. So I don't see how a formal descriptive statistics should be used anymore. But I could be wrong, that's why I'm asking.",AskStatistics,2022-09-18 06:58:05,4
"inverse transform is one-dimensional

there are no good non-MCMC methods for general multivariate
(there are good methods for a few, like multivariate normal)",7,xgxyf0,I seem to be confused about why the latter exists.  I can't think of a situation where [MCMC](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm) sampling would work but [Inverse Transform](https://en.wikipedia.org/wiki/Inverse_transform_sampling) wouldn't.,AskStatistics,2022-09-17 14:18:12,9
"It depends on what you mean by ""misc. statistical questions""

If you're asking for random demographic, economic, geopolitical etc *facts*, no.

If you're asking questions about the subject of statistics itself, sure. (Some kinds of lay questions are quite common, you might try a search of the subreddit for 
some of those.)


For what's on topic, in particular see [rule 2](https://www.reddit.com/r/AskStatistics/about/rules/):

> 2\. Posts must be questions about statistics

>  This subreddit is to answer questions asked here about the theory or practice of statistics, such as questions about statistical analysis. It is not a place to get/give random demographic, economic etc facts, nor for challenges or brain-teasers. Use other subreddits to share results of your statistical analyses or to request research participants. If you want participants for a survey try /r/SampleSize (get your self-selection bias for free). If you are seeking data, you could try /r/Datasets.",2,xh8fg4,,AskStatistics,2022-09-17 22:48:40,2
"It’s not ANCOVA, but you can compute the pooled within-school covariance matrix and compute the correlation matrix from that by dividing covariance by the product of relevant sd’s.  If you ignore schools and just correlate the variables mean differences between schools will inflate the correlations.",2,xgsph7,"So if I am interested in the correlation of two continuous variables but want to separate them by location groups to account for site-specific effects?

i.e. test results in Maths and test results in Chemistry,  in 5 different schools? 

The hypothesis being that Match results affect/are correlated with Chemistry results.",AskStatistics,2022-09-17 10:38:38,2
"No.  You forgot about order.  The probability of 100 tails is indeed equal to the first 50 being heads and the next 50 being tails.  Or any prespecified order like odd flips are heads and even tails.  But there are

    > choose(100, 50)
    [1] 1.008913e+29

different specific ways to get 50 heads and 50 tails, so 50-50 is 10^29 more probable than 0-100.

Learn about the binomial distribution.",21,xgtp8q,"Suppose we have a coin flip (heads vs tails). The probability of having either of the 2 results is 50%.

It does not matter how many times we flip a coin, the probability between heads or tails is always 50%.

&#x200B;

Now lets expand this a bit.

Lets say we have a set of 100 coin flips and we will call it S1 (set number 1).

The most common output of that set would be 50 tails and 50 heads.

&#x200B;

Now lets have 100 out of those sets which means (100 sets of 100 coin flips each).

&#x200B;

Statistically the probability (we call it P) of having a set full of tails, is the same of the probability of having 50-50, right?

&#x200B;

So theoritically the Probability(set of 100 tails) = Probability(set of randomly assigned heads/tails (most likely 50-50)).

&#x200B;

Why doesnt this sound right to me?",AskStatistics,2022-09-17 11:20:00,6
"1. Don't choose what test to run on the basis of other tests or the p-values won't work right.

2. The t-test is pretty well-behaved under a variety of model violations, there's plenty of discussion [here](https://stats.stackexchange.com/questions/38967/how-robust-is-the-independent-samples-t-test-when-the-distributions-of-the-sampl).

3. The Wilcoxon is *not* a nonparametric alternative to the t-test despite what is written all over the place. Read, for example, what [Wikipedia](https://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test#Null_and_alternative_hypotheses) says when it actually defines the hypotheses. An actual nonparametric equivalent would be a permutation or bootstrap test.

4. Should this be unpaired or paired?",3,xgui3z,"Hi, I ‘m new in statistics. I have to compare scores from before and after a training. I ran a unpaired t-test and got a skewed bell curve graph. I read somewhere that in that case I have to run a Shapiro test and check if the data is normally distributed (pre and post scores). Well, that failed. Should I run the Wincoxon test instead? I have approximately 98 scores.",AskStatistics,2022-09-17 11:53:41,6
"1. Why do you believe that needs fixing? 

2. If you're using ordinal *logit*, then the assumptions are as for logistic regression plus a proportional odds assumption. 

  If you're using ordinal *probit* then the assumptions modify to fit that model, and so on for the connections between the various other binomial/multinomial glms.",1,xgr75g,,AskStatistics,2022-09-17 09:38:10,2
[Here](https://tsoo-math.github.io/ucl/poisson.html) is an introduction with simulations in R.,3,xgosb2,"Does anyone have an idea on how to generate a random number from a poisson point process with a defined arrival rate?
I searched about this topic and i cant even understand what is the poisson point process.",AskStatistics,2022-09-17 08:01:38,5
"Let's say we have some function:

Y = 3 + 1.4 * X + 33 * P 

This functions says that when we have a value for X and P we can calculate Y. So to know Y you depend on X and P, making Y the dependent variable and both X and P are an independent variable. So in a nutshell it is easiest to remember: I want to say something about Y with the information of X (and P or any number of X; X1, X2, X3 etc)

So in your example, what information (X) is used to make a prediction on what outcome(Y)?",2,xgsjys,"Stat newbie here. Sorry if the question sounds dumb.

But if a study is titled ""effects of caffeine on running speed"", what would be the DV and IV?
If you're studying just the speed pre and post caffeine, would caffeine be IV and speed be DV?",AskStatistics,2022-09-17 10:32:16,3
Double integral over the region where one is bigger than the other,8,xg6q2j,,AskStatistics,2022-09-16 16:43:51,9
"Do you have a reason to be concerned about these polls? Like, do you disagree with the methodology?",7,xg2wqx,"This question has been on my mind for a while and I wanted to ask. I'm not an expert on statistic or even have academic qualification on statistic. So I have no idea. By the way, I'm not talking about government data. I'm talking about independent polls. I can't help myself but be sceptical of polls about religious beliefs in developing countries, especially when it has a high percentages of irreligiosity. I just want to know if my concern is valid, and can provide an explanation for me about my scepticism.",AskStatistics,2022-09-16 14:02:58,15
"I believe you may be confusing the distribution of parameters with the distribution of the data.

Linear regressions are models for conditional expectations. That is, you’re making a function that describes the average value of y as a function of all x variables. The parameters are the coefficients/weights in this function (usually with one additional parameter describing the variance in the conditional distribution). The more data you have the more certain you are about those parameter values. This is as it should be! That’s why we like having data and why we like having more than less of it.

But the parameter distributions are not the conditional distribution of the data! That is described by the conditional mean, the variance parameter, and the chosen distribution used as the likelihood. This distribution does not shrink forever as you add new data, you just become more certain about what it looks like. If you want to display *this* you can do things like display the marginal quantiles of the data-generating distribution (where you marginalize over the posterior parameter distributions).

It’s basically the same idea as the difference between standard deviations and standard errors, they’re related but one has a 1/sqrt(n) in it (the one pertaining to the inferred parameter) and the other doesn’t. It’s related to the difference between confidence intervals and prediction intervals.",5,xfr5yc,"I have a data scenario where the targets, y, have a large amount of intrinsic scatter about a trend line. Because there are so many points and a strong correlation between predictors x and targets y, standard Bayesian linear regression leads to a posterior distribution with a small scatter in weights (I.e. polynomial coefficients) so the range of models is quite small. 

I would like the range of models to actually reflect the dispersion of data points. I’m struggling to figure out how to accomplish this.",AskStatistics,2022-09-16 06:20:49,4
"1. >  I am given a set of SPSS outputs, and I must write a hypothesis and paper utilizing them.

  Be warned, this appears to be teaching you to formulate hypotheses after you see the results, i.e. training you to engage in HARKing, a form of p hacking. Please understand that you should *never* be doing this in practice.

2. Comparing t-values: in general you should not do that. Nor p values.

  Coefficients might be compared if the models are the same apart from the pair of terms you're comparing.",3,xg1yu0," Hello,

I'm a student working on a paper in which I am given a set of SPSS outputs, and I must write a hypothesis and paper utilizing them.

The dataset consists of PISA data, with a set of hierarchical multiple regressions on different student variables. I am provided with separate regression outputs for the total population, the male population, and the female population. I'm planning to compare the male and female outputs.

My question is, can I compare or say anything meaningful about the relations between these outputs? To what extent can I compare the Beta and T values for the same coefficient between the two models?",AskStatistics,2022-09-16 13:25:00,2
It's on a scale of e-15 which means it's probably just a floating point error.,10,xfyy5v,"&#x200B;

https://preview.redd.it/6r6jl7kpj9o91.png?width=692&format=png&auto=webp&s=8566d675db2a0758adcaefad13b8cfd6fa4c1ce7

So far in the examples I've seen in GLMs the residual deviance was always positive so this result (even though it is close to 0) puzzles me. Can that be a legit result with negative residual deviance or have I done something wrong that I need to explore? Is there a meaning/interpration to negative residual deviance?",AskStatistics,2022-09-16 11:25:31,4
"This reminds me of

 https://otexts.com/fpp2/moving-averages.html

And 

https://otexts.com/fpp2/ses.html#ses

If this is what you are after.",1,xfwxym,"I believe you can sometimes define a quantity X as a weighted average of some random variables X1,...Xn such that it has the form 

X = k1\*X1+...kn\*Xn

and -- assuming X1,...Xn are not strongly/perfectly correlated you can choose k1,..kn such that X has lower (relative) variance than any of the original random variables.

Does this sound right and if so would anyone be able to point me towards a topic/search-term/etcetera?

I'm looking for a name for this technique and ideally for some math to go with it (estimating the resultant variance and such) -- although I could conceivably re-derive that myself with the right background information.",AskStatistics,2022-09-16 10:06:56,3
"I personally find the MIT courses on edX quite nice:

https://www.edx.org/course/probability-the-science-of-uncertainty-and-data

https://www.edx.org/course/fundamentals-of-statistics",5,xfgcnc,"Hi everyone, I have a master of public health in epidemiology and biostatistics and am currently working in a healthcare organization as a data analyst/stats programmer. I've taken a lot of statistics in both undergrad and graduate school, but most of it was application focused and only spent a short time discussing the mathematical foundations. I would like to strengthen this aspect of my knowledge (perhaps to eventually try to move to a biostatistican position, admittedly I am unsure how realistic that is). I am currently reviewing calculus and linear algebra (using these two courses: https://www.coursera.org/specializations/expressway-to-data-science-essential-math and https://www.coursera.org/specializations/mathematics-machine-learning), and I am wondering if anyone could recommend me good math-based/statistics for statisticians courses? 

For example I've found the following John Hopkins course sequence that seems promising:

https://www.coursera.org/specializations/advanced-statistics-data-science

It notes that: ""This specialization requires a fair amount of mathematical sophistication. Basic calculus and linear algebra are required to engage in the content.""",AskStatistics,2022-09-15 20:31:37,2
Is the response variable a count of something?  What's the raw data?  What represents the variation within a sample?,1,xfs429,"I suppose this is a very dumb question but I am confused. I am using Rstudio to compare the number of bacteria isolated from 10 different samples using Kruskal Wallis test and Dunn test.

I am seeing pvalue of 1.2e-07 for example when comparing 2 samples, but somehow my boxplot shows them having the same significance letter (a,b,c). Did I make a mistake in my R script? Can someone please help me make sense of this?",AskStatistics,2022-09-16 07:00:27,3
"You could compare each treatment to the control using Dunnett’s test assuming you care about B versus control which is not clear from your description. Alternatively you could use the Tukey hsd to do all pairwise comparisons. Although ANOVA is a special case of regression, tests based on the studentized range distribution are not.",1,xfoeua,"Hello,

My PI is asking for this (replication work) but I have no where to ask and have no experience with RCTs...

Let's say there are treatment A and B (so control, A, and B). How would I measure:

1. The effects of Treatment\_A relative to control
2. The effects of Treatment\_A relative to Treatment\_B

My intuition tells me that I obtain 1) from regressing Y=Treatment\_A, and 2) from Y=Treatment\_A+Treatment\_B but I cannot be sure. Also, the intercepts are always significant but that obviously does not imply that the effects of control is significant right? How can I measure the effects of control then? Am I supposed to reverse the model i.e. Y=control+Treatment\_A or B?

Thank you so much for your help!

Edit: sorry the title is not that informative but I cannot edit anymore...",AskStatistics,2022-09-16 04:12:41,1
"My understanding is that stem and leaf plots were popular between 1940-1990 because they can be printed using just monospaced font, which for a long time was the best computers could (easily) do. Today, virtually all computers have fancy graphical engines, so the main draw of stem and leaf plots disappeared.",6,xfhz06,"It seems like every basic stats textbook teaches Stem and Leaf Plots, but I rarely see professionals use them in published papers, business reports etc (Obviously I've only seen a tiny subset of published material).

Is that because they are only really of use for datasets of a specific size or are there other reasons ? I know it seems silly, but from my perspective they just don't look that good compared to other types of graphics (too busy).",AskStatistics,2022-09-15 21:53:35,4
"I think you can use the census data and segments as labels, train a classifier, and then use that model to predict the labels from the PCA variables of the customer data",2,xfk5ho,"Hi there, I am working on a project that involves cluster analysis for two separate databases. One is a client customer database and the other is a census representative database based on survey data. 

I have created clusters based on the census representative data and would like to reuse the same segments in the customer data, which I know is usually possible. 

However the clusters are built using principal components due to having a large number of variables. Is there a way to keep the same clusters for a different set of data (same variables) or would this be impossible due to using PCA?",AskStatistics,2022-09-15 23:56:44,9
"If you're planning to do graduate school you could always do a math bachelors since that seems to be the aspect you're most interested in, itd prepare you well for graduate studies in either CS or Stats.",3,xffvu5,"I love the math in both majors, that said I have never had a huge interest in Computers nor have I tried coding, but I am interested. I’ve heard CS can lead to quite a stressful career. Thoughts?",AskStatistics,2022-09-15 20:08:22,4
sounds like a job for extreme value theory,3,xf9uos,"The variables that I am working with are discrete (numbers between 10 and 50, each associated with a probability) but can be made continuous by fitting the data to a normal distribution if that simplifies things.",AskStatistics,2022-09-15 15:23:22,12
This happens once per year in 9/10 years that have 4 digits.,1,xfd47n,"Hi,

  
I was curious how often the following occurs, the short version of a person's birthday is the same as the year they were born.  


IOW: My birthday, 1/9/64 is the same as the year I was born, 1964.  


I searched Google and found another person who asked this question, but there were no answers. I see a lot of articles about when the age you turn matches the year.  


Figured if anyone might know, someone here would.  


I'm embarrassed to admit I'm 58 years old and only a couple of years ago I realized this about the numbers.  


Thanks in advance.",AskStatistics,2022-09-15 17:55:19,3
"You do understand that ""everybody is talking about"" the stuff that is still confusing.  What is well understood doesn't need a lot of discussion.  So that is a very poor guide to what is useful.",1,xf4upe,"I want to improve my statistics because I really enjoy when I understand new concepts. But I am really stuck - my education focused on null hypothesis testing. t-tests, ANOVAs, and multiple regressions in order to get p-values and effect sizes. I know this is not ideal and that there is more to the story.

Reading in this subreddit and elsewhere online, it seems like everyone now is talking about linear mixed effects, parameter estimation, maximum likelihood, model comparison. Very little interest in p-values. I want to understand why, and I want to learn about those things - when and why to use them.

I have started reading Gelman's regression analysis book, and I have also started reading McElreath. But I want to see concrete examples and guides on actually doing real statistics, for example with real experimental data and realistic questions. Not playing with throwing globes into the air or struggling to install R libraries and having to revert to an older version of R just to follow along... I don't want to have to read through 300 pages just to get to something realistic.

Is there any decent online course out there that goes into using LME, parameter estimation, MLE, or model comparison, with real examples and code-along to learn R in the process? I am fed up of being stuck in my basic t-test/ANOVA mode of thinking.",AskStatistics,2022-09-15 12:00:30,7
"Sounds like you could use left truncation, with entry time being the start of follow-up. You really don’t want to use exposure variables defined after the start of your follow-up (it’s sometimes called “using the future to predict the future”), but there are a few issues with defining all your predictors at birth. 

Age at disease onset is probably an important predictor. But if you’re defining your exposure variables at your time zero (birth), everyone is age zero. A number of other highly predictive variables would also be meaningless if you used the birth values. 

It would probably be better to define time zero as the time of disease onset, and to try to define your non-time-varying exposures as of that date (age, etc.).",2,xf3o89,"Hi, 

I'm currently in the process of doing prediction models with my data, but it's the first time I have come across this specific situation.

I am using an ongoing clinical database where all participants have the same genetic disease. When a participant is recruited, he starts being followed, and information is collected about various clinical manifestations of the disease. But, when the participant joins, they also go back in time, from their birth, and collect information about all the clinical manifestations that occurred before they joined.

This means that I have several clinical manifestations with a binary variable (Present/absent) and an age at diagnosis variable, which can either have occurred before or after the participant joined.

I intend to do Cox regressions to model the risk of developing a severe manifestation of the disease that occurs in adults, using less severe manifestations seen in children as predictors. The start time would be birth, and the end time is either the event's occurrence or the date of the last follow-up.

I've been told that I shouldn't use birth as a start time since some participants were diagnosed with my outcome before joining. But, with how I explained my data was collected, isn't it the same as if each participant was followed since birth?

Is there another aspect I don't understand that makes this situation different? Is the time-to-event I'm using necessarily wrong?",AskStatistics,2022-09-15 11:12:22,8
Why would ordinal characteristics as an independent variable stop you from using linear regression? You just have to dummy-encode them,2,xf3big,"Hello everyone,

Imagine that you have historical data on sales of collectibles, they have several categorical and other ordinal features.Really the one who decides if something is better or worse is the community, there are trends that last one or two weeks. If it is better it sells more expensive.There is also data from the offer, since anyone can sell a collectible at the price he wants (although it may not be sold if the price is not fair according to people). If it is sold, it goes to historical data.


Is linear regression a good way to predict that an object given its categorical and ordinal characteristics has an approximate value of x?I understand that you have to train the model every few hours since trends are changing 


What do you think? Do you see it possible?


Thanks to everyone",AskStatistics,2022-09-15 10:58:16,10
"I don't do a lot of survival analysis, but if you suggested this for a linear regression I would say:

* make one model
* include the period of birth as a predictor
* interact all other model terms with birth period

If the period of birth has an effect on the HR associated with the mutation then the mutation x BirthPeriod interaction will be significant.

Perhaps a cox/survival expert can weigh in on if there's anything more needs considering.",4,xew3cv,"Some background: I've inherited part of an old project where we are evaluating the risk of developing cardiovascular disease in depending on the presence of a particular genetic mutation.

Before I joined, four exact similar cox models were fitted, though each subsetted by calendar period of birth (1970-80, 80-90...) and a HR for having the mutation vs not were produced. These HR are increasing for each period and now the PI has asked for a p value for this ""trend"". Anyone have an idea if this is possible?",AskStatistics,2022-09-15 05:53:24,3
"I have no clear idea what you're asking.

What do you mean by ""3 different frequency distributions""?",2,xf4pj1,,AskStatistics,2022-09-15 11:54:53,3
"The problem is you didn't specify the event you were looking for until after you found it.   You could have been just as suprised (and so ask about) any number of other slightly odd events (what if the rolls had gone 2,4,6,8,10? What if they'd been 2,2,2,2? etc etc)

Hence, the relevant probability is closer to ...""what are the chances that in a long sequence of rolls, *some* mildly surprising pattern of coincidence would happen at some point?""

I don't know the answer to that, but that's not a small number.",3,xf9sw3,"So the other day I was in a dnd game and one of my friends rolled a d20 that came to a 9, he rolled again and got an 8, then rolled a d30 that came to a 7. 

I was curious on what the chances were on something like that happening and what would the formula be to figure that out?",AskStatistics,2022-09-15 15:21:10,1
"This makes no sense.  What is the *dimension* of the parameter spaces of the null and alternative hypotheses?  Parameter = 1 does not say explicitly that you have one parameter, that is, a one-dimensional parameter space.  And HA != 1 doesn't say anything useful.  What is the *dimension* of the alternative hypothesis?  After you have answered these questions correctly, you can do the subtraction yourself.

Note: both parameterizations have to be identifiable (no collinearity).

Edit: And the hypotheses must be nested.  Every parameter of the null hypothesis is also a parameter of the alternative hypothesis (or could be if reparameterized).",1,xf26j3,"likelihood test statistic will be asymptotically chi-squared distributed 𝜒2 with degrees of freedom equal to the difference in dimensionality of parameter spaces of H0 & H0 U H1. Now in case my H0: Parameter = 1, HA=! 1, would the degree of freedom be zero or one? I’m confused, can someone help me here.",AskStatistics,2022-09-15 10:10:39,3
"> Can you calculate statistical significance for single values? 

sometimes. It depends on the specific hypothesis and the model.",4,xez1k5,"Hello!

&#x200B;

I'm trying to figure out if there has been a statistical increase in Sales between two groups given different treatments.

Group A (n=191083) treatment resulted in $167,049 sales.

Group B (n=235919) treatment resulted in $322753 sales.

Is there a way to show statistical difference?",AskStatistics,2022-09-15 08:01:44,9
"They are fundamentally different:

- credible intervals are a summary of the Bayesian posterior distribution but do not necessarily have any guarantees about containing the true value ( you can always pick a prior distribution to give a different interval). They instead communicate an interval consistent with prior information and the new data.

- confidence intervals are a procedure that gives a guaranteed coverage probability when the assumptions that go into them are true. That doesn't mean that any given interval covers with the specified probability, but that the procedure does in repeat use, in the long run. It's easy to make contrived examples where you know for sure that the true value is in the interval, but the confidence is still only 75% or something like that.",12,xehzne,"I've been reading a bunch about this and, from what I can tell, the theoretical difference is that credible intervals consider P(hypothesis|data) whereas confidence intervals consider P(data|hypothesis), which seems similar to the problem with p-values in hypothesis testing -- they don't directly answer the question we usually want the answer to.  However, in actual practice, I don't see how this is actually the case: If I understand correctly, to actually compute a credible interval, you just integrate the conditional PDF.  So, for example, if I want a 95% credible interval and I have the PDF of the posterior distribution, I just want an interval corresponding to 95% of the area under the curve.  Simple enough to understand the concept.  But if I instead compute a confidence interval for the same data, it seems like the only difference is that, rather than trying to figure out the actual posterior distribution, we instead just assume that, given a sufficiently large sample size, we can approximate the distribution as normal due to CLT.  Hence, it seems like confidence intervals are really just a special case of credible intervals, where the posterior is normally distributed.  

But if this is the case, that is, if confidence intervals are merely an approximation to credible intervals that are only valid in the case of approximately normally distributed data (or a large enough sample size to rely on the CLT), then why do some many resources say that confidence is different from probability and that confidence intervals and credible intervals answer altogether different questions?   Why the emphasis on not interpreting confidence intervals as ""There's an approximately n% chance the interval contains the actual parameter""?  Or, if it's not the case, what am I missing; How are confidence intervals different from credible intervals where the posterior is normally distributed?",AskStatistics,2022-09-14 17:25:23,5
"Ultimately the standard deviation is still just a measure of how far your data points disperse from the mean in the units of your original measure. 

If you consider absolute mean deviation, this is another measure of how far your data points differ from the mean, but you simply sum the absolute value of your mean minus your data point;  Σ(xbar-x)/n

In this way, if you have a mean of 39 and an absolute mean deviation of say 15, it means your data points are on average 15 away from the mean. It is quite literally the average of all the distances of each point from the mean.

The standard deviation is the same except it squares and then square roots;   √(Σ((xbar-x)^2) /n)

This effectively puts extra weight on data points further away from the mean. So if you have a mean of 39 and a SD of 14, its somewhat saying that on average your data points are 14 away from the mean, but this has put more weight on data points further away (and means you don't have to calculate an absolute value each time).

Sometimes looking at absolute mean deviation makes it easier to understand what SD is doing.",7,xeeq58,"I'm trying to better understand some basic statistics stuff, starting with Standard Deviation and what that tells me about a dataset. I have a random group of percentages down there. If my SD= 14% and my Mean = 39%...with a SD of 14%, what does that tell me about my dataset relative to the mean?

Here's how I think it goes as one example using the first # in the below set:

24% is equal to 1.7% standard deviations (got this by doing the percentage divided by the SD so 24% / 14% = 1.71) 

But that's all I know.

So, ultimately, I'm just trying to figure out how I could use SD to better understand the dispersion of my data...

Or, given my example, how can I use 1.71 to understand the significance, or lack thereof, of 24% in the dataset

This may not make any sense at all, if not, I'll need to delete & take time to rephrase my question....

https://preview.redd.it/bv6v0yxcawn91.png?width=65&format=png&auto=webp&s=35bc079f3aa0be2f31845f420c11aba19d051772",AskStatistics,2022-09-14 15:03:10,5
"> If you have to find out the probability of something, what is the first technique you think about to use?

The ""figure out what problem we're solving first"" technique.

If you're trying to pick some probability rule with no context whatever, you're wasting your time.",13,xeyq7v,,AskStatistics,2022-09-15 07:49:04,11
"AIC is a model selection tool based on some asymptotics of divergences between the truth and our estimates. You can use the AIC to choose a beat model, or to provide weights to combine the models. But otherwise, just think of it as a complexity-penalized  descriptor of model fit.

How are you proposing to use it as a descriptor of predictions?",3,xei5d0,"So, I know I can use AIC to decribe the representation by different models when I'm doing data regression. After I have the parameters, I may want to use those values (or combinations thereof when appropriate) to make predictions on new data sets. Is AIC still an applicable descriptor for those predictions, since I'm not correlating anything, or is it better to stick with something like RMSE even though the models providing the predictions are still using different numbers of parameters? Or, since the parameters are already regressed, does the difference in number of parameters even factor into the predictive case?",AskStatistics,2022-09-14 17:32:37,7
"It's definitely possible to treat these events as independent events for the most part, in which case, the formula you gave above (multiplying them together) would be a correct approach. 

However, it gets a bit muddy if you have multiple players on your fantasy team who play on the same team in real life. For example, if Patrick Mahomes has a great game which is worth a lot of fantasy points, Juju Smith-Schuster and Travis Kelce probably also had games which were worth more points in fantasy, so it's a bit hard to say that all of your players are performing entirely independent of one another.

Another thing to possibly consider as a variable which affects your player's probability to do well in a given game is the defenses which your team's players are facing in that week, but that may make your model significantly more complex.",4,xed6i9,"Hey yall!

So I'm doing a little personal project for fantasy football (american) exploration.  

Some background info: 
I've calculated the average points per game for each position over the last 3 years. 
I've then created a table that calculates a probability of a player scoring +/- their position average based upon their own average and standard deviation. 

My question:
How would I calculate the probability of my whole team scoring more than their positional average for a given week? Is it simply by multiplying each players probability (from the table I mentioned above) with the other players on my fantasy team? 

For instance: 
P. Mahomes: P(fantasy points > position ave)
D. Cook: P(fantasy points > position ave)

Would I just multiplying these two guys together?",AskStatistics,2022-09-14 14:01:05,4
"AFAIK ""reasonable"" is not a technical term of statistics.  I guess it means whatever the textbook author is looking for, and who knows what that is?",13,xe70kt,"Basically the title, I'm really stuck on the wording with this one.",AskStatistics,2022-09-14 09:52:37,14
"I dont know what you are asking.  Can you fit AR models without using the MLE?  Yes.  There is also Bayesian inference via MCMC or INLA for example.  

I dont know what you mean about AR models and GPs.  What kind of model do you want to fit here?",1,xe2mux,"Due to my research, I've been around wondering about auto-regressive and distributed lag models. The only similar thing I have seen is LSTMs and the family of recurrent Neural Networks. I've seen also some work performed on CNNs for time series but I am trying to avoid Neural Networks given the small amount of data from the phenomena I am studying.

Are there any literature suggestions for something like Distributed Lag Models models but using things such as Gaussian Process or Decision Trees? Also it would be nice if they had a computational implementation. 

On a similar note I have been wondering, from a very naïve implementation on GPs I thought of doing it in a way:

Y\_t ~ Y\_{t-1}, Y\_{t-2}, X\_{t-1}, X\_{t-2}

Using GPyTorch that way I would have as an input a ```tensor([input_size, 4])```

Because of how I see the implementation of the matrixes in GPyTorch I was wondering if it is necessary in the response of vectors Y_{t} for them to have the same amount of time between observations. I know it is necessary from a vector autoregressive modelling perspective but I am a bit puzzled if these assumptions need to hold under all the modelling frameworks.
I don't know if the question is rather ridiculous but it is slowing down my research a bit.",AskStatistics,2022-09-14 06:55:11,5
"Statistical significance is not the same as practical significance. Assuming you don’t have something weird going in with scaling so that that small number is in fact a reasonable effect, you have simply precisely estimated an effect that is extremely small but still identifiably different from 0.",5,xe5fvu,"See the title. How is this possible? Does an estimate of 0.004010 not indicate that it basically has no influence on the dependent variable? How can it still be significant on the *p* < .01 level? Please enlighten me, I have no idea what this implies theoretically.

Edit:

It may be important to know that I'm doing Beta regression (and new at this) so the dependent variable lies between 0 - 1 and there are 304 observations

Edit edit:

Is there a way to transform them to get more intuitive estimates? In another model, I now even get estimates as small as 7.790e-02 and I have no idea how to report this.

I'm doing beta regression so my dependent variable are proportions (0.43097466, 0.63038119, 0.27758503 etc.) would it help to reduce them to (0.43, 0.63, 0.28) or would that make no difference at all? Thanks

Edit edit edit:

I tried and sadly the answer is no",AskStatistics,2022-09-14 08:51:09,9
"It depends on how the data points are turned into information and knowledge.  Are they being averaged, trended, used to characterize the state of control?  Look at the information that the one-month data points were contributing.  Test that they were contributing nothing.  


Unfortunately, you might be able to pencil-whip the data to make the historical one-month points contribute nothing.  It means you know the outcome that want you and you get it by selecting the test that you know the data would fail.


A more honest approach is to hypothesize that future one-month points will contribute nothing, then collect data for a specified sampling period (which is powerful enough to detect whether they contribute), then after the sampling period calculate the test.",1,xe1c09,"Say data is collected on week 1, week 2, week 3, 1 month, 3 months, 6 months, 1 year and so on for each group. If say for example, I no longer want to collect 1 month samples, what test can be used to show that the new data set is valid / that the 1 month data points are not critical?",AskStatistics,2022-09-14 05:58:02,1
"There is discrete statistics also called [categorical data analysis](https://www.amazon.com/Introduction-Categorical-Analysis-Probability-Statistics/dp/1119405262/) but you are barking up the wrong tree.  Trying to discretize the fundamental concepts of probability theory (probability and expectation) is not the way to do it.  Those concepts apply to all random variables, discrete, continuous, or neither.

For example, when you learn about the Poisson distribution, you learn that its usual parameter is the mean and the mean can be any positive real number.   If you said the mean could take only integer values, then you wouldn't have the whole family of Poisson distributions.

To say Geiger counter clicks at your lab bench follow a Poisson distribution with 2.7 clicks per second does not mean there there are 2.7 clicks in any particular second (what you are complaining about) but rather the number of clicks per second gets closer and closer to 2.7 as longer and longer time intervals are observed.

But it also says that the probability of the number of clicks in any particular interval one second long has the Poisson distribution with mean 2.7.

    > x <- 0:8
    > names(x) <- x
    > cbind(dpois(x, 2.7))
              [,1]
    0 0.067205513
    1 0.181454884
    2 0.244964094
    3 0.220467685
    4 0.148815687
    5 0.080360471
    6 0.036162212
    7 0.013948282
    8 0.004707545


of course any nonnegative integer is possible but the probabilities get really small for large n.",3,xdzwqh,"I am learning statistics, and start to wonder. I often here a phrase like: ""The average family consists of 5.4 persons"". Is it ok to say this? I understand how it is calculated, but is it correct?

So, here are my questions:

1. Is it ok to report a non-whole mean for a discrete integer variable?

2. Are there any ways of calculating discrete means and variances? (just like there exist discrete optimization, shouldn't there be discrete statistics?)

3. What are the better ways of reporting central tendencies for such variables?",AskStatistics,2022-09-14 04:50:28,3
"This doesn’t answer your question directly but [here](https://www.tqmp.org/RegularArticles/vol15-3/p174/p174.pdf) I wrote about how to graph data from that type of design. One thing you should make sure of is your graph represents the variability of pre-post differences. If I understand you correctly, the SE you want is based on between-subject differences which are likely to be large and inconsistent with your inferential statistics.",1,xdvo5j,"Hey everyone! I have a basic question as i am new to r graphs: I would like to show descriptively my preliminary results (N=20) for two groups (EG n = 9, CG n=11) in a pre-post design, Outcome = Score of a questionnaire.

For this I would like to show in one graph both groups (x-achsis: pre & post, y-axis: Score of a questionnaire). For this i would like to show M and SE and a line between pre-post M for each group. Moreover I would like to show the single measurement points of the participants. Unfortunetly, i did not find any code or tutorial online (probably because i search the wrong way..) - could you give me a code or any link/tutorial?  
Would be very, very helpful! Thanks!",AskStatistics,2022-09-14 00:42:04,1
Comparing AICs for nested models is equivalent to a LR ratio test with an alpha (significance threshold) of around 0.14.,3,xdv7ya,,AskStatistics,2022-09-14 00:13:29,1
"because the notion of correlation you are working with presumes that variance has to be non-zero. if all of the values of one of your sets is the same, the variance of your sample is zero and this puts a zero in the denominator of the equation for the [pearson correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient#Definition).

Correlation is a relationship between random variables. if you know what the variance of the ""all zeros"" set is supposed to be, use that instead of trying to estimate it from your sample. If the ""all zeros"" set always evaluates to zero and can never be any other number, it's not clear what correlation would even mean since your two variables have different support. 

Found some discussion of what correlation could mean in the context of a dirac delta function (which your ""always zero"" 'sampler' is an example of). warning: calculus. https://physics.stackexchange.com/questions/283274/dirac-delta-function-and-correlation-functions",11,xdgmnx,"I have a data set in which one array of values ranges randomly from -5 to +5, the other array of values is all zeros.

If I try to calculate the correlation between the two arrays, I get an error because you can't divide by zero. Not sure what you do there. Two options I can think of:

1. Interpret the result as 0 correlation. 

2. Change one of the values from 0 to 0.1.

If you do the latter, the end result is around r = .46 That's a huge difference from 0! 

So I have two questions:

1. What do you do to prevent the error when one set is all zeros?

2. Why would there not be 0 correlation if one set of values is all the same and the other is random? Clearly there is absolutely no correlation between them, so why does the correlation coefficient show such a high result? Am I doing something wrong? I thought 1.0 is a perfect correlation and 0.5 is a moderate correlation, is that not correct?",AskStatistics,2022-09-13 12:45:11,22
"This has nothing to do with logistic regression.  It is a general property of maximum likelihood estimation under the ""usual"" regularity conditions ([one parameter argument (19 slides)](https://www.stat.umn.edu/geyer/5102/slides/s3.pdf#page=32) and [multiparameter argument (7 slides)](https://www.stat.umn.edu/geyer/5102/slides/s3.pdf#page=86))",3,xdukks,"See title. I know everything else pertaining to this including why the Fisher Information matrix equals X^(T)VX where X is the design matrix and V is the variance matrix of y (diagonal matrix). I do not understand why the diagonal of its inverse gives the variance of the coefficients. I can't find an explanation anywhere it is [mentioned](https://stats.stackexchange.com/questions/89484/how-to-compute-the-standard-errors-of-a-logistic-regressions-coefficients).

Any help would be appreciated.",AskStatistics,2022-09-13 23:33:24,5
"You should clarify exactly what you did in your ""chi-squared test""; please give details of what you did and why.

Be careful: 

(1) The two sets of residuals are not independent of each other so if you're using a test of equality of variances that assumes they're independent it will be wrong.

(2) Even in that case, it would generally be a form of F test, not a chi-squared. 
I don't see any good basis for claiming that there should be a chi-squared distribution of some test statistic here, except perhaps asymptotically (in which case you should explain the basis for that justification in this situation)

(3) the test R would be reporting (assuming you're looking at the right thing) would be a correct test to do, as long as the assumptions are approximately correct.",3,xdo1kp,R tells me that Model B (including 3 variables) significantly reduces error as opposed to model A (only 1 variable). However a chi square test of variances on the residuals from model A vs residuals of Model B shows no significant difference. Should I conclude that this my error is indeed reduced in model B?,AskStatistics,2022-09-13 17:59:36,1
the first chapter of Deep Learning is an excellent review of background fundamentals.,1,xdlfdv,"Hey All,

I'm starting a masters in CS this upcoming January and looking to bring my stats knowledge up to par to understand texts like Deep Learning- Goodfellow, PRML, ML A Probabilistic Perspective, etc. 

My background is civil engineering and in undergrad took Calc 1-3, Diff Eq, Linear Algebra and a non-calc based stats 101 type class. When I got my masters in Civil-Transportation I took some stats modeling classes that covered regression (linear, logistic, poisson, negative binomial, etc) pretty heavily using R+NLOGIT.

At this point my stats knowledge is pretty disjointed and feel like I dont have a solid understanding of the fundamentals. From what I can tell with some research the most popular recommendations I've seen are:

1. Statistical Inference- at first glance this book feels way above my level, like I can probably work out some things with effort, but I'll probably need another book to fill in the gaps.
2. All of Statistics- Looks solid as a reference for someone who's taken way more stats than I have, super concise but super dry read.
3. Mathematical Statistics with Applications(Wackerly)- This is one that feels like could be a possibility
4. Introduction to Probability(Blitz)- This feels like the other contender

So far books 3 and 4 seem like best options, is there anything else I'm missing or votes one way or another?

&#x200B;

Thanks!",AskStatistics,2022-09-13 16:00:00,1
"You're not interested in *testing* -- it sounds like you want to do some kind of clustering. If you have some specific notion of ""similarity"" in mind (e.g. there is some specific property of these variables that you're interested in, like the mean age), then you can tailor your similar metric to those features. If you're really just interested in similar distributions, then the Wasserstein metric might be one possibility.

A more fundamental question is do you want to compare e.g. the *overall* age distribution, or are things like the age distribution broken down by sex interesting to you (i.e. do you care about the joint distribution)?",2,xdepfj,"Using census data, what's the best way to find cities or postal codes that are the most similar and dissimilar in binned population distribution by age and sex?

I've tried using wasserstein distances but I'm not sure if it's the appropriate test/metric to apply.",AskStatistics,2022-09-13 11:28:22,2
I don't see a hypothesis here to test.,1,xdix0l,"

Hi everyone,

I have a question regarding a Type of Test for significance.

From the Manuscript:

In 2018, we identified 56 hospitals offering either therapy a (31), b (13) or c (11). In total 391 clinics Performed therapy y in 2018.
In 2018, 62% (8/13) hospitals offering b increased therapy y, while 82% (9/11) of hospitals offering c increased therapy y. In contrary, only 48% (15/31) hospitals offering a increased therapy y and 47% (159/335) of hospitals without offering a/b/c increased in therapy y.

Thank you!
Best Regards",AskStatistics,2022-09-13 14:16:53,3
"It depends on what the denominator of the original figure wa to get the average. It may not gave corresponded to  38 million (was it instead, out of Candian adults? Canadians with a license?)",1,xdf1sz,"If I see that the average Canadian spends 380 hours a year driving, is it statistically valid to multiply the number of Canadians (38M) by this figure (380) to arrive at a **rough** estimate for the total annual time spent driving by all Canadians (38,000,000 \*380 = 14,440,000,000 or over 14B hours)?",AskStatistics,2022-09-13 11:41:47,3
Your stats teacher can’t figure it out? That’s kind of embarrassing.,14,xcxoin,"A kid in one of my classes said that he thinks there is one day of the year that no one in the school has a birthday, i proposed this to my prior stats teacher and we've been trying to figure out what the probability of there being any one day in the year that, assuming there is an equal probability of having a birthday on any one day i.e 1/365 for one person having a birthday on any given day, no one has a birthday out of 3300 people

I was just curious if any statisticians out there knew how to calculate this

Edit: My teacher miss understood what the question of interest was from the start, when i asked her today tho she know how to do it and we got a 3.6% cahnce (we rounded to .0001 of there being one day in the year out of a group of 3300 people assuming equal probability of a birthday on any day where no one has a bday

Edit Edit: Yall are so awesome, i think its so great that you just want to do random stats problems that is so cool",AskStatistics,2022-09-12 21:15:04,15
"I would start by creating two histograms, one with bare minus shoe times for sprinting and one for running. For statistical tests you could use paired t-tests but first consider how greatly the difference scores deviate from normality. If highly skewed you could consider a transformation such as log or square root.",4,xdc3n2,"About my study:

Participants: 100

4 tests done on the same participants:

1) Sprinting barefeet
2) Sprinting with shoes 
3) Long distance running barefeet
4) long distance running with shoes

Essentially I want to study if running with shoes is better for speed.

What test should I run?",AskStatistics,2022-09-13 09:28:59,3
"They're equivalent by definition. The likelihood L(θ|X) is *defined* as the joint density f(X|θ). Note that the text you posted isn't *strictly* accurate, since the density is not a probability, but it's a reasonably good intuition in any case -- a parameter is *more likely* if it makes the data *more probable*.",6,xdb697,,AskStatistics,2022-09-13 08:52:27,12
"Seems like a silly thing to ask for.

Benfords law is about large collections of numbers which must span orders of magnitude, or it doesn't work.

Following video explains things nicely, and why you can't always apply it.

https://youtu.be/etx0k1nLn78",1,xdaf9e,"So performing a benfords law analysis (first two digits) over a large set of balances to identify digit sequence outside the expected curve. 

Can someone explain to me what exactly does “identify digit sequence outside the expected curve” even mean?",AskStatistics,2022-09-13 08:22:20,1
"> for (i in 1:length(k)) s = s+campione$phishing[i]

This is just the sum over campione$phishing[1:k]. Why put it in a loop?

> lam = s/length(campione$phishing)

Just compute the mean directly.

In any case, are you asking what the MLE *is*? Or how to perform maximum likelihood estimation more generally? For the Poisson, the MLE (and the MM estimator) is just the sample mean, so you can compute it directly. In general, if you don't have a closed form, you'll need to do some kind of optimization, and R has a few libraries (e.g. optim) that can optimize your likelihood (or log-likelihood) for lambda (or some other parameter). If you're asking how to *derive* maximum likelihood and method of moments estimators, then the basics with be covered in most rigorous textbooks on statistics (e.g. Wackerly or Bain & Engelhardt), assuming you have some background in calculus.",2,xd9dhx,"I cannot understand how I should go about estimating (numerically) the lambda parameter of a certain set of values in R, and if there is a difference between discrete and continuous distributions to make this estimate.

for example (to give an idea), if given a sample k coming from a discrete distribution (let's consider Poisson), precisely with the unknown lambda parameter, does such writing make sense to estimate lambda?

    # MLE
    L_pois = function(lambda) (lambda^sum(k) / factorial(sum(k)))*exp(-lambda)
    s = 0
    for (i in 1:length(k)) s = s+campione$phishing[i]
    lam = s/length(campione$phishing) #lam = mean(k)
    
    # MM
    lam = mean(k)

In case it is correct, if I had other distributions (ex. Bernoulli, binomial, exponential, ...), how should I do?",AskStatistics,2022-09-13 07:41:38,3
"The coeff isn’t significantly different from 0, so the sign doesn’t matter. There’s no evidence of an interaction here, so you could default to the model with just x1 and X2 and no interaction.",1,xd395w,"First of all, thank you taking the time reading this!

I am new to stats and I try to wrap my head around the problem of interaction effects when interpreting regression summaries.

Am i right in the following conception?  
Lets say we have this regression summary:

Intercept: 0.15 \*\*\*  
Coefficient1: 0.16 \*\*\*  
Coefficient2: 0.19 \*\*  
Coefficient3: -0.85 p=0.6

Conclusion:  
Coefficient3 is an insignificant interaction term because of its p=0.6 and the negative correlation compared to coef1 and coef2. Therefore coefficient3 should be excluded from the model. IF the coefficient were significant AND positive correlated to coef1 and coef2 it should be included.

Does the coefficient have to be positive/negative correlated relative to the other coefficients or is it only the p-value that determines whether or not it should be included? Or am i totally lost about this?

Again, thank you so much helping me out!

Best regards",AskStatistics,2022-09-13 02:49:31,4
"Your outcome / response `cell_type_frequency` is a continuous variable and not a binary variable. Logistic fit should throw a warning for that (an error would be more appropriate I guess).

BTW, it would help to know the goal of the analysis. I re-read the beginning of the text few times and honestly still don't get it. Want to predict `cell_type_frequency` from the `cell_type` with other covariates & random effects?

Also I see `time_point` predictor variable listed as a factor. Should it be numeric?",2,xcx5cx,"I've collected data during the course of a study of 2 different types of subjects. The test subjects have a specific disease while the control subjects have a fundamentally similar disease that is governed by a distinct biological mechanism to that of the test subjects. For each subject, and at multiple time points (in days), I isolated cells from biopsies and determined the number of a certain kind of biological cell present out of a total number of cells collected. I plan to use logistic glm to analyze this data. I would appreciate any opinions and advice the community might offer on the proposed analysis.

Structure of the data: 

    # A tibble: 72 × 6
       subject_id subject_type time_point cell_type cell_type_frequency total_cells
       <fct>      <fct>        <fct>      <fct>                   <dbl>       <dbl>
     1 test01     test         224        Cell A                0.161          1647
     2 test01     test         224        Cell B                0              1647
     3 test01     test         224        Cell C                0              1647
       ...        ...          ...        ...                   ...            ...
    70 control08  control      260        Cell A                0.118          2537
    71 control08  control      260        Cell B                0.00473        2537
    72 control08  control      260        Cell C                0.0146         2537

Plot of the data:

[points represent individual \`subject\_id:time\_point\` observations.](https://preview.redd.it/gf5eywsgqjn91.png?width=2560&format=png&auto=webp&s=adf4000c60fa0bce743db78fe07b597ebdb3b404)

I plan to apply logistic glm to the analysis of this study. I will specify the functional form of the model like this: 

    cell_type_frequency ~ subject_type + cell_type + (1 | subject_id) + (1 | subject_id:time_point)

where the response `cell_type_frequency` is predicted by the fixed effects `subject_type` and `cell_type` with random effects for `time_point` nested within each `subject_id`.

I would then implement it like this:

    lme4::glmer(
      formula = formula,
      weights = total_cells,
      family = binomial(link = ""logit""),
      data = data
    )

where I pass the `total_cells` column to the weights parameter and use the Binomial probability distribution with a logit link function.

After doing that I check the summary:

    Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
     Family: binomial  ( logit )
    Formula: cell_type_frequency ~ subject_type + cell_type + (1 | subject_id) + (1 | subject_id:time_point)
       Data: data
    Weights: total_cells
    
         AIC      BIC   logLik deviance df.resid 
      7308.8   7322.5  -3648.4   7296.8       66 
    
    Scaled residuals: 
        Min      1Q  Median      3Q     Max 
    -23.961  -2.802  -0.229   1.677  49.434 
    
    Random effects:
     Groups                Name        Variance Std.Dev.
     subject_id:time_point (Intercept) 1.25043  1.1182  
     subject_id            (Intercept) 0.02807  0.1675  
    Number of obs: 72, groups:  subject_id:time_point, 24; subject_id, 16
    
    Fixed effects:
                        Estimate Std. Error  z value Pr(>|z|)    
    (Intercept)         -1.86124    0.31006   -6.003 1.94e-09 ***
    subject_typecontrol -0.33264    0.48214   -0.690     0.49    
    cell_typeCell B     -2.72072    0.02240 -121.437  < 2e-16 ***
    cell_typeCell C     -2.57945    0.02105 -122.531  < 2e-16 ***
    ---
    Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
    
    Correlation of Fixed Effects:
                (Intr) sbjct_ cll_CB
    sbjct_typcn -0.609              
    cll_typCllB -0.006  0.000       
    cll_typCllC -0.007  0.000  0.092

and I see that the estimates for `cell_type` have large negative effects that are statistically significant, and that the estimate for the `subject_type` effect  is more modest and not statistically significant. To me this could mean the nature of the cell types I enumerated from the subjects have more to do with their frequencies in each subject than the nature of the type of subjects.

Next I check the residuals to see if the model is not bad:

https://preview.redd.it/hldww8wjojn91.png?width=698&format=png&auto=webp&s=afbcf87ada00b1632140395b6b5b68507a1ad81d

heteroscedasticity! Though I'm not sure if this is a problem or not. I think it might be present because the `cell_type_frequency` has such a large range across the levels of `cell_type.` One question I have is whether it would make sense to do 3 separate logistic glms for each `cell_type` given this result.

Next, I want to probe the effect of `subject_type` further with pairwise comparisons within each `cell_type` level:

    pairs(
      emmeans(
        object = glmer, 
        specs  = ~ subject_type | cell_type, 
        regrid = ""response""
      )
    )

resulting in:

    cell_type = Cell A:
     contrast       estimate      SE  df z.ratio p.value
     test - control  0.03426 0.04889 Inf   0.701  0.4835
    
    cell_type = Cell B:
     contrast       estimate      SE  df z.ratio p.value
     test - control  0.00285 0.00407 Inf   0.699  0.4848
    
    cell_type = Cell C:
     contrast       estimate      SE  df z.ratio p.value
     test - control  0.00327 0.00468 Inf   0.699  0.4848

which seems to make sense given the overall estimate and P value of the `subject_type` fixed effect from the model summary.

So is logisitc glm an appropriate method of analysis for this data? If not, what would be a valid alternative? If yes, is my implementation of the glm method and the following pairwise testing OK? Or am I so far off base here that the only responsible thing to do is to delete R and turn in my statistics privileges? Thank you r/AskStatistics community.",AskStatistics,2022-09-12 20:48:21,2
"I dont think you will be able to compare the count (kg cut) directly since this is what is affected by covid. 

In my opinion you could try to find a way to transform it into a rate and compare it that way (ex, kg cut per machine or per person). Then you can compare the covid rates more directly between covid affect and non covid affected years",2,xd1xhk,"Hello this is my first post so if I make any mistakes or the question has been answered please point me in the right direction.

Let's say I work in a factory that cuts cheese (not a euphemism lol). I have weekly production data (kg cut) for the last 6 years. I want to know how my fingers are for the current year but the issue is that with COVID over the last 2 years we have cut a lot less so my year to date shows a massive excess. How can I get my figures to give a correct comparison  I was thinking of excluding the COVID years but it won't be correct. 

Thank you in advance.",AskStatistics,2022-09-13 01:26:04,1
"The [explanation at Wikipedia](https://en.wikipedia.org/wiki/Fisher's_exact_test#Example) is pretty good.  


The expected frequency is that each of the cells in a row have the same proportion to that rowʻs total as the column totals do to the grand total.",5,xcq2cq,"I was trying to make sense of a statement I came across in a paper. The author states that ""The difference between the observed frequency of zero and the expected frequency can now be tested for significance using any statistical test appropriate for contingency tables. For example, the Fisher-Yates exact test yields a probability of error of p = 1.96E-20"". Can somebody explain me how to use the Fisher-Yates test to stimate the significance of the difference between observed and expected frequencies?",AskStatistics,2022-09-12 15:22:22,6
"That is generally not possible, unless you either used some form of nonparametric correlation, like Spearman or Kendall, or the regression includes covariates.",16,xcg93i,"I am trying to make sense of my thesis results, one of which involves a significant correlation between X and Y but a nonsignificant regression between X and Y. I am confused about how to interpret this, as shouldn't the regression be significant if the correlation is? Thank you in advance!",AskStatistics,2022-09-12 08:48:09,10
"You should probably ask the ""panelists"" this question now-- we have no idea what the expectations are at your school/program/etc.  

However, generally speaking, there are fairly standard formulas for determining necessary sample sizes, and there is nothing wrong with using a command in a stats program or a website to punch those numbers into the calculator for you.",9,xctgxx,"I am in the middle of our writing my thesis and I decided to use an online sample size calculator to determine my sample size. However, I am afraid that I might be questioned for doing so (instead of computing it manually) during my defense. Is it okay that I did this? Or will the panelists disagree with me?",AskStatistics,2022-09-12 17:53:55,6
"I'd say you can separate out your question into two parts: distances and variances. 

Given a distance measure, you can use [Frechet variance](https://en.wikipedia.org/wiki/Fr%C3%A9chet_mean) to measure the variance. Finding the actual Frechet mean can itself be a pain, but if you don't have too many sets you can use the [relationship between variance and pairwise distances](https://stats.stackexchange.com/questions/20108/link-between-variance-and-pairwise-distances-within-a-variable) to get around that.

That's at best half a solution, because it offloads a lot of the work to figuring out an appropriate distance metric. I'm not fully sure I understand what you want out of your distance, though. Can you have repeats? Or are you looking for more of a presence/absence distance? That is, do you care about {1,1,1,2} or is that just {1,2} as far as you're concerned? Depending on which of those you want, and assuming you don't have a way to differentiate between d({1,2},{1,3}) and d({1,2},{1,4}), then something like a Hellinger distance (counts matter) or a Hamming distance (counts don't matter) could work. In either case, I think you can get around things that don't appear in either set. For a Hellinger distance that just gives p and q both 0 so the contribution of that dimension would be 0. You might need something that isn't technically a hamming distance, but there's something that could work I'm sure.",2,xci83r,"Hello, first time poster here!

I'm interested to see if there is a usual measure of variance across a sequence of sets of discrete data. Kind of like Shannon entropy but for sets of data. For instance, let's say a given customer purchases multiple sets of items (identified by distinct integers) across three transactions: {1,2} , {1,2} , {2,3}

I'd like some kind of variance measure to compare how stable this sequence of transactions is compared to a set like: {1,2} , {1,2} , {1,2} , {1,2} , which I would want to have ""zero variance"" under this framework.

The order within the sets doesn't matter (so wouldn't want to use like Levenshtein distance or something like that) and also the number of sets and the length of each set may not be fixed. Not sure if this is an established concept in statistics but hopeful someone has seen something like this before! Let me know!",AskStatistics,2022-09-12 10:04:43,3
"ANOVA doesn’t assume equal group sizes, although it is more resistant to assumption violations if they are. With such a big imbalance, you should think carefully about how your model is set up.",3,xcjpgy,"Let's say I want to compare the means of three groups. Mean is a continuous variable (anxiety), group is categorical (gender). 

The sample is distributed like:
Group 1 (men): n=5
Group 2 (women): n=50
Group 3 (other): n=5

Would this be possible with an ANOVA?",AskStatistics,2022-09-12 11:03:11,5
"Look *closely* at Bayes theorem. Notice there's 3 components on the right hand side.

Look carefully now at the relationship between posterior, prior and likelihood.",2,xcqm01,"Hello everyone,

I'm trying to plot a posterior distribution which I'm obtaining by multiplying a prior and a likelihood together. I've normalized the posterior distribution but I'm not sure the plot I'm looking at is correct. I have two normal distributions for my prior and likelihood distributions but my posterior seems... small?

Thank you very much!

    #Exact same distributions, normal
    x <- seq(-10, 10, by = .1)
    prior <- dnorm(x, mean = -5, sd = 1)
    likelihood <- dnorm(x, mean = 5, sd = 1)
    posterior <- (prior * likelihood) / 2
    
    #Standardize the posterior distribution
    std_post <- posterior / sum(posterior)
    
    plot(x, prior, type = 'l')
    lines(x, likelihood)
    lines(x, std_post) #unimodal, looks small!",AskStatistics,2022-09-12 15:45:43,2
"It’s OK to report whatever you want as long as you don’t claim something was a planned analysis when it was not or omit analyses that are inconsistent with the conclusion you or your advisor were hoping for. It’s important not to interpret a difference in significance as meaningful (such as significant for treatment but not for the control). As many including Gelman have stated, a difference in significance is not significant.",2,xcpnc9,my advisor initially wanted me to do within group analyses on an rct. findings were null after controlling for baseline characteristics. he wants me to present this data in the paper but also look at within participant changes within the treatment group alone and then the control group alone. is it frowned upon to do this? or is it okay as long as all results are presented? i’ve been in grad school for ~3 months so still trying to learn best practices for stats. thank you.,AskStatistics,2022-09-12 15:05:10,1
"I am playing with a toy example to see how modeling could be potentially different for causal inference and predictive exercise. But I found myself ran into an identification problem in the causal case. I wonder if I missed something.   


  
Suppose the true relationship among random variables y, e1, z1, z2 are as the following:  
y \~ e1 + z2  
x \~ e1 + z1   
Except e1, all other variables in the above are observable.   
e1, z1 and z2 are assumed to be jointly independent.   


  
For the predictive modeling case, we want to predict y. For causal inference case, we want to understand the effect of z2 on y.   
Right off the bat, we know that z1 is independent of y. Interestingly, for predictive modeling, it is better to add this seemingly irrelevant variable z1 to the model y \~ f(x, z1, z2).  


  
But for causal case, it appears that x is a collider. It might not be wise to open up a backdoor between e1 and z1 by including x in the model. But we know the true model requires us to remove the effect of z1 from x to precisely recover e1 if we add both x and z1 to the model, hence to better capture the effect of z2 on y. 

&#x200B;

  
Perhaps my understanding of DAG is wrong. Or in this case, do we actually have an identification problem?

&#x200B;

Edit: format
Edit 2: replace all e2 with z1.",7,xbwg4d,,AskStatistics,2022-09-11 15:58:53,37
Testing differences in proportions and computing confidence intervals on differences appears to be what is most relevant. [This page](https://online.stat.psu.edu/stat100/lesson/9/9.3) will compute a CI on difference between proportions. Note that confidence intervals of individual proportions can overlap and the difference still be significant.,2,xcgqmm,"Hello Stat Gods

I work with various shipping carriers and say I want to know with statistical confidence if their ""packages lost"" rates are different from each other?

What is the best way to do this? Ideally i would like to see a confidence interval of each carrier's rate. This way i can look at any 2 carriers and say they are similar (if the CI overlaps) or not. So chi-squared doesn't work for me i guess.

This is what i have done so far. I calculated the confidence interval for each metric separately. How far off base am I?

[https://docs.google.com/spreadsheets/d/1nRdp0NgZkM8o7Er3A0YwV0MK36ZVo\_Gs/edit?usp=sharing&ouid=113412420602982341380&rtpof=true&sd=true](https://docs.google.com/spreadsheets/d/1nRdp0NgZkM8o7Er3A0YwV0MK36ZVo_Gs/edit?usp=sharing&ouid=113412420602982341380&rtpof=true&sd=true)

Something to note is that I am counting every single shipment in 2022 from my company's data warehouse. Does that make these rates population proportion? or are they still sample proportions?  I am really confused. 

Thanks in advance for any suggestions!",AskStatistics,2022-09-12 09:06:46,6
Simulation (including Monte Carlo simulation) is about using small known pieces and putting them together to get a picture of a bigger and more complex question. It's not a silver bullet for solving problems where you have no information at hand.,1,xcerld,"For example, we can assume that the money spend in marketing is negative for the profit of a company but maybe not, how do we know that?",AskStatistics,2022-09-12 07:48:19,2
Maybe clustering,1,xcdo0l,"Hi,

I am working on a project and also trying to incorporate some data science techniques to improve analysis. The problem we are trying to understand is that, let's say a user listened to a particular song or podcast, what other songs or podcasts the users listened to? Is there a particular trend?

We can use standard data analysis for this, but curious to see if there are any stats techniques\\visualization methods we can use to solidify the analysis.

Some of the techniques I was considering were associative learning",AskStatistics,2022-09-12 07:03:02,1
"I think it's 1:5 odds which is the same as 1/6 chance. You can calculate the probability of something not happening by subtracting it from 1. So probability of rolling a 3 is 1/6, probability of not rolling a 3 is 1 - 1/6 or 5/6. You can calculate the joint probability of independent events by multiplying. So not rolling a 3 in 25 rolls is 5/6 multiplied 25 times, or 5/6 to the 25th power.",6,xca4n5,"I know for a single dice roll the odds are 1:6 for any number.  If I wanted for example to calculate the odds of NOT rolling a 3 with 25 rolls, how would you calculate that?",AskStatistics,2022-09-12 04:19:13,9
"Yes, the sums of squares formula accounts for negative values.",1,xc58ll," I am setting up a study where I want to measure if a boundary between 2 habitats (e.g. shoreline edge) moves over time, and if there are comparable differences between locations (differences in mean distances from a fixed points). So, I intend to set establish 5 sites (2 test and 3 control sites) each with 5 replicate line transects, which cross the boundary from fixed points. The first survey measurement will establish the initial distance (0m) from which the 2 subsequent survey measurements would be compared. So, if a shoreline would accrete, subsequent distance measurements would be positive values and erosion would represent minus values. A hypothetical dataset for the second survey could look like this, where all second survey measurements are distances from the initial shoreline reading (for sites 1 to 5 values in cm); Test Site 1: 2, 5, 3, 4, -3 Test Site 2: 10, 6, 8, 6, 11 Control Site 3: -5, -4, 0, 0, -1 Control Site 4: 0, 1, 0, 2, 1 Control Site 5: 8, 6, 10, 7, 8. My question is- is Anova suitable to detect if there are differences in mean distance (from fixed point) values between 2 surveys? Regardless of the negatives and zero's, I will still get a measure of variance within sites and among sites. Ideally, I would like to test individual sites and not as grouped control and test sites. Depending on the distribution of data, I could then use a post hoc test to test for significant differences between individual sites.  Hope it makes sense!",AskStatistics,2022-09-11 23:22:17,2
"It would be useful to know what model you are actually running, and whether there are covariates, but I assume you have a configuration where the effect of a variable X on a variable Y is mediated through a third variable M, and there might or might not be covariates included. The fact that the total effect is significant while the direct effect is not indicates that the mediator acts as a confounder in the model that estimates the direct effect. So in essence, the effect of X and M on Y overlap. The total effect can be misleading, as it is calculated solely from the equation Y \~ X, without taking covariates or the mediator into account.",1,xc0dpj,"I have a total effect that is significant, with insignificant indirect effect and insignificant direct effect.

What does this mean?",AskStatistics,2022-09-11 19:03:24,1
"Unfortunately ""density"" is used twice but means different things. There is ""probability density"" and ""density of contour lines.""


The probability density is the graph on the left, it look like a hill.  The height of the hill is the value of the probability density function.  That height depends on foot size and literacy score.  Reading the contour plot, we see there are spots on the hill that are at the same height.  A foot size of 15 and a literacy of 85 seem to have the same probability density as foot size of 20 and literacy of 100.  Those (and other locations) are where the probability density are constant.  There is a different constant probability density at 15 and 90, it's the same as 15 and 110.  This explains the contour plot on the right.


Density of contour lines means how closely spaced they are on the contour plot.  The contour lines are closely packed together near (18, 95), but widely spaced near (12, 90).  ""Local density"" means how dense the contour lines are at any point on the contour plot. 


If you want to walk up that hill and you want to walk in a straight line to the top at (15,100), you'd start from (0,60) rather than from (30,60) because the path to the top would be less steep, meaning the gradient is lower.",11,xbkfsn,,AskStatistics,2022-09-11 07:47:39,11
"I would put all of them in the model but only look at the significance of IV2. It is also possible to do a [fully multivariate analysis](https://www.researchgate.net/publication/285820380_A_truly_multivariate_approach_to_MANOVA) rather than depend on univariate follow-ups. If your purpose for the MANOVA was to control the type I error rate there is an alternative of,separate ANOVA’s with the Bonferroni correction.  Neither approach is universally more powerful. Even following a significant MANOVA, you still should be concerned with multiple univariate tests. For example, suppose one has 20 DV’s for which in the population there is a gigantic effect of one and no effect of the others. The MANOVA is likely to be significant and if followed up with univariate ANOVA’s, there would 19 chances for a Type I error.",5,xbqu3a,"It is a massive problem that my supervisor is away for 3 weeks now and he did not tell me in advance.. so I am thankful to find some answers here.

Situation: 2x2 experiment, 2 DVs

The question is just: If I do a manova and the results are like this

IV1: not significant 
IV2: significant 
IV1:IV2: not significant 

-> for the follow up univariate ANOVAs, can I include all terms again or do I have to refrain to conducting it with IV2 only, as it is the only one significant?

I am using the car package in R. 

Any  help is greatly appreciated 🙏",AskStatistics,2022-09-11 12:09:43,5
"They are not the same. The assumption of equally likely outcomes is kinda important.


Imagine you are going to have surgery. According to the classical definition of probability, the probability of surviving would be 50% - you either live or die. However, in practice something like 99% of people undertaking a surgery (in the USA) will survive, which would correspond to 99% probability of surviving under the frequentist definition. That’s a pretty big difference.",3,xbmt2n,"The Classical probability is assigning the likelihood of an event by taking the total number of elements in the sample space that satisfy the condition over the total number of occurrences, assuming each element is equally likely, and the relative frequency is the limit of the proportion of times that event A occurs in a large number of trials.

Are these probability definitions equivalent? or is it the case that they are equivalent only when each element in the sample space is equally likely, and otherwise they will always yield different probabilities?

&#x200B;

Basically are these just two interpretations of probability that ultimately measure the same thing, or are they completely different concepts with completely different implications?",AskStatistics,2022-09-11 09:24:36,6
You could use multiple regression or ANOVA which  is the same thing but may be easier to set up if you plan to include interaction terms in your model.,2,xbgu9o,"If I have 4 binary independent variables that are based on questsions like ""Did you smoke during the last 12 months? Yes/No"" and the dependent output variable is the weight gain and loss in percent, how can I find out if there's a significant influence of these binary variables on the body weight? Is it best to use multiple linear regression or are there other tests that are specifically suited for this applicatin that deal better with binary independent variables?",AskStatistics,2022-09-11 05:05:47,12
Look up Poisson distribution.,-1,xbehaz,"Hello,

I need to measure manual EPC (earning per click) against machine work. The issue is that the time frame I use to aggregate this ratio (EPC) changes the results dramatically.

Some issues:

\- if I measure EPC daily then the variance is very high (small data). For the same decisions the EPC can be $5 today and $1 tomorrow.

\- if I measure EPC annually then the gap is very high, it seems that some periods affected strongly the complete period. And it does not feel reasonable to multiply the annual EPC by the sum of clicks in the year.

Would love to hear your opinion on it ...

Thanks",AskStatistics,2022-09-11 02:50:54,1
"Does your research question address non-binary participants? Most frequently everything that is not M/F is omitted as the sample size is just too small to use in the analysis (e.g., your problem right now).

The question is: does your RQ or hypothesis concern this group?",3,xb7kd4,"Hi everybody, 

I am analyzing results of a 2x2 experiment.

DV~IV1*IV2  . Design is slightly unbalanced, so Type III SS. N=260.

To check for Gender influence, I want to include Gender as a third factor. However whereas male and female are both approximately 130 observations, non binary are only 3. 

What can I do? If I remove those 3, I cannot compare the results to the initial ANOVA. If I code them NA I still have an empty cell. 

Help is so much appreciated. This question is driving me crazy since hours and I don't find appropriate information anywhere.🙏",AskStatistics,2022-09-10 20:04:17,8
"A frequentist can say ""I want to use this sample statistic, T, as my test statistic"" ... irrespective of where that statistic came from. The relevant questions for a frequentist are (a) how to set it up so the significance level is what you choose it to be\* and (b) making sure it has good relative efficiency compared to other options (otherwise why would you not take one of those other options?)

There's nothing stopping a frequentist from contemplating those considerations for essentially any sample statistic whatever; if that happens to be a posterior mean for some parameter (say), that should pose no major conceptual difficulty, aside from the technical aspects of evaluating the significance level and power.

---

\*  or does not exceed it, or is never more than a little above what you choose, etc",3,xb8zwq,"The scenario I am thinking of involves regression coefficients estimate generated via Bayesian  MCMC. Hypothetically, I could just plug this into the equation of the Wald test for statistical significance the way one might use for a frequentist generated parameter. So what prevents this? Is it because doing this makes the interpretation explode? It seems the p-value would cease to have much semantic meaning other than a couple numbers after a decimal. I would assume doing something like this is a big no no, but I dont think I’ve ever gotten a technical answer other than “that’s frequentist, this is Bayes, so just don’t.” 

In the scenario where I go completely renegade and do something like this, what would be the consequences?",AskStatistics,2022-09-10 21:19:40,4
"I’d not disaggregate and do this all with one model, or a few models. The problems with disaggregating include but are not limited to (a) reduced statistical power resulting from the smaller sample sizes for the individual data sets, and (b) not having a simple way to test whether the coefficients differ across the various models. 

So I’d suggest not disaggregating…..",1,xb0d1z,"Hello people who know stats more than I do,

&#x200B;

I'm confused on if I should use a multiple regression, or just compare simple linear regressions by group.

I have a dataset with two continuous quantitative variables (Y & X2) and two categorical variables (X2 & X3. Let's say crop size (Y), amount of fertilizer amount (X1), soil type (X2), and crop species (X3).

Currently I am separating my data into individual datasets by crop species and soil type. So I have a dataset for corn-loam, a dataset for corn-clay, potato-loam, potato-clay, etc.

I am then running individual linear regression on crop size by fertilizer amount (Y\~X1) for each of the individual datasets, giving me individual regression coefficients and p-values for the relationship between size and fertilizer. I can compare these coefficients and p-values and see if differ between the groups. So if potato-loam and potato-clay have different coefficients, or different significance, I can say soil type has an effect on the relationship between size and fertilizer.

&#x200B;

But I'm not sure if this is the right way. Which is where I think multiple regression comes into play. But what is the difference between a multiple regression and comparing individual regressions? Is there a benefit to one over the other?

&#x200B;

Edit: Looks like the test I'm trying to find is an ANCOVA, not a multiple regression. Since I'm working in R, and can't separate output by species, I can test by soil type still.

    library(stats)
    library(car)
    library(multcomp)
    model <- aov(Y ~ X1 + X2, data = X3disaggregate)
    Anova(model, type=""III"")
    meanssep <- glht(model, linfct = mcp(X2 = ""Tukey""))
    summary(meanssep)
    

&#x200B;",AskStatistics,2022-09-10 14:19:58,3
"Neither. In many cases you'd have a single 1 or 0 at each value, for which the logit is infinity or minus infinity.

It doesn't transform the response in that fashion (transformations do crop up in the usual algorithms but not in that way).",1,xb2lpb,"I understand that logistic regression coefficients are linear in the logit of their respective probabilities. My question is does the computer calculate the logit for every single value of the independent variable or does it ""bin"" the data then find the logit of those groups?",AskStatistics,2022-09-10 15:59:35,2
"You're asking for a function f(x1,x2,x3,x4), such that 
y = f(x1,x2,x3,x4), for all your known (x1,x2,x3,x4,y) values.

There's an *infinite* number of functions for which this is true.


The problem is that if you only know the values at the specified inputs; there's no restriction here on what happens at any other set of inputs.

You have to restrict the problem space a bit or you're wasting your time asking. 


Note that because you say you know the *exact* output that goes with those numbers, implying that if we observed the same inputs we would get the same output -- there's no error, no approximation, no fitting -- this is not a statistics question and is off topic here.

> My end goal would be me pluging in x,y,z,a and it properly spitting out the correct output.

You make a list of every known combination, and record the known output that goes with it (there are ways to do that efficiently). It can't give you outputs for combinations you don't already know, of course, unless you restrict the sort of functions you could have, they might be anything.

---

Edit: it's unclear to me why so many of the answers assume that f is *linear* in x1-x4. No such restriction is mentioned in the question.",7,xaseq3,"Simple question, wondering if anyone can help me: I'm trying to google a problem, but I don't exactly know the name of it. Let's say I have 4 variables an an output:

x,y,z,a = b

I have various values for each of the variables and the exact output for each combination, but I don't know the formula as to how those values arrive at the output:

12, 1.3, 41, 11 = 7.8

3, 47, -11, 3.2 = 6.7

78, -99, 14.67, 12 = -314.5

I'm searching for how to find/calculate the formula in R, but have no idea what the name of this is; What should I be googling? 

Edit: My end goal would be me pluging in x,y,z,a and it properly spitting out the correct output.

&#x200B;",AskStatistics,2022-09-10 08:40:27,10
[deleted],1,xay78u,,AskStatistics,2022-09-10 12:45:41,1
Why not make the significant values bold or slightly larger. You could use different degrees of darkness for different significance levels.,1,xb02nh,"Hi all:

I’m fitting a simple descriptive regression to a huge dataset, and the vast majority of explanatory variables are highly significant. Standard practice would be to place a *** or a * behind each significant point estimate. But this will cause the table to be rather crammed. 

For presentational purposes, it makes more sense to show which point estimates are *insignificant*. 

Is this ever done?

Thanks",AskStatistics,2022-09-10 14:07:08,2
Condition is a between-subjects variable and pre-post is within. You could do a repeated measure MANOVA which is equivalent here to computing post minus pre-test scores and doing a between-subjects MANOVA on difference scores. Also possible is a MANCOVA with pre-tests as covariates or two ANCOVAs using a Bonferroni correction. You can never show that there is no effect for the control so I would modify the hypothesis to be that the effect is bigger in the experimental group than the control. From a test just with the control group you may be justified in stating there is no evidence of a difference for controls but not that there is no difference.,1,xauirq,"Hi, I'm a bit confused about what test is best for this analysis. 

For this study, participants were randomly assigned to either the experiment or control condition. In the experiment condition, they received a treatment and in the control they did not. There were two dependent variables (A and B). Scores were collected pre and post treatment for both dependent variables. My hypothesis is that scores for A and B will rise post treatment for the experiment group but not the control.

I'm confused if a a one-way repeated measures MANOVA is ideal or not. I’m also having a hard time differentiating between within and between participant conditions.",AskStatistics,2022-09-10 10:10:07,4
"> I'm reading a paper about personality testing, and it says that the test-retest reliability is 0.91.

> Is it referring to the Pearson product-moment correlation coefficient between the 1st and 2nd score?

It might be Pearson, but ""test-retest reliability"" is not specific. Personally I wouldn't expect it to be Pearson correlation, because correlation doesn't register changes in the actual value. I.e. if you measured peoples height and then remeasured it 5 minutes later you could still have a perfect Pearson correlation if all the measurements from the second set were each 10cm larger. I wouldn't call this perfect test-retest reliability though.

In my experience it's more common for test-retest reliability to be an ICC based on ANOVA (of which there are many types): https://en.wikipedia.org/wiki/Intraclass_correlation",6,xakvi3,"I'm reading a paper about personality testing, and it says that the test-retest reliability is 0.91. It's referring to the consistency between testing the same person twice, but what exactly does it mean, statistically? 

Is it referring to the Pearson product-moment correlation coefficient between the 1st and 2nd score?",AskStatistics,2022-09-10 02:21:10,6
"
Focus on the sum for the moment, the integral is essentially the same kind of thing though. 

In the sum, you're multiplying each probability by the corresponding value you're looking at the probability of. 

If a value has more probability, it gets greater weight in the average.

Consider I have two fair 12-sided dice, labelled as follows:

Die 1: 1 1 2 2 3 3 4 4 5 5 6 6 

Die 2: 1 2 3 4 4 5 5 5 6 6 6 6

The first die has equal probability on all outcomes. Consider, for example, if you tossed the first die say 12 billion times (where such a thing feasible). Then you'd get about equal proportions of each value (to about 5 decimal places with so many tosses), and the average would be 3.5.

The second die does not have equal probability on all outcomes (the die is fair in that each face is coming up in about equal proportions in the long run, but the face-labels are unequally distributed over the faces; there are more high numbers and fewer low numbers)

Consider, for example, if you tossed the second die 12 billion times. Then you'd get a '6' about 4 times as often as you got a 1, and so in the sense of an ordinary average, '6' will get 4 times as much weight as 1 does. 

(The long run interpretation of probability is unnecessary to any of this but it can help motivate why it must be the way it is.)",3,xat06j,,AskStatistics,2022-09-10 09:05:21,6
"Because the physical concept of *density* -- that is, mass per unit of volume -- doesn't make sense at a point. Mathematically, probabilities are obtained by *integrating* a density function, and the integral of a function at a point is zero. 

Regarding the ""impossible vs. possible"" comment, I'll repost an earlier comment of mine:

---------------------------------------------

There is a pedantic sense in which it does not make sense to say that events with probability zero ""are possible"", but the main issue is that ""are possible"" doesn't really have a rigorous definition in probability theory. Strictly speaking, ""has probability zero"" means ""has probability zero"" and nothing else, but interpreting probability zero as meaning that an event is impossible is mostly reasonable, whereas saying that events with probability zero *are* possible immediately leads to mathematical problems.

It's late and it's an interesting topic, so some elaboration. Most of the comments on stack (the ones that aren't riddled with basic errors, like the idea that a uniform distribution assigns ""infinitesimal"" probabilities to individual values) go on about how events with probability zero have to be ""possible"" because we have to get some value when we draw from a continuous distribution, and so these values have to be ""possible"", but this is completely irrelevant. Distributions are models, and we can't draw from them.

The Lotka–Volterra equations are a set of differential equations used to model the population evolution of interacting predator and prey species. Note that they model the populations of each species as continuous quantities, because they're differential equations. Actual populations are discrete, but we can sometimes model them as being continuous and get reasonable predictions. It makes no sense to ask the model ""how many prey animals exist at time t"", because the equations don't model the populations as discrete quantities, though we can get a continuous value that is hopefully relatively close (if the model is good).

Random variables are models of uncertainty, and continuous random variables don't model the real numbers as a set of discrete points. There is a rigorous sense in which it doesn't even make sense to talk about a random variable taking a value at a single point, and if you do try to talk about it, then you run headlong into problems with the math. Almost everything in probability theory involves equivalences up to probability zero, and the way equivalence is defined for random variable means that, given an event of probability zero, I can construct another equivalent random variable in which that event doesn't even exist (in which case it must be impossible). It's not just that it doesn't make sense to talk about events with probability zero being ""possible"", it doesn't even really make sense to talk about them at all. They don't exist. The normal distribution, for example, doesn't ""see"" individual points, it only sees the relative density of a region of space.",7,xasso3,,AskStatistics,2022-09-10 08:57:04,12
"Because the way it is worded, it is a density function defining the probability of falling (through the ice?). Hence, as ice is thinner, the probability that you fall (through?) is higher.",13,xasm2g,,AskStatistics,2022-09-10 08:49:12,7
"You should have two answers here: one for male professors and one for female professors. Right?

For a normal distribution, a score that’s about 0.84 standard deviations about the mean is at the 80th percentile. You can verify that with a table of areas under the normal distribution. You’ll see that the area to the right of Z=0.84 is about .20, or 20%. That means the area to the left is 80%, which corresponds to the 80th percentile. 

So the answer for men = male mean + (0.84 x 5200) For the female professors, it’s the same except you’d use the female mean. 

Hope that helps.",3,xalu10,"Im studying for a quiz and i came across this question : 

The average salary of a male full possessor at a public four-year institution offering classes at the doctor level is $99,685. For a female full Professor at the same kind of institution, the salary is $90,330. Is the standard deviation for salaries of both genders is approximately $5200 and the sellers are normally distributed find the 80th percentile salary for male professors and for female professors. 

My answer is 104,053 but im not sure if thats correct , pls help me !",AskStatistics,2022-09-10 03:18:28,5
"In short, any distribution is defined by its probability density function (PDF). I'm not good with formatting equations in reddit markdown, so you'll have to do a search for ""Normal distribution PDF formula"" to see what it looks like. If a distribution has that PDF, it is normal. The standard normal distribution is just a special subcase of the normal distribution where the mean parameter (mu) is zero, and the variance parameter (sigma) is 1. Kind of like saying that If an animal has wings and feathers, and it flies, it qualifies as a bird, but if it has black feathers and also caws, it is a crow, which is a special case of bird.",5,xajamv,"I am confused between these two terms, like how they are different from each other. Anyone can explain in a layman term with a real-world example ? :)

TIA",AskStatistics,2022-09-10 00:45:03,4
In real life you don't know the true value (else why try to measure) so all of this is nonsense AFAIK.  Outlier rejection has a lot of literature but is a really tricky area.  Nothing is rigorous.,4,x9zwyt,"My level of understanding of statistics is relegated to a single course in undergrad so bear with me.  Lets say I have compound  at 20 g/L determined by a highly accurate mass spectrometer.  I add this compound to all my samples in that concentration as a gauge of how well I prepared my samples for analysis.  Anything one standard deviation above or below the true value would signify a bad sample prep and be thrown out of consideration during analysis.  Now lets say I get a bunch of samples back from analysis where 50% are very close or equal to the expected value (20) and 50% are less than the expected value to varying degrees.  Since I know what the true value should be, when I calculate standard deviation from the mean, should I still toss out data on the + end (one standard deviation above the mean) and the - end (One standard deviation below the mean) or should I just throw out the values on the - end since there are no values above the true concentration. Hope this makes sense :S",AskStatistics,2022-09-09 09:27:42,10
"Just wondering — 26 drinks a week is defined as “moderate?” Whose scale is this? That’s an average of  3.7 drinks per day. Seems weird.

Edit to say your calculation is definitely wrong but I can’t explain why. Your result just isn’t reasonable.",13,xa0oua,"Not sure if this is the right place to ask but I’ll give it a shot.

As someone with Asian flush, drinking a moderate (14-26) amount of drinks a week puts me at a 40 fold chance of developing oesophageal cancer. 0.75% of deaths are caused by this cancer. Does this put my chance of death by oesophageal cancer at 30% of death? Doesn’t seem to add up considering smokers have a 30 fold chance of developing lung cancer but the increase only goes from 3% to 5%.",AskStatistics,2022-09-09 09:59:42,16
"There are a lot of different ways to do this but they’re all going to be a bit subjective. Come up with your definition of what makes the best writer and then build your formula around analyzing that definition.

Some examples.
1) The best writer is the one responsible for the best episodes by rating. Formula: Find the episodes with the ten highest ratings, then find whose name shows up most as the lead writer.
2) The best writer is the most consistent at producing quality episodes. Formula: Each writer gets a list of numbers, the ratings of episodes they’ve worked on. Exclude any writers who have fewer than five entries, or any single rating below 6/10. Out of the remaining writers, whoever has the lowest standard deviation for their list wins.
3) Same as 2 but take highest average instead of lowest standard deviation. Not sure what I’d call it, the word consistency feels wrong.

I’m not convinced views matter? Whether I watch an episode of a series is determined by how the series has gone so far. I don’t know how good the episode itself is because I haven’t seen it yet. Afaik it’s typical for early episodes of a show to have the highest view count and then fall off over time because people decide the show isn’t for them.  But idk if you have views from initial airing or some other measurement.",1,xa187t,"Sorry for the vague title, I couldn't think of a better one!

I have a dataset containing all episodes of a TV show. There have been many writers, with different writers on each episode, some of which write for many episodes and others who write for only 1 episode.

My data contains the number of views and the ratings (0-10) for each episode. I want to find out who the best writer is by somehow combining the writer's view count, the number of episodes they have made, and the ratings of those episodes. Is there some elegant way to do this to come up with some overall writer score? I could get estimated views per episode created and then multiply by (rating/10), but this feels a bit arbitrary, although it probably does the job.",AskStatistics,2022-09-09 10:21:47,2
It looks like you should display more decimal places. The coefficient for gini2 is how much the partial slope between gini and the dv change for a unit change in gini. From the graph it appears negative because the function is negatively accelerated. The slope decreases as gini increases.,2,x9zlnp,"So I'm having a brain fart and can't remember how to interpret models with a quadratic term. In these models (LM), **life satisfaction** is the dependent variable (continuous from 1 to 10, 10 being higher satisfaction), **giniWB** is just the GINI measure of each country of the sample according to the World Bank, and **gini2** is the quadratic term. How should I interpret the output on model 3? I also leave a photo of the distribution of the means of life satisfaction according to GINI. 

https://preview.redd.it/h0kjnkpcyum91.png?width=1531&format=png&auto=webp&s=7e85d49be73e54f8e2a2766eeecb6b8ca84ce747

https://preview.redd.it/z7j9yggewum91.png?width=1358&format=png&auto=webp&s=2745f9b8bfd815ff622b16ee24d05fc675865209",AskStatistics,2022-09-09 09:15:07,1
"Eta-squared doesn’t, in general, exist for a mixed model.",1,xa4pzu,"I usually calculate eta-squared (also called R-squared) manually by: 

eta-squared SSbetween/(SSbetween+SSerror)

It's easy for standard one-way and multifactor ANOVAS. I know there is a way to calculate it using F values as well. However, I used a Reduced Maximum Likelyhood (REML) Mixed Effects Analysis in Graphpad Prism. It does not provide an ANOVA output table (traditional SS, MS, df, and F), and so I'm at a loss as to how to proceed. I do have G-power, but it's a little user-unfriendly for me . What would be the best way to proeed?",AskStatistics,2022-09-09 12:49:00,5
"If you are counting seeds and have lots of zeros, a zero-inflated Poisson model would be better suited",10,x9rz9n,"The data was collected based on the seed setting capacity of 7 different flower varieties and it was done with three replications. Previous literatures have shown that all these varieties set seeds without any issues. So I had planned to run an anova test on this study. However, while collecting the data, it was observed that out of the 7 varieties, only three of them set seeds and other four did not (may be environmental influx) so it has been ruled out as zero seed settings. Can anova test still be applied to this or is it statistically wrong to use anova here?",AskStatistics,2022-09-09 03:31:47,8
"Any data you interpolate here is going to have to be justified in some manner, ideally based on an extremely simple metric and fairly obvious relationship. An average of surrounding entries when there's a clear trend (or general stability) isn't too difficult to swallow. 

This one, however, shows quite a bit of movement even with the limited data you do have. Is this something where stock counts only go up at the beginning of the week? Do you have other, comparable weeks you could assume a relationship from?",1,x9ysg9,"I am missing a weeks worth of data of daily stock counts, I'm wondering if there is a way in excel to fill in this missing data using the data that I already have?

&#x200B;

DATA SAMPLE EXAMPLE:

|Date|Stock|
:--|:--|
|August 13|250|
|August 14|185&nbsp;
|August 15|173&nbsp;
|August 16||
|August 17||
|August 18||
|August 19||
|August 20|80|
|August 21|300|  
  
  
I am looking to fill in the 16, 17, 18, and 19.

Thanks!",AskStatistics,2022-09-09 08:42:47,3
"I would approach this by including an interaction term in a general linear model and formally testing whether control/intervention has an effect on the differences between the successive years. If the interaction is significant, you could then probe it by running two separate univariate models for year with pairwise comparisons, appropriately adjusted for type I errors, and compare differences in differences in that way.",1,x9t7l6,"Hey all,

I'm working on an impact analysis of an intervention. The programme started in October 2019 and continued into 2022. Below is a graph of the number of 'bad outcomes', shall we say, in the location where the intervention took place, as well as a comparator location from 2018 to 2022.

&#x200B;

https://preview.redd.it/rzviqsaoftm91.jpg?width=1384&format=pjpg&auto=webp&s=a38ed8bd548f676614bf42c68bcab85d4ccf7f10

The locations have different population sizes so my understanding is that we need to conduct a difference in difference analysis to see the impact of the intervention. Diff in diff, as I understand, is based on the assumption that the trend in the intervention location would follow the comparator location trend were it not for the intervention. So we start with the pre-intervention number at the intervention location and then start counting the difference between that and the superimposed counterfactual?

The figures for each year are based on end of March so really the first measure of impact is 2020. I've calculated that the differences between the comparator location aat 2019 and the next point and so on until the end is: +8, -18 ,+11. Counting onwards frrom 2019 for the intervention location: +1, -9, +1.

Comparing the actual intervention trend to the superimposed counterfactual then is: (8-1)+(-18--9)+(11-1)=8.

Does that mean the difference in difference overall is 8 and that the impact of the intervention is 8 fewer bad outcomes if we follow the assumptions of the difference and difference model? (e.g. that the intervention location would follow the comparator trend, were it not for the intervention).

I've seen more complicated versions of this analysis so I wonder if my attempt here is still valid or I need to follow the more complicated calculations?

Many thanks!",AskStatistics,2022-09-09 04:39:46,6
"If you're ignoring the potential effect of the time points, the most usual approach would be a 2xk chi-squared test of homogeneity of proportion (with k groups).

There are other options.

If you want to allow for heterogeneity over the time points then a suitable analysis wouldn't correspond to a one-way ANOVA but something else.",2,x9jr6m,"I have a data set that I am trying to test significant group differences for accuracy at 3 time points. The data are independent (not repeated measures just collected at different times from different people) but although I would typically use one way ANOVA for testing group difference like this, the accuracy scores are binary 0/1 for each participant (inaccurate, accurate), so even though I have many participants across the 3 time points (groups) I’m not sure what the best way to test significant differences would be. Any advice would be great!!",AskStatistics,2022-09-08 19:51:56,3
"> A colleague told me that if the mean of group2 is much larger than that of group1, then group1 will not be significantly larger than the control mean but if the mean of group2 is about the same magnitude than the mean if group1, the difference will be significant


In general this won't be correct. There might well be some multiple comparison method for which it might sometimes be possible that this happens but I don't think this is correct for the ones I can think of right now (nor would you want it to be, typically). 

What method of multiple comparisons did they have in mind?

You can check for yourself in your favourite software using whatever method they're using by taking a set of data which fits the description - three groups,  with mean(g2)>> mean(g1) > mean(c) and where the  g1 vs c comparison is *just* not significant (why *just*? to give the claim the best chance of working, if it is to work at all) and then subtract mean(g2)-mean(g1) from all the data in group2 and redo the analysis and see if that was able to make any difference at all.

For the ones I'm thinking of right now it won't but there's a lot of approaches to multiple comparisons, so maybe it can happen with some particular one.",2,x954df,"

So if for example we want to compare the means of group1 and group2 to the mean of a control group, does the significance change with the range of the magnitude?
A colleague told me that if the mean of group2 is much larger than that of group1, then group1 will not be significantly larger than the control mean but if the mean of group2 is about the same magnitude than the mean if group1, the difference will be significant.
I thought the significance test is independent and doesn't take into account the magnitudes of the tested groups.

I hope you understand what I mean.",AskStatistics,2022-09-08 09:35:34,3
Diff for both assuming nested and lots of other assumptions,3,x92s2m,,AskStatistics,2022-09-08 08:02:28,1
You can try fitting your model with a  regression ?,1,x93igj,"I'm currently trying to test how various variables such as revenue from ATM/PoS,Internet Banking, and Mobile banking may Influence Return on assets of commercial banks in my country. 

However the data I'm using is compressed data of ALL 30+ commercial banks (As individual banks won't disclose revenue per item) for every quarter from 2013 to 2022

Now can I still use panel data analysis? Or what appropriate test in Stata/SPSS can I use to capture what I'm trying to establish.",AskStatistics,2022-09-08 08:31:59,1
The relevant excel functions for most of these are easy to find online or even from within Excel. They are basic statistical functions. Wouldn’t take you long to just go through them and make your own sheet.,1,x963tx,"\[Question\]  Hi all, I am in an MBA program that requires that I take statistics. It's an introductory level course but my brain does not grasp any of the concepts. My two saving graces (I hope) are that I can use excel/TI-84 and I can have one page of notes front and back.

So here's where I need help. I figure my best bet is to have that one page of notes just be the excel shortcuts/formulas. But here's the problem-I don't know them and our textbook doesnt have them. Our book explains how to do the stats by hand but our professor encourages we use excel.

[Here are summaries of chapters 1-4](https://imgur.com/a/RdOvyVV), anyone have any one-pagers of how to do this stuff in excel so I can practice? Any/all help would be great and I am happy to share any class information that shows this is not against class policy.

Edit to add:

Frequency Distribution, Cumulative Frequency, Stem and Leaf Displays, Measures of Central Tendency, Weighted Mean, Measure of Variability, Variance, Coefficient of Variation, Z Score, Chebyshev's Theory, Measure of Relative Position, Quartiles, Sample Covariance, Correlation Coefficient, Empirical/Marginal/Joint/Empirical Probaility, Multiplication Rule, Bayes Theorem",AskStatistics,2022-09-08 10:13:56,8
"It depends how you are collecting the data. How are you measuring their perception of the interaction (ex. 5 point likert scale). 

If you are measuring it by likert scale, you could use an ANOVA. First divide the data into groups based on their perception of negative interactions (categories for each scale point) then run ANOVA to see if the mean time spent in the gym is different between each satisfaction group. Here is a link to a page that goes over ANOVA [ANOVA](https://www.statisticshowto.com/probability-and-statistics/hypothesis-testing/anova/).",2,x95l0y,"I’m collecting data on women that are starting to work out, and would like to know if their time on the gym is reduced by what they perceive as negative interactions with other gym goers.   The hypothesis is that the worse the interaction is perceived the less time the person will spend in the gym in the future, but I don’t know how to test this. Any ideas?",AskStatistics,2022-09-08 09:53:47,3
"Typically you drop the lowest variant vectors after rotation.  Intuitively you're getting rid of the  rotated vectors that don't  contribute much to the solution. Dropping these vectors  also reduces the dimensionality of the regression being solved.  


* Rotate  to get an orthogonal basis set of vectors
* Drop low variant vectors from regression 
* Solve the reduced place regression
* Back project the reduced place using the rotated matrix into the original solution place



So you can start with say 12 unknowns, 4 highly correlated. After rotation and reduction you solve for just 8 unknowns. Back project the 8 vector solution into the original places to get values for all 12 unknowns.",2,x95ep3,"When dealing with spectral data,  I get that PLS is better than PCR since small principal components which are ignored in PCR, may be highly correlated with outputs, and PLS overcomes this limitation by extracting the components that have a significant correlation with outputs in the latent space. But how do any of these two methods compensate for spectral overlap? More specifically, why does changing the basis vectors to the direction of maximum variance (PCR) or covariance (PLS) resolve the spectral overlap? ",AskStatistics,2022-09-08 09:46:52,3
"small recommendation regarding how you describe this issue to statisticians: when I first read the phrase ""drop out"" I thought you were talking about a particular technique commonly used while training neural networks. To reduce confusion, I suggest you consider referring to this as ""survey non-response""",1,x91kp8,"Hi all,

I'm trying to analyze drop outs within my data at the moment and can't decide for the right statistical approach.

The question is whether people who dropped out of the survey (n=8) differ from those who completed the survey (n=115) in regards to two variables obtained at the time of recruitment.

Also I'll impute missing values via multiple imputation. Should I run the drop out analysis before or after running the multiple imputation?

Any help is highly appreciated. Thanks!",AskStatistics,2022-09-08 07:12:53,1
"I'm a little bit confused as to what your L2 measure is. Is it a percent of people in that grouping (amount of time spent abroad) that people used the linguistic feature correctly? 

If so I would consider doing an ANOVA analysis. This page goes over the difference: [Chi Squared vs ANOVA](https://www.statology.org/chi-square-vs-anova/). Look at the Chi Squared test for independence. 

If you are looking at Regression it depends how you are setting it up. I do not thing Poisson is the right model for you. Poisson models are used for count data, which means data that is only whole numbers. Which model you choose would depend how you set up your dependent variable, or what you are trying to predict, is it the percentage of time that the lingusitic feature was used correctly or is it how much time the person spent abroad? If so then you would need to look a little bit more at your data. If you choose regression, this website goes into many different model types and when you would want to use them [Stats by Jim](https://statisticsbyjim.com/regression/choosing-regression-analysis/). 

Overall, I would probably lean towards the ANOVA method if you are only looking to determine if time aborad impacts the correct use of the linguistic feature. Otherwise if you are looking to add in other variables (ex. Age, gender etc.) That is when regression would become more useful.",1,x98l63,"Hello! I'm an undergrad linguistics student working on a paper in corpus linguistics. My professor gave me total carte blanche when it comes to the statistics bit, and I’m feeling very out of my depth. I’ve been doing a lot of research and looking up statistics resources for linguists, doing some very basic work in R, but I’m stuck now and was hoping someone might be able to point me in the right direction.

**The study**: I’m looking at whether second language (L2) learners produce a particular linguistic feature correctly. The corpus exports data in tokens, and I grouped them by time spent abroad (no time spent abroad, 1-3 months, 4-6 months, etc. all the way to over a year), but each token also includes other information on the participant who wrote that particular essay (age, sex, years of exposure to the L2). I evaluated each token by whether the feature was produced correctly or not. 

**The hypothesis**: I’m trying to show whether stays abroad positively affect the production of that linguistic feature. So, a person who spent time abroad would be more likely to use it correctly than a person who hasn’t, and a person who spent more time abroad is more likely to use it correctly than a person who spent less time abroad (hence the grouping by different amounts of time). In other words, the null hypothesis would be “a person who spent time abroad has an equally good chance of getting it right as someone who hasn’t”.

**The problem**: I’m not exactly sure how to get the p-value or what to do about the fact that the participants have different years of exposure to the L2 (my professor told me to just exclude outliers so everyone has the same average time of exposure but I'd rather not). I used [this wizard](https://www.socscistatistics.com/tests/what_stats_test_wizard.aspx) to try and find out what test to use but clearly I’m missing something, because none of the tests I got make sense. I don’t even truly understand if my data is considered nominal or not, much less what a chi-square test is supposed to do for me here…

I know I could chuck any set of numbers into a test and it would probably spit something out, but that’s completely worthless if you don’t understand what it’s for, and that’s exactly my problem :( For example, I saw someone with a (relatively) similar study do a Poisson regression, but I don’t understand why I would do that/what it would mean if I did. I’m so sorry for the amount of handholding I’m asking for here and my fundamental lack of understanding- I’m really hoping someone who actually understands statistics could give me a little nudge, and I'll handle the actual research/work myself (as I should) :'D",AskStatistics,2022-09-08 11:58:02,4
"A difference in means is still an effect size, even though the term effect size is more commonly used as shorthand for **standardised** effect size. See e.g. https://scc.ms.unimelb.edu.au/resources/effect-sizes-and-confidence-intervals/effect-sizes_confidence-intervals

In medical research, the term “clinical significance” is used to refer to a difference which matters for clinical practise. So we might describe a study as being powered for “the smallest clinically significant effect size”. “Practical significance” is sometimes used. I haven’t heard “industrial significance” used that way but I don’t work in that area and it wouldn’t surprise me if that term or a similar one was used analogously to clinical significance.

Edit: if you are in a situation where you can easily obtain statistical significance for difference which don’t have practical significance, (1) you’re lucky! (2) it’s entirely reasonable, in classical statistics land, to choose an alpha (p-value threshold for statistical significance) which is more stringent, thus also reducing the risk of false positives. Alternatively, you could ditch statistical significance and report confidence intervals for the effect of interest.",2,x8w7q6,"I would swear that there was a term related to the effective change in a meassure for comparing populations that was along the lines of ""industrial significance"" or something like that, but I cant seem to remember it nor find it in my notes (I lost some of them).

It was very interesting cause it added a minimum diference in real -unit- terms along the lines of:

We have two patches of... Idk, pumkins, A, and B types. We need to meassure if there is a difference between the sizes of those pumkins, but since our N is huge, the more pumkins we test the difference spoted by the test becomes minimal. Ofc, we are only interested in changing the sistems A to B or B to A if we spot a minimum diference of,  say at least 10 kg. 

That is what I am looking for, but I cant find. Because otherwise despise I can say that there is a difference in my populations size with a t-test or a nonparametric approach (although >30 probably for pumpkins we can say its close to normal distributed) ; the quantification of the difference can be absurdly minimal.

&#x200B;

Any ideas on this? Or maybe there is some sort of p-value correction such as the ones I do remember for multiple hipothesis testing?",AskStatistics,2022-09-08 02:53:12,4
">  it is a generalization of the rank test that accounts for the sample variances.

You say ""the rank test"" as if there's only one. There is not. Presumably by 'the rank test' you mean the Wilcoxon-Mann-Whitney test.


I'm aware of the test (edit: BM not WMW) but have never had occasion to think I needed it (I do occasionally use nonparametric procedures but I don't use rank tests much at all, not that there's anything I specifically dislike about them; mostly I need models for situations with multiple IVs and have reasonable parametric models for the conditional distribution). I would likely use this test if I -- (i)  had a straight two sample problem, (ii)  didn't have any good idea of a suitable parametric model (at least one which could inform the choice of a better nonparametric test) and (iii) thought there could be substantively different spreads ***under the null***, particularly if sample sizes were not similar. 

I think a lot of the impetus to use it is based on a (common) misconception combined with some (common) poor practice and a (again, common) misunderstanding of when parametric vs nonparametric procedures make sense (indeed the word *parametric* is itself very often misunderstood by nonstatisticians including ones who write textbooks on the subject, promulgating confusions). 

That's not to say I think there's anything wrong with the test itself per se, but  it will often be used when there's no need to (and on the other hand sometime not used when it probably should). In more detail:

- *it is based on a misconception*

  specifically the assumption that the Wilcoxon-Mann-Whitney assumes spreads are the same. This is not so. It only assumes that *under the null* but an equality null (which is nearly always how it's used) is nearly always false, so we should not expect to see nearly identical spreads in the data.

  In many situations the assumption of homogeneity of distribution under the null is quite tenable, or at least approximately so. For example, it might be tenable  if there's a treatment that should either affect the parameters of the distribution in such a way as to change P(X>Y), or it's not effective and doesn't do much of anything to the distribution, even when the distributions would have different spreads under the alternative (that it was effective). 

- *combined with some poor practice*

   . . . that of looking at the data (the same data you're testing) to choose assumptions; this practice itself impacts the properties of the tests you might choose between (causing the sort of problems it seeks to avoid) and makes the mistake of confusing what you assume under the null with what you can find in the data.

  The assumptions relate to the populations... in a situation you're almost certainly not in. Information and ideas about what to assume should generally come from things other than the specific data at hand. If you must use your data, you need to think about strategies to avoid data based selection of assumptions (and thereby, of hypotheses)

- *a misunderstanding of when parametric vs nonparametric procedures make sense* 

  Many people think ""non-normality"" -> nonparametric. Not so. There are numerous situations - and corresponding parametric but non-normal models - where heteroskedasticity is to be expected and can be suitably modelled.

  There's also the problem that many commonly-used nonparametric procedures correspond to quite different hypotheses than the ones you might have used if you'd chosen a suitable parametric distribution and a corresponding procedure to use with it. Hypotheses are not things to be swapped out on a whim. Make your test match your hypothesis, not the other way around. Especially, don't make your hypotheses so vague that your direction of conclusion under a rejection - e.g. from 'A bigger than B' to 'B bigger than A' - could *flip* because your hypothesis was changed in just that way -- i.e. don't make it so vague you don't even know what you mean by a difference(!).

There's a lot more could be said on this topic (and some related misconceptions) but I don't want to go off on a tangent.


> such as running the permutation version if the samples are less than 10)

I would run the permutation version (either evaluating the exact p-value or using random sampling of the permutation distribution with a *very* large simulation size) unless the sample sizes were quite large. There's no good reason to avoid doing it.

> , as far as I understood there are no problems using it if the variances tend to be similar since it should behave like a normal rank test. Is it correct?

There would likely be some loss of power at small effect sizes if the population spreads were essentially the same but it should generally be a only a small loss. If you don't do the exact permutation distribution thing you may also have some issues with both significance level and power at small-effect sizes from that.

---

Whether it makes sense to use it depends in part on *how* the different spreads arise (should they arise at all in the populations); do they occur under the null or are the difference in spread and the difference in P(X>Y) from P(Y>X) both related to the treatment? If so, there may be a power difference - which could be investigated, but there's a more important question of (since we're caring about power) what might be an even more suitable model for that situation.",2,x94wge,"Dear statisticians,

I would like to ask you for an impression of the [Brunner Munzel test  (BM test)](https://www.statisticshowto.com/brunner-munzel-test-generalized-wilcoxon-test/). For those who don't know what it is (as I did until few months ago), it is a generalization of the rank test that accounts for the sample variances.

My question is then, is there something I should be aware of, something I must absolutely check before running this test or any critiques? (Some tips and an explanation are detailed [here](https://www.statisticshowto.com/brunner-munzel-test-generalized-wilcoxon-test/), e.g. such as running the permutation version if the samples are less than 10)

Do you have experience with it? Any feedback would be appreciated. For instance, as far as I understood there are no problems using it if the variances tend to be similar since it should behave like a normal rank test. Is it correct? Could I technically replace all the rank tests with it? But if not I don't know why.

The motivation for this post is that I accidentally performed some p-hacking by reusing some code for a second totally unrelated group comparison. In the first the differences between the rank test and BM were minimal so I kept it because my samples had very different variances but when I reused the code with a totally different study the BM test had quite a few significant p-values while the rank had none after FDR.

Thanks",AskStatistics,2022-09-08 09:26:35,3
A piece of wood takes on the form of the uniform distribution,3,x8uw33," Hi, I am looking for videos/animations or visual examples of statistical distributions which do occur in nature. Similar to the Galton board (e.g. [https://www.youtube.com/shorts/Vo9Esp1yaC8](https://www.youtube.com/shorts/Vo9Esp1yaC8)) but for other distributions. Do you have any hints/links/names for me?",AskStatistics,2022-09-08 01:36:04,3
"It's an interesting question, and there is likely a lot of models that could be used for this. One that comes to mind is [KNN](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm). Basically, the idea is to look for similar players, and you could then use an average (or possibly a weighted average according to similarity) to form a prediction. 538's [model](https://fivethirtyeight.com/features/our-nba-player-projections-are-ready-for-2018-19/) for projecting the value of basketball player uses a method similar to this if you'd like to do some reading.

Regardless of which method you choose, one factor that could confound your results is injuries/games played. A player that produces 40 points over 80 games is obviously quite different from one that produces 40 points over 40 games. I would suggest accounting for that in your model.",2,x8l5a3,"Hi everyone! I'm looking at building a model to predict the points (goals and assists) for NHL players this year based on detailed statistics in their previous years. I was wondering what would be the best model type to use? 

I tried buildong a Poisson count model for previous season data and applying the model to predict this years points, but did not get great results. I figured I'd try a few other model types if anyone has any suggestions. 

Analysis is being done in R.",AskStatistics,2022-09-07 17:20:09,3
"What framework and software are you working with? You can absolutely do this in Bayesian SEM with Stan or Jags, but I’m not sure you can with frequentest estimation and Lavaan.",1,x87p8a,"Hello,

I am wondering if SEM would support a multi categorical dependent variable. I have a dependent variable with 4 levels. I know I can use multinomial logistic regression, but I wanted to do a path model with mediators, moderators, leading to a multicat DV. Is this possible?

Thank you so much in advance!",AskStatistics,2022-09-07 08:14:21,1
"1. [Suggestions](https://grammar.yourdictionary.com/grammar/punctuation/when-not-to-use-an-apostrophe.html)

2. If you have no data on ""non-diagnosis"", and are not allowed to use any other data on non-diagnosis, then you have completed what you can do, and can present the client with the bill for wasting your time. 

It is just like if I had data on how many people were given tickets for speeding, but asked how many **other people** were also speeding, but were not caught.  There is no there there.",1,x8ked7,"Working on a dataset for a client. They want to know prevalence of non diagnosis in dataset. Only thing my variables consist of population size, people with a disease, and people without insurance. Either my brain fried and I’m over thinking but some reason I’m stuck. Client also doesn’t want to use outside sources, instead use what’s in the dataset.

Anything helps.",AskStatistics,2022-09-07 16:46:45,5
"I see no reason why sample size in each zone would need to be the same, but you should explain more about your data 

Are you repeatedly measuring the same individuals in each area?

What sort of things are you measuring? I presume that some quantities will tend to be inherently right skew (I don't mean in your sample, generally you shouldn't be looking at your sample to choose what you do with it); this is not of itself a problem.


There's probably no need for them to be specifically normally distributed, since (a) other distributional assumptions likely make more sense and (b) you have repeat measurements, so distributions of averages might be more relevant, depending on several things which are not yet clear


---

>  calculate the significance of my data


Note that *data* are not themselves significant or insignificant. They're just data. You reject or fail to reject some hypothesis; some people call the condition under which you reject 'significance' but it relates to the specific test 


(that *significance* is not a particular attribute of the data can be easily seen, you could ask an infinite number of hypothesis-test-type questions of the same data set and some may result in rejection while others do not - clearly the data cannot be both inherently 'significant' and 'not significant'; the attribute attaches to something else. Its what the data have to say about your hypotheses through your specific choice of test statistic and rejection rule that get you that reject/not reject decision; so the test, if anything, would be what you ascribe significance to, but I'd - strongly - suggest simply never using the word 'significant' in relation to anything and you'll save yourself a good deal of confusion/trouble.)",2,x8gsct," 

Hello, I'm ending my Master's Thesis, and I'm trying to figure out which test to use to calculate the significance of my data.

Basically, we measured the Free Kick biomechanical parameters of several football players in different pitch zones.

I was going to perform a Wilcoxon test, seeing as the data is not normally distributed, but the number of Free Kicks is not the same for each zone, and N would have to be the same between zones for it to work.

Basically, which test should I use to compare my parameters between pitch zones?

Thank you for your help!",AskStatistics,2022-09-07 14:16:37,2
You can never prove that the mean of the sample is equal to the mean of the population.,3,x8e2q7," Hello everybody, I am trying to generalize some statistics I found on a sample to his population.

I have found on the internet that we have to do a 'Hypothesis testing' so I started looking for how it works and I have got -more or less- how it does.

Now I am trying to apply what I have learned to my situation (find if the sample's mean X is the same as the population's mean):

1. I have collected other samples with a sufficient length (> 30)
2. I did with each one a hypothesis test with H0 => µ=X

RESULT: I could not reject H0 on any of these samples.

And now I am stuck: I don't know if I can deduce that the mean of the population is X.

(I am new to statistics and thank you for your help 😉)",AskStatistics,2022-09-07 12:28:51,9
"> (each with equal probability output within the range)

I presume you mean the distributions are uniform but it's unclear whether these variables are discrete (integer, say) or continuous. If they're discrete, you need to specify what happens with ties (if A ties N at the top, did A come out 'higher' than all the others or not?)

To answer the question you'd also need to know not just their marginal distribution but their *joint* distribution -- e.g. if you mean for them to be independent, you must say so.

(However, once you get more than a few events, there's a lot to be said for simulation. Even if you do calculate an algebraic answer, you still ought to simulate as a check on your calculation)",1,x828yo,"Hello, I was wondering if anyone could help with something that is surprisingly burning my brain, I think I have a way to calculate this (albeit heinously inefficient) but...

If I have N ranges with varying limits (each with equal probability output within the range) such as

A: 10-20,

B:15-25

C: 5-14,

N: 12-40

What is the best way to calculate the probability that A's output, for example, is higher than all the other ranges?",AskStatistics,2022-09-07 04:06:44,3
"> Should I use And or should I Or probabilities

Probabilities are numbers. You use operations like add, subtract, multiply with numbers

You calculate probabilities of events (which are usually written in terms of sets)

AND and OR are *set* operations, and can be used to make compound events out of simple events.

> if I have 3 horses from 3 different races, say 0.5, 0.5, and 0.5 

No, horses are not numbers. You presumably mean for each of these numbers to represent the probability of some event but you have not stated what the event is for each. It's important to be careful with these things.


> But can also be called mutually exclusive events.

No, these events you should have defined are not mutually exclusive since you can get 2 horses win, or 3 win. If A, B and C are *mutually exclusive events* it means 'if one happens, the others *cannot happen*'. That's not the case here. If one horse wins its race, the others still might win theirs. Indeed if they were mutually exclusive, they could not be independent\* and vice-versa.


Let A be the event ""the first named horse, ""a"" wins race 1"", let B be the event ""the second horse, ""b"", wins race 2"" and let C be the event ""the third horse, ""c"" wins race 3"".

- Then  P(all three win) = P(A) x P(B) x P(C) 

  (from the chain rule of probability, the simpler form for independent events)

- P(exactly two win) = P(""a"" & ""b"" win, ""c"" loses) +  P(""a"" & ""c"" win, ""b"" loses) +  P(""b"" & ""c"" win, ""a"" loses)   
  = P(A and B and C^(c)) +  P(A and B^(c) and C) +  P(A^(c) and B and C) 

  where C^(c) indicates the complementary event 'not-C', and so on.

  (we add here because we split ""two win""  into three mutually exclusive events)

  and then we can write P(A and B, C^(c))  as P(A) x P(B) x (1-P(C))  (because they're now independent events)

   and similarly for the other two probabilities P(A and B^(c) and C) and  P(A^(c) and B and C) 

In short, to answer P(""two win"") you need to do both -- add and multiply -- but unless you're very careful about what your events are you can't hope to get it right.


----


\* at least not while the probabilities are >0",2,x8745o,"I’m doing a coding project on horse racing, I’ve obtained and calculated the odds of a horse winning by using implied probability. My problem is I’m confused with if I have 3 horses from 3 different races, say 0.5, 0.5, and 0.5 for arguments sake and respectively. 
What are the odds at least one wins, and that 2 win, all 3, and none win.
I understand that they are independent events. But can also be called mutually exclusive events. So do I use “P(A) or P(B) “ or do I use “P(A) and P(B)”

Thank you so much for the help",AskStatistics,2022-09-07 07:50:33,3
In what way do you think the result is being biased? What survival function are you using?,1,x86m60,"Hello, in a survival analysis, how do I decide how much follow-up time is necessary and how does that affect the analysis?  I am dealing with a dataset where some people were only followed for 3 months (and nearly all of them didn’t have the event), and some people were followed for 3+ years.  I keep wondering those who were only followed for 3 months are biasing the result.  Is my intuition right or wrong?",AskStatistics,2022-09-07 07:29:52,5
I suggest probability first. Stats relies on probability but probability does not rely on stats.,16,x7xn4o,"My ultimate goal is to learn & refresh my knowledge of stats, I’m not sure if I should just jump right into stats or learn/relearn probability first.",AskStatistics,2022-09-06 23:24:32,8
"Since the chances you don't get hit is 6/7 per day, and the chances are independent, there's a (6/7)\^7 chance your house won't get hit in 7 days, and thus a 1-(6/7)\^7 chance that it will, which is about 34%. If you have a 1/100th chance of hitting, the total over 100 days is about 36.7%. As you choose large numbers, this probability approaches 1/E which is Euler's number.

However, this doesn't answer your intuitive question: why isn't the probability 1?

In general, this is because you're overcounting the possibility of something happen two or more times. If there's a 50% chance something will happen today or tomorrow, there's a 25% chance it'll happen on both days (assuming independence)

Your case is slightly different: if your house gets hit on Day 1, I'm assuming it can't get hit again since it no longer exists. So, is the probability on Day 2 really 1/7th? Or is it 1/7th times the chance your house survived day one, which would be 6/7\*1/7 or 6/49th. If you make similar computations for the following days, you'll see the numbers don't add up to 100%",4,x835rp,"Ok so I have no experience in statistics at all so this is probably dumb and maybe not even statistics. But, say, there is a week. On Monday, there is a 1/7 chance that, say, your house will be a hit by a plane (obviously unrealistic, but I can’t think of anything else). And on Tuesday there is also a 1/7 chance that a plane will hit your house. And so on. What is the chance of a plane hitting your house at some point that week? At first, it seems that you just take 1/7 and multiply it by 7. But that would be 1, and it seems obvious that no event which is not guaranteed on any one day would somehow be certain to happen over the course of a week. What is the solution? (To take a more intuitive example- maybe there’s a 1 in 1000 chance of winning some casino game. Playing that game 1000 times would not guarantee a win- but then, what actually is the probability?)",AskStatistics,2022-09-07 04:53:50,3
"BLUPS are not conditional *modes* but conditional *means*.  Best unbiased predictions are conditional means (that's a theorem).  if those conditional means happen to also be linear (as when all variables are jointly multivariate normal), then they are BLUPS rather than BUPS.  *Modes* have no such properties.    They are not *best*, not *linear*, not *unbiased*.  Of course *anything* is a prediction if you use it that way.

GLMM are goofy.  They have been in use for decades, but there are a lot of unsolved problems with them.  This one in particular.  It is an open research question AFAIK.  The reason these authors are doing something like this (or for that matter I and co-authors do things like this) is because we haven't thought of something better.  The conditional modes (they aren't really even that, so those *thingummies*) are convenient to calculate.  So we use them.

You get a gold star for realizing there is an issue here.

You can call these thingummies ""BLUPs in scare quotes"" to explicitly say they are *not* real BLUPs.",2,x82mm2,"De Waters et al. ([2017](https://www.sciencedirect.com/science/article/abs/pii/S1053811917303038) ) and Mies et al. ([2019](https://www.tandfonline.com/doi/full/10.1080/09297049.2018.1508563)) used conditional modes (BLUPs) from a generalized linear mixed model to further examine individual differences. In their experiment, they performed a temporal delay task (a marshmallow test for adults with money), manipulating the **amount** of immediate reward and the duration of the temporal **delay**. After extracting the conditional modes for each participant, they correlated them with an fMRI signal. The beauty of this approach is that it allowed them to examine individual differences in the influence of **amount** and **delay**. To do this, the conditional modes of the random slopes for **amount** and **delay** were each correlated with the fMRI signal.

However, I am somewhat unsure whether such an approach is appropriate. For example, [Kliegl et al. (2011)](https://www.frontiersin.org/articles/10.3389/fpsyg.2010.00238/full) writes:

""\[...\] Conditional modes of different subjects are not independent observations, but values weighted by distance from the population mean. Therefore, statistical inference must refer to the estimates of the LMM parameters; it is not advisable to use the conditional modes for further inferential statistical purposes (e.g., to correlate them with each other or with other subject variables such as age or intelligence).""

I understand that conditional modes corresponding to shrinkage are not independent.

**Question 1:** However, I don't quite understand why further analysis of conditional modes is not appropriate. Could someone elaborate on this or provide a

**Question 2:** Does anyone have an idea how to analyze such data as from de Waters et al. (2017) without using the conditional modes.",AskStatistics,2022-09-07 04:26:28,3
"I like to write notes as I read, not just copying what’s written but also my commentary on the ideas. This is how I learn—others learn better from visual or audio input, in which case you might prefer simulating and plotting in Excel/python, drawing, or watching a YouTube lecture on the topic. I also find that explaining the content of what I read in different/simpler terms to someone (or even an inanimate object, hence the term rubber-ducking in programming) helps me too.

Also, to the extent it’s possible, make sure you understand things before reading on. Math can be very elegant and understated, so it can be easy to brush past a very carefully-worded statement that, if fully understood, is rich with insight. 

Best of luck!",13,x7fp0c,I am having a tough time understanding the text i have been reading. What strategies do you use?,AskStatistics,2022-09-06 09:59:14,17
"I'd probably be considering at an ordinal logistic regression ('ordered logit') model

https://cscu.cornell.edu/wp-content/uploads/91_ordlogistic.pdf

https://stats.oarc.ucla.edu/r/dae/ordinal-logistic-regression/

https://en.wikipedia.org/wiki/Ordered_logit

There are other possibilities. If an (approximately) proportional odds assumption is not reasonable\*, then you might consider an ordinary multinomial logit model perhaps.

\* nor equivalent assumptions for other ordered-link regressions, such as ordered probit etc",2,x7zkcp," 

I’ve got a data sample from a questionnaire. I’ve selected a dependant variable based on a question with 5 different possible answers, varying from 1 “very good” to 5 “bad”. The goal is to analysis the connection of a row of independent variables (from similarly coded variables) on the answer of the dependant variable. 

Im unsure which model I can use and what I need to keep in mind when working with the model. Do you guys have any suggestions?",AskStatistics,2022-09-07 01:23:06,1
" It depends on what you mean by merge. Are you adding more rows? 

If so, the proper way to handle the weights would be to give everyone in the convenience survey a weight of 1, assuming you are using population scaled weight in the other survey (big numbers as weights).",1,x7z61i,"Suppose I have two surveys data sets. Both ask similar questions mainly around consumer behaviour. Survey A, is a random sample of the entire population. Survey B, is a convenience sample of mainly affluent segment of the population (25% of the richest of the entire population) . Survey A is conducted with F2F interviews and leave behind questionnaires. Survey B is filled online. Survey A, takes more time and people are paid to take part in the survey. Because of this, survey A, has sparse data on the higher income end. People with high income do not have the time and the money incentive does not mean much to them. Survey B is more convenient and offers a draw to a bigger prize so they complete that one.

For completeness, we want to merge two so that we have a representation in all income areas. Question is, is this statistically allowed? If so, how does one fuse the two sets? Is there some sort of weighting that needs to be done?",AskStatistics,2022-09-07 00:58:24,1
"A pie chart is a particularly poor choice on several grounds. Let us abandon it without further consideration.


Note that the options are ordered. Something around a circle won't do.

You will need small multiples; perhaps 11 rows and 4 columns (edit: or fewer rows and more columns as needed) of a small histogram-like plot, but since the distribution is 'discrete' using vertical lines rather than bars (emulating a pmf)

that is, a set of plots more or less like this:

https://i.stack.imgur.com/Zr6VD.png

They could be compressed vertically considerably more than I have them there, by the way.

-- you should be able to fit at least 6 columns and 8 rows of something like that on that size of paper, even
with at least half an inch of margin all around, so 44 on a page should be possible.

Whether anyone will be able to get anything useful out of such a display is another matter!",2,x7ubm0,"The issue that I am encountering currently is that I am working with a standard 8.5 x 11 piece of paper and have about 44 likert scale questions that I need to try my best on fitting onto the one page. My initial thought was use a pie chart, but that proved to be just as ugly on the page since there would have to be 44 of them. Is there a better visualization tool that I am not thinking about?",AskStatistics,2022-09-06 20:24:08,3
"> when to use each one

A central issue is ""does it correspond to my specific hypotheses?"". 

So often I see people choosing between tests that respond to entirely different questions, as if their hypotheses were fluid and their test options fixed to some extremely tiny list of options. It's the other way around.\* 

A lot of 'lists' I have seen of when to use tests fail to mention any  explicit hypothesis and as a result many of the ones that focus on the form of the data rather than first on the hypothesis will lead people to choose tests that are not suitable for what they want to find out. 

One quick check that shows how bad most such lists are is to try to see what they do when you want to compare variances. About 9 in 10 of of them will have you doing a test that doesn't correspond to that question. Variance-tests is not the main problem, but it does serve as a quick illustration of the underlying issue.

---

\* Don't modify your hypothesis to fit a test, find a test to fit what you actually wanted to find out. So many people don't spend any time at all on the most critical part of the statistics (clearly identifying the question they wanted to answer) -- the ones that do spend more than a few moments thinking about it will still tend to abandon their real question at the first molehill. The same books, websites, videos etc that encourage the bad practice of choosing tests without consideration of what you wanted to find out, also encourage not explicitly identifying your hypotheses properly, so that you *can* engage in that jumping around asking different questions to fit in with the tiny list of tests they deign to tell you are available. Figure out your question first. This is the main task! In some cases, that step might call for the most investment of thought and energy. 

Tukey said ""Far better an approximate answer to the right question [...], than the exact answer to the wrong question"". He was making a slightly different point than I am here, but the basic point applies here. Answering the wrong question may be essentially useless; or in some cases, worse than useless -- it risks you misunderstanding (and so misrepresenting) what it tells you, as your mind will tend, in conclusion, to unconsciously slide back toward the original question of interest that you abandoned.",4,x7llvw,"Hello! Does anyone have resources or videos on statistical tests and when to use each one? Something that will simplify it would be so greatly appreciated. 

Additionally, anything on regression techniques and examples!",AskStatistics,2022-09-06 13:57:02,6
"It seems like what you are trying to do is incorporate additional information into your model beyond the point observations, which is reasonable. The important question to ask is what does this third piece of data represent?

Is it some measure of the resolution of the measurement? For example, if one performs a measurement with a scale that truncates everything to integer values, then a reading of 20kg, could in actuality be a measurement in [20, 21). You can incorporate this sort of information by building a likelihood that uses censored observations.

If your std is a summary of measurement variability of some underlying data, i.e. the std of 10 measurements of the same phenomenon, then you might want to see if you can incorporate the underlying observations rather than a summary if you have them.

If the observations are coming from different measurement processes, then you have a situation where you may have different observational models and a hierarchical model could be appropriate.

There are numerous ways to incorporate the idea that ""all observations are not made equal"" into your model, but if you want to be principled about it, you likely need to be careful about how you set up the model to incorporate the manner in which these observations differ.",1,x7a7v0,"Hi, i  i am training a bayesian neural network by maximizing the ELBO

&#x200B;

and  i was wondering if it made sense from a statistical point of view  to replace in the formula for the posterior distribution  the likelihood of a dataset ( it's a regression problem so the elements are supposed independent and identically distributed  with a gaussian distribution around their respective mean )  a weighted  version of it like it is often seen in the least squares method.

&#x200B;

My question arise from the fact that my dataset has a third column (x,y, \\delta\_y) that report the std  of the measurements and i would like to give more weights to the likelihood corresponding to the measurements that have a lower error.

In laboratory 1 class we often used weighted least squares when we had different errors over the measurements, and i think it can be done, but i would like to have your advice on this. does it makes sense?

the formula would become  p(\\theta,D)=  \\prod\_i L(y\_i,x\_i)\^{w\_i}  \* p(\\theta)/Evidence

&#x200B;

where w\_i would be the normalized weights calculated from the errors",AskStatistics,2022-09-06 06:08:30,4
"Let me save you some effort:

Simply never calcuiate 'observed power' (a misnomer if ever there was one). It's (a) misleading and (b) uninformative. It tells you nothing useful that the p-value didn't tell you already and *it does not tell you anything useful about power*.

Now, if you follow that good practice, you have no need to worry about how to calculate it *and* you avoid doing something egregious.",2,x7n22h,"I am trying to figure out a power calculation from the below online power calculator: 

[https://abtestguide.com/calc/](https://abtestguide.com/calc/)

&#x200B;

They have written formulas for everything else apart from 'Observed Power'. can someone please help me understand the step by step calculation for 'Observed Power'?",AskStatistics,2022-09-06 14:55:15,1
What else are you going to divide by?,2,x7kev0,,AskStatistics,2022-09-06 13:08:34,5
"Please see [rule 1](https://www.reddit.com/r/AskStatistics/about/rules/):

> 1. This sub is not intended for homework help

> This sub is not intended for homework help (""homework"" is interpreted quite broadly; among other things, it would include any coursework you're supposed to be doing yourself).   [...]

---

I will give a brief hint. Notice that if you know the mean and the sample size, you can calculate the sum of the values

For the standard deviation, it's worth thinking about the fact that you can relate the variance to the sum of squared observations (and other quantities).",6,x7dcdc," **There are 15 numbers in a list whose mean is 35, median is 32, and standard deviation is 4. The smallest number in the list is changed from 30 to 3.** 

1)**By how much does the mean change from the original mean of 35? What is the new mean?** 2)**What is the change in the standard deviation? What is the new standard deviation?**

&#x200B;

**So i assume that the dispersion is right skewed considering that the mean is larger than the median, so 50% values are between 30 and 35. I can assume that the mean would be lowed , but how can you get an exact number? is it a percentage difference? would I use the difference between the mins? Secondly, how can deviation be calculated with this? Ive read chapters of textbooks but it doesn't offer any insight on how to approach it.**",AskStatistics,2022-09-06 08:23:14,1
"If you’re simulating them and know they’re different, why do you want to *test* differences? Is this some kind of power analysis? A simulation study of a method? Or is the interest more in describing the difference rather than testing? 

Regardless, you say it’s best described as discrete so KS is right out. And since it sounds like you’re talking about a 2D distribution (?) then the others may also be inappropriate, at least unless you’re being careful to apply them in sensible ways. KL and Hellinger are, in their usual descriptions, measures of difference between 1D probability distributions. You may need methods from spatial statistics. 

The bottom line, whether you’re looking to test or describe, is that the nature of the data and the nature of the differences between the distributions matters a lot.",1,x7go0t,"I have a monte carlo simulation which has a specific distribution of points (**x1**,**y1**) and an altered simulation which has another distribution (**x2**,**y2**). There aren't really any known parameters to these distributions, no expectations, and imo it's best left as a discrete distribution. I just want to find how different these two distributions are from each other.

I'm not too sure which test to use when comparing the similarity (especially the shape) of these two distributions.

Some options I came across were the 2D KS (e.g. Peacock), Hellinger distance, KL divergence. There is no expected distribution so I can't really use a Chi2 test. I tried the 2D KS at first but it quickly got hard to compute since I'm working with 100k+ points and it needed to turn all these into 2D continous functions. I was thinking of maybe turning these distributions into density functions that I could use in some test.

Wondering if you know which one (including any I didn't list) that would be the most appropriate for this kind of situation.",AskStatistics,2022-09-06 10:37:22,2
"> I was thinking of maybe utilizing some form of regression analysis

Some form of binomial regression to model the proportion seems like an obvious candidate.

Logistic regression is popular but a logit link might not be ideal for this situation; you might need a different link (or you might be able to transform the size to get a reasonable fit)",1,x7cf1b,"Hi everyone,

I will preface this by saying my stats knowledge is relatively limited and I apologize if this seems like a basic question or if I’m being naive in any way. 

I have a data set of about 100 values where I am recording the volume of an object relative to its overall outcome. There are 2 general outcomes: outcome 1 = good and outcome 2 = bad. And I have a spectrum of sizes from 0-100, where I have an outcome for each data point. For example, data points are generally like size 10 with a good outcome and size 90 with a bad outcome, etc. 

I have noticed a general trend where larger sizes start leading to the bad outcome, and I want to apply some statistical significance to results. Is there a good way to determine the exact size at which there starts to be a statistically significant (p<0.05) difference in outcomes based on the size? Say, once size >50, outcomes are statistically more likely to be bad?

I was thinking of maybe utilizing some form of regression analysis but I’m really not sure of the best way to go about it. Again, I would definitely consider myself a stats amateur so I wanted to see what other people thought. Thanks!",AskStatistics,2022-09-06 07:44:57,6
"You shouldn't need to create new dataframes, just rerun the ANOVA without the factor you are considering dropping. You can also run av plots to determine which parameters can be dropped. I'm not sure what you mean when you say you want to find out where the interaction is coming from, you should be able to derive the estimated mean from the parameter estimates, however 4 way interactions can be tricky to explain or replicate so some people will stop at two way interactions.",2,x6yvf7," 

Hi all!

I’m writing my thesis at the moment, and I’ve received a 2(A) x  2(B) x 2(C) x 2(D) significant interaction. I’ve filtered the data based  on the two levels of A, and created two new data frames (A1) and (A2).

Both in A1 and A2 I ran separate two 3 way Anova’s of BxCxD and received significant 3 way interactions.

I then ran separate anova’s for both A1 and A2: CxD at each level of B, and received significant interactions.

Where do I go from here?

This seems quite slow and doesn’t feel like the best approach - any ideas on  the best way to continue analysing to determine where the significant  effect is coming from, without running more anova’s?",AskStatistics,2022-09-05 19:43:05,3
"Not sure how many electives you are going to take but if I were to rank them into definitely and helpful if you can take, I'd say:

Definitely 8, 9, 12, 15, 17, 18
Helpful 6, 7, 10, 20",3,x6udqr,"I'm a stats undergrad and considering pursuing a master's in ML in the future but I have no idea what electives would be useful for that.  


Electives I can choose from are the following:

1- Statistical Quality Control

2 - Census

3 - Biostatistics

4 - Reliability Theory

5 - Financial Statistics

6 - Operations Research (2)

7 - Categorical Data Analysis

8 - Non-Linear Regression

9 - Stochastic Process

10 - Decision Theory

11 - Order Statistics

12 - Bayesian Statistics

13 - Medical Statistics

14 - Actuarial Statistical Models

15 - Data Mining 

16 - Econometrics

17 - Time Series Analysis

18 - Applied Multivariate Analysis 

19 - Spatial Statistics

20 - Queuing Theory",AskStatistics,2022-09-05 16:14:43,3
Just divide sales by branch clients…which will give you sales per client. The highest sales per client will be the person who gets the prize,6,x6omkw,"Let's say I'm trying to evaluate sales results of a company and the person with most sales will be gifted a prize.

There's one vendor in each branch, and each branch with a fixed number of contacts/clients. 

Now, the situation is:

Vendor A - 24 sales - 1300 branch clients 

Vendor B - 48 sales - 600 branch clients

Vendor C - 43 sales - 1800 branch clients 

Vendor D - 23 sales - 300 branch clients

How to compare them more fairly? 
The percentage of sales by branch clients is reliable?",AskStatistics,2022-09-05 12:17:31,3
"I think what you're looking for is closely related to a *regularized estimator*, a class of estimators to which a Bayesian estimator belongs.  

IMDB uses an algorithm of this kind to rate movies so that a movie with a single ""10-star"" rating is not regarded as more highly rated as e.g., ""The Godfather"", which has ~2M ratings that average out to 9.2 stars.  I invite you to review it:


https://www.fxsolver.com/browse/formulas/Bayes+estimator+-+Internet+Movie+Database+%28IMDB%29

Note that IMDB's algorithm is not the *only* way to construct a Bayes estimator (and it probably isn't the ""best"" way, either, depending on how you define ""best"").  The way in which you interpret a Bayes estimator is slightly different from most (usually frequentist) statistical estimators so be sure you understand the intuition behind before you apply it blindly.",2,x6ui9n,"I'm having a hard time googling for an answer to this because I don't know the correct terms, hence the vague title.

Say I have a number of items {A, B, C, ...}. Each item has two pieces of data associated with it: a set of tags {x, y, z, ...} and an integer popularity score, p, where a higher number is better (imagine it represents likes or upvotes or something similar).

Is there a way to get meaningful information about the relative popularity of each tag while controlling for the popularity of the items?

To be concrete, imagine this toy data set:

* A: ({x, z}, p=72)
* B: ({y, z}, p=6)
* C: ({y}, p=40)

If I just went based on the total popularity score for each, I'd have (p_z=78, p_x=72, p_y=46). If I went based on average, I'd have (p_x=72, p_z=39, p_y=23). Both of these options seem less than ideal to my (naive) eye, as they are really affected by the overall popularity of item A. But I'm not sure there's a better way.

If it helps, in the real data set, the size of both the item set and the tag set is in the thousands, and the distribution of p is right skewed with a range of approximately [0,2000].

And in case it helps in knowing how technical or not to make your answer, I have an undergraduate-level understanding of math in general but stats is probably my weakest area and I haven't done any in a looooong time.",AskStatistics,2022-09-05 16:20:19,3
"You don't need a distribution there. You just need to know his many manufacturers there are.

If 20% of the manufacturers produce 60% of the total, on average they produce 3 times the emissions of the overall average, which is

       Total emissions 
     ----------------------
       Total manufacturers

In their case it's ""0.6 total emissions""/""0.2 total manufacturers"", or three times that overall average",4,x6pu5z,"Let’s say I know the total CO2 emissions from industrial manufactures in my country, and I also know that approximately 20% of those manufacturers account for 60% of the country’s total emissions. Can I estimate what are the average emissions of those 20%, say using a Wilbur distribution, with this data? How? Why or why not? Thanks in advance!",AskStatistics,2022-09-05 13:05:56,4
"Comparing what percentage?


Proportion of selecting A among two group?

Proportion if selecting A over, say B, in a single grp?",3,x6r53m,"Say I’ve got questionnaire data where one question was multiple choice of 3 categories.
If I wanted to compare to see if there’s a statistical difference between the number of people who chose option A, how would I do this?
Independence chi square test?
Thank you!",AskStatistics,2022-09-05 13:59:03,8
"Just two different ways of assessing the variability. Neither is ""right"" or ""wrong"" as a general rule, though in certain contexts one might be more natural than the other.",7,x6cv7l," I'm trying to understand the difference between Standard Deviation and Mean Absolute Deviation (MAD) in reality. 

I read somewhere that in SD square makes difference i am thinking that if we square then we also take square-root.

please explain",AskStatistics,2022-09-05 03:29:34,19
"Your question is somewhat unclear to me. 

 It's a test of a specific hypothesis.
 Hypotheses don't talk about comparing  observed vs expected. It tests for independence/association. 

Some  test statistic might measure discrepancy from independence using observed and expected - that is a test  statistic can explicitly compare observed and expected. There are many such test statistics and its not difficult to make more if you have some specific idea of the particular way  you want to measure the discrepancy. If you don't have one in mind the usual chi squared is generally a pretty good default choice.

If sample observed are far from expected values calculated under a null of independence in that contingency table framework, the Fisher exact test  p value will tend to be small, as would the p value for the chi squared test, the G test, the Freedman-Tukey test, the Neyman test, etc etc. 

However, the Fisher test doesn't have a statistic that directly compares observed to expected in the way it sounds like you intend. The others do.

You can certainly look at observed and expected after a Fisher test or any of the other tests. Indeed in a 2x2 table there's a fairly direct correspondence, in spite of the fact that the Fisher test is just based on the likelihood of tables under the null, conditioning on the marginal totals; it doesn't do a direct comparison of observed to expected like most of the other statistics (it does come quite close to a comparison of odds ratio to expected odds ratio, but even that's not quite exact).

If you already have a suitable  statistic in mind that does make such an observed/expected comparison, you can construct an exact test directly from it; it's not at all difficult (just a few lines in R, for example).

It might be best to explain more about your circumstances. How do your expected values arise? Why use the Fisher exact test rather than something else? 

What size is your contingency table?",1,x6tce1,"Can the Fisher's exact text be used to determine if the difference between observed and expected frequencies is significant? If so, could somebody explain how to do that?",AskStatistics,2022-09-05 15:29:56,2
"The basic concepts are reasonably straightforward. 

I don't think it's inherently harder than frequentist statistics, just different.",4,x6kdjs,,AskStatistics,2022-09-05 09:25:45,5
"If your question is how to draw the right conclusion from this test then you will need to look at the variances and the p-value. In your case, the variance of variable 2 is at least six times as large of the variance of variable 1. The p-value is smaller than 0.05 so this means that we reject the null hypothesis that the variances are equal. I.e. there is enough evidence to conclude that the two variances are not equal. If your intention is to do a t-test after this would need to use the two-sample t-test assuming unequal variances. 

&#x200B;

If your question is about how to report it, APA generally asks for the effect size (variances), test statistic (F-value), and the p-value. I don't know about other citation styles though.",9,x67cpt,,AskStatistics,2022-09-04 21:53:49,2
"2. Consider an example or two. e.g. 

 consider rolling a six-sided die

 Let A = roll a prime (i.e. 2,3,5). Let B = roll > 4

 A' ∩ B is the event ""roll a 6"". P( A' ∩ B ) is 1/6

 P(B) = 2/6   P(A) = 3/6. P(B) - P(A) is *less than 0*.

1. Try drawing a Venn diagram

   ... the part of A that is not in B should not be subtracted from B.

    You should be able to convince yourself it is instead P(B) - P(A ∩ B)

----

please note rule 5 of the sub:

> 5. Use an informative title

> Use a title for your post that very ***briefly describes the statistical problem you need help with***. It should not be about your emotional state (""Desperate""), personal circumstances (""Bad at stats"", ""I'm a beginner""), nor how urgent you think your problem is, nor your assessment of how easy you think it is. It should not consist largely of redundant information like ""Quick Question"" or ""Please Help"". If personal context is essential, put it in the body of the post.

A suitable title in this case might have been something like ""why doesn’t P(A’ ∩ B) = P(B) - P(A)?""",2,x6mgq8,"Hi everyone,

I was reading a statistics textbook and was having trouble understanding something. 

When there are two events, A and B, why doesn’t P(A’ ∩ B) = P(B) - P(A)?",AskStatistics,2022-09-05 10:49:17,1
"You could test the hypothesis that the variances of the two groups are the same (the 19 and the 2), and if they are then pool all 21 into the total.  If the variance of the 2 is not the variance of the 19, then you have a repeatability problem and it's time to stop and eliminate the cause before you move on.


Of course you can also do the same with the bias and if there's a mean difference, you got some 'splainin' to do.",1,x6hie2,"I'm analyzing the repeatability of dissolved oxygen measurements and I'm wondering if I screwed myself over. Sample size is 21 with me testing 19 and someone else doing the other 2. Since it's a repeatability study (yea I know), should I just omit those 2 data points since I have reason (but they are not outliers)? Should I do a test on bias?

I already know how I want to analyze the data, Just wanting to know others opinions.",AskStatistics,2022-09-05 07:26:18,4
"By r coefficients I assume you are referring to Pearson's correlation coefficient. A possible reason that you may not be noticing any significant correlation is that there may be confounding factors.

As an example suppose I'm trying to measure association between number of trees and happiness and I have access to data from various states. A possible confounding factor may be healthcare spending, which may vary per state and mess up the association that I'm trying to test.

A simple way to deal with this would be to run a linear regression,  which allows you to control for these confounding factors, by including them as predictor variables. The regression would have outcome XYZ is the response/independent variable, and you would include the geographical characteristic and any confounding variables as predictor/dependent variables.  

As for using data from previous year, this would go into the realm of time series analysis. If you're familiar with time series analysis (or willing to learn) than go right ahead! Otherwise I would probably stay clear and stick to date from the same year. 

There are more complex statistical techniques you could consider but it's hard to recommend without knowing your level of statistical knowledge.",1,x6cedk,"I am an undergraduate student of psychology from Indonesia.

Right now I am doing a mini research project where I correlate  geographical data to outcomes at the provincial level. The design is that geographical characteristics A is associated positively with outcomes XYZ. However, there are only 34 provinces in Indonesia, which certainly makes it very difficult to get reliable results, and probably impossible to detect small effect sizes. Right now, my analyses have yielded non-significant positive r coefficients.

However, I do have data on provincial characteristics from different years, e.g 2015-2020. I am wondering if there is a statistical method where I can somehow ""synthesize"" the nonsignificant results from different years into reliable(?) significant r coefficients?

I hope I articulated my intentions clearly. I have no educational background in mathematics so I'm wondering if any of you here could help me, and pardon if I misunderstood some statistical concepts here. Thank you very much in advance.",AskStatistics,2022-09-05 03:00:21,1
"There's plenty of things you could work on involving multivariate time series: forecasting, dimension reduction, anomaly detection, classification, etc.

Take a look at what your professor has published as it may give you some ideas.",3,x6bguw,"Hello guys,

I'm thinking about a possible topic for my master thesis. I'm doing MSc in Econometrics, but the most interesting subject and the most approachable professor teaches the course Multivariate Analysis, so I would like to write my thesis with him. Do you have any suggestions for a topic that combines these two areas?",AskStatistics,2022-09-05 02:01:25,1
"If you search calculus on this subreddit or r/statistics, you'll find what you need. This question gets asked frequently so there's no shortage of answers.",3,x6a4l5,"Hi!

I'm a business student hoping to focus on data science. Econometrics is my desired major, but due to some circumstances, I didn't get the chance to learn fully calculus in school.

Where can I start to learn calculus that will help me catch up (or even get ahead hopefully) in my field of study?",AskStatistics,2022-09-05 00:34:37,2
"Given a fixed number of total subjects, equal n gives the most power. However, increasing the number of subjects in the control group gives you more power than not increasing the number. There is no requirement for equal n.",12,x5q7de,"I am designing an experiment to compare the difference between two proportions. I determined that I need 200 users each in the control and treatment groups for the difference that I would like to detect. However, if I can afford more users in the control (say, allowing 1000 users in the control), does it matter that I restrict the control to 200 users?",AskStatistics,2022-09-04 08:50:36,6
This behavior of kappa is a known problem.  [Here's a critical appraisal with lots of references.](http://www.john-uebersax.com/stat/kappa.htm#procon) Look in the section **Cons**.,1,x5sc7y,"I've been calculating Kappa values to quantify agreement within my data set and have been thrown by one set of results. 

Out of 72 data points, 71 show the category AA and 1 has the category AB. This gives me a kappa value of 0, meaning slight agreement. However
98.61% of the observations are in agreement.

For the same factor I have another data set, where 69 of the isolates are AA, 1 is BA and 2 are AB. This gives a kappa of -0.02, meaning no agreement. However again a most (95.83%) of the observations are in agreement.

Can anyone help me to understand this as I'm a bit confused how other factors (with worse % agreement) have had better Kappa values.",AskStatistics,2022-09-04 10:19:24,1
">	Some say buying late is the right choice, as it narrows down the pool a bit, and some say it’s to buy early.

They’re both wrong. It doesn’t make any difference when you buy. Let’s use an ultra simplified example to illustrate why. Imagine you have two envelopes. One has a $1 bill in it, and one has a $100 bill in it. You and a friend are each going to choose an envelope. You can choose whether to go first or second. Do you want to go first, or do you want to go second?

What if there are three envelopes? Two with $1 and one with $100, and still just you and a friend? Do you want to go first or second?",2,x5rhbm,"Hi everyone! I've been thinking about this situation a lot, as I always get different responses from different people. This isn't homework, just a scenario that I have come up with.

____________

You and your friends are deciding to take part in a sports card draft. (Draft meaning you randomly select cards from a pack, and in this case, you are picking 2) There are 4007 cards in this pack, and each card has a different rarity. 

The probability of each rarity is as follows:

R1: 32.6% (lowest rarity)
R2: 27.2%
R3: 20.8%
R4: 13.4%
R5: 5%
R6: 0.95% (highest rarity)

In this draft, multiple people can pick cards. (Anywhere from 1-5, but picking more costs more) You, your friends, and ~300 to ~500 people get to pick the cards 30 minutes early before they are released to the public.

*Assuming that you are picking two cards, when would be the best time to buy? (Best being defined as having the highest chance to get a higher rarity.)*

_______

This is a question I have been thinking about for a while, as every time I talk about this with people, they give me different answers. Some say buying late is the right choice, as it narrows down the pool a bit, and some say it's to buy early. 

Hope you guys can give your two cents on this!",AskStatistics,2022-09-04 09:43:25,6
"A couple quick checks:
1.	Is the alpha input one-sided or two? Are there other defaults that are used in gsDesign that are quite different than used in other calculators? 
2.	as you said, is the output sample size per arm? 

(I’m less familiar using the package for sample size calculations, so there might be other straightforward checks.)",2,x5jfd2,"I'm learning sequential testing so that I can apply it in an online AB testing environment. I've been studying gsDesign, an R package for sequential testing. I'm confused by the sample size calculations that don't seem to agree with a sample size calculator for fixed testing. I'm aware they shouldn't be exactlty the same, but gsDesign predicts I need half the volume, which is too large a difference and in the wrong direction. I'm sure this is user error, but can't figure out where I'm going wrong.

Here are the two cases-

Fixed calculator:

[https://select-statistics.co.uk/calculators/sample-size-calculator-two-means/](https://select-statistics.co.uk/calculators/sample-size-calculator-two-means/)

&#x200B;

https://preview.redd.it/xi7g0378gtl91.png?width=1044&format=png&auto=webp&s=06129c8299c5d6a3df7633907407ca2510da1491

Other calculators seem to agree with this, they all give sample size estimates per variant that are greater than 100k.

&#x200B;

gsDesign:

https://preview.redd.it/h4i10ojbgtl91.png?width=458&format=png&auto=webp&s=5da33c0418c0eca270ad232441e1fb371ba14758

Which gives n.fix = 50078.71, which I'm guessing has to be per treatment?

I believe delta should be the standardised effect size, which I think is the hypothesised difference in means divided by the pooled sample standard deviation. I think I've applied these consistently across both tools.

Where am I going wrong?",AskStatistics,2022-09-04 03:10:14,6
"It can be used as data analysis ( to identify trend/cycles)
It can also be useful to create an explanable model ( this is our general trend + our weekly cycle so we expect X by week 21 )
Aka it's an easy way to interpret or explain to someone why you see those values in the data.
It's a simplication of the data into someone easily understandable",3,x5goxy,"I need to forecast the daily traffic on my employers website so I'm trying to learn forecasting but I'm a bit confused about this. After making sure the series is stationary, I've seen most people recommend decomposition but then the output isn't used anywhere. For modeling (arima, ma, etc) the original data is used rather than just the trend. The what was the point of decomposition?",AskStatistics,2022-09-04 00:15:38,10
"Typically an online Masters falls under the realm of a terminal Masters. Terminal Masters typically have more applied options (useful for industry) in their curriculum.

A Masters enroute to a PhD is usually more theoretical, as you apply to a PhD program that has a curriculum designed for research and publishing, follow their curriculum, and at a certain point they decide you have done enough to get a Masters.

It is still hard to get a PhD incrementally in stem and quant fields in the US (it can be easier in other countries), that is expecting to transfer from a terminal Masters to a PhD program with any advanced standing. At best, you might get unit credit that minimizes the length of a PhD from 4-6 years to 4 years, and in that circumstance you still need to complete the comprehensive/qualifying exams.

Really, if you want a PhD, it is better to choose to do it, do any prerequisites, do your GRE, get your letters of recommendation, and start the PhD by age 22-32, while holding off having kids until at least the last two years of the PhD. At the end, you will have the status of a PhD, a higher income base in industry, and more teaching options.

Regarding a terminal Masters, including an online Masters, it is often the experience of many that what happens is that your bachelor's degree and past work experience gets you in the door to new career opportunities but the skills from the Masters keeps you in the door by giving you the skills to do the job.

With your background in EE and econ, are you sure you don't want a Masters in Software Development or Engineering (software or some other engineering field)? Maybe a graduate degree, including a JD, that can help you monetize IP?",4,x5i339,I am currently a data scientist/software developer; I double majored in college in electrical engineering and economics. I'm interested in doing a MS Statistics online now through NC State (since my company will pay for it) but I don't want to preclude my chance of a PhD. Is it common to go from a MS Statistics to a R1 Statistics PhD program?,AskStatistics,2022-09-04 01:45:07,9
"""We had enough"" means they had more than someone who answered ""We didn't have enough"". You are measuring a quantity, and there is a sensible, meaningful order to the possible answers. That's not the case for nominal variables.",3,x5e0el,"If I ask you via a survey what your socioeconomic class was growing up, but I word it in a way that is not quantitative is this still ordinal? For example ""we didn't have enough, we had enough, we had more than enough."" I'm still new to stats, and this is driving me crazy. I am leaning towards nominal, but I am unsure. what separates this from a question such as what is your favorite season if it is not nominal?",AskStatistics,2022-09-03 21:35:23,6
"There is a lot of misinformation about likert scales floating on the internet.


Firstly, parametric tests don’t generally assume normality. Some of the popular ones do, but it’s by no means a rule. For example, you can analyze likert items using chi squared test or fit an ordinal regression model and then use likelihood test. Neither of these require assuming normality. So the whole parametric vs nonparametric debate is pointless.


The main problem with treating likert items as metric is that we have generally no way of (a priori) knowing if people perceive response categories as equidistant and if the perceived distances are the same across all group of people we want to compare. Empirical research on this is mixed - sometimes this assumption seems to be viable and sometimes it’s wildly off. In other words, there is no general answer to this question and if you want to use techniques that assume likert scales are metric, you should demonstrate that, for your specific case, the assumption is (approximately) correct. If it isn’t, the computed means, as well as any analysis based on them, are meaningless.",5,x55kbv,"I've read in a few articles that you can't really use parametric tests (like an ANOVA) to compare groups on Likert scale outcomes because the sampling distribution of sample means will not be normal. But at the same time I have read articles that say you can use parametric tests with Likert scale data. 

[https://statisticsbyjim.com/hypothesis-testing/analyze-likert-scale-data/](https://statisticsbyjim.com/hypothesis-testing/analyze-likert-scale-data/)  


I have a scale where people rate images from positive to negative (1=positive, 3=neutral, 7=negative). If I want to compare 3 groups of people, could I use an ANOVA, or would I have to use a nonparametric test?",AskStatistics,2022-09-03 14:38:42,8
"Do you plan to use PWB and LS as some sort of combined variable? If so, SEM might be useful. If you plan on predicting them separately (which is probably what I would do), then I would just use OLS regression, at least to begin with. One of the benefits of SEM is that you can create latent factor variables (e.g., a variable representing PWB and LS), as well as measure and purge measurement error (which can be useful for some psychological variables, which tend to have quite a bit of measurement error). I'm not sure your study needs this. So, I would just tackle it first using OLS regression and then *maybe* use SEM depending on what you find.",1,x5b2lh,"I want to do a quantitative study on how experiencing toxic behaviors in online gaming affect the psychological well-being and life satisfaction among male and female players. My independent variable is the toxic behaviors and my dependents are PWB, LS, and gender. I was wonder if doing a regression analysis will already do the trick and if I should do a SEM analysis? Or is there a better method to use for this study?",AskStatistics,2022-09-03 19:02:13,1
"You’re going to need to simplify the question into statistics terms. 

Is the likelihood of transmission uniform across people? Like if you engage once each with two people, each with said STI, are you equally likely to  see transmission?

If the answer is yes, and the number of interactions is the same weather it’s with the large number of people each with unknown status, or if it’s many times with the same person with known positive status, then the former is less likely to see transmission.  

The basis here is that with someone of unknown status you can see ‘no transmission’ through two ways, one that they don’t have anything to transmit and the other i they were positive, because of the safeness of the interaction, nothing was transmitted. This latter part is true in both circumstances uniformity (I’m assuming, that’s the question I have above), and then adding the possibility of the other partner potentially not being positive means the likelihood of transmission is even lower. 

To use some numbers. Let’s say the odds of transmitting if someone is positive per act is 5%, and the average rate of being STI positive in the group of people is 20%. Then per act with a positive person has likelihood of transmission of 5% ( 20% * 5%), per act with a person of unknown status has a 5% rate, and the likelihood of transmission from someone who is negative is 0%.",11,x4vwll,"Hello,

Feel free to use arbitrary infection rates, this doesn't have to be perfectly scientifically accurate.

Assume the person asking the question is using condoms.

Is it riskier to engage in ""ignorant bliss"" acts where the STI status of a partner is totally unknown due to lack of STI testing in 3+ years with multiple partners and non-condom use.

OR

Is it riskier engaging after STI testing gives positive for HSV2 and clean for everything else.

...Asking for a friend.

For more details and a list of stupid decisions continue reading.

I'm currently in a polyamorous argument.  

* Partners A and B add Partner C.  Partner C has an unknown STI history with known higher risk factors.
* A+B already have HSV1 from very early childhood.  Both are asymptomatic and no prior partners have contracted HSV1 from them.
* A+B engage in multiple/repetitive acts with C while waiting on STI results (C takes a while to get tested).  
   * A doesn't use protection with C but they're both female presenting.
   * B is 100% condom use with genital sex, not with oral.  
   * C told A that she saw anti-virals at a former partner's house.
* STI Testing reveals C has low values of HSV2 IGG (within the potential false-positive range).
* A+B both get tested and are clean for HSV2.
* C claims to have never had an outbreak, however C might be a liar.
* A abstains entirely from further physical contact with C, but continues seeing them as friends+snuggles.
* B continues kissing C, but avoids genital contact with any mucous membranes.
* A+B also go to an unexpected sex party, both engage with the people who have unknown STI status. One person even says she's lucky she's clean since she was formerly a sex worker.
* Later that week, B gets a BJ from C after reading that HSV2  
   * Is less likely to transmit \*to\* guys from girls in general
   * Is less likely through oral sex (if the infected party is giving).
   * That HSV1 (which A+B have) may provide some protection against HSV2 (which C has).
* A has openly performed oral sex on an HSV2+ partner prior to dating B and seems to think this is okay or is even apparently proud of it for destigmatizing HSV2.
* B is in trouble for being risky and violating safe-sex boundaries of A by engaging in oral sex with C.
* B thinks the blowjob is lower risk after knowing which STIs are in play than prior to not knowing and all the other behaviors mentioned.  But B is an idiot that gets blowjobs from people with known STIs, so he's probably wrong.  

And no, knowing this answer will not help B get out of trouble and he is at least smart enough not to bring it up.",AskStatistics,2022-09-03 07:35:33,5
"The first part refers to the fact that there isn't any number you can write that is infinity. Infinity is an abstract idea - you just keep going and never stop. That's the same idea as a limit - you keep going in some direction and never stop. 

In the context of Bayesian models, they are pointing out that in many cases, models with different priors will converge to the same posterior with finite amounts of data, and depending on things like quality of data, 'correctness' of the likelihood function, etc., that convergence may happen relatively quickly. In such cases, the data quickly 'dominate' the priors.",5,x4vphl,,AskStatistics,2022-09-03 07:26:27,6
"> It would be wrong to do that because of the differences of sizes?

Not inherently. Your power may be low, since two of your groups are quite small and the groups are very imbalanced, but there is no inherent problem with unequal groups sizes.

> Could i randomly take let's say, 50 individuals of group C, use them for comparison

You would almost never want to do this; this is just throwing away data for no good reason.

> Or the ""non-parametric"" approach allows me to do it?

Parametric or non-parametric has nothing to do with it. Whether the Wilcoxon is actually a good choice given your particular data and research question, we can't say, but unequal groups sizes will not invalidate your use of the test, though the power will be lower than in the case of equal samples.",6,x57j8z,"Hi! Currently, i have a dataset consisting of 50 measurements (continuous variables) for a total of approximately 4000 individual. Of those, 30 belong to a group ""A"", 70 to a group ""B"" and the other 3900 to the group ""C"" (also, it would be our ""control group""). I want to perform some tests to compare this measurements, with the aim of comparing group A against C, and also group B against C. The kind of test that i want to perform are non-parametric (for example Wilcoxon's Rank Sum Test for each variable). Can i use ""directly"" this tests to compare the small n groups (A or B) against the large n group C? It would be wrong to do that because of the differences of sizes? Or the ""non-parametric"" approach allows me to do it?

I've tried to search literature for a similar case but because I'm recently starting to understand statistics, I'm still lost and need some help. If you have any books, papers or links that could help me with this it would be very much appreciated.

Final thought: Could i randomly take let's say, 50 individuals of group C, use them for comparison, and iterate this ""random"" process? If this would be a viable approach, then how could i interpret the results? Thanks to anyone for reading this long! Any help is appreciated!",AskStatistics,2022-09-03 16:08:46,7
the plot https://postimg.cc/cvp8tKbT,1,x51vy5,"Hi fellow data scientists,

as a take-home interview exercise, I am asked to forecast **stock returns.**

my dependent variable is **Return\_1Month\_Forward** (stock returns) and my independent variables are **delta-dividend-growth** (daily dividend growth ratio) and **Dividend-Yield** (Dividend / Stock price) as you can see plotted in the picture in a comment below.

The two independent variables were not given but I extracted them from other variables as suggested in the exercise. I checked that their values y make sense by comparing their range to some real data I found online. i.e I looked at the time series of the dividend ratio of some company's stocks.

Now here's where I am lost, I a supposed to find a model that explains the relationship between these variables and I just don't see it.

I don't see any clear apparent relationship by just looking at the plot. I also tried to fit a polynomial regression model but the results were very bad.

Is there anything that I am missing here?",AskStatistics,2022-09-03 11:54:52,3
How is this monthly average recorded? E.g. is it in whole dollars or binned into ranges? Are respondents just guessing or were they asked to keep a record prior?,1,x545re,"Hello, I am a social work major and I am taking a social statistics course for my final semester in college, so I am utilizing SPSS. my question pertains to understanding a histogram I just made. This particular data set which is groceries, measures the monthly average spent on groceries, and the results were taken via survey. I made the histogram, but interpreting it is difficult for me. I believe it is either interval or ratio, but I am confused. One of the variables is zero as in zero spent, wouldn't this mean that it is Ratio data as it has an absolute zero, and you can't spend less than zero on groceries?",AskStatistics,2022-09-03 13:36:47,9
"Upton and Cook’s Oxford Dictionary of Statistics is good. 

Perhaps I am being too concrete about your A-Z term. 

It’s a good read. Other than indexed look-ups I have read through to Bradley.",2,x4t4j8,"Do we have an A-Z guide on statistical analysis? A guide that can help a novice to understand the link between scales of measurement , variables and appropriate statistical tests? One that can help a masters and PhD student to confidently write the Methodology section, select statistical test and justify the choice, analyze and interpret data?",AskStatistics,2022-09-03 05:21:49,7
"1. Why do you want to do a transformation?

2. your data has values -3 to 3? Is it some ordinal Likert-like scale? Or is it some discrete random variable? Or something else?

3. You want to ... compare *type I error rate* between conditions? That doesn't make sense to me. Type I error rate of what test? and why would that make sense to do it at all?",9,x4o5z3,I want to compare type I error rate between transformation exist and nonexistent conditions. But my data has -3.... to +3.... values. I have to add a constant but how and where ? Is there a way to solve this problem ? Thanks.,AskStatistics,2022-09-03 00:09:18,22
"it's impossible to tell whether the topic is too specific or too generic without knowing the field fairly well. if you are unsure, create the search terms based on your PICO and search the databases with it, and see how many articles you have left over after sifting. if it's too many, narrow down the question. if it's too few, expand it (but don't it to just add articles - consider whether expanding the question actually makes sense).",1,x4gby2," Hey guys, i want to do a systematic review with meta-analysis to improve my curriculum for a competitive residency, my question is: how do i know if the topic i chose for research is relevant? (I used PICO) and if is it too specific or too generic?

PS.: I've never written an article, this will be my first.

PS1.: I don't know if this is the right subreddit for this kind of question, sorry.",AskStatistics,2022-09-02 16:58:41,4
"My real question is, given the limited data, can I tell my boss that I'm 99% sure this is correct?",1,x4j9a0," I have a sample where my algorithm correctly predicted a value of 0-9, 7 out of 7 times. I had none of these samples before building the algorithm, and I have exactly a 1/10 chance of being randomly correct on any single sample. How do I calculate the chance that I have a correct algorithm? Is it 1-1/10\^7, or much lower?  I don't know what a Z-value is, that's stumped me in the Google results.

This is not a homework question, it's a real-life question from a novice programmer with limited data who doesn't have any education or background in statistics.",AskStatistics,2022-09-02 19:27:40,2
"total number of ways to do the two draws is (78 choose 9) \* (78 choose 10)

to get the total number of ways where there are exactly 4 common cards, you could first choose the 4 common cards from the 78. of the remaining 74, choose 5 cards for cards in the first draw that's not in the second. and finally from the remaining 69 choose 6 for the second draw. the number of ways would be (78 choose 4) \* (74 choose 5) \* (69 choose 6)

divide the two to get the probability. i'm getting \~0.012 as the answer

below is a table of probabilities for different possible number of common cards

|number of common cards in both draws|probability|
|:-|:-|
|0|0.2702282|
|1|0.4053423|
|2|0.2392184|
|3|0.07202274|
|4|0.01200379|
|5|0.001125355|
|6|5.771053e-05|
|7|1.498975e-06|
|8|1.677957e-08|
|9|5.483519e-11|",3,x467fc,A deck of tarot has 78 cards. 2 draws were done. The first draw was a 9 cards draw and the second draw was a 10 cards draw. How do I calculate the odds for 4 of these cards having come up in both draws?,AskStatistics,2022-09-02 09:41:19,2
"Also, if those dots are individual cells, you need to think about whether they represent individual experimental units

https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3321166/",15,x3kg9v,,AskStatistics,2022-09-01 15:23:27,26
"What exactly do you intend this 2.5% effect size to mean in a 2x2 table?

\--

I am familiar with phi. There's a 'w' in Cohen's book; I'm assuming that's the one you mean. 

Note that there's also his effect size measure 'h' for a difference in proportions (in the previous chapter in the book); this may be more suitable depending on what you want to specify about the effect size.

As far as I can see from Cohen's definition (and a little algebra), w = √[χ^(2)/n] (the chi-squared statistic divided by n, then take the square root, where n is the total count), which appears to be the same as |ϕ| in the 2x2 case.

That is, you'd convert ϕ to w simply by dropping its sign.

ϕ is often called Matthews Correlation Coefficient (MCC) in machine learning, if that helps locate information on it",1,x3xzgv,"Hello,

for a chi-square test I would like to estimate the sample size necessary to have an alpha level of 5%, a power of 80% and an effect size of 2.5%. I am aware that the effect size is typically indicated in phi or w, but my understanding is that that this can be calculated into one another. But I can't really wrap my mind around how. Anyone?

&#x200B;

Thanks",AskStatistics,2022-09-02 03:27:01,5
"Hi we are similar in that we are not core statisticians -  except I am a doctor who likes stats. 

Here’s what I am doing - I am now studying part time for a PhD by published works in “Quantitative analyses of restraint reduction in hospitals” 

Taught myself to code Python then Julia by doing stats projects which I love.

Got a few papers out in my field using stats or on stats 

Buy any old book on stats from 2nd hand shops that I see 

Buy cheap text books for important stuff, kindle is pretty good

Don’t be too proud to read school level stats to get the basics if you skipped that class

Watch YouTube videos like Khan academy and 3Blue1Brown 

Wikipedia is pretty good but double check it because some people edit it who should know better

Set up a stats WhatsApp group for me and other nerds at work 

Started teaching basic stats to medical students 

Follow this sub - there are a few regulars who really know what they are talking about

Enjoy it, stats is beautiful

Watch out for an effect where people like you and me think we know more than we do - take a humble approach when a “real” statistician cuffs you round the ear it’s good for you.

I think autodidacts probably skip proofs too much, I think they are like katas in martial arts, they are worth the time. 

Knuth on The Art of Computer Programming is expensive but mind blowing and covers loads of interesting maths that applies to stats. Binomials, combinatorics, and logarithms and such.",3,x3s174,"
Hello,

I am an electrical engineer from India graduating this year. My interest lies in the field of data science and I have acquired some of the skills that help in data science such as Python(pandas,bumpy,seaborn) , Power BI, Tableau, SQL. I understand how important it is to understand statistics and probability for Data science. Being from a STEM background does give me a bit of basics but I want to learn statistics from the very basic to a good level. If any of you guys can help me with a path or the resources on how to get started and from where to study, it will be of great help!",AskStatistics,2022-09-01 21:20:14,5
"Ah, now I can see it. (That second page didn't show for me before)

Yes, that's clearly a typo. It should definitely be '3' not '2'. I don't imagine anyone would be much bothered by it (don't expect it to be mentioned in later papers, except perhaps in passing). I don't know if there were errata issued for the paper. I didn't locate any with a quick search.

(Your original query had me looking through the paper for a 3 when I should have been looking for an anomalous 2.)

Here's a double check in R:

    a <- pwilcox(0:9,3,3)
    names(a)<-(0:9)
    a
       0    1    2    3    4    5    6    7    8    9 
    0.05 0.10 0.20 0.35 0.50 0.65 0.80 0.90 0.95 1.00 
    
You can see that the values 0-5 are the same as that column of the table in the paper for m=3",11,x3gle6,"I have outlined the supposed typo in red. Looks like it should be a 3 not a 2 in that series. It’s a paper from 1947 with 13k+ citations. 

I mainly ask because I love non-parametric models and I am worried I don’t understand it. Basically this “2” should be a “3” I think. 

Normally I’d shrug it off but this test is central to something else I am doing that I need to have a watertight understanding.  

Here is the citation:

On a Test of Whether one of Two Random Variables is Stochastically Larger than the Other
H. B. Mann, D. R. Whitney

Ann. Math. Statist. 18(1): 50-60 (March, 1947). DOI: 10.1214/aoms/1177730491",AskStatistics,2022-09-01 12:42:10,13
"took me a while to parse ""MDE"" as minimum detectable effect

Usually if I am in a situation where a simple formula is not easy to obtain, I calculate power using simulation:

Given specific hypotheses and a specific model and and a specific test statistic you progress as follows:

Specify a specific alternative, or a sequence of them if a power function is required (i.e. specify an effect size or a collection of effect sizes). Specify a sample size, n. Specify alpha (or state a rejection rule). Then:

for each such effect size:

- Simulate many times (I typically use tens to hundreds of thousands) 

  \- generate data from the model at the specific alternative. 

  \- decide whether the test is rejected or not on that sample 

- Compute the proportion of rejections (and usually, estimate the standard error of the proportion). This is an estimate of power at that alternative (with an estimate of its s.e.; this is useful for a number of things, not least telling you if you have sufficient accuracy on your estimate of power for your purposes or you need a larger simulation size)

You can also check the closeness to the nominal alpha this way (some tests will be exact when you simulate 'from the model' but some are not, and some may surprise you how far out they can be)

You can also investigate behavior (power an significance level) when assumptions don't hold in particular ways.


In R, the tool I mostly use for this is the built in function `replicate`. If I want a whole power curve, it goes inside a loop, and then some smoothing steps\* are added.

If some other quantity besides power is needed, modify as appropriate but the general concepts are much the same.

---

\* Typically, I start with suitably precision-weighted locally linear approximation of Φ^(-1)(p) where p is the power, as a function of some measure of raw or standardized effect size d (as relevant), (sometimes with the fit being updated when the power is better approximated); if I'm trying to find n to attain a given power at a fixed effect size, generally I look at Φ^(-1)(p) vs 1/√n instead. Usually a few well-chosen points suffices for solving that case  (typically just two once you bracket the desired power, but getting to that point doesn't need a high simulation count).",2,x3nksn,"Hi all,   


I'm struggling to find a textbook that might teach me how to manually calculate power for various research designs. I've always used PowerUpR which is like... fine if you want to plug and play but there's so much that I do not understand which is a frustrating situation to be in. If anyone has recommendations for resources (articles, texts, etc.), that teach you the logic of calculating MDES / Power manually, that would be incredible. Thanks!",AskStatistics,2022-09-01 17:43:38,1
"There is no reason why likert scores should be approximately normally distributed (they can’t be exactly normal since they have both lower and upper bound). This isn’t a mistake, just a fact of life. Skewness also doesn’t necessarily lead to negative correlation.


Your results may be surprising in the context of the theory you are working with, but nothing you have written suggests there is a statistical problem.",2,x3i89p,"I have a lab report due Monday, I'm relatively new to stats but I know roughly how I should be treating and analysing the data.

Essentially, I have to compare three sets of questionnaires.

2 of them should correlate, the other one shouldn't.

I've run an initial cronbach's,
Reverse scored the questions that needed reversing
Run a second cronbach's and a correlation heat map which shows good results so far

But then when I move on to the boxplots and histograms theres a lot of skewness, the data isn't normally distributed, so then when I go to analyse the 2 questionnaires that should correlate, the scatter plot is negative. I don't know where I'm going wrong

Can someone please help?",AskStatistics,2022-09-01 13:50:44,9
"You're asking if abstract algebra is useful for statistics? Probably not. Sometimes there's reason to talk about groups, but outside of some niche topics I think it's very low priority.",5,x38o8y,,AskStatistics,2022-09-01 07:10:43,8
"You need to do a t-test for difference of means for two different samples.

To figure out which test you need though you first need to do a a different test to see if the variances of the two samples are the same. 

So first figure out if the variances of each season are the same (look up f-statistic). Once you do that test you can then pick which t-test you need to test the difference of means.",1,x39usf,"Hi,

I’m completely new to statistics, I understand basic concepts like average, median, variance, regression.

I’m trying understand a very basic example of using statistic-based thinkig in sport. 

Example: let’s say we have a basketball player. Let’s say it’s his 2nd season in the league, same team, same position etc. In his 1st full season he averaged 20 points per game. After couple games of new 2nd season his points per game average is 30 points. 

My goal is to estimate if the probability of this player scoring more than 20 points is higher than before start of current (his 2nd) season.

How would you approach this problem? Does his new average after couple games means that he became a better player? Or was he just lucky in new season?",AskStatistics,2022-09-01 08:00:38,1
"I’m ABD for my statistic PhD, and I’ll share my perspective. 

1) Broadly, my research was in methods for precision medicine. The methods themselves are pretty wide ranging, but those were the applications I was interested in so that guided my research. 

2) My coursework is almost all theory. 90% proofs and 10% computing, with the exception of my statistical computing course. Dissertation work was a bit more balanced, because I’m coding up methods and running simulations, etc. 

3) There’s a ton of interesting problems out there to work on, but if I was starting at the beginning, I would be focusing more on “machine learning” side of things. Causal ML seems to be a hot topic this year at JSM. 


4) Find a good therapist now. Getting a PhD can be a grueling task, and having an established relationship with a mental health professional will be incredibly helpful when navigating grad school.",13,x2p2qf,"Hello r/AskStatistics!

I'm applying to graduate school this year, and my undergraduate background has left me with a couple of options for which programs to look at. I have done research in mathematics and computer science before, and I've taken many classes in the statistics department, but I'm curious about what the day-to-day life of a PhD student in stats is like. So, my questions are:

1. If you are pursuing/have a PhD in Statistics, what field did you do research in? What did you write your dissertation about? 
2. How much computing v.s. theory can I expect from graduate statistics?
3. What are some of the more interesting parts of research in statistics?
4. Any other comments you might have for a prospective PhD student :)

Thank you in advance!",AskStatistics,2022-08-31 14:22:42,4
"If the relationship is a widely studied one, search for meta-analyses. Regression weights are unstandardized, making comparisons across contexts problematic, especially when looking at continuous measures (e.g. money allocated). This is why you will find that most comparisons are made using correlations.",1,x336u4,"Hi, I am studying the correlation between two variables and would like to compare it to previous findings (from literature), how can i compare between those correlations /regression analysis?",AskStatistics,2022-09-01 02:36:01,3
"> Also is there a possibility to test for interactions?

Yes, if you put them in your model. Which interactions did you want in the model?

> I can't do a ""normal"" plot for a logistic regression because I don't have a numeric variable on the x axis. 

Possibly you can depending on what you're looking to see.

Imagine you had an ordinary regression with (as here) 5 IVs (plus some interactions), not necessarily binary. What plot would do then?",1,x2zuxg,"Hi everyone!

So I got a dataset where my **all my variables are binary (0/1)**. I have one endemic aquatic species (present = 1, not present = 0) as my dependent variable and a few neophytic species (present = 1, not present = 0) as well as two diceases (present = 1, not present = 0) as my predictors/independent variables. I want to determine (in R) how the presence/absence of the neophytic species and diceases influences the presence/absence of my endemic species. I tried to it with a glm, but I'm not sure if that is the right approach, since I'm quite new to statistical tests. I did it like this:

*mod <- glm(data = data, endemic\_species \~ neophytic\_species1 + neophytic\_species2 + neophytic\_species3 + dicease1 + dicease2, family = ""binomial"")*

Also is there a possibility to test for interactions? What test would you use? And my biggest problem is, that I have absolutely know clue how you would visualize it! I can't do a ""normal"" plot for a logistic regression because I don't have a numeric variable on the x axis. Maybe just a barplot?

Any tips/ideas/comments are greatly appreciated! Thanks in advance!",AskStatistics,2022-08-31 23:03:43,2
"It's the same model (`aov` is a wrapper for `lm`). The `summary` method for `lm` returns tests for the coefficients, whereas `summary` for `aov` returns the anova table. If you do:

    mod <- lm(y~x*z, data = df)
    anova(mod)

You'll get exactly the same output as with `summary(aov_mod)`. But beware: These are type I ANOVAs which are rarely interesting. To get type II ANOVAs, use `car::Anova`, for example. In this particular case, it doesn't make a difference, though.",1,x2oq44,"Hello,

I ran an experiment with a two-factor setup: treatment x (not applied or applied) X treatment z (not applied or applied). I am interested in whether the baseline response is different than zero (i.e. is there a significant intercept), the unique effects of x and z, as well as whether they function additively.

It seems like the easiest way to do this is with lm. It gives me the intercept and an associated p-value, the estimate for the effect of x and z on their own, as well as the deviance from additivity. For example, doing this:

    df = data.frame(y = c(-1, 0, 1, 1, 2, 3, 3,4,5, 1,2,3),
                x = rep(c(FALSE,TRUE), each = 6),
                z = rep(c(FALSE,FALSE,FALSE,TRUE,TRUE, TRUE), 2))

    df %>%
     lm(y ~ x * z, data = .) %>%
     summary

which returns this easy table:

    Coefficients:
              Estimate Std. Error t value Pr(>|t|)   
    (Intercept)  1.026e-15  5.774e-01   0.000  1.00000   
    xTRUE        4.000e+00  8.165e-01   4.899  0.00120 **
    zTRUE        2.000e+00  8.165e-01   2.449  0.03997 * 
    xTRUE:zTRUE -4.000e+00  1.155e+00  -3.464  0.00852 **

In contrast, the ""normal"" way I was taught to do this was with aov, which means there will be a significant interaction, but then you can't interpret main effects like I'd like to above, since they are averaged across the whole dataset. 

So my question is, am I interpreting the results of the lm summary correctly, and can I keep doing it this way? Specifically, does the pvalue for xTRUE:zTRUE tell me whether there is a significant deviation from additivity? Thank you!",AskStatistics,2022-08-31 14:08:16,1
"This seems like one of those posts where a guy sits behind a table with a sign that says, ""God is dead. Change my mind.""",2,x2pjxf,"I think the best technique is monte carlo simulation, it can do anything.",AskStatistics,2022-08-31 14:42:46,3
"In assetmanagement a Sharpe ratio is defined as the ratio of expected return (minus the risk free rate, which we can ignore for this purpose) divided by the standard deviation of the expected return. So the higher the standard deviation the lower the Sharpe ratio (if we were to keep the expected return the same)

In your example I expect the Sharpe ratio to be low. But not sure how useful this is. Sharpe ratios are used to compare similar strategies (eg one quant strategy vs another). But comparing gambling in a casino vs investing in the stock market isn't that useful.",2,x2yium,,AskStatistics,2022-08-31 21:46:15,4
"It is not the right place to ask; by rule 2:

> 2\. Posts must be questions about statistics

> This subreddit is to answer questions asked here about the theory or practice of statistics, such as questions about statistical analysis. It is not a place to get/give random demographic, economic etc facts, nor for challenges or brain-teasers. Use other subreddits to share results of your statistical analyses or to request research participants. If you want participants for a survey try /r/SampleSize (get your self-selection bias for free). If you are seeking data, you could try /r/Datasets.


For help on how to use reddit you might try /r/help, perhaps, but check the FAQ first.

If you're a programmer, you might want to look into the reddit API 

e.g. https://alpscode.com/blog/how-to-use-reddit-api/",2,x2v94e,"Not sure if right place to ask. If not then please redirect me.

It's a good question someone (a usernamed dumquestions) asked me [here](https://www.reddit.com/comments/wrkevd/comment/ikwkkgn/?context=999): How often do I post in a day? I tried redditmetis, but it didn't quite seem to help.

Is there any app or site or whatever that tells you how often a reddit user posts in a day or something?

Tried asking here but so far nothing:

[https://www.reddit.com/r/help/comments/x28lx7/how\_often\_does\_user\_x\_post\_in\_a\_day/](https://www.reddit.com/r/help/comments/x28lx7/how_often_does_user_x_post_in_a_day/)

Suggested to go here from there:

[https://www.reddit.com/r/datascience/comments/x2o08j/how\_often\_does\_reddit\_user\_x\_post\_in\_a\_day/](https://www.reddit.com/r/datascience/comments/x2o08j/how_often_does_reddit_user_x_post_in_a_day/)",AskStatistics,2022-08-31 19:00:31,2
"Its a flipped around version of one variant of the coupon collector problem. To be honest I'd just use simulation for this one  (as you thought already), *if you know enough about the circumstances to simulate it*.

If you don't know enough to simulate it you can't really get any precise answers by other means either.",1,x2oiru,"There's about 750 unique pokemon total, and what is given is she has caught the shiny variant of about 300 of those, however number of duplicates is not listed. I want to figure out how many she's most likely gotten in total with duplicates included. 

My first thought was I could code a random number generator to run with odds of 750/750, then 749/750, then 748/750 etc until stopping after reaching 450/750 (300 unique catches), decrementing if it's a success (new unique) and rerunning the same if it's a failure (duplicate), and finally counting the total number of tests (then running many times and taking an average). However, I feel there has to be a more efficient method for finding the answer that doesn't require running tests. Not asking for someone to do the work for me (though if you feel so inclined to I certainly won't complain), just to point me towards the right method/formula/principle to figure it out.

&#x200B;

&#x200B;

(This paragraph is context, irrelevant to the question) Odd question I know, and yes I'm aware it's very inconsequential, but I wanna know anyways. Honestly the other factors at play (only certain ones appear at certain areas so there'd be way more duplicates of some and less of others) make this unreasonable to draw a conclusion from, but I've been thinking about how to solve this for long enough it's more for its own sake (and learning) at this point.",AskStatistics,2022-08-31 14:00:04,1
"The first step in the analysis fitting the dose response curve, and deriving the parameter of interest- in this case the maximum, but it could also be the EC50- from the fit.

The second is performing a statistical hypothesis test of the means of the derived parameter.

This is different than a one step process like multiple linear regression because the model of the dose response has too many parameters for a one step process to work.

Does that help?",1,x2f013,"A simple two step analysis is described as follows: a dose response curve is created by testing different concentrations of a drug in many subjects. For each subject a plot is made which shows the individual curve response for the subject vs dose for each drug. The maximum response is found for each subject. The max. response can be compared by t test or another test.

Does it go by another name perhaps? 

&#x200B;

Not homework help, but it will help with homework.",AskStatistics,2022-08-31 07:26:36,3
"Right, you can’t do this the way you’re trying to do it. The outcome is knee-level so the rows in your data need to be knee-level too. You’ll need to either do some dimension reduction (this could be as simple as averaging all of the cartilage scores, or combining the A and B so you medial and lateral as your two predictors) or treating the four predictors separately (this would be the first thing I’d try tbh). 

FYI the other thing you may have got wrong is your random effects formula, even if you had a compartment-level outcome. (1|id) + (1|knee) is only correct if each knee has a unique identifier. Otherwise you’ll need to specify a nested RE, ie (1|id/knee) indicates that patient 1’s left knee is different from patient 2’s left knee. However since your outcome is knee-level, you’ll only need the patient random effect.",4,x29rz5,"\[~~Created~~ Greatly\]

Hi there - and thanks in advance!

I have a pretty tricky multi-level modelling problem, and could do with some hive-mind, internet galaxy brain advice as to how best model the large volume of data I have.

The research question is to assess whether MRI scored knee cartilage in 4 knee compartments predicts self-reported knee pain amongst individuals with or at risk of osteoarthritis of the knee.

The issue I have is that I want to use a level 1 predictor (compartment-level cartilage) to predict a level 2 outcome (knee-level pain) (see below). I obviously also want to account for the fact that knees are nested in patients, and compartments nested in knees, but for both - I am not too sure whether 2 and 4 levels is too few to treat them as random effects.

The theory we are assessing (lower cartilage = more pain) is supported  when completely ignoring dependency and modelling  a compartment\*cartilage interaction, i.e.

lm(vas\_pain \~ 1 + cart\_z\*compartment, data = d\_long)

I think one way of doing it would simply be:

lmer(vas\_pain \~ 1 + cart\_z\*compartment + (1|knee) + (1|id), data = d\_long)

But I'm not sure that adequately/efficiently captures 1. the fact that the outcome is on a different level to the predictor, and 2. between-compartment variation.

Some more details:

**\*\*Structure\*\***

Level 1: compartment (4 per knee, n = 36000)

Level 2: knee (2 per patient, n = 9000)

Level 3: patient (n = 4500)

**\*\*Primary predictor\*\***

Cartilage Z scores \\\[note the scores were generated within compartments\\\] (Level 1: compartment, n = 36000)

**\*\*Outcome\*\***

Vas pain (Level 2: knee, n = 9000)

The data is laid out, at present, as follows (fake data with 2 patients):

&#x200B;

|id|knee|compartment|vas\_pain|cart\_z|
|:-|:-|:-|:-|:-|
|1|Left|LateralA|8|\-2.9|
|1|Left|LateralB|8|\-8.2|
|1|Left|MedialA|8|\-1.2|
|1|Left|MedialB|8|8.2|
|1|Right|LateralA|5|2.7|
|1|Right|LateralB|5|\-1.0|
|1|Right|MedialA|5|\-2.4|
|1|Right|MedialB|5|0.2|
|2|Left|LateralA|7|0.1|
|2|Left|LateralB|7|6.1|
|2|Left|MedialA|7|2.2|
|2|Left|MedialB|7|0.3|
|2|Right|LateralA|3|1.2|
|2|Right|LateralB|3|0.5|
|2|Right|MedialA|3|\-0.4|
|2|Right|MedialB|3|\-0.2|

Thanks again everyone.",AskStatistics,2022-08-31 03:11:21,17
"Addig a constant will not change the p-values as long as  it is addes to every single variable. It's a linear transformation, you're just rescaling the variables.",3,x2kcx5,"Hypothetically, let's say I ran t-tests to compare several ""items"" based on a score of up to 45 points, and I get p-values for each comparison... Now let's say I added an equal number of freebie points to EACH of those ""items"" scores (for example, 5 points, or 55 points, or even 1,000 bonus points)... **Would my p-value change?**  Or should it remain the same because the difference between ""item A"" and ""item B"" is in the *difference of their scores (up to 45 points)*, which will vary (and not in their bonus points, assuming it is equal number across the board)?",AskStatistics,2022-08-31 11:07:45,2
"There are two alternatives that I am familiar with. The first is the Scheirer-Ray-Hare test, which is not widely known or employed because it has a very low power to detect interactions even at fairly large sample sizes. A more promising avenue would be to try an Aligned Rank Transform ANOVA, which I've had more success with in the past. The only problem is that it tends to run into convergence issues when the data contains many ties, so I cannot wholeheartedly recommend it for ordinal data.",2,x2iv20,"My google-fu suggests there is not, but I am not sure. I know kruskal-wallace is for single-factor data. 

The reason I would like such a thing is because I am specifically interested in testing whether there is additivity. So alternatives that allow that would also be great. 

Thanks!",AskStatistics,2022-08-31 10:06:18,7
"Neither chi squared nor Mann-Whitney test assume that the compared groups are of equal size, so I don’t see a problem here.",3,x29pgk,"Hello,

In order to control for differences in numbers between two groups (238 participants vs. 10/clinical population), I have read that several methods could be adopted: Bootstrap or monte carlo corrections.

I would like to perform Chi2 tests (categorical data) and mann-whitney tests (continuous data) on my two groups of participants. What do you think is more relevant to limit the difference in size in R? I'm a little confused",AskStatistics,2022-08-31 03:07:06,13
"1. Start with a model.

2. Realize that the model has no hope of being correct

3. Go to 1.",13,x299kq,"Hi,

     How does one go about finding the 95% Credible Interval for Population of the World in 4000 CE?",AskStatistics,2022-08-31 02:40:07,13
"Neither, quite, though the second is a bit closer to what it does mean.

What's missing is the fact that the interval that the margin of error gives you  (the eay you calculated your 50-70%) is not certain to overlap the true population value\*. 

 If you generated many random samples from the population (of the same size as that one), most estimated proportions would be within the margin of error of the population proportion.  

A typical figure is that at least 95% of the sample estimates should  be within the margin of error of the true population proportion but other coverage values are used (and yield wider or narrower margins).

Any bias in the sampling will of  course ruin such long-run probabilities.

See the Wikipedia article on *Margin of error* for more details

---

\* assuming all the conditions for it to be valid hold",4,x22hga,"Suppose you ask people who work 5 day work weeks, whether they like the idea of working a 4 day work week.

* Results indicate that 60% of the people sampled like the idea of a 4 day work week
* Margin of Error = 10% (±)

There are two ways I can interpret this, but I'm don't know which one is right.

The margin of error means that if we surveyed the entire population:

1. Between 50% and 70% of the respondents would agree with our results that 60% of the people sampled like the idea of a 4 day work week
2. Between 50% and 70% of the respondents would like the idea of a 4 day work week

Between #1 and #2, which of these are correct?",AskStatistics,2022-08-30 20:01:02,5
"The lack of context here is a problem.

Usually you would specify the coverage ('confidence level') not compute it.",11,x21u6i,,AskStatistics,2022-08-30 19:30:14,14
"Strictly the standard error √[p (1 – p) / n] has the population proportion in it but if you need an estimate of it (rather than a bound, say), you would use the sample proportion in your estimate of the standard error

> Sometimes I see the formula written as it is above, but sometimes I see the ""p"" variable written as p-hat, meaning the letter ""p"" with a caret (\^) above it. Which notation is correct?

There's no ""correct"". It depends on how you define your notation. If you adhere to the convention of Greek for parameters, Latin for sample quantities you'd typically use lowercase pi for the population quantity and p for the sample quantity. This used to be very common going back some decades. If you follow a lot of the less mathematical books in recent times, where people decided using pi was somehow a problem and convention be damned, then p tends to get used for the population proportion and p-hat for the population proportion

> If n is the sample size (a sample of your population), then which of these variables in the formula represent the population itself?

I am not sure what you're asking there. If you don't know the population proportion (and if you do, why do you need a margin of error for something you know?), you need to use an estimate of it which will generally come from a sample

> If p is the sample proportion, then how does p differ from n?

Umm, p, being a *proportion* will be a count in some category divided by the total count, n. e.g. if you survey a random sample of 200 people from your population of interest and 80 say they are in favor of some proposition, then n=200 and the sample proportion in favor of it is 80/200",3,x20s7g,"Here is the formula for Margin of Error:

**Z \* √((p \* (1 – p)) / n)**

Then **p** is supposedly called the sample proportion and **n** is the sample size.

&#x200B;

1. Sometimes I see the formula written as it is above, but sometimes I see the ""p"" variable written as p-hat, meaning the letter ""p"" with a caret (\^) above it. Which notation is correct?
2. If **n** is the sample size (a sample of your population), then which of these variables in the formula represent the population itself?
3. If **p** is the **sample** proportion, then how does **p** differ from **n**?",AskStatistics,2022-08-30 18:40:39,6
Repost this to /r/iopsychology the gang will have a field day and you'll have people begging to help you with this.,1,x1rzm6,"I have been given a project involving about 10 years of corporate employment data, including variables on events like hiring  date, training outcomes, promotion, retirement, etc. Also included and  the primary focus of the study is a battery of standardized tests meant to predict job success. The employees took these tests at onboarding.

The primary question is whether the standardized battery is valid and reliable at predicting training, job success, and retention over the years. Also of interest is test fairness for underrepresented vs.  majority groups.

Would really appreciate pointers on selecting an overall design,  considerations (e.g., cohorts?), appropriate statistical tests, etc.  General idea is to investigate relations between the standardized tests  and specific milestones, e.g., for training phase 1, there was a  moderate association, etc. etc. Pointing me at readings is great. Thank  you!",AskStatistics,2022-08-30 12:25:52,2
"> In the plot of the best fit line, include a layer with the original datapoints. 

Do this.  It's generally good practice to show the data along with the statistical summary.",1,x1qxea,"Hi guys,

**TLDR**: How to communicate in a journal article that the relationship you modeled as linear is indeed linear, and not, say, a curvilinear relationship that you incorrectly modeled as linear and got a significant result?

**Full question:**

Sort of embarrassed to be asking this question, but here's the situation. I'm writing up an article for publication (field is cognitive psychology / psycholinguistics) and I need to report the results of some linear models (mixed effect models specifically, but that's not really relevant here).

In the past, while I always do model diagnostics after fitting a linear regression to confirm a good linear fit (visual inspection to confirm normality of residuals, homogeneity of variance, etc.), I've never mentioned any of that in the actual writeup--and I never see this kind of thing mentioned in papers in my field.

In the present context of the paper, however, I feel it's important to make clear that it is indeed a linear relationship, and not curvilnear, etc. Basically I want to let the reviewers know before they have to ask that it wasn't a just a curvilinear model that was incorrectly modeled as linear and generated a significant B1, etc..

Do any of you have any tips for how this kind of thing is communicated? Some possibilities I was thinking of:

1. In the plot of the best fit line, include a layer with the original datapoints. A little tricky because I have multiple observations per participant, but might be able to make it work. Maybe this is enough on its own?
2. Include model diagnostics plots? 
3. Language to use -- is it weird to say something like... *visual inspection of residuals and QQ plot confirmed a good linear fit*?
4. Some sort of formal testing to indicate good linear fit? This seems overkill, but some sort of nonparametric test on the residuals to confirm normality?

Like I said, I haven't really seen any discussion of this in my field, but maybe those in other fields have experience?

Sorry if this is totally dumb.",AskStatistics,2022-08-30 11:43:18,5
"The difference you notice between the estimates from both models is most likely due to partial pooling (also called shrinkage). Check out [this other post from Michael Clark](https://m-clark.github.io/posts/2019-05-14-shrinkage-in-mixed-models/) and [this one from Tristan Mahr](https://www.tjmahr.com/plotting-partial-pooling-in-mixed-effects-models/) for a good illustration/overview of this effect.

The gist of it is that random effects pull the cluster-mean estimates (here, estimates by student) towards the grand-mean (mean of all students), more so when the cluster means can't be estimated accurately (i.e. a specific student has few data points), or when a specific cluster-mean is very different from the grand-mean.

It's a form of regularization: you biais the coefficients (towards 0, since random intercepts model a deviation/variance/gaussian ""noise"" around 0 after having ""removed"" the grand mean) to avoid having too much variance between the coefficients of each student (i.e. to avoid extreme values in some students' coefficients). You effectively ""shrink"" the spread of the parameters. This avoids over-fitting, thus giving your results, theoretically, a better generalizability (to unseen data or unseen levels of your clusters).

Whether or not one should model something as a fixed or random effect is a whole other can of worms, and, as many other decisions/assumptions in stats, is very problem dependent.",9,x1gsbj,"**Cross-posting from StackExchange because I've not yet had any responses there..** [**https://stats.stackexchange.com/questions/585850/understanding-fixed-versus-random-effect**](https://stats.stackexchange.com/questions/585850/understanding-fixed-versus-random-effect)

&#x200B;

I am trying to get my head around the difference between a fixed effect versus a random effect. To do this, I am looking at the Student GPA example seen \[here\]\[1\] (from m-clark.github). This example is ""assessing the factors that predict college grade point average (GPA). Each of 200 students are assessed for six occasions (each semester for the first three years), so we have observations clustered within students.""

&#x200B;

Note that the data for this example can be downloaded \[here\]\[2\].

&#x200B;

The example starts by fitting a standard linear regression, with only \*occasion\* as a fixed effect.

&#x200B;

\`\`\`

load('data/gpa.RData')

gpa\_lm = lm(gpa \~ occasion, data = gpa)

summary(gpa\_lm)

\`\`\`

&#x200B;

This produces the following output:

&#x200B;

\`\`\`

Call:

lm(formula = gpa \~ occasion, data = gpa)

&#x200B;

Residuals:

Min       1Q   Median       3Q      Max

\-0.90553 -0.22447 -0.01184  0.26921  1.19447

&#x200B;

Coefficients:

Estimate Std. Error t value Pr(>|t|)

(Intercept) 2.599214   0.017846  145.65   <2e-16 \*\*\*

occasion    0.106314   0.005894   18.04   <2e-16 \*\*\*

\---

Signif. codes:  0 ‘\*\*\*’ 0.001 ‘\*\*’ 0.01 ‘\*’ 0.05 ‘.’ 0.1 ‘ ’ 1

&#x200B;

Residual standard error: 0.3487 on 1198 degrees of freedom

Multiple R-squared:  0.2136,	Adjusted R-squared:  0.2129

F-statistic: 325.3 on 1 and 1198 DF,  p-value: < 2.2e-16

\`\`\`

&#x200B;

Subsequently, the example demonstrates the fitting of a mixed effects model, where \*occasion\* is again included as a fixed effect, but this time \*student\* is included as a random (intercept) effect:

&#x200B;

\`\`\`

library(lme4)

gpa\_mixed = lmer(gpa \~ occasion + (1 | student), data = gpa)

summary(gpa\_mixed)

\`\`\`

&#x200B;

Producing the following output:

&#x200B;

\`\`\`

Linear mixed model fit by REML \['lmerMod'\]

Formula: gpa \~ occasion + (1 | student)

Data: gpa

&#x200B;

REML criterion at convergence: 408.9

&#x200B;

Scaled residuals:

Min      1Q  Median      3Q     Max

\-3.6169 -0.6373 -0.0004  0.6361  2.8310

&#x200B;

Random effects:

Groups   Name        Variance [Std.Dev](https://Std.Dev).

student  (Intercept) 0.06372  0.2524

Residual             0.05809  0.2410

Number of obs: 1200, groups:  student, 200

&#x200B;

Fixed effects:

Estimate Std. Error t value

(Intercept) 2.599214   0.021696   119.8

occasion    0.106314   0.004074    26.1

&#x200B;

Correlation of Fixed Effects:

(Intr)

occasion -0.469

\`\`\`

&#x200B;

Now I can clearly see that in both models, the estimate for the global intercept remains the same (2.599). And I understand that in the mixed effects model each student gets their own intercept estimate; I can see these by running the following code:

&#x200B;

\`\`\`

coef(gpa\_mixed)$student\[1:5,\]

\`\`\`

&#x200B;

Resulting in the following output (only the first 5 students shown):

&#x200B;

\`\`\`

(Intercept)  occasion

1    2.528319 0.1063143

2    2.383636 0.1063143

3    2.687471 0.1063143

4    2.412573 0.1063143

5    2.629598 0.1063143

\`\`\`

&#x200B;

\*\*My question, hence, is what is the difference between the above mixed effects model, which includes \*student\* as a random effect, versus a fixed effects model that includes \*student\* as a fixed effect?\*\*

&#x200B;

In both cases an intercept effect is estimated for each \*student\*, so what is it about the two models that is different?

&#x200B;

To exemplify, here is the fixed effects model:

&#x200B;

\`\`\`

gpa\_fixed = lm(gpa \~ occasion + student, data = gpa)

summary(gpa\_fixed)

\`\`\`

&#x200B;

This model produces the following summary – note that again only students 1-5 are shown, with the estimate for student 1 being represented by ""(Intercept)"":

&#x200B;

\`\`\`

Call:

lm(formula = gpa \~ occasion + student, data = gpa)

&#x200B;

Residuals:

Min      1Q  Median      3Q     Max

\-0.8676 -0.1474  0.0072  0.1438  0.7325

&#x200B;

Coefficients:

Estimate Std. Error t value Pr(>|t|)

(Intercept)  2.518e+00  9.892e-02  25.450  < 2e-16 \*\*\*

occasion     1.063e-01  4.074e-03  26.096  < 2e-16 \*\*\*

student2    -1.667e-01  1.392e-01  -1.198 0.231301

student3     1.833e-01  1.392e-01   1.318 0.187967

student4    -1.333e-01  1.392e-01  -0.958 0.338196

student5     1.167e-01  1.392e-01   0.838 0.401995

\`\`\`

&#x200B;

From this I can see that the estimate for the slope effect of \*occasion\* remains unchanged (0.10634 in both the fixed effects and the mixed effects model), but the estimates for the intercept effect for each student are slightly different.

&#x200B;

For example, in the mixed effects model, student 1 has an intercept estimate of \`\`\`2.528319\`\`\`, whereas in the fixed effects model the intercept estimate for student 1 is \`\`\`2.518\`\`\`.

&#x200B;

Similarly, in the mixed effects model, student 2 has an intercept estimate of \`\`\`2.383636\`\`\`, whereas in the fixed effects model the intercept estimate for student 2 is \`\`\`2.518-0.1667=2.3513\`\`\`.

&#x200B;

Admittedly these values are very similar, but they are not identical.

&#x200B;

Are these values different only because of the way that they are estimated? i.e. partial pooling/shrinkage in the case of the mixed effects model versus least-squares in the fixed effects model?

&#x200B;

My understanding is that when you include \*student\* as a random effect, the estimate for each \*student\*'s intercept is drawn from a normal distribution centered on the global intercept estimate. Whereas when you include \*student\* as a fixed effect, each \*student\*'s intercept can be estimated with whatever value is best (i.e. best according to least squares) without any constraint as to what distribution that estimate is drawn from. Is my understanding here correct?

&#x200B;

Why would it matter, from a statistical point of view, if I modeled \*student\* as a fixed effect, rather than as a random effect? Is it for example because with the fixed effect you cannot ""borrow strength"" from other groups? And because with the fixed effect you are having to estimate a greater number parameters than with the random effect, which only considers variance around the global intercept estimate?

&#x200B;

Thank you in advance!

&#x200B;

\*\*Edit\*\*: I also have another question - how exactly do you interpret the value of the random effect?

\`\`\`

Random effects:

Groups   Name        Variance Std.Dev.

student  (Intercept) 0.06372  0.2524

Residual             0.05809  0.2410

Number of obs: 1200, groups:  student, 200

\`\`\`

The value for the variance effect size is \`\`\`0.06372\`\`\`. Does this mean that, when variance of all the individual \*student\* intercept estimates is \`\`\`0.06372\`\`\`? I tried that using \`\`\`var(ranef(gpa\_mixed)$student)\`\`\` but this returns a value of \`\`\`0.0553\`\`\`. If anyone could shed some light on how to intuitively interpret the random effect estimates, that would really help!

&#x200B;

&#x200B;

\[1\]: [https://m-clark.github.io/mixed-models-with-R/random\_intercepts.html#example-student-gpa](https://m-clark.github.io/mixed-models-with-R/random_intercepts.html#example-student-gpa)

\[2\]: [https://github.com/m-clark/mixed-models-with-R/blob/master/data.zip](https://github.com/m-clark/mixed-models-with-R/blob/master/data.zip)",AskStatistics,2022-08-30 04:38:10,6
You are more likely to get an answer at /r/spss,1,x1efp9,"Hello, can anybody help me how to create a graph like in the picture attached with SPSS? The two variables on the x-axis belong to the same person hence the different from first and second measurement should be connected to show how the value of the person changed. Sorry for my English and thanks for the help in advance!

&#x200B;

https://preview.redd.it/7d23beafktk91.png?width=1354&format=png&auto=webp&s=0402cc2b77213bb851b42240e53de07a1cfcfd78",AskStatistics,2022-08-30 02:24:13,2
"There's a lot of text in your image that should be part of the text of your post. In fact there's nothing there that could not have been posted as text. Please don't make it impossible for visually impaired users why rely on screen readers to see most of your post unless it's absolutely necessary.

(Indeed, even for people who just don't have good eyesight and a big screen your image is quite hard to read)",1,x1fp0b,"Hi all,

Engineering PhD student using stats as a tool, so apologies in advance for my ignorance!

Hopefully the attached image covers my aim in enough detail :)

I know the introduction of a second variable will cause change in theory, but this is about seeing whether discounting the influence of this is acceptable as I expect any difference to be very small.

I've researched into the issue of testing whether there is a difference between two groups by using dummy variables etc. However, the condition I am interested in is a continuous variable and I am struggling to find any methods for answering my question.

https://preview.redd.it/bzeosxdpxtk91.png?width=677&format=png&auto=webp&s=9ac020425f1122ae202c333fdd70c58de1d613b0",AskStatistics,2022-08-30 03:40:08,1
"You're assuming independence, which might not be true. 

For example, on a worldwide basis, let's assume that 2.1% of the population (150M people) have the first name ""Muhammad"" and 1.4% have the last name ""Wang"" (100M people). That does not mean that the number of people with the name ""Muhammad Wang"" can be calculated by multiplying these numbers: 7B \* 2.1% \* 1.4% = 2.1 million people?? 

I doubt there are more than a handful of people named ""Muhammad Wang"" in the world. That's because the first name is not independent of the last name. If your last name is ""Wang"", you are much less likely to have ""Muhammad"" as your first name.",14,x1r495,"Context: imagine that, in order for someone to have a particular intellectual skill, it is 1/800,000 chance. Now imagine you find someone that seems to have this skill, but they're a famous youtuber who readily employs the skill. Assume being a famous youtuber of that degree is 1/1million. Is it right to just simply multiply those two stats together to get an extraordinarily low number and then say 'see, it's so ridiculously unlikely that it's more probable to believe that the youtuber doesn't have the skill.' But if you can do that, why can't you just narrow it further and say 'that youtuber is white, which means less likely, and also they have green hair, and they're skinny, and they went to college at X university,' so now it's essentially 0 chance that this person exists...but they do, presumably. 

What's going on here, exactly? Is it right to say that the low probability is true, but whether u can dismiss it cuz unlikely is the problem?",AskStatistics,2022-08-30 11:51:00,17
"[The direction of the null and alternative hypotheses matter](https://www.itl.nist.gov/div898/handbook/eda/section3/eda358.htm) when defining the rejection region.

As to getting the p-value, do you understand how rejection regions work? The relationship between using critical values for cutoffs versus comparing p-values to alpha? You've already got the distribution needed.",2,x17ck0,"**Problem:**

 If sample of size n=20 has sampling standard deviation S=0.12, test the null hypothesis σ <=0.1 compared to the alternative hypothesis σ > 0.1, with a = 0.05

 **I run the calculations and got the (correct) results:**

 X = 27.36 and χ^2 _a, n -1 = 30.1435

 And i have a table that says that when H_1 is σ > σ_0 and we get X >  χ^2 _a, n -1 then we reject H_1. But this isnt the case here, so why the textbook says that we reject H_1???

 **Bonus question:** how do i find the p-value here?",AskStatistics,2022-08-29 19:34:27,9
42 data.,12,x1bcu4,"Was just curious. I know there's gotta be a sweet spot somewhere in between too selective and Heaps of data. 

How do we structurally gauge the pattern-observable subset of the data that can be then scaled to the entire data.
Could be experimental setup data or captured observations.

What is the thought process like? I kow the domain knowledge would play an instrumental role in this. But was wondering if there's a unified thought process, irrespective of the domain.",AskStatistics,2022-08-29 23:06:46,17
I think the recommended textbook for that course is “All of Statistics” by Wasserman. The textbook itself isn’t the best; it just brushes over material without an in depth explanation of anything. I read the book and just looked up more information about each topic when I needed to. The course mostly follows the book.,2,x1awvd,"I'm watching the lectures on youtube, and doing the assignments as provided by MIT OCW. However, this course is a bit theoretical and I'd prefer to spend time complementing the course with a text book of same stature which covers the concepts as taught in the class. I see no recommendations on the course website, so I'm wondering if any of you have taken the course and can recommend some book, that'd be great help. Thanks.",AskStatistics,2022-08-29 22:41:02,1
"Do you mind describing your regression model, and what each variable’s type is? 

If I understood your post correctly, you’ve run a regression model and calculated the pairwise sample correlations. There seems to be a third variable “watching the video” that’s acting like a mediator. Depending on what your regression model is, the answer to your question may change",1,x17m0c," 

so my thesis supervisor sent me this;

""Regression wise: Mach does not predict changes to positive affect (b= -.11, ns), but it does predict changes to negative affect (b=.19, p =.013).

Correlation wise:  You’re right, there is more of an association of mach with negative affect over positive affect, but these associations are not significant. Mach is not significantly correlated with negative affect before watching the video (.10, NS), or after watching the video (r=.24, NS). Mach is not significantly correlated with positive affect before watching the video (r=-.10, NS) or after (r=.01, NS).""

 Ok so, the dependent variables are Machiavellianism and negative affect. We predict Mach will mitigate increases in na before and after a stimulus. Meaning participants showing Mach traits will increase levels in na after experiencing the stimulus.

What I'm confused about is the difference between predictability and correlation. why is the predictability significant but not the correlation? What I think it means is that where there are Mach traits there is a likelihood of na. But the different levels of mach don't dictate the different levels of na?

I dunno. But seriously thank you so much for replying. At least you took the time to say that I needed more context",AskStatistics,2022-08-29 19:47:07,2
"""Compare quartiles"" of what / what's the response variable exactly? What's the specific hypothesis about population quantities?",1,x18n5e,"I'm working with a restricted group (age-restricted/recruited extremely healthy people) and I want to assess the impact of diet quality.

As everyone eats relatively well, I'm hoping to compare quartiles (lowest to highest diet quality)

I'm wondering what the best way to do significance testing would be?",AskStatistics,2022-08-29 20:37:56,4
"A chi-square test of proportions. If there are some channels with a very small number of customers you may need to remove them or combine them into ""Other"", but with that much data you probably won't need to.",2,x0u59s,"I’ve got roughly 90k rows of data, where each one represents a customer. One of the columns tells me the channel that brought that customer to my company. 

I’m looking at the channel mix by quarter (eg, for Q1, x% of customers came from events while y% came from social media). Overall, I’ve got about ten different channels that contribute customers. 

What I want to figure out is whether the percentage breakdown changes significantly from quarter to quarter. What method should I use to investigate this?",AskStatistics,2022-08-29 10:12:40,2
"The answer should be 1 - P(X<300), which, according to the graph, is 1 - 0.0235 = 0.9765. However, without the graph, my answer would be 0.975 as you are proposing.",7,x0lia0,,AskStatistics,2022-08-29 03:41:32,19
"I DM’ed you

It’s really gnarly because the pathways of detainees are not linear, there’s loads of comorbidity and the data is patchy 

https://academic.oup.com/policing/advance-article-abstract/doi/10.1093/police/paac016/6534129?redirectedFrom=fulltext

https://www.justiceinspectorates.gov.uk/cjji/wp-content/uploads/sites/2/2021/11/Mental-health-joint-thematic-report.pdf

https://www.researchgate.net/publication/328841444_Referrals_to_a_Mental_Health_Criminal_Justice_Liaison_and_Diversion_team_in_the_North_East_of_England

Edit: even if you don’t want to chat these are some reading materials for your colleague",2,x0wlro,"Hey all. Trying to help out a lab member who is attempting to launch a project soon on a dataset that contains pre-booking arrest suicides and pre-booking arrest homicides. The focus of the question is whether the same factors that have contributed to suicide v homicide (for example, charge, race, county etc.) have remained the same during the beginning of the dataset, which is 2005, are still the same factors influencing risk of suicide in recently, taking a year by year approach.

That is, the first level of analysis would be a binary logistic regression, but the second level - covering the rate of influential variables over time - has gotten me turned around. The point of the project is to examine whether influential and notable cases of police brutality and increasing trainings around de-escalating arrests, as well as the increasing concentration of police officers as de facto mental health crisis workers, has changed who tends to die by suicide vs homicide during arrests.


I am unsure if you can conduct a time-series model with a categorical outcome, but i’ve seen a few with a categorical predictor. Some of the other ideas i’ve thought about are a basic logistic regression incorporating a time or year variable, an MLM with the groups being the years (16 groups, all relatively similar size, give or take a few), but i also could be missing a different type of analysis. What would be the best way to focus on the rate of change over the year",AskStatistics,2022-08-29 11:52:13,1
Would John Fox's book on applied regression and GLMs be anything near what you seek? Or is that more mathematical than you want?,1,x0xkl5,"Just looking for peoples favorite introductions to GLM. Teaching a course and I want to hear some opinions on what people liked and why. 

The course is a masters course for applied GLM in the behavioral sciences.",AskStatistics,2022-08-29 12:31:58,3
You should check the items structure through Factor analysis or item response theory model. That will tell you if the items are collapsible into a single index.,2,x0qbea,"I’m using the PANAS (Watson et so 1988) scale - it has 20 items - 10 items measure positive affect and 10 items measure negative affect. 

I have reverse scored the negative affect items. Now that I’ve done this, can I use the overall mean score? So if the score is higher does that mean the person has greater levels of positive affect and visa Versa. Or do I have to split the positive and negative questions up after reverse scoring? 

Thanks",AskStatistics,2022-08-29 07:34:32,16
"> The problem is that the data contains a fair amount of overlap. Although this isn't specifically what happened, the best analogy would be those surveys where you can ""check all that apply""

You can't do a chi-squared test on directly on a contingency table like that; category-levels have to be mutually exclusive. This is not negotiable, in that the derivation of the distribution of the test statistic relies on it.

>  Does it invalidate the test entirely? 

Yes in that the test statistic won't have the claimed distribution asymptotically under H0 so you won't get a test with the significance level you want; and power properties will be similarly impacted.

I don't know that there's any easy way to ""fix"" that. If you have the original individual-level data there's some things you may be able to do (e.g. along the lines of McNemar tests and its generalizations).",3,x0goq0,"I'm trying to analyze some data to find whatever statistically significant relationships I can, be them positive or negative.  The data I have is categorical, and shows the frequency of co-occurrence.  There are a LOT of categories though, 9 rows and 39 columns, which gives a whopping 304 degrees of freedom if my math is correct (r-1)(c-1) yes?  

The problem is that the data contains a fair amount of overlap.  Although this isn't specifically what happened, the best analogy would be those surveys where you can ""check all that apply"".  So, you might have initial questions about the subject such as occupation, sex, etc.... and trying to match those demographical statistics up with ""which TV shows do you watch? (check all that apply).  This sort of situation.   So my question is how does having such ""check all that apply"" data affect the results of a Chi-square test?  Does it invalidate the test entirely?  Is there a way to compensate for such data?  Is there a more appropriate test for this occasion? 

I'm in the field of teaching English, so statistics are not my strong suit... any help would be greatly appreciated!",AskStatistics,2022-08-28 22:35:52,7
"Multiply the probabilities together and re-normalize to ensure the outcomes sum to one. Basically p1 * p2 / (p1 * p2 + (1-p1) * (1-p2))

More importantly, to compute the input probabilities, you should be using properly calibrated probabilities that come from your inputs. For example, if something says it's  99.9% likely that something is true, that imparts a lot of confidence. They shouldn't need to be weighted since the stronger methods are more likely to be near 0 or 1 and thus more confident",1,x09c7n,"Hi there, I have a question that I thought was simple but I can’t seem to find an answer. 

Let’s say I have N number of models that will predict a binary event and i have an accuracy assigned to each model, for example:

Model 1 is 82% accurate
Model 2 is 60% accurate
Model 3 is 40% accurate
Model 4 is 20% accurate

How would I combine them in such a way that if model 2 and 3 gave a notification, but models 2 and 4 didn’t, I would get a percentage of probability of the event occurring while taking into account model 1 is the most accurate and model 4 is the least. Any help would be appreciated.",AskStatistics,2022-08-28 16:29:00,3
Check Google scholar,1,x0clgi,"Hello people, any suggestions about this subject. Papers, coding, notes, etc…",AskStatistics,2022-08-28 19:03:32,1
"Yeah, gamma would be an obvious choice. 

However, be very careful not to conflate the marginal and conditional distributions. While the marginal distribution of  speed may look quite right skew  the *model* is for the conditional distribution. That may be almost symmetric! 

More important than the distribution, however, is to get the linearity (on the scale of eta) right - i.e. the link function is normally the most critical part of the model, but many people don't even consider it, for some reason. Second most important would be the variance form (and with it, also the assumption of independence). Last comes the particular shape of the conditional distribution. You can't even assess shape if you get the link or the variance function wrong.


> The residuals of the Gamma model are normally distributed:

No, they're not. That's not even possible for all the definitions of residual I can think of. Do you mean to say that they look similar to a normal distribution?

Which residuals are you looking at here, specifically? With GLMs there's more than half a dozen things you might be calling 'residuals'. Some will look close to normal when the conditional distribution is gamma, some will not.",2,x0a32h,"Hi all,

I'm trying to model speed data. Speeds are always right skewed, positive-bound and so I've considered using the Gamma distribution. I've plotted PDF's of residuals vs. theoretical distribution. 

The residuals of my OLS model are right skewed:

lmer(cv \~ factor1\*factor2 + factor3 + (1|id), data = df)

https://preview.redd.it/xm5e8ma2mjk91.png?width=1438&format=png&auto=webp&s=f6845ea94a4c43f918ca305ec9e45e5d195638e6

The residuals of the Gamma model are normally distributed:

glmer(cv \~ factor1\*factor2 + factor3 + (1|id), data = df, family=Gamma(link=""inverse""))

https://preview.redd.it/a8kk6ecjmjk91.png?width=1344&format=png&auto=webp&s=89efb036b2ed3a5ffd9f13949d21fea4550c2161

Here's the response variable: 

https://preview.redd.it/k3bgz0ikmjk91.png?width=1344&format=png&auto=webp&s=55495979eed7f493bf5a5d6ede671998c2658d5a

So should I be modeling this with Gamma? Are there any other diagnostics I should be looking at?",AskStatistics,2022-08-28 17:03:33,2
"What distribution over x are you using?

What are you using this to do?

I can see a point such an activity but the way you're discussing it makes me concerned you're trying to do something else; there's many things this would not be relevant for)",2,x05it5,"I'm not quite adept at statistics as it is not my field, but I've been pondering lately if it is possible to calculate a distribution for the moments of a discreetly sampled function.

Say I have a sinusoid (or anything like that), y(x) = A•sin(x), with x~U(0, 2pi).

If I take 10 random samples (or any fixed n), evaluate them in y(x) and calculate the standard deviation. If I repeat this experiment infinite times, I would expect my calculated stds to be a distribution around the true value of the std of y(x)... As n gets bigger, the closer and tighter that distribution should be.

Is it possible to analytically calculate this relationship? If so, how would you go about it?",AskStatistics,2022-08-28 13:41:36,9
"The p-value (*not* the significance level) of 0.48 means ***if*** the two variables have no correlation in the population, then the chances of observing a sample correlation of 0.144 *or larger* is about 0.48. Keep in mind this probability is calculated based on certain assumptions about the population, namely that the population has a bivariate normal distribution (if you're using Pearson's R and standard parametric procedures) and that the pairs of observations forming the sample are independent of one another, so that if these assumptions are violated then the 0.48 is muddied up a bit in the interpretation.

Also keep in mind that the p-value is calculated based on the tentative assumption that the two variables are in fact uncorrelated. It doesn't tell you that they are or are not correlated, only how likely your sample is (or rather, how likely your observed sample correlation, or something more extreme, is) given the *assumption* that they are uncorrelated. It is all too common to think the reverse, that the p-value gives some statement about the chances of the variables being correlated (you need a Bayesian framework to make such a claim), but that is incorrect. Rather the p-value is making a statement about the observed sample given some assumption about the population.

EDIT: And I just noticed you're using Spearman's &rho;. The assumption about the bivariate normal distribution in the population can be ignored in that case.",13,wzwfax,"I am trying to understand the results I got from an SPSS correlation matrix using spearman's correlation. From my understanding, these results would mean that: 

*there is a* ***48% probability*** *that the correlation of 0.144 was calculated* ***by chance****.* 

Though I am not so sure, can anyone confirm or correct me?",AskStatistics,2022-08-28 07:10:33,17
"> Say the highest was 51 people for libra moon, but the others are around 35-38 people per sign, does that count as anything? 

Assuming the participants were selected *at random* from the population you wish to make inference about (which may be a dubious assumption), you can test for whether the proportions  by sign are consistent with no bias in astrological sign.

The usual approach would be a chi-squared test. However, the exact number of days per sign is not *quite* equal and births are not *quite* uniform across the year, so the expected number of people in each sign is not constant; to do it correctly you'd need to account for both those effects. At best taking the expected proportion as 1/12 for each sign would be a decent first approximation, but not definitive

> Say the highest was 51 people for libra moon, but the others are around 35-38 people per sign,

The other 11 can't all be 35-38. Maybe there's a couple like that?

It would be quite rare to see 51 in a single sign for a single random sample of 300 but my first suspicion would be that the sampling of the population of interest was not actually at random, and that there could be a variety of ways that it might be biased. For one example, sometimes calendar effects can impact some kinds of careers and that can (by knock on effects) then induce opposing calendar effects into other choices of career. 

There's a lot of such APEs to account for.",1,x0c1u4,"hi guys! this isn't necessarily statistics (i mean it is but). I'm doing a research on astrology and musicians, to see which moon sign arises the highest. There are 12 signs altogether.

I compiled 300 people and listed their moon signs. Now my question is... how do I analyse this? Say the highest was 51 people for libra moon, but the others are around 35-38 people per sign, does that count as anything? 

Since 38 isn't THAT far from 51, and the average would be around 25.. Does it still count or should I abandon this research?

Also please no comments on the astrology thing, it's just a past time. Thank you.",AskStatistics,2022-08-28 18:37:38,2
"> I would think that there is some way to get estimates of all three together in one regression. 

Not without some form of regularization; in effect you're trying to balance a door flat on top of a fence; it's always going to fall off. But even with regularization, the interpretability of the estimates would still be lacking.

The model simply isn't identifiable (there's an infinite number of sets of coefficients with the exact same fit); adding some form of regularization will make it 'identifiable' (will make the fence a bit fatter) but will not necessarily result in coefficients that are *sensible* or *meaningful*.",2,x04s4w,"I have a dataset where some columns are the exact sum of other columns. Is there a way to model this in a regression? If I had three columns A, B, C, where C = A+B, I would like to model all three together. Of course, there is going to be multicolinearity. I could just drop C, or A and B, but I would think that there is some way to get estimates of all three together in one regression. Is this possible, or should I do more than one regression?

If they were categorical, B and C would be nested in A and I could do a hierarchical model, but these are integer counts. I don't actually know a lot about hierarchical models. Thanks.

Edit: I found 90% of my answer. I just needed to keep working at the google machine.

[https://stats.stackexchange.com/questions/54990/how-to-perform-multiple-regression-when-one-predictor-is-the-sum-of-two-other-pr](https://stats.stackexchange.com/questions/54990/how-to-perform-multiple-regression-when-one-predictor-is-the-sum-of-two-other-pr)

Basically, it can't be done in OLS. The last unanswered 10% the is the question, !s there some type of hierarchical/nested model that can model this?",AskStatistics,2022-08-28 13:09:50,3
"Did you miss a word?

Are you asking if two variables which are, individually, non-normally distributed can jointly have a bivariate normal distribution? Then the answer is no. If two variables are jointly normal, then their marginal distributions are normal.",5,wzyoyj,Is there any study or book that I can rely on if this is the case?,AskStatistics,2022-08-28 08:51:26,8
"For Bonferroni (whatever the test), as originally performed you'd divide alpha by k  and leave p alone, the aim being to control the overall type I error rate.

Some people multiply p by k and leave alpha which on an individual decision basis has the same outcome, but does alter what they represent. It does, however have the advantage of not having to specify an alpha if you were just looking to present results in terms of p values. Individual readers could still choose their own alpha (as long as  they're aware what you did).",1,x06m9c,"When calculating adjusted p-values for a spearman’s rho test, do we multiply the original p-value by the number of comparisons or divide by the number of comparisons?   (The bonferroni correction guidelines are unclear in regard to a spearman’s rho correlation).

Also to get the bonferroni- corrected threshold for Mann Whitney u do you divide by the number of comparisons? 

Thanks!",AskStatistics,2022-08-28 14:28:32,1
"If the question you're asking is whether the proportion varies between university, then there is a statistical method.  It is the [chi-square test of independence](https://www.scribbr.com/statistics/chi-square-test-of-independence/).


The normalizing indicator occurs by calculating the expected amount and then the difference between expected and observed.


**Observed**


University | Like apples | Dislike apples | Total at uni |
--- | --- | --- | --- |
1 | 5,000 | 5,000 | 10,000 |
2 | 5,000 | 15,000 | 20,000 |
3 | 10,000 | 40,000 | 50,000 |
Total | 20,000 | 60,000 | 80,000


We see the total proportion that like apples is 20,000 / 80,000, that's 25%. We expect all universities to have 25% like apples and 75% dislike apples.  For each cell in the Like column we multiple the university's total number by 0.25, and for dislike multiply by 0.75

**Expected**

University | Like apples | Dislike apples | Total at uni |
--- | --- | --- | --- |
1 | 2,500 | 7,500 | 10,000 |
2 | 5,000 | 15,000 | 20,000 |
3 | 12,500 | 37,500 | 50,000 |
Total | 20,000 | 60,000 | 80,000


The normalized difference is (observed - expected)^2 / expected.  If the difference between expected and observed is large, this number gets large.  The sum of those normalized differences is chi-square.  If chi-square is large, then we reject the idea that all the universities have the same preference for apples.",2,wzx37h,"Lets say that I have three different samples that are equal to the population. These are the students in universities. They were asked if they like apples. These are the results:

&#x200B;

|University|Students|Students that like apples|Percentage of students that like apples|
|:-|:-|:-|:-|
|1|10.000|5.000|40%|
|2|20.000|5.000|25%|
|3|50.000|10.000|20%|

&#x200B;

If I just looked at the amount of students that like apples, it would be misleading as it would give me the impression that students from University 3 like apples the most, because they are 10.000 and the others are only 5.000 each. But the percentages show me the true results: in fact students from Uni1 like apples the most.

Now lets imagine that in my actual set the percentages of students that like apples are tiny (below 1%) and showing that in a graph looks bad. Therefore, I want to ""normalize(?)"" or ""maximize(?)"" all the results according to the maximum value in an observation: in this case its 40% of students that like apples.

Therefore, I use 40% as the max possible value and create an *indicator (?)* that tells me how many students *from the other universities* like apples *per every 100 students from Uni1 that like apples*.

Based on this I find that 2.5 is the factor to multiply each set for (in order to make 40% the new 100%) and with this, I create the following table (or graph):

|University|% of students that like apples|*My new nameless coeficient (?)*|
|:-|:-|:-|
|1|40%|*100*|
|2|25%|*62.5*|
|3|20%|*50*|

First, is this a valid process? I really need to show my data in this way. Second, does this process have a name? Is there a way I can use it in a methodology section? 

I know this is something very basic and must have a name or something. Sorry its been like 6 years since I had my statistics modules in uni...

Thanks so much for your help!",AskStatistics,2022-08-28 07:40:44,6
"I don’t know much about logistic regression but in a typical regression you’d want to have all the predictors in the model at the same time. The reason is that by including them all you control for each of them thus the estimate you get for “substance abuse” is in isolation from “mental health” etc. Basically the estimates for the regression are estimates after controlling for all the other variables in the model. If instead you do regressions one at a time (ie don’t put all your predictors in at once) you will get incorrect estimates because then the effects of other predictors will pollute the estimates. For example if substance abuse and mental health are highly correlated then an analysis of mental health or substance abuse individually will also include the effects of the other variable.

TLDR; if running a regression you should include all predictor variables at once. The resulting estimates will tell you the effect of that variable when everything else is held constant.",1,wzui1r,"I   have a dataset of around 80 individuals, which shows the prevalence of  8  needs (e.g. mental health problems, substance use) that can co-occur   for each person. I'm pretty sure there are no cases in which a person   has one need. It is therefore difficult to tell whether each need is   influencing the outcome (binary variable, lets say good/bad) on the   basis of being intrinsically a condition that increases the chances of a   bad outcome, or whether some needs are associated with a greater  number  of other needs and it is this clustering of need that explains  how each  need is impacting the outcome.

I   recognise it may be impossible to determine which one is happening  based  on my limited data, but I wanted to check, as then I can say in  the  report that there is no way to tell, but on the hypothesis that it  is  the higher incidence of needs we can say this and that.

Anyway,   I have conducted logistic regression test and have established that   there is a significant relationship between number of needs and   likelihood of the bad oucome. Should I now conduct a logistic regression   of each need against the outcome, and take a value from that which   reflects the influence on the outcome (coefficient, z-test, not sure   which), and compare this value for each need with the average number of   total co-occuring needs for each need? If these align then we cannot   rule out that it is clustering rather than intrinsic propertiies of each   need influencing the outcome, right?

Thanks to anyone willing to help :)",AskStatistics,2022-08-28 05:38:06,11
Since the patients decide for them selves what medicine they take it is cohort and not case control. Case control requires randomisation (strategized) of the administration of the medicine.,1,wzqz6g,"Recently, I have been handed a research project mid-way. The data was already collected and they have already got an ethical approval (Study Containing Human Subjects).

However, I have noticed that they named the study (Cohort). The question rose to me as I didn’t feel that the study is a Cohort, rather it was a Case-Control.

&#x200B;

The study is about:

Certain patients taking medication (X). Small percentage of those patients experienced side-effects. Through the study we want to study the percentage of the patients who took medication (X). Also, we want to study the predictors that might have helped in the increase of those side-effects such as height, weight, blood sugar, haemoglobin etc.   


I read online, but it didn’t really help me and I couldn’t find an example that would give me the definite answer.",AskStatistics,2022-08-28 02:06:34,4
"LSTM is not going to work well.

ARIMA is a good starting point.",1,wzg5qh,"I work at a steel distributor (reseller) in an very small country in Central America.  We buy steel products (mainly plates and pipes) and resell to companies that operate in our country.  

Sometimes I even wonder if it’s worth the trouble of using sophisticated statistical modeling to forecast sales but I would like to start by knowing if Arima or something like LSTM would be more appropriate for forecasting sale of units (because sales in $ is more complicated due to fluctuating steel prices and currency exchange rates.  Thx",AskStatistics,2022-08-27 16:06:02,2
"So, an ANOVA will help to confirm whether there is a significant difference at least SOMEWHERE between groups, but then it will be the post-hoc analyses that identify WHERE that difference lies.",1,wz4otr,"What statistic test is best to use to compare the final grades of one class to all of the others?   
For example, to compare class X with classes A, B, and C, and to see if there is a statistically significant difference

A: n 18, avg 82.64, SD 4.38

B: n 24 avg 80.32, SD 8.92

C: n 15 avg 87.27 SD 7.37

X: n 40 avg 74.27 SD 14.12",AskStatistics,2022-08-27 07:48:31,2
"The issue is controversial but my opinion is you’d be fine with either Spearman or Pearson. It would be an extremely rare situation for, in the population, Pearson to be positive and the correlation with a true underlying scale to be negative or 0. See also, [this article.](https://pubmed.ncbi.nlm.nih.gov/20146096/)",2,wz3byj,"Hey, I am analysing data for my thesis and I'm investigating the relationship between psychopathy and 3 emotions - empathy, guilt and shame.

The IV would be psychopathy as measured by a 32  question inventory with 3 subscales - egocentric, callous, and antisocial. Each question is answered on a four point likert scale (strongly disagree to strongly agree). The total score was found by averaging the mean of the 3 subscales.

The three dependent variable were the three emotions, guilt, empathy and shame. Each of these was asked by one likert style question each (7 points, strongly disagree to strongly agree).


I want to see if there is any relationship between the emotions and psychopathic traits(the 3 subscales and the total). 

Am I right in saying a spearmans Rho correlation would be appropriate here? Because the data would be non-parametic because the DV is ordinal. 

I would then have 12 correlations - 3 emotions x 4  psychopathic trait variables (total, egocentric, callous, and antisocial). Should I do something else such as a bonferroni correction since there are so many correlations? 

Am I on the right track? Please help lol.

Thank you :)",AskStatistics,2022-08-27 06:49:11,3
"You are confused in thinking that means imply a normal distribution.  Means make sense for any distribution that is not too heavy tailed (the Cauchy distribution has no mean).

When you start talking about variances, you are (more or less) assuming normality.  Although not precisely, learn about the Gauss-Markov theorem.

When you use means rather than raw data, you have thrown away a lot of information.  Sometimes this does no harm (when the means are sufficient statistics).  But other times you have thrown away all of the information in the data about the parameters you want to estimate.  More bad statistics is done by users pre-processing of data than perhaps any other reason except plain old error (typos and so forth).",4,wyy9wx,"I've been working on teaching myself stats and while it was a struggle at first, I'll sometimes have breakthroughs where everything I learned falls into place, allowing me to press on. The impetus has been to learn data science and classical machine learning. I say all of this to point out that my learning path isn't as structured as you'd find in academia, so I often find that I accidentally skipped over some fundamental concept(s) and need to go back to build up a familiarity. With that said, I've got a few (hopefully) easy questions that I'd like some guidance on.

1) For an arbitrarily large dataset where it isn't feasible to derive the population mean or standard deviation, could sample means work as estimators of the population, basically treating them as a normally distributed subset of the population? 

2) Would the sample means be appropriate to use to train certain ML models on, specifically ones that require or make assumptions that the features are normally distributed? I'd love for this to be the case since almost all of the data that I work with isn't normally distributed and I worry about the effects it would have on the data by forcing it into a normal distribution.

3) Because they become normally distributed, can a sample mean be used to determine probability? Say that there's a labeled dataset with 2 labels and I'm comparing a stacked histogram of sample means for a single feature across 2 labels. Suppose the means are sufficiently separated. If I wanted to know if a particular observation would belong to a certain label, can I derive the probability for each label assignment from the label's normally distributed sample mean for the given feature?

I apologize if I minced terms or concepts but I've been noodling over this for the last few days and haven't been able to resolve it or find anything useful online.

Thanks!",AskStatistics,2022-08-27 02:12:10,5
"In theory there's nothing wrong with using this data. 

The way you measure and collect it will be the challenge.",1,wyyw6b,"I am trying to start a project that is centered around time series analysis and I am interested in using conflict data from the US. This involves events like police brutality, gun violence, protests, etc. Would this type of data be appropriate for a time series analysis? I have no interest working with economic data and I was hoping to work with something that has more of a social background. If this isn't a good idea, do you have any recommendations on topics that are similar? Thank you",AskStatistics,2022-08-27 02:51:19,1
"This is called a case-control study, you may want to read up on the design and analysis of this type of study. You can calculate odds ratio for each risk factor and try logistic model building to take into account multiple risk factors (this will also produce odds ratios).",3,wz082i,"Hi all, newbie here

May I ask if T test is a good tool to demonstrate association between risk factors and a binary outcome?
For example, I have one group of subjects having a disease (case group) and a group of subjects without the disease (control group) in my retrospective study. If there is a significant difference in T test in terms of their exposure to one of the risk factors (e.g. smoking), can I draw the conclusion that smoking is associated with the odds of getting that disease? 
On the other hand, is logistic regression a better way to demonstrate such an association? If I have multiple risk factors to be tested against such a binary outcome, is it acceptable to do a number of bivariate logistic regressions, each time between each risk factor and the outcome? Or is it better to do a multivariate logistic regression? I am worried that my small sample size would not be able to produce a meaningful multivariate analysis. 

Thank you in advance for your help.",AskStatistics,2022-08-27 04:09:24,5
"It depends on how precise you want your measurement, usually based on some preconceived idea of what the data will look like. Generally ""what the data will look like"" relates to the variance, and possibly the relative difference of any means you are comparing.

Generally this is called [statistical power](https://en.wikipedia.org/wiki/Power_of_a_test), which refers to how likely you are to reject a null hypothesis for a given sample size and set of other conditions, including a true alternative hypothesis.

You can also be ""safe"" and go a bit beyond whatever sample size is determined to be ""powerful"" enough, but make sure that you decide on the sample size before actually looking at the measurements, lest you risk [p-hacking](https://en.wikipedia.org/wiki/Data_dredging).",2,wyg31f,"Hi everyone. I’m a bit confused on what stats to do in order to find out how many observations/counts are needed per sample. I know we can do power analysis to determine sample size. Is there a way to determine the number of counts per n is needed? I am wondering, for example, if you are calculating an average muscle fiber size for a sample, how many fibers do you need to count, (as there are thousands of fibers and can’t possible count them all)? Thank you!",AskStatistics,2022-08-26 11:09:49,7
"The regression won't care or be bothered. Just understand that when you interpret the coefficient estimates, you should do so with the respective unit in mind.

If you want, you can convert one of the units in your data beforehand.",8,wydb19,"Let's say i am running a multiple linear regression to predict a house price. 

Now two of the predictors are distance from the railway station (km) and distance from the city town centre (miles).
Since one predictor is in kilometres while the other is in miles, do i need to do any kind of scaling for distance or regression will take care of it?",AskStatistics,2022-08-26 09:16:29,5
"One possible scenario: Your single covariate is significant, and might be significant if it was in the model by itself (you can test this), but once you lose two degrees of freedom to estimate the other non-significant covariates, the joint effect is no longer significant. 

Think of it this way: suppose you constructed a regression model using twenty completely random variables with no association with your outcome. On average, you'd expect one of these to be 'significant', even though (95% of the time) the F test for the model as a whole will not be significant. Does that example help illustrate what might be going on?",4,wygvj8,"Hi! 

I am confused and struggle to wrap my head around a statistics-related question. 

I have been running multiple linear regression in which one of the coefficients turns out to be significant, but yet the overall F-test of significance is insignificant. I have 3 predictors in my model, and there are no issues with any regression assumptions. I read somewhere that the T-tests for each coefficient can turn out different from the overall F-test because the first examines the effects of each coefficient *independently*, whereas the latter examines the *joint* effect of all coefficients. This leaves me scratching my head. Could this really be the reason? Or does anyone have another explanation, or ideas for things I should consider to make sense of this output? 

Yet, it is also my understanding that a if a single coefficient turns out to be significant in multiple regression, it means it is significant *while controlling for* the other predictors in the model. If that's true, then how is the ""joint"" effect of the F-test different from the ""independent"" T-test of each coefficient, if these T-tests control for the effects of the other predictors? Pardon if this is a silly question :) 

Fingers crossed that this makes sense to someone, thanks!",AskStatistics,2022-08-26 11:42:47,8
"Yes, it's quite common. You can generate predicted probabilities for various combinations of values on the predictors. In your case, the different groups, which can be plotted as bar graphs (with the predicted probabilities on the y-axis).",2,wyfj9d,"I was asked to plot several logistic regressions for a colleague. They’re looking at the odds of having different diagnoses (y) as predicted by age, gender, race, income bracket, and their interactions (x’s). So, I make several plots of their logistic regressions, plotting the classic “S” curve for each model. However, my colleague came back and asked my to “make these bar graphs”. 

I’ve never seen/heard of plotting logistic regressions as bar graphs and am not even entirely sure what this would be plotting (avg odds ratio of diagnosis for a given group I guess?). 

I’m newer to the professional world and may just not be familiar, but has anyone done/heard of doing this before? It feels odd (no pun intended) to me.",AskStatistics,2022-08-26 10:47:18,4
"I don't know what you mean by ""practical"" here. It happens with probability .5^(10^20), which means I would never bet on it happening, but it is, strictly speaking, possible.

>  They are saying that mathematically it’s possible, but practically it’s impossible.

This is a fair summary. If you could actually flip a coin this many times, you would almost certainly never observe only heads.",21,wyd62x,"Forewarning: I know nothing of the mathematics behind statistics. Is it impossible in a practical, real life situation for a coin to be flipped 10^20 times and always land on heads?

Settle a bet for some friends and I.  They are saying that mathematically it’s possible, but practically it’s impossible.

My argument is that since mathematics are practical, the opportunity exist for this to actually occur in real life.

We are not good at math haha.  Please help settle this",AskStatistics,2022-08-26 09:10:58,15
"Without more specifics about your interests, it's hard/impossible to answer this. 

I assume you're referring to Summer 2023? Every major financial firm will start their internship recruitment in the fall. Reach out to the ones that visit your campus, and talk to your career center. Almost every job in finance, consulting, etc. will be interested in those majors.",1,wycoom,"Hey all!

 I'm not sure if this is the right type of post for this sub, but I'm also not sure what jobs I should be looking for/ are available to me. I'm in my junior year, working though my Applied Stats and Econ double major, and would ideally like an internship this summer. Any suggestions?

Thanks in advance.",AskStatistics,2022-08-26 08:51:07,1
You could add a random effect to adjust of county or state level effects.,1,wyblce,"Hello, I am looking for help deciding a type of model to create with my data for my thesis.

I am looking to examine childhood vaccination rates before and after pandemic, with sociodemographic
characteristics for further examination of the cause in vaccination changes. My model would look something like this:

vaccination rate= alpha (constant term)+
beta1*(covid-19 dummy variable, equals to 0 before covid-19, 1 - after covid-19) + beta2*Xi (a vector of controls including but probably not limited to % of highly educated, annual income, % voted to leave, % non-white, language status) + epsilon (errorterm).

I have completed OLS regressions in STATA but want to know what I can do further?",AskStatistics,2022-08-26 08:07:04,3
"The advantage is that it is *nonparametric*.  When the assumptions for the GLM are *false*, that is when the linear part of the GLM is false (so the regression function is incorrect) the GAM does the [Right Thing](http://catb.org/jargon/html/R/Right-Thing.html) and the GLM does the [Wrong Thing](http://catb.org/jargon/html/W/Wrong-Thing.html).

OTOH, if the assumptions for the GLM actually hold, it will have smaller standard errors, more powerful tests, and so forth.

BTW, this has *nothing* to do with GLM and GAM.  The same explanation works for any nonparametric procedure and its competing parametric procedure.

Edit: Should have emphasized that nonparametric procedures have assumptions too.  For GAM there is the *additive* part of the model specification, which can fail.  In that case, you would need something even more nonparametric.  But GLM assume this too.  GLM have all the assumptions of GAM plus one more: (generalized) linearity.",5,wxt0w9,"I understand the differences between a GLM and GAM on a technical level, but often have trouble articulating it for a broader audience. I also understand that GAMs are not always advantageous, etc. How would you describe the advantages, though, when they do exist?",AskStatistics,2022-08-25 16:31:11,1
This is not a perfect solution but you could do an ANCOVA with pre-test scores and age as covariates and post-test as the dv.,1,wy51qr,"
Hello! I am conducting a study on the effectiveness of an online course. I have a pre- post design (so my dependent variable is measured at T1 before the course and T2 after the course) with a treatment and a control group. My hypothesis is that the treatment group has a greater reduction in stress  than the control group. I would have actually used a mixed repeated measures anova to test my hypothesis, but my problem is that my treatment and control group are significantly different in terms of age (which unfortunately also is measured in age categories). If I conduct an mixed anova without considering the age differences my results are significant. However, how do I account for the age problem here? Thanks in advance !",AskStatistics,2022-08-26 02:56:29,1
"
>Normally with confidence intervals I know to check if the interval crosses 1. That's why I'm confused here. Is this example saying that 95% of values were between 24.8 and 28.5?

Confidence intervals are notoriously tricky, not so much because of the math, but because of the weird inverted intuition you have to use to understand what they *actually* mean, which is different from what it *feels like they should* mean.
So here (and in general) it's a good learning technique to try to understand *why* you apply any heuristic, such as checking to see if the confidence interval contains some critical value. I'd say the most typical check, and in fact the one you need here, is whether the interval contains zero, not one. 

A q% CI states, somewhat technically, that q% of a hypothetical infinity of such intervals - constructed based on new independent samples from the same population - would contain the true parameter. You can kinda sorta think of that as justifying the statement ""I am q% confident that this particular interval contains the parameter,"" which is importantly different from ""the parameter is contained in this particular interval with q% probability"" and relies on a fuzzy understanding of ""confident."" 

In this case, it seems like the parameter of interest is the mean within-subject difference across two time points. Under the null - if the intervention does nothing - then the differences should be centered on 0, so a 95% CI that does not include **zero** represents a significant difference at alpha=.05. (That critical value might be *1* if, for example, you were comparing the *ratio* of scores across time points, where a difference of zero would produce the ratio `x/x =1`.) So your CI here is giving you a range in which the true within-subject score difference is likely to fall. It doesn't say anything (directly) about the proportion of actual difference scores that fall in that range, just about the mean of those differences.

>2nd question, if my primary outcome is a ***difference*** in change from baseline between two groups, and the p value is 0.2, does that mean a statistically significant difference wasn't found between the two groups meaning the 2 groups performed the similarly?

Yes, it means that, even if both groups showed significant change from baseline, the size of that change (the difference in differences) was not statistically different from zero.",1,wy0ow9," Hi, so I'm looking at a study that's reporting the change in pain from baseline and it provides the mean change, the confidence interval and p value as 26.7, (24.8 – 28.5), < 0.001.

I wanted to make sure I'm understanding this right. Normally with confidence intervals I know to check if the interval crosses 1. That's why I'm confused here. Is this example saying that 95% of values were between 24.8 and 28.5?

2nd question, if my primary outcome is a ***difference*** in change from baseline between two groups, and the p value is 0.2, does that mean a statistically significant difference wasn't found between the two groups meaning the 2 groups performed the similarly?",AskStatistics,2022-08-25 22:34:56,2
"Why not just poll everyone? Usually sampling is used when it's impractical to survey the whole population, but this is just 80 people.

As an aside: what are they using as criteria for putting teachers on the improvement plan?",4,wxwlnq,"I am a special educator who has been placed on a school improvement plan. As you can imagine morale has been down, this is in education in general. This year I was placed on the school improvement team and we are tracking a wellness goal. We have 80 staff members divided between 6 departments. I wanted to randomly select a number of staff to take a short survey to track the climate of the school throughout the year. How many staff would I need to poll to have an accurate climate of the school?

Thank you for any insight!",AskStatistics,2022-08-25 19:12:41,5
"Why do you want to simulate from a regression model?

There are different ways you might approach the simulation depending on the purpose. To my mind the simplest/easiest would be to start with the model's predictions given some observed X, then add random gaussian noise to the predictions scaled at the model's regression standard error. 

This would probably work for something like a simulation approach to power calculation. It wouldn't capture uncertainty in the coefficients, that would need something different.",4,wxr56y,"So I fitted a linear model to predict variable Y given the predictors X. Now that I have my parametric model, with its linear coefficients Beta, how should I simulate new Y's, based on my model? 

Should I simulate the coefficients as normal rvs and multiply by the predictors in the sample? Would it be better to resample the predictors and multiply by the estimated coefficients?

Thanks",AskStatistics,2022-08-25 15:11:59,10
"What is ""real life""? If you're taking courses in statistics department, real life presumably involves performing some kind of statistical analysis. I work with sums and transformations of random variables almost every day, as will you if you perform a lot of data analysis. It might help if you explained what your goals are, then we can give some specific examples of where these concepts might be useful to do.",6,wxl1sa,"I am doing undergrad in statistics course and have been learning things like mgf, cgf, transformation of variables, random sums etc, but I can never see how they could be useful in real life. 

Can anyone explain? Thank you!",AskStatistics,2022-08-25 11:03:44,5
"VAR is not strictly an econometric model, it is a model that can be used whenever the assumption of autocorrelation is violated.  Have you confirmed that your residuals have remaining autocorrelation?  If so, can it be handled by differencing and/or inclusion of seasonal dummy variables?",1,wxh5ss," Hello community, I am studying the impact of subsidies on the sales of electric vehicles on stata, I first did a simple linear regression to see the impact between its two variables then I added three other control variables, but I do not know which econometric model to choose more, VAR, PVAR ...?

I really need your help guys

Thank you",AskStatistics,2022-08-25 08:26:31,4
Try r/datasets,2,wxmdg2,"I want to run a new project that would be based around analyzing the event industry prior and post COVID in each country. I focus mainly on sector in Nordic, can anyone guide me to some good datasets from Denmark, Sweden, Norway and Finland? I focus on festival, conferences and events over 1000 attendees.",AskStatistics,2022-08-25 11:57:34,1
"First, you need to *please* **begin all posts** with enough contextual information that a human might need to try to help **you** by answering a question. In general, we don't know what you are reading or why.

Now that you have posted a link to something more than 1 equation as a followup, we can help.

Just replace ""Be"" with ""P"" for ""Probability"". In its simplest form, it is just saying that 

P(A or B) ≥ max{P(A), P(B)}

So, the probability that a card is a Queen or a Heart must be at least as large as max{P(Queen), P(Heart)}.  The addition rule:

P(AUB)= P(A) +P(B) -P(A⋂B)

Now, they are NOT saying that **Be**liefs must follow all laws of probabilities and follow the additional rule... but the intuition is very similar.",3,wxagot,,AskStatistics,2022-08-25 03:24:00,7
"Yes, they’re the same because there’s only 1 way to get each sequence. So the probably of each sequence is just 1 divided by the total number of possible sequences (here, there are 8 possible sequences: HHH, HHT, HTH, HTT, THH, THT, TTH, TTT). The probability of each is 1/8. 

This is true only when you're considering the specific sequences. If you compare the probability of, say, getting two heads in the three tosses, that’s different than getting three heads in the three tosses. That’s because there’s >1 way to get two heads, but only one way to get three heads.",2,wxg0ka,"So is a heads, heads, heads as equally likely as a heads, tails, heads?

&#x200B;

I can see it mapping out the possible outcomes, but for some reason I feel intuitively that they should be different. Is it because each trial is independent, so its always a .5 chance of seeing whatever toss you get in a given sequence?",AskStatistics,2022-08-25 07:40:41,2
"As a sequence of approximating regressions\*. See the classic book by McCullagh and Nelder for details of the algorithm in terms of working residuals.

\* the regressions themselves could them be done via say a Gram-Schmidt process as with any other by-hand regression. It's not the most efficient method *possible* but it's still going to be reasonably efficient and it's pretty easy to do in a fairly automatic way so you're more likely to be able to keep straight where you're up to, and because you can lay it out in tabular/spreadhseet fashion you can more readily backtrack to identify errors",5,wx7m66,How would you do multiple logistic regression by hand? What would be the step by step algorithm to arrive at the coefficients and their respective p values? (I already know you wouldn't do this in reality and that we have R/Python but I need to understand what's happening under the hood.),AskStatistics,2022-08-25 00:26:29,2
"I’m a data scientist at a small tech company.

60% of my time is data collection/cleaning/storage/engineering in support of our analytics. I’m a good enough programmer and we need the shit done. It’s not fun, but it’s gotta happen.

20% is research of my choosing from the same data stores as I’m building for other people.

20% is ML projects and modeling. I get to decide what directions are feasible and worth pursuit which is pretty rewarding.",26,wwqlk5,"Hi all, I’m deciding whether to apply for masters in stats next year. I would like to know what are your job titles, and what do you do on a daily basis at work? How crucial are the stuff you learnt in school for your job scope?  

Thanks!",AskStatistics,2022-08-24 11:14:49,10
"> Could an estimation be made based on the sample size of only one?


In some circumstances, yes. 

Indeed for a Bayesian, such a thing is even fairly natural but it's sometimes feasible even for a frequentist.",2,wx7ffd,"It came up because I was arguing that since there is only one example of life in the universe - Earth, we can't estimate whether there is other life somewhere else. I tried Google, but didn't find a question that fits to my situation.",AskStatistics,2022-08-25 00:14:39,8
"Hi guys, been struggling to figure out this equation and my lecturer has been no help at explaining it. Youtubers go over calculating PPV but use whole numbers and their examples seem less complicated. Help would be greatly appreciated",1,wx4avw,,AskStatistics,2022-08-24 21:15:44,1
Here's my attempt at a [solution ](https://i.imgur.com/U15Dgel.png). Check out this [link](https://math.stackexchange.com/questions/647587/sum-of-a-power-series-n-xn) if you want to know how the formula for the infinite series is derived.,11,wwd4k8,,AskStatistics,2022-08-24 00:38:52,6
"Maybe you can run a regression of X on all the other factors and include a dummy variable for each group. Then, include interactions between the dummies and the other predictors to check if there are significant differences between the effect of these for each group.",2,wwwxuy,"I have data from six different experimental groups that were collected by two different companies (three groups were collected by company A e the other three by company B).

I'm interested in compare a variable X present in all of the groups.

But I need to check if eventual differences in sex, age, race and income(six categorical values) variables in the samples collected by each company affected X.

What is the best way to do this?",AskStatistics,2022-08-24 15:34:38,1
"Your approach of measuring students before and after a treatment is called a paired sample procedure, and I think that would be a fine design. You can use the paired sample t test",2,wwlp0k,"I have to design an experiment which aims to measure the efficacy of an initiative in improving the grades of students. Grades of students is not a necessary metric, I could also use a custom assessment for the task. FYI, I know basics of statistics and have read research papers but haven't designed an experiment before. 

The problem is this: if I randomly select students in the control and experimental groups, I risk putting ""smart"" ones into the experimental group, that is, those students who would perform better than others regardless of the new initiative. This would give an optimistic result of the initiative. On the other hand, opposite situation might give pessimistic results. 

So, how do I select these groups so as to get accurate estimate of the impact? One solution that comes to mind is to measure students' performance against their own. Like measure the performance on the same or similar questionnaire before and after the program. Is it a better approach? Or is there any way I can do with the former approach, or is latter the only choice? Thanks.",AskStatistics,2022-08-24 07:57:31,4
"Note that distributions of incomes tend to be highly skew, that their spread should be expected to increase as the mean increases\*, and that incomes are clearly quantities for which ratios make sense.

\* Indeed, given their properties should not depend on whether you measure income in bucks, centibucks or kilobucks, you should expect that constant coefficient of variation models make sense.

These, taken together, may provide a reason to either consider distributional models where the standard deviation is proportional to the mean - models with constant shape and mean proportional to (or equal to) a scale parameter (gamma, Weibull, lognormal etc), where population-parameter ratios like those you discuss relate to natural quantities (e.g. parameter differences under a log-link) or to take logs and work with differences of logs (converting back at the end). Having used both quite a bit, I lean toward the first for a variety of reasons. 

Which is to say, given you want a GLM, a gamma GLM with log-link would be the obvious first thing to do.",3,wwvsci,"Hi,
what would in your opinion be the best practice for modeling ratios (not necessarily of count variables), preferably using GLM framework?


For example, let’s say I want to model income inequalities between genders. For interpretability reasons, I don’t want difference in means, but difference in ratios. Eg. rather than saying ""women on average make X less randobucks"", I would like to say something like ""average income of women is 0.82 the income of men”.


Is there a nice model for this or is the best option fit a normal (e.g. mean difference focused) model first and than use something like delta method?

Thanks in advance!",AskStatistics,2022-08-24 14:47:09,2
"Yes those hypotheses are the correct ones, it's just a left tailed t test. As alway calculate the t-value as (observed value - hypothesized value)/standard error and go from there",1,wwpf02,"Hi there,

I have problems understanding the wording and concept of a problem when it comes to identifying the null and alternative hypotheses of two variables that have a negative relationship.

y= B1 + B2 x, B2  = -200.  
Test the 1% significance level if “an extra unit of variable X is associated with a decrease in the variable Y of OVER 100”.

This is a bit confusing to me because their relationship is negative.  Would   I consider the null hypothesis B2 = -100 and the alternative B2 <   -100

Thanks for taking ur time to read it!",AskStatistics,2022-08-24 10:26:29,2
"For starters, you can conduct a research project about people who are addicted to the internet.


That’ll be 200€ for my services, thanks.",4,wwteo7,,AskStatistics,2022-08-24 13:10:04,3
"> I have grouped the people and food into categories.

> I'd like to know what types of people eat what types of food.

They're not even ordinal. You're looking for association not correlation

There are a number of measures suitable for such contingency tables.",2,wwdytl,"Hi there! I'm not sure if I'm asking this correctly, but I have two groups, say people on one side and foodon the other.

I have grouped the people and food into categories.

I'd like to know what types of people eat what types of food.

For example, are people from rich backgrounds more likely to eat pastas?

How would I go about doing this? I looked into correlation but they seem to require a linear scale.

&#x200B;

[Linear Scale](https://preview.redd.it/i39fvwu5imj91.png?width=1267&format=png&auto=webp&s=be38cec2c924f09cf6f65525d31af598409628eb)

But I don't have a linear scale for ""people types"" vs ""food types"".

Thing of using `df.corr` from `pandas` to find the correlation.

Thanks! :)",AskStatistics,2022-08-24 01:31:15,3
"There's two entirely different issues in your question

1. > I'm thinking of this as a situation where you flip a coin 100 times and any combination of heads or tails is equally likely and I can't seem to see this in any other way

  This is not the case. Consider a much smaller problem. 

  Toss a (fair) coin twice.

  These four outcomes are equally likely: TT, TH, HT, HH

  **but** once you ignore the order and count the number of heads:

         0H, 1H, 1H, 2H

   then we see that 1H occurs twice as often, because there was two ways to get that (TH and HT).

  Now look at 3 dice. There's 8 equally likely sequences:

          TTT, TTH, THT, HTT, THH, HTH, HHT, HHH 

  but when we count heads, we get:

          0H, 1H, 1H, 1H, 2H, 2H, 2H, 3H

   so the middle two outcomes occur in 3 times as many sequences as the end two. 

   With 10 dice there's 252 sequences that contain five heads (and five tails) but only one sequence that contains 0 heads or 10 heads. 

2. It's not normally the probability of individual outcomes that you worry about to determine what cases you should reject in a hypothesis test\*.

  Imagine you roll a 24-sided die\*\*, and you're interested in whether it's fair. If you roll it once and something pretty unusual happens should you conclude it's unfair? 

 Each outcome on a fair 24-sided die has less than 4.2% chance of occurring, so if your significance level is 5% you should reject the null of fairness, right?

 Well, no. We can construct other examples where all the individual outcomes are fantastically unlikely (not necessarily exactly equal like that one) but don't of themselves indicate anything is amiss. Instead what we need to do is consider which cases are more or less consistent with the specific alternative hypothesis of interest.

  So, for example, imagine we'd used the die in a game and we had got fairly low rolls - plenty of 1's and 4's and 7's and 11's but very few high values like 15s or 23's. We are worried that perhaps the die isn't fair in that it rolls low numbers too much. We could conduct an experiment -- roll the die and see if it's low. If we just roll it once, there's not much information there. Even if we did roll a 1, it's not a lot of evidence for our hypothesis -- just rolling a single 1 isn't very convincing; and worse, it's not just 1's we're concerned about. We're worried about 2's and 3's and 4's and so on. But if we want to reject the null hypothesis (and so conclude the die is not fair in the way we suspected) on all of those outcomes, we'd conclude a fair die was unfair *half the time*. Useless.
  Instead we should roll it multiple 
times and count how often it rolled low numbers (let's say in the bottom half, 1-12). Now if the die is fair, that should happen half the time. Let's say we roll a total of 13 times and count how often we got 1-12. So we could get that outcome of concern 0 times, once, twice, three times, ..., up to 13 times.

 Which outcomes are the *most* indicative that the die rolls too low? If roll low *lots* of times. So if we rolled low 13 out of 13 times, we'd definitely want to conclude there was a problem. What is we rolled low 12 times? Is that a concern? Well, yes, that's still pretty weird. But with a fair die, rolling low say 8 times could still happen pretty often, so we probably wouldn't want to say the die was indeed rolling low too often to be fair in that case. 

 Where do we place the cutoff, then, between the cases that most strongly indicate our suspicion was correct and those that don't suggest it very strongly? That's where we choose our significance level. 

 In this case, here's the possible critical values -- the values for the number of times we rolled low -- and the probability we'd roll low that many times or more for a fair die:

           #low    P(at least n)
           rolls     on a fair die
             9        0.13342
            10        0.04614
            11        0.01123
            12        0.00171
            13        0.00012

 We might, for example choose 10 as our cut off; if we roll 10 or more ""low"" outcomes out of 13 rolls, that would be somewhat indicative of a die that rolls low, and only occurs 4.6% of the time with a fair die. (Or we might want more evidence before we started throwing out accusations of unfair dice and choose say 11; that would only wrongly call a fair die unfair a bit more than 1% of the time.)

 Let's say 10 for this example; giving us alpha just a little under 5%.

 So now we have a useful test. The rejection rule ""reject H0 and conclude the die rolls low too frequently if you get at least 10 low rolls out of 13"" works in that it does collect the cases that are most indicative of what we wanted to test for, without doing it too often overall.

---


*That's* how you do it with your births example:

- Specify your alternative of interest, the thing you want to pick up. Like ""girls occur more than half the time"", for which the null would be that there's no tendency toward more girls (this is like our die example). Or maybe we are instead interested in whether *either* sex occurs unusually often. 

   Let's say for now that we are particularly interested in too many girls and the cases ""equal proportions of each"" or ""more than half males"" are not notable (this potentially makes sense since more than half males is what happens in practice so if we were in a special situation where we suspected too many girls, there would be nothing interesting about picking up too many boys; it's already the slight default). So for now our alternative hypothesis is that girls occur more than half the time.

- With 100 births, which cases would make us *most* interested in rejecting the null? The ones with the *most* females: 100, then 99, then 98, those cases. We would want to reject all of those. But 51 or 52 might be expected with equal probability of boys and girls. 

- So how close to 50 can we put the cut-off, where we reject anything more extreme? That's where our significance level, alpha comes in.

          #girls     P(n or more)            
             n       if P(G) = 0.5
            51       0.38218
            52       0.30865
            53       0.24206
            54       0.18410
            55       0.13563
            56       0.09667
            57       0.06661
            58       0.04431
            59       0.02844
            60       0.01760
            61       0.01049
            62       0.00602

 Let's imagine we wanted no more than a 5% type I error rate. Then we would choose 58 as our critical value; if we got 58 or more girls in 100, we'd reject
H0 and conclude that the proportion of girls was more than 1/2.



---

\* Why did I not say ""are statistically significant""? The phrase itself is in part contributing to the conceptual issues you're having. I advise simply never using the term. It's not conducive to thinking about things clearly nor to expressing them clearly.


\*\* there's several different kinds of symmetric 24-sided dice you can get; [here's one kind](https://www.tarquingroup.com/media/catalog/product/cache/1/image/9df78eab33525d08d6e5fb8d27136e95/8/Z/8Z-SB0E-2MLO_l_2.jpg)",3,wwduov,"98 out of 100 births are girls is statistically significant because the chance of that occurring is 5% or less but I don't understand why 52 girls out of 100 aren't because doesn't any combination of girl births and boy births have an equal chance of occurring so shouldn't all possible combinations be considered statistically significant? Currently, I'm thinking of this as a situation where you flip a coin 100 times and any combination of heads or tails is equally likely and I can't seem to see this in any other way. It would be nice if I could get an explanation and receive a different example of something that is statistically significant as well.",AskStatistics,2022-08-24 01:24:04,4
"> But I'm confused with this; 99.7% of the values recorded should be in the 3sigma deviation centred around the mean with rest of 0.3% of values outside of it.

There's no reason for this to be the case.

The numbers you give there are for a *normally distributed population*. Here instead of a population, you have a sample (of unknown size) and that sample is surely not drawn from a normal distribution.

>  The distribution for the data should be normal because there are many processes which affect the data.

That does not of itself suggest any distribution at all. You'd need considerably more than that to actually suggest even approximate normality (many small, almost-independent, *additive* effects with no term dominating the sum of the variances would -- potentially -- suggest approximate normality if the number of them were large enough to overcome the effects of the most skewed and heavy-tailed components). And that would just be for the population. In a sample whose size is unknown, you don't have any guarantee that the 3sd figures will be anywhere close to that 0.3%-ish outside.",3,wwadzp,"I have this data for a batch of samples.
Maximum value recorded: 3, minimum value recorded: 2.2, Mean: 2.64, Standard deviation: 0.20(approximated to 2digits), 3SD:0.605823
But I'm confused with this; 99.7% of the values recorded should be in the 3sigma deviation centred around the mean with rest of 0.3% of values outside of it. But the bounds(2.034,3.246) are crossing beyond the range(2.2,3). The distribution for the data should be normal because there are many processes which affect the data. This data has not been recorded or analysed by me. And thus I don't have the batch count too. Similar statistics are there for different batches too. Am I understanding something wrong or is my assumption wrong?",AskStatistics,2022-08-23 21:58:18,1
"Your theoretical probabilities are based on flawed assumptions

You've listed all the ways to win. Now list all the ways to lose.",5,wvz02v," 

Hi all,

I am having trouble calculating the statistics for this game. The game is a mine sweeper type game with three total picks (not lives), meaning each time you select an tile you subtract 1 pick. There is 3 rows and 3 columns with 2 mines per row and 1 safe tile, to progress to the next row you must find the correct tile. What is the probability of winning the game (making it to the final row and selecting the correct tile), what is the probability of winning the second row and the 1st row (first row would be 100% since even if you select the land mines you still have the third pick which would be the final correct tile)

My calculations had said 100% to win row 1, 1/3 will win row 2 and 1/27 will win row 3. However running the game it seemed that the chance to win row 2 was much higher.

Example:

1               2                     3

4               5                     6

7               8                     9

Safe tiles are 1, 5, 8

There for combos to win row one would be (what is the prob of each of these)

1

2,1

2,3,1

3,1

3,2,1

Combos to win row 2 would be (what is the prob of each of these)

1,5

1,4,5

1,6,5

2,1,5

3,1,5

Combos to win row 3

1,5,8

What is the total number of possible combinations with the above parameters, what is the percent chance to beat the entire game and what is the chance to win on row 2. Thanks in advance.",AskStatistics,2022-08-23 13:22:00,1
Are you doing this as an intermediate step to compute odds ratios?,1,wvkl7x,"(screenshot below!) So I am struggling with interpretation of my GLM. For background - choose2 is a either yes or no.

So far I think I have established that my intercept gives me the ln(odds) of choosing when Codea, CostHigh, CategoryDairy. Is this correct?

The main thing I want to work out is the odds of:

1) Codea being chosen when CostLow 

2) Codea being chosen when CostHigh

Would I need to fit another model without Category to do this? 

Thanks a lot in advance!!

https://preview.redd.it/mi3xcsnvqfj91.png?width=926&format=png&auto=webp&s=2ac845b8b87f1fd7dce3b78002080b6d6a642071",AskStatistics,2022-08-23 02:53:36,12
"The term ""bell curve"" is just slang: not everything that looks like a bell is a normal curve. The normal distribution has a very precise shape: if I tell you its width then you know *everything* about its shape. (You might also want to know where the peak is located, but that doesn't really have to do with the ""shape"" as such.) Physical bells, even the flared curvy type, come in a wide variety of shapes.

If you're interested in acoustics, you might get a better answer in /r/AskPhysics. I suspect the classic ""bell"" shape has something to do with creating a cavity for air to resonate. Generally speaking, this sort of analysis can get pretty complicated, but [interesting](https://en.wikipedia.org/wiki/Hearing_the_shape_of_a_drum).

Edit: Anyway, if we list out some of the properties of the normal distribution, it starts looking ""bell-shaped"" very quickly. The normal distribution has a single peak and is symmetrical and smooth (no hard corners). It's a valid probability distribution, which means the total area underneath it is equal to 1 (100%). But the curve never actually reaches zero, so the only way for the area to be finite is for the curve to ""flare out"" and get closer and closer to zero when you get far from the peak. 

Any curve that meets those criteria will look somewhat ""bell-shaped."" And if we look at what the normal distribution *is* and what it's used for, these sorts of properties seem pretty natural.",2,wvqw7m,"Hello! Silly question here. Are there properties of the normal distribution that lend themselves to an actual physical bell’s shape?  Is a bell curve just the best acoustic distribution for the ringing?

I suppose cymbals are also normal distributions too…

Thank you!",AskStatistics,2022-08-23 07:53:51,5
"P(blue) = 0.15, P(green)=0.85,  
P(saw blue|blue) = 0.8, P(saw blue|green) = 0.2 (those two numbers don't normally add to 1 by the way)

P(saw a blue cab) =  P(saw blue|blue).P(blue) + P(saw blue|green).P(green) 

P(blue cab involved|saw blue) =  
 P(saw blue|blue cab) . P(blue cab)/P(saw blue) 
= 0.8 x 0.15/[0.8 x 0.15 + 0.2 x 0.85]  
=  0.4137931

~= 41.4%",2,wvytwb,"CAN SOMEONE EXPLAIN HOW THE RIGHT ANSWER IS 41%? WHAT IS THE FORMULA? THIS IS FROM A BOOK IM READING, ""THINKING FAST AND SLOW""
A cab was involved in a hit-and-run accident at night. Two cab companies, the Green and the Blue, operate in the city. You are given the following data: 85% of the cabs in the city are Green and 15% are Blue. A witness identified the cab as Blue. The court tested the reliability of the witness under the circumstances that existed on the night of the accident and concluded that the witness correctly identified each one of the two colors 80% of the time and failed 20% of the time. What is the probability that the cab involved in the accident was Blue rather than Green? This is a standard problem of Bayesian inference. There are two items of information: a base rate and the imperfectly reliable testimony of a witness. In the absence of a witness, the probability of the guilty cab being Blue is 15%, which is the base rate of that outcome. If the two cab companies had been equally large, the base rate would be uninformative and you would consider only the reliability of the witness, concluding that the probability is 80%. The two sources of information can be combined by Bayes’s rule. The correct answer is 41%. However, you can probably guess what people do when faced with this problem: they ignore the base rate and go with the witness. The most common answer is 80%.",AskStatistics,2022-08-23 13:14:57,5
"You don't know.  Marginal distributions don't determine the joint distribution.  Similarly, knowing two off-diagonal elements of a 3-dimensional correlation matrix, does not determine the other.  It can have any value that makes the resulting matrix positive semi-definite.",8,wvpam4,,AskStatistics,2022-08-23 06:47:51,3
"'I have read...' -- Where did you read this? What justification for the claim  did they offer?

There is a thing where min(r,c)-1 does come in: it impacts the maximum value of the chi-squared statistic and hence the scaling that should be applied to make V have a maximum value of 1. This effect should be unrelated to the d.f. in the test.

Consequently I think the thing you read is in error, but it's *essential* to see exactly what they wrote, and *how they justified the claim*.

You have to be *really*  careful about automatically believing random pronouncements you read\*  but at the same time we shouldn't just dismiss the claim out of hand. We have to see exactly *what* was said and *why* (what the basis was). Possibly there's a point in there, but we're missing *all* of the context. Maybe there is some special situation in which the df is impacted; without context we don't have a good way to guess if there might be something to discuss or if it's just nonsense.

---

\*(lots of stuff that's written about stats is just wrong - *lots* of it, even in books and published papers; there's too many people with no real stats education at all writing about stuff that they simply don't understand, and they have no way to even check when they get things wrong -- because they lack the tools of an actual statistician, they don't have any way to identify their own errors. Mistakes get repeated over and over, and a new generation of books and papers in that same application area go on to repeat the same unchecked errors.)",1,wvw6ca,"Hi! I'm looking at a Cramer's V value of 0.189 for a 4x4 crosstabs w/ chi-squared that I ran on SPSS. 

I see a lot of resources that would interpret this Cramer's V value as a ""weak"" association between my variables. However, this interpretation usually appears to not account for the degrees of freedom of a table. I might just be having an off day, but can someone help me with the degrees of freedom of a 4x4 crosstabs table? 

I've read that: ""the degrees of freedom used \[in interpreting Cramer's V\] is not the one from the chi-square test, **but it is the minimum of the number of rows, and number of columns, then minus one.""** 

So, even though SPSS tells me I have 9 degrees of freedom when I run Chi-squared, it seems this isn't the right 'type' of DF for interpreting Cramer's V. I'm not sure why, but that quoted phrasing for calculating degrees of freedom is throwing me off! I'm also a stats novice and don't have a ton of experience using SPSS until now. 

So... in a 4x4 crosstabs table, what are my degrees of freedom? I would appreciate any help!! I know this might be such an obvious question but I'm quite a novice with stats. Thanks! 

https://preview.redd.it/mg7tau1f7ij91.png?width=832&format=png&auto=webp&s=135b387608f7a7a76e2e1758e5fc449a26eb41c6",AskStatistics,2022-08-23 11:27:41,2
Here we assume that our prior has a beta distribution with expectation a/a+b (ie our prior guess). W is our observed data. So we are essentially creating a weighted sum of what we expected to see (prior) and what we actually observed (w). This provides our posterior expectation.,3,wvlagp,,AskStatistics,2022-08-23 03:32:51,2
You can do tests of adjacent age groups but beware of accepting the null hypothesis when a difference is not significant. You could specify in advance how large a difference would have to be to be meaningful and then compute confidence intervals to see if all values in the interval are less than that.,1,wvncol,"The title of my thesis is ""the age-related changes in upper body grip strength and lower extremity power in healthy adults aged 18-70yrs"".

I have 5 age bands (18-29, 30-39, 40-49, 50-59, 60-70). Firstly I need to test for any differences between the youngest and oldest age band in strength and power. I then must test at which age does the decline in strength and power become noticeable/ significant from its peak. Also I would need some sort of correlation to show the strength and direction of aging on strength and power. These tests would need to show the difference between males and females too.

&#x200B;

Thanks in advance",AskStatistics,2022-08-23 05:20:16,2
Equating,1,wvfdts,I want to compare the result of my survey which utilized a five-point likert scale to a secondary source (6-point likert scale).,AskStatistics,2022-08-22 21:40:11,3
"The bigger question might be 'why should means change but spreads not?' ... Its homoskedasticity that's the surprising state of affairs.

As for why spread might increase as mean increases, this is what you should expect when variables have a lower limit (typically zero). 

Consider an imaginary relationship between an x and a y variable that's constrained to be positive, as if you were plotting data. You might like to take out a piece of paper and a pencil and try this (or you can use some paint or draw program if you like). 

Draw a set of x-y axes. Start at some x value in the right half of the plot and draw a dashed vertical line there, so were looking at a  vertical 'slice' through your plot. Consider points in a very narrow strip around that slice, with some  population mean about (or a bit higher than) halfway up the plot. Mark  in either mean ±  2sd limits (if you envisage a distribution that's continuous, unimodal and not very skew), or a boxplot or even  2.5 and 97.5 percentiles of the distribution in this strip (imagine a pretty wide spread of values keeping in mind that all y values must remain positive).

Now move left to a new strip where the y- mean is half as big and do it again; you can imagine the shape changing somewhat as you go if you wish, but not too radically; avoid crossing the 0 line again; remember we still have to obey y>0. Then repeat again (halve the y mean, keeping the means forming a straight line on the plot since you were thinking about linear correlation) and again draw the limits of your choice, but respecting positivity of the y values 

What do you notice about the spread as y increases? Did you have much choice in that?",10,wv7dhv,I'm dealing with a simple correlation between two variables and i'm noticing that points spread while variables value increase (such a typical situation),AskStatistics,2022-08-22 15:33:34,8
"I think your question is basically “what the the pros and cons of collecting more samples than I organically calculated that I needed?”

One possible pro: Since the original sample size calculation is based on a desired power and effect size, more samples collected will give you a little more room in case the actual effect size or power are a little worse than your estimates.

A fairly major con: collecting more samples has a cost. Whether it is delaying a result, replacing the ability to do another experiment, or simple racking up more costs to continue running the experiment, it costs something to collect more samples than is needed to test a hypothesis.",2,wviy1m,"Essentially I can’t find much info on the drawbacks of continuing to collect experiment data. For example, I calculate I need a sample of 1m per variant for my mde. What problems arise if I choose to measure my result at 2m per variant instead?

Sort of like peeking, but after we have our sample size. 

I’ve tried to search for past Q’s and stuff on Google related to this but im getting trapped in a web of optimizely and vwo seo content :(",AskStatistics,2022-08-23 01:12:03,3
"It kind of depends on what is specific about a ""specific type of student in my school"".

Often in psychology the population is ""all humans"". Often you have to be more specific such as ""all high school age humans"", or ""all American humans who go to high school"".",1,wvl7zq,"Hello, I am in need of your help. I am a high school student conducting a study on a specific type of student in my school. The problem is, there is no existing list of these students so I do not have a population number. We will be using purposive sampling despite it being a quantitative study.

&#x200B;

I have read that the minimum sample size for an unknown population with a 5% margin of error is 384, but since the whole population of our whole school is just 529, can this sample size be lessened?

&#x200B;

Thank you so much in advance!",AskStatistics,2022-08-23 03:29:10,1
"I think getting up to speed in linear algebra will be your best bet. You can check out [3blue1brown](https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw) on YouTube for some helpful content. In particular, the ""Essence of Linear Algebra"" series.",1,wvjn37,"Hey everyone,  hope you are all doing well.
Im starting my masters in AI in a couple of weeks and courses feature some pretty advanced stuff for my level.  I have a bachelor in BI and almost entirely forgot every math related subject.
My courses ( for the first semester) include :
- data analysis ( PCA,  CA,  MCA,  SVD)
-  maths for data science : linear models, numerical optimization
- advanced algorithms
- graph theory and complexity


Im also working so my time will be limited in terms of studying.

 

Whats the most time effective way i can learn these concepts and be somewhat fluent in them?

Do you recommend any yt channels / books / lessons that go from high school to grad level?

Thanks!",AskStatistics,2022-08-23 01:56:49,1
"When computing an estimate of the population variance rather than computing the variance of the population when you have the entire population of data?  One simple (maybe too simple) rule is that when you estimate a parameter you start with N degrees of freedom and then lose one for each additional parameter estimate needed.  When you compute the sample estimate of the population variance, you need also estimate the population mean, which costs you one degree of freedom",3,wvd6wz,"Recently I think I've developed a deeper understanding of what degrees of freedom are at least in the context of sample variance. 

Sample variance is a function of all residuals and we know that the sum of all residuals is 0. Because of this, it's only necessary to know all but one of the residuals to calculate sample variance; the condition that residuals sum to 0 allows you to calculate the last residual.

I also understand that if you were to calculate sample variance with the formula for population variance that it would almost always be an underestimate.

Why does dividing by degrees of freedom correct the underestimation in sample variance or in general? It seems like a lot of formulas in stats involve dividing by degrees of freedom.",AskStatistics,2022-08-22 19:52:01,4
"I'm going to fill in some details to your question, and then answer it. If the details I add are incorrect, please provide a clarification so I can provide a better answer.

Here's the question I'm going to answer:
I have some continuous outcome y and a set of predictors x_1,...,x_p. Oh, and there are repeated measures, so my observations are not independent. I could take two approaches
1) Fit a linear mixed model, where i specify my predictors, and for some of the continuous predictors I model them using restricted cubic regression splines (also known as natural cubic regression splines). I use a random intercept to account for the repeated measures.

2) Fit a generalized additive mixed model, where I specify that I want to model all of my predictors are smooths. I use a random intercept to account for the repeated measures.


The real difference between these two approaches is this: linear mixed models operate under the assumption that you know roughly what the functional form of the association between the predictors and outcome is. On the other hand, generalized additive models are an attempt to do nonparametric regression, where you do not pre-specify the functional form of the association between the predictors and the outcome, in a way that isn't crushed by the curse of dimensionality.

When Tibshirani and Hastie were developing GAMs, there were nonparametric regression methods, but they suffered from the curse of dimensionality. Ideally, what you want from a nonparametric regression method is something that for any given point in space, it looks at a small window around that point to estimate the shape of the regression function. That is, you want your estimate to be local, and not influenced by things happening far away. Which is doable in one or two dimensions. But as the dimensionality of your predictors p gets bigger, the space around any given point becomes really sparse. So you have to increase your window size, so that there are a sufficient number of observations inside the window to estimate the shape of the regression function. But as p gets bigger, this window gets bigger, and your estimate is no longer local.

So Tibshirani and Hastie made a gamble. Instead of trying to solve the problem of a p-dimensional nonparametric regression problem, let's ignore interaction terms and assume the model is additive. Once you do that, you can solve this modeling problem by only considering one dimension at a time. And in the one-dimensional case, we've got great tools to non-parametrically estimate the association between one predictor and the outcome: smoothing splines for continuous predictors and step functions for categorical predictors. This led to the backfitting algorithm, which is essentially a form of coordinate descent: figure out what the best fitting smooth is for the first predictor, then take the residual and figure out the best fitting smooth for the second predictor, and so on. Iterate until convergence. 

TLDR: LMM is for the setting where you roughly know what the functional form of the association is. GAMM is an approximation to fully non-parametric regression that works well when you assume that everything is additive, i.e. no important interaction terms.",6,wuwj7d,"Hey everyone,

I've been reading up on generalized additive mixed models and I'm wondering if there is a difference between running a linear mixed model with added splines and running a GAMM? I realize that in practice there will likely be software-dependent differences due to different implementations of e.g., the basis functions, but is there a difference just on a conceptual level?",AskStatistics,2022-08-22 08:23:50,10
"The real problem is that all observed samples are correlated because you’re only drawing samples that involve one player. In other words, your samples are **not** really drawn independently from the population of games.

EDIT: added a very important word",1,wvf9rl," 

Let me form the scenario:

&#x200B;

We have a two-team game with many people on each team, so many that the individual performance of players doesn't matter. Each team can play as white, or black. Each game takes an hour to complete but there are 1000s  of games played by different groups each day.

&#x200B;

Someone has a theory that white wins more than black, with teams performing equally; so he starts to record his game's results.

&#x200B;

Each game he plays takes an hour so he only completes  \~4 games each day. He collects data for 50-100 games and finds that white is winning about  2/3 of the time with a p-value of \~0.005 using the binomial distribution.

&#x200B;

He has no way of finding out the results of others' game results, the  1000s of game results are gone with the wind after they are played.

&#x200B;

Can he make an argument that white wins more than black using his data? He doesn't claim why white wins more than black, just that there may be a  win rate skew. Or will he never be able to make the argument because he is not collecting 1000s of game results and the sample size, for his time invested, will always be too small compared to the big picture of what is happening?

&#x200B;

I  know that they always say n needs to be at least 30, but when you have huge numbers of something occurring and no way to get to those numbers in sample size, won't people just destroy your argument by stating you don't have enough samples in comparison?",AskStatistics,2022-08-22 21:34:08,1
"In essence, you want to compare two percentages of two independent groups, right? A Chi-Square test or a z-test for two proportions should be fine (if no continuity correction is used, they are equivalent in this case).",1,wv24ce,"Hi there, 

I believe this is a fairly basic question, but my stats knowledge is lacking. Basically, I have 2 categorical variables, and I would like to understand if there is a significant difference in Group A's results for a single variable and Group B's results for that same variable.

Based on [this](https://stats.oarc.ucla.edu/other/mult-pkg/whatstat/), I believe I should be running a Chi-Square test. However, much of the literature I can find on Chi-Square tests discusses how to determine if there is a difference between two *distributions,* which is not really what I want to do. 

As an example, here is an [example chart](https://www.statisticshowto.com/wp-content/uploads/2018/06/grouped.png) which doesn't include my data but is similar to what I am working with. Referring to this example, I am wondering how to determine if there is a significant difference between the percentage of 70-79 year-olds who report mobility issues and the percentage of 80-100 year-olds who report mobility issues. (Note that in my actual data, I would be able to convert percentages to number values, if necessary)

What, if anything, would be an appropriate way to do this?

Thanks!",AskStatistics,2022-08-22 12:03:18,6
"What software are you using? I just found tutorials for [R](https://www.youtube.com/watch?v=GZMXEq7GPvY), [STATA](https://stats.oarc.ucla.edu/stata/faq/how-can-i-perform-mediation-with-multilevel-data-method-1/) and [SPSS](https://www.youtube.com/watch?v=bAzOekt39fQ) on Google.",1,wuvypf,"Hello all,

I'm a little confused when it comes to multilevel modelling (MLM). It is my understanding that MLM can be used to test mediation and can also be used to analyze longitudinal data. However, I am having trouble finding resources that explain how to use MLM to test mediation with longitudinal data. Is it that if I want to test mediation with MLM then I won't be able to capitalize on the longitudinal nature of my data?

Any help would be GREATLY appreciated. Thanks guy.",AskStatistics,2022-08-22 08:01:17,4
"Yes, you need an X and Y value for each case to find the correlation. If one is missing, you could figure out a way to replace it — there’s no one best way to do that. It would depend on your knowledge of the data, the phenomenon being measured, etc. 

What you propose seems reasonable, as does using the average of the previous whatever-number of days, etc.",2,wv4aj4,"More specifically, I can calculate the correlation coefficient, say via Pearson's method, for two equities since they will have the same timestamps over a period, and thus the same number of data points.

The same holds true for two crypto assets.

But how are correlations performed between a traditional equity (which will not have price data on weekends or holidays) and a crypto asset (which will have price data for every single day over some period)?

I suppose one such solution would be to repeat the Friday price for Saturday and Sunday for the traditional equity, such that number of points between the two data sets will be equivalent. Is this how it is typically done?",AskStatistics,2022-08-22 13:30:13,3
"You did not mention the MLE under the null hypothesis.  That is a bit complicated.  If you write down the log likelihood with p_2 = 2 p_1, differentiate, set equal to zero, and solve for p_1 you get a messy quadratic equation.  So that means you can do either a Wilks (likelihood ratio), Rao, or Wald test.  The latter is simplest because it only uses the MLE for the alternative hypothesis, which is trivial unless it violates the inequality.  But in that case you get P = 1 for the P-value (no evidence whatsoever in favor of the hypothesized alternative).

No matter how you do this, this is a tricky math stats problem.  If I wanted to show how to do all three tests (Wald, Wilks, Rao) I would probably have a three page or more write-up.  Maybe I will set this a PhD qualifying exam question some year.",2,wup1ae,"Say I have a standard hypothesis test for a difference in proportions:

H\_0: p\_1 = p\_2

H\_a: p\_1 < p\_2

What if I wanted to test that a proportion is at least twice as large? So...

H\_0: p\_2 < (p\_1)\*2

H\_a: p\_2 >= (p\_1)\*2

If I already have n1, n2, p\_1, and p\_2, do I just go back and change the mean and variance of the other sample? Where p\_2 (new) = 2\*p\_2 and V\[ sample 2 \] = p\_2 (new) \* ( 1 - p\_2 (new) ). Then plug all this in and run the hypothesis test as I would normally?",AskStatistics,2022-08-22 02:36:05,4
"Are you testing a hypothesis? Your choice of test will depend on what that hypothesis is. And also will depend on how you are scoring depression (and mood). For example, maybe you suspect that acute depressive symptoms lead to a few days of lowered mood. Or maybe you're measuring trait depressive symptoms, and suspect that  high scores are associated with smaller day-to-day variance in mood. Totally different analyses, with others possible too.

Your use of ""ESM"" suggests you're maybe working from a guide of some sort? What kinds of models are you building in general? I haven't done experience sampling analyses (and so I apologize if I missed something obvious), but I imagine part of the point is to average out things like time-of-day effects by randomly sampling within/across participants. So at some point you'll probably want to take some averages of mood scores. But that doesn't prevent you from also looking at more fine-grained effects; just decide what you're looking for before you actually look for it, or you risk invalidating any findings. (Well, ideally you make that decision and design an analysis *before* you collect the data, but that ship seems to have sailed.)",1,wuqno9,"I am using experience sampling methodology (ESM) to collect mood data at multiple point throughout the day for 7 days. At the beginning of the 7 days I also collect depression scores. 

How can I examine the relationship between the 1 time depression measure and the repeated mood measures without just taking a total average for the mood data across the time range?

&#x200B;

Thanks in advance!",AskStatistics,2022-08-22 04:06:34,1
I would say non-parametric statistics and stochastic processes,25,wuhccp,,AskStatistics,2022-08-21 19:23:04,14
u/efrique helppp,1,wuppne,,AskStatistics,2022-08-22 03:14:59,2
Maybe try using python pandas? If the dataset is public I can help you with that :),1,wuu5wc,"
I'm trying to convert the large FDA Adverse Drug event data available in a large xml format to a user-friendly format for my research analysis, but I've tried different methods that can convert it using the command line/bash, and it's not working.
Can anyone recommend how to convert this or what step to follow.",AskStatistics,2022-08-22 06:49:08,2
"Based on what you know about  the content domain, you should consider (1) the chance that the relationship of the transformed variable is positive but the relationship with the untransformed variable is not and (2) the variable with the stronger relationship with the criterion untransformed is not  the variable with the stronger relationship  when transformed. If both answers are “very unlikely” then don’t worry about transforming the variables.",1,wup9p0,"Just to elaborate: I’m doing an OLS-test to determine the following things:

1. Do my independent variables have a significant effect on the dependent variable?
2. What’s the direction of the effect of my significant independent variables (positive/negative)?
3. What’s the order of my independent variables by strength of effect? (e.g.: Which independent variable has the strongest effect, and which one has the weakest effect?)

Now, I know that there are other statistical models that could come into play, but for all other reasons, I decided to make OLS work if possible.

Since my residuals are definitely not normal (but I would like to interpret p-values, etc. from my model), I’m thinking of using a Box-Cox transformation. However, I’m a bit confused about whether I can interpret the coefficients of the significant independent variables after doing a Box-Cox transformation on the dependent variable to answer my research questions. Again, I’m not looking to build a predictive model with precise predictive power – I’m just looking to:

1. Establish whether an independent variable has a statistically significant effect on the dependent variable
2. Establish the direction of this effect
3. Establish whether one independent variable has a stronger effect than another one (without saying anything about ‘how much stronger” that effect is)

Thanks!",AskStatistics,2022-08-22 02:50:12,3
"""Probability that it doesn't rain on either day"" means it doesn't rain on the first day AND it doesn't rain on the second day. It needs to NOT rain on both days to satisfy that. Both are only satisfied at the intersection. 

""P(not A ∪ not B)"" is the probability that it doesn't rain on at least one of the two days... No rain on first OR no rain on second.",5,wufx92," 

I saw an example in a textbook :

1. There is a 60 percent chance that it will rain today.
2. There is a 50 percent chance that it will rain tomorrow.
3. There is a 30 percent chance that it does not rain either day

let's define **A** as the event that it will rain today, and **B** as the event that it will rain tomorrow.

The book defines the 3 as P( not A ∩ not B )= 0.3

So why do not define 3 as P ( not A **∪** not B) = 0.3 ?

Hope to get an answer, thanks.

&#x200B;

The link of this example: [https://www.probabilitycourse.com/chapter1/1\_3\_3\_finding\_probabilities.php](https://www.probabilitycourse.com/chapter1/1_3_3_finding_probabilities.php)",AskStatistics,2022-08-21 18:15:06,6
"What do you mean by least significant? Least statistically significant (highest p value?), or do you mean smallest effect size?",14,wu351v,,AskStatistics,2022-08-21 09:04:51,29
">  but the degrees of freedom that I attained is way too large to use the normal tables.

 By 'normal tables' I assume you mean t-tables. You should in fact just use normal tables (which don't have degrees of freedom) once your d.f. go far off the end of the t-table.

If your total d.f. was only mildly off the end of the table (say at d.f. = 180 for a table that only went to 60 or 100), I'd suggest considering inverse interpolation, but unless you're going a fair way into the tail (using a very low alpha or doing a lot of multiple comparison adjustments) it's probably not a big deal. 

(If your table only goes out to 30, you should consider using inverse interpolation in the d.f. more often -- at least in cases where there's some opportunity to be near a cutoff)

---

If you're not *required* to use tables, use a computer to find the critical values or p-values for you once you have a statistic. Even most spreadsheet programs can do this much.",2,wubl5r,"I'm trying to compare the population means µ1 and µ2 by looking at the sample means, but the degrees of freedom that I attained is way too large to use the normal tables. Should I have standardized the values given for the sample size and the standard deviation? Please take a look at my working out → I first got the pooled estimate, found the standard error and then calculated the test statistic.

(I didn't know how to upload pics so here's a link)

[https://www.tumblr.com/blog/view/lmaoooooo12oooooooooooooo/693232917092188160?source=share](https://www.tumblr.com/blog/view/lmaoooooo12oooooooooooooo/693232917092188160?source=share)",AskStatistics,2022-08-21 15:00:25,10
"I would read the Armitage paper - what you’re missing is the density function provided in the Armitage paper, as they cite. 

But if you’re tight on money, it might be possible look around for free articles on sequential monitoring and/or read through the documentation for available sequential monitoring software. You’ll definitely be able to find the density somewhere! 

A completely different direction to learn more generally about sequential monitoring would be to read the textbook about sequential monitoring by jennison and turnbull (2000), which continues to be well cited even though it’s been several years since its publication. But that would also cost money and a good chunk of time, I’m sure.",3,wub5is,"I'm trying to understand how to run sequential testing in an online setting. The test statistic is a difference of two means and we'd like to check the test multiple times prior to the full collection of the sample.

[This paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4024106/) tells me that I need to solve this equation:

https://preview.redd.it/jomrn4f9y4j91.png?width=1004&format=png&auto=webp&s=9003e90127afe810642b8b92230de793f3596ec2

To do so it says I need to obtain the joint distribution of:

https://preview.redd.it/9td2igcey4j91.png?width=342&format=png&auto=webp&s=1d9c339f0642ea350ad3dba1160e85028446773b

(I presume the first term is meant to read Z(1), but I could be wrong)

Although very motivated to understand and solve this, I fear I'm out of my depth. Is the solution already known or knowable/lookupable for a two sample test of the difference between means?

If not, would someone be able to help me to the starting line in trying to research or calculate the solution? The paper goes on to say this:

&#x200B;

https://preview.redd.it/krw28490z4j91.png?width=1718&format=png&auto=webp&s=c344c6e6077fa35ea40d8a3106e9723becdd4cfc

I'm encouraged to know that the derivation is ""relatively straightforward"", but I'm not at all sure how to proceed. I'm wondering whether to purchase access to the cited paper or not.

Any help or advice would be much appreciated!",AskStatistics,2022-08-21 14:42:12,23
"> I just read that regression analysis is for determining function, or cause and effect. 

""Establish casuation"" is definitely one thing regression *doesn't* do, outside some very particular situations. People often act like it does but they're not correct.\* 


There's no big barrier between correlation and regression; indeed, simple regression is very closely related to correlation (to the extent that they should give the exact same p-value for a corresponding test) and similarly, multiple regression is very closely related to partial correlation. (Some application areas seem to have erected huge conceptual fences between them but they're not actually there mathematically. What's important is being able to take the parts of either that you need for your specific purpose.)



\* Sadly, it's necessary to be very cautious about things you read -- how do you know the person who wrote it understands what they're talking about? What reason do you have to just accept their word? (Is getting it from a textbook or paper a good reason to think it's correct? Sadly not. Lots of people write about stats who don't really know what they're doing. There's a very strong tendency for people who have very limited background in the theory to think they understand well enough to actually write books and teach others.)",4,wubd1x,">**Hypothesis 1:**  Positive social feedback for previous outrage expressions should predict subsequent outrage expressions.  
>  
>**Hypothesis 2:**  Norm learning processes affect online outrage expressions.

Here is the actual paper: [https://www.science.org/doi/10.1126/sciadv.abe5641](https://www.science.org/doi/10.1126/sciadv.abe5641)

**My Problem:**

I started writing my paper with the assumption that these were correlational studies, because they are large-scale, and because the researchers cannot manipulate the variables, like in a small scale experiment, which also means they cannot be sure of cause & effect. Then I started talking about using regression analysis, specifically, the coefficient of determination. However, I just read that regression analysis is for determining function, or cause and effect. So, as of right now, my paper is looking retarded.

What do?",AskStatistics,2022-08-21 14:50:59,1
If you're worried about that you could use error proportion,3,wukadp,"Can somebody help me with this variance problem.  I am trying to measure the variance of order delivery qty.  However, I realized that the error increases together with the increase of order qty. For example,

Order Qty (5, 10, 500, 1500...etc)

Delivery Qty (3, 5, 400, 850...etc)

Error (2, 5, 100, 650....etc)

&#x200B;

Should I still use standard deviation to measure the error variance?  It seems not random error, biased by order qty.  ",AskStatistics,2022-08-21 21:51:48,2
"Tutor for what?  Math stats?  In that case there should be done in the textbook an example like this where the support of the PDF depends on the parameter.  The usual example is Unif(0, theta)
and this is similar.

Here is are some [course slides about that](https://www.stat.umn.edu/geyer/5102/slides/s3.pdf#page=26).  Your problem has the same issues.

First for this use the likelihood rather than the log likelihood.  Make sure you put in an indicator function that indicates where the likelihood is zero or nonzero (like in the example).  And don't differentiate.  Derivative equals zero only finds a local maximum when that occurs at a point where the likelihood is differentiable.  Not the case here.",14,wu2w7w,"Consider the density f(x;θ) = 2x/(1-θ\^2), θ ≤ x ≤ 1; 0 < θ < 1. Find the Maximum Likelihood Estimator for θ.

The likelihood function, given x1, x2, … xn would be:

2\^n \* (1-θ\^2)\^-n \* x1 \* x2 \* … \* xn

The log likelihood would be:

n\*log(2) - n\*log(1-θ\^2) + log(x1) + log(x2) + … + log(xn)

The derivative with respect to θ is:

2nθ/(1-θ\^2)

which doesn’t depend on the x’s, which is weird. But even more perplexing is that setting that equal to 0 and solving for θ, I get simply, θhat = 0. That can’t be right, can it? θ can’t even be 0, it’s always strictly greater than 0.

Anyone have any insights?",AskStatistics,2022-08-21 08:54:41,11
"You should not be taking cohort studies seriously regardless. RCTs are the only credible evidence for a drug intervention.

If this is homework, consider the biases introduced by the lack of randomisation. Point out that the CI is calculated assuming random allocation when there was no random allocation and is therefore uninterpretable. Point out that even if it was randomised, the study is too small to reliably estimate the effect or rule out small but important differences.",3,wu6uwg,A cohort study is investigating the relationship between a newly marketed antibiotic and standard of care for impact on pneumonia hospitalizations. A relative risk (RR) of 1.3 with a 95% confidence interval (CI) of 0.90–1.81 is reported for the new drug compared to standard of care. Can you say the new drug is associated with an increase in hospitalizations? Or is there not enough information since the relative risk is not statistically significant?,AskStatistics,2022-08-21 11:41:30,22
"Nothing is actually distributed like, say, the normal distro, just like there are no true circles in nature. 

🤯",2,wu1721,One lesson can be the difficulty of winning the lottery.,AskStatistics,2022-08-21 07:40:24,3
"> "">0.01""

You got both the direction and the value wrong there. 

It's ""<"" not "">"" *and* it's 0.001 not 0.01. 

(It is important to get the right way around, and it's important to get the value correct.)

It means the p-value is less than 1 in 1000. In psych (and other areas that tend to follow its practices along) they seem to have collectively decided that smaller p-values will not be explicitly stated. Which is mostly fine unless you want to use a type I error rate of say alpha = 1/2 % (as a lot of papers have recommended in recent years) and also to do a bunch of multiple comparisons.",1,wubtab,"He everyone,

Sorry if this is a dumb question as I'm pretty new to stats. I am doing a dissertation and was doing a regression analysis and my p value for coefficients and  F statistic is >0.01 and there is a small b next to it. What does this mean? I know p of less than 0.05 is statistically significant but the > sign is throwing me off. Posted screenshot if that helps

Any help would be thoroughly appreciated

Thanks !!!1

&#x200B;

https://preview.redd.it/9254nl5g55j91.png?width=1920&format=png&auto=webp&s=926daa9b429b615ad37f0e50617d75f4f105364a",AskStatistics,2022-08-21 15:09:43,3
"In future please try to use more informative titles, per [rule](https://www.reddit.com/r/AskStatistics/about/rules/) 5:

> 5.Use an informative title

> Use a title for your post that very briefly describes the statistical problem you need help with. . . . [*snip*] . . . It should not consist largely of redundant information like ""Quick Question"" or ""Please Help"".",2,wtvjoe,"So we are doing a study regarding a medication (X). In literature 47% of patients who receive medication (X) experience hypersensitivity reactions. 

After collecting our data we collected all patients data that received medication (X), and found that 33% of them experience the hypersensitivity reactions. 

&#x200B;

We would like to find the predicting factors that are associated with this 33%. Is it age? weight? or no. of doses given?

How can we translate this type of data to a research? is it considered a case control? or other types, and what types of statistical analysis shall we do? ANOVA?",AskStatistics,2022-08-21 02:35:59,10
"What does ""histogram"" mean to you? To me, those are not histograms.",3,wty89g,"
Hello good folks,

I have a few questions related to three histograms I have created, I want to know how to identify the histograms. Are they multimodal? bimodal? unsymmetric multimodal? Is there inverse/ reverse multimodal histograms? my supervisor is away and I have a deadline and I'm a bit stuck.

The histograms are for correlation between:

1. Deviation of line size from the mean vs. line angles ( [https://imgur.com/hL9lLxd](https://imgur.com/hL9lLxd) )
2. Deviation of brightness level from the mean vs. angles ( [https://imgur.com/fzcK6bJ](https://imgur.com/fzcK6bJ) )
3. Deviation of brightness level from the mean vs. segment size ( [https://imgur.com/djvMdPG](https://imgur.com/djvMdPG) )",AskStatistics,2022-08-21 05:16:39,3
"Yes, at the most basic level, you could account for covid by having a variable that is 1 for the years 2020 and 2021, and 0 for all other years.

I'd also look into how you'd account for the time series element in your data too. Remember regression typically assumes all data points are 'independent' of each other, but if I had your salary in 2018, that usually tells me something about your salary in 2017 or 2019, so those datapoints are not independent. You'd perhaps want to include `year` as a fixed or random effect, or you could do something fancier",1,wtv3kc,"Hi friends, 

I am working on a research but I am not that skilled in data analysis. 
My aim is trying to use a regression (I will have to work on a panel data but I still have to check whether OLS, fixed effect, or random effect would be better for my model) to study the determinants of corporate executive remuneration (For instance, Salary = B1xSize + B2xTenure + B3xGeographical Diversification…).

Do you have any suggestion in how I could study the impact of covid in my model? I was thinking about creating a dummy variable 0 (Years before covid, in my sample 2013-2019) and 1 (Years after covid, in my sample 2020-2021).  
Do you have any other ideas?

Thank you in advance!",AskStatistics,2022-08-21 02:06:48,3
What do you mean by instruments here?,1,wteel3,"Good afternoon all! I was hoping to get your opinion on the best way to compare different instruments running the same test. We have 6 instruments and we want to be sure we get the same results no matter which instrument we run.  On Day 1, we run samples 1-7 in triplicate and samples 8-13 in singlicate on each of our 6 instruments. Then, we do the same thing on Day 2. Normally, we would take the average of all of the instruments, calculate SD, and then %CV. But, is there a better way? Is there a way to say, ""There is no statistical difference between all 6 of our instruments?"" Do I have to do a T-Test with all variations of data? (I.e. 1 and 2, 1 and 3, 1 and 4, 1 and 5, 1 and 6, 2 and 3, 2 and 4, 2 and 5, 2 and 6, 3 and 4, 3 and 5, 3 and 6, 4 and 5, 4 and 6, 5 and 6).

Here are our data. I am not sure what happened on Instrument 1 Day 2 with Sample 2, but I am pretty sure something went wrong.

Any advice on the best way to compare these data is greatly appreciated.

Thanks!

https://preview.redd.it/pn8c5xgbywi91.png?width=808&format=png&auto=webp&s=16dade3c9585bf111dfce28c40fa39e952a4a456",AskStatistics,2022-08-20 11:46:57,2
"it would be redundant- if you added it alongside the time dummies, you would have perfect multicollinearity and the variable will be automatically omitted by any statistical software",5,wt3v8d,"Hey everyone,

it is my understanding that it is possible to account for omitted variables that evolve over time but are constant across entities by adding time fixed effect. If one of these variables was actually measured, would it be sensible to add it to the model alongside the time dummies or is that redundant?",AskStatistics,2022-08-20 03:28:30,1
"With your last paragraph, the difference in differences in the log of the outcome variable becomes a ratio of rates when you exponentiate back.  Eg a difference in differences of -0.5 in the log of the outcome variable is a ratio of rates of e^(-0.5) to one. This would represent an approximately 39% reduction in the outcome variable. The ratio of rates would be refered to as a incidence rate ratio",2,wtb64t,,AskStatistics,2022-08-20 09:26:22,6
"Principal components is an exploratory method that should not be used to,test hypotheses. It can be used for data reduction so that a small set of components may capture most of the variance of a larger set. This can be used in a regression context to have fewer variables and avoid collinearity. On the other hand, there is no guarantee that the variance captured is the portion most predictive of the criterion.",1,wt2xp3,"Would someone be able to help me out with my masters degree statistics/data analysis questions? I have my conceptual model and hypothesis done and now my mentor said ""do a principal components analysis, then cronbach alfas and then multiple regression for some hypothesis I only need to do a simple linear regression, which I kind of understand. But I never had data analysis before and I am not really sure especially what a principal components analysis is for and even when reading about it I quite can't understand how I am supposed to read the informations and what kind of conclusions to take out of it.

&#x200B;

I could send my conceptual model and hypothesis if someone is down to help me out with it! 

&#x200B;

Thank you",AskStatistics,2022-08-20 02:27:59,2
"Psychology PhD turned data scientist here.

Psych only teaches a very narrow part of statistics. A key thing you probably won't have been taught that you should get your head around soon is distributions. understanding different distributions and what they mean.",27,ws9epv,I’m a psychology student and I plan to shift to BS Statistics next year. Any tips/advices will be appreciated. Thank you!,AskStatistics,2022-08-19 02:20:38,19
"A power calculation\*  is in respect of a specific analysis (in particular a specific hypothesis test, particular population effect size, particular significance level). The power is not the same for doing something else with the same data.



\* though here it looks like you mean a sample size calculation given a particular choice of power -- i.e. you didn't calculate power at all, you specified it, sample size was what was calculated; the basic issue is the same though",2,wsvoq1,"I know a very broad question but I have a specific thought in mind. Lets say you calculate you need 125 people to achieve 80 % power.

You combine two population groups A and B into the control n =130 and intervention n =135.

Control group has population A=60 and population B=70

Intervention group has A = 45 and B = 90.

You find these results: 1. Death in intervention is 25/135 (18%) and death in the control is 40/130 (30%).

you then separate it further and say:

Death in group A of the intervention is 15/45 (33%) and group B is 10/90(11%).

Death in group A of the Control is 35/60 (58%) and group b 5/70( 0.07%).

if you spit the overall composite outcome of what is said in line 1 into what is said in line 2 and 3. Don't you effectively loose power since it is now less then 125 to detect power or does power carry over and cover all break ups of the composite?",AskStatistics,2022-08-19 19:17:03,4
Bro. What did you smoke?,12,wt5nwa,"According to the Law of Large Numbers something can happen in the long run a certain number of times. 

If the age of universe is 13787 billion years and the year 2021 happened 1 year, does that mean that in the long run we will live a year similar to the year 2021 again at least 1 time?",AskStatistics,2022-08-20 05:14:42,11
"> Apparently, a pseudo-additive decomposition stabilizes the variance just like a multiplicative decomposition but does not have the issues with zeros and small values.

What is this apparent from? (/ where have you found this term? What are you looking at?)",1,wso34a,"Hello, I am convinced that decomposing my time series will greatly benefit my forecasts, however, there are too many zeros, small values, and some negative values that are messing with the decomposition. Some years, a multiplicative decomposition works excellently, especially compared to an additive decomposition, but other years, there are too many zeros and small values. Apparently, a pseudo-additive decomposition stabilizes the variance just like a multiplicative decomposition but does not have the issues with zeros and small values. The problem I have now is that I cannot figure out how to implement a pseudo-additive decomposition. I'll explain where I get lost below:

To my understanding, a multiplicative decomposition is done as follows:

1. Estimate trend with a moving average with a window that is appropriate for the seasonal component. I have a weekly seasonal component so I used a window of length 7 for my daily data.
2. Detrend the series by dividing out the trend. This is where I believe everything breaks. If there are too many zeros/small values then the trend returns zero or a small number and dividing by either of these creates a mess.
3. Estimate the seasonal component by averaging each period of the season in the detrended series. In my example, this would be taking the average of all Sundays, Mondays, Tuesdays, etc. These values are then looped throughout the weeks of the year(s). After this, there is some sort of adjustment required which I cannot find a good explanation for. I was told by the fpp3 book to adjust the seasonal component so that it ""adds to m"" (m=7 for me). I accomplished this by subtracting the total from each period and then multiplying by 7. (This adjustment doesn't seem to accomplish anything, however, every text on pseudo-additive decomposition talks about how the seasonal component is centered at 1 and therefore 1 should be subtracted so that certain terms in the decomposition are centered at zero.)
4. Estimate the remaining variation (ideally just noise) by dividing the series by trend and season (remainder = series / (trend x season)).

By following the above I was able to decompose some years so that the remainder was just noise. In other years, the decomposition fails at step two. An additive decomposition leaves too much noise (additional variation) in the remainder component where the level of the series is greater, while a multiplicative decomposition has the same amount of noise throughout the series (homoscedastic). If anyone knows how to implement the pseudo-additive decomposition that would be great. Any suggestions on forecasting with this kind of data would be greatly appreciated as well. Thanks in advance.",AskStatistics,2022-08-19 13:30:00,5
"1. *Significance level* is a property of the ""population"" behaviour of the test, and *has no relationship to the data* -- change the data all you like - take new samples again and again, but your significance level alpha is still what you chose it to be. 

   Indeed you should always choose it before you see any data.

2. The p-value is information from the sample. You don't choose it, you calculate it.

Clearly, your test has to look at the sample in order to see what it tells you about the plausibility of H0 as an explanation for the data, so if the sample is to contribute anything to your decision you will want to look at the p-value (or equivalent information such as the test statistic).

On the other hand, when you look at sample information, such as the p-value, you also need a rule by which you can conclude that the data are inconsistent with H0 (sometimes called a *rejection rule*).


The significance level, alpha, provides a threshold for just such a rule; if p ≤ alpha, you conclude that the sample is not sufficiently consistent with the null hypothesis (i.e. reject H0).",4,wsl58p,Why is p value important for rejecting null hypothesis? Why can’t null hypothesis be rejected only on the basis of alpha (level of significance)?,AskStatistics,2022-08-19 11:25:54,5
"If your question is simply does Y increase or decrease with increases in X and not details about  the form of the relationship, it is unlikely that simple regression treating both variables as quantitative will give you the wrong answer. Granted, contrived examples with weird mappings between a true underlying scale and the ordinal scale can show a wrong answer is possible. It’s a judgment call, but I believe that in most realistic situations this is very unlikely. Your decision should be informed by your domain knowledge.",2,wsdzxe,"I would like to assess the relationship between one quantitative continuous variable (blood biomarker 1) and one ordinal variable (blood biomarker 2) but with non proportional gaps between elements, i.e. 4 categories with the following values: 0.1, 0.33, 0.4, 0.9 (values are increasing but gaps are not proportional between elements).

Which statistical method should I use to assess the relationship between both variables? I want to know e.g. if biomarker 1 increases when biomarker 2 increases too or the other way round. Should I treat the biomarker 2 as a quantitative or qualitative ordinal variable? Looking at the plot ordinal would seem better to me. I thought of Spearman's rank correlation of Kendall's rank correlation. Would these tests be appropriate? Or should I use ANOVA?",AskStatistics,2022-08-19 06:26:05,3
"I would say they imply an association and nothing more. Causality is always something the researcher carefully infers using knowledge, statistics, etc etc. There’s no such thing as a statistical method that can a priori prove causal relationships - the researcher always has to make some assumptions, guesses, whatever, then do the analysis and revise, conclude, whatever. Causal inference is really just about trying to make that process as clear, transparent, and principled as possible. But it’s still researchers inferring causality. At best associations might hint at something you haven’t thought of, but then you need to think about it, do extra work, to prove causality.",2,ws9kry, The question is not whether association rules ‘proves’ a causal relationship. But the question is whether a causal relationship is implied (antecedent-consequent does suggest this) or whether they ‘just’ co-occur. Is there some implied meaning (from the data analysis technique or theory) behind the allocation to these categories?,AskStatistics,2022-08-19 02:31:11,4
"This happens all the time. It's important to understand regression well enough to understand what things can cause this or you will misinterpret regression a lot, and do things that *can directly mislead you* about what's going on in the relationship between variables.

There's several separate things that can do this.

1. Omitted variable bias.

    https://en.wikipedia.org/wiki/Omitted-variable_bias

    Also see the first diagram here, which illustrates an example:

    https://en.wikipedia.org/wiki/Simpson's_paradox 

    (which is to say, the results from a simple regression may be completely misleading about the size and even direction of effects, if you have left out other potentially important variables that could correlate with them)

   This is a constant bugbear with observational studies (with designed experiments, you can generally avoid it)

2. When you add a new variable, you reduce the error sum of squares and typically along with it, the mean square error estimate.

   If the second variable is *usefully* reducing the size of the estimated error variance, this makes F ratios or t-ratios for coefficients noticeably larger (since the denominator is smaller). Ergo, more likely to have a p-value below some threshold.

   It can easily happen that both variables do this (make MSE smaller), in such a way that it happens to take you from not significant to significant in either order.

---

You can also go the other way (significant to not significant). e.g. not only can item 1 above take you the other direction, moving a coefficient closer to zero, there's also multicollinearity which inflates the variance estimate of coefficients and some linear combinations of coefficients.",3,ws2e2z,"A linear regression conducted wasn't significant(a
~ b), however a multiple regression with those
two variables as the predictor variables and a new
variable as the outcome was significant (o ~ a +
b). Just wondering how the two predictors work
together to predict the new outcome, but don't
correlate in the original linear regression?",AskStatistics,2022-08-18 19:44:24,6
"A summer off? You'll be fine. I've been doing this for a lot longer (about 20 years), but I've basically taken 2 years off because of the pandemic. Say a prayer for me as I try to jump back into it this year!",16,wrl5u9,"I have had a summer off for the first time in 5 years. I start my graduate program in Statistics next week. With that being said, I have not done any coding all summer & I’m walking into this way too cocky not being nervous about it. 
Should I be worried? 😂

btw, I have a Bs in Stats",AskStatistics,2022-08-18 07:34:56,6
"Want to test the two factor theory, yep, do confirmatory factor analysis.  Not sure what you mean by ""the values of Factor A and B"" -- loadings maybe, or do you want estimated factor scores for each of your cases.  I have rotated many an initial exploratory factor analysis, but never with R, only with SAS or SPSS.  I am confident that there are R users here who point you in the right direction.  Wishing  you the best with your upcoming adventures with factor analysis",1,wrzr5n,"I have an 18-scale item - Self-Monitoring score, with 10 items loading into Factor A and 5 items into Factor B based on previously established literature. Given that the two factors are already known, using R, how do I calculate the values of Factor A and B by doing factor rotation on the 2 eigenvector values? I'd like to use the two values as separate DVs in a regression.

I am new to factor analysis and thought that it may belong to Confirmatory Factor Analysis. I am unsure of the steps needed to take to calculate the final score of the two factors drawing from the 15 items. Thank you!",AskStatistics,2022-08-18 17:39:45,1
"Please note the rules:

https://www.reddit.com/r/AskStatistics/about/rules/

specifically these ones, since they may each be relevant:

> **Rules for r/AskStatistics**


> \1. This sub is not intended for homework help

> This sub is not intended for homework help (""homework"" is interpreted quite broadly; among other things, it would include any coursework you're supposed to be doing yourself). You can try /r/HomeworkHelp but be aware of the rules there before posting there. [Occasionally a legitimate question may be deleted by this rule; try not to just post ""textbook"" style questions; if you're doing one for your own study & get stuck, identify what stopped you and ask about that. Showing your work may help.]



> \3. No solicitation of academic misconduct

> This includes but is not limited to asking people to do exams, tests, quizzes, assignments and other coursework for you, or offering to do them for others. Breaking this rule may result in being banned from /r/AskStatistics. Any indication that /r/AskStatistics is being used to locate people for this purpose may also result in such a ban.


> \5. Use an informative title

> Use a title for your post that very briefly describes the statistical problem you need help with. It should not be about your emotional state (""Desperate""), personal circumstances (""Bad at stats"", ""I'm a beginner""), nor how urgent you think your problem is, nor your assessment of how easy you think it is. It should not consist largely of redundant information like ""Quick Question"" or ""Please Help"". If personal context is essential, put it in the body of the post.

---

What you need to do is make a serious attempt at these (which will involve going back over materials like notes and textbook, practicing examples, doing more basic exercises of similar kinds, etc)  and identify what it is in particular that is stopping you from completing these exercises. Then ask about one of *those* specific issues, externally to the exercise that led you to identify that problem in your understanding.",2,wryxuh,"Can someone help me with two exercises?

I am trying to solve 2 exercises and I can’t. Could you please help me? And, if possible, explain me.

1) You run a factory that makes high-quality radios. Only 1% of radios are defective when they roll off the assembly line. You then have a quality-control unit that tests the radio. The test is 90% reliable; that is, 90% of good radios will pass the test, and 90% of bad radios will fail. If a radio passes the test, how likely is it to be a good one?

2) (related to question 1) If a radio fails the test, how likely is it to be a bad radio?


Thank you so much for the help!!",AskStatistics,2022-08-18 17:00:36,3
"Try computing difference scores for each scale and do a MANOVA on the difference scores. (As an aside, an ANOVA on difference scores on a scale is equal to the Condition x Trials test in a repeated-measures ANOVA.) A MANCOVA with pre-tests as covariates may be better but probably not what is expected of you.",2,wro5f8,"I am doing a mock stats analysis on two groups (one with a disease and a group of healthy controls). ALL participants complete the experiment and we are analzying their moods into four different likert-type scales (A scale, B scale, C scale, and D scale). They complete each scale before and after the experiment. I was told to do a post-hoc MANOVA of all the scales but it's not working and when I soley do the MANOVA my findings are insignificant. However, when I do a repeated measures ANOVA or a T-test my findings are significant. Can anyone please help me?",AskStatistics,2022-08-18 09:36:09,1
"Im unclear what youre asking for here.

'As close' as the ecdf? You can't get closer to the data than the ecdf.

So far you don't have anything but data to judge closeness to the population. If you want 'the closest' cdf, you have it already. The ecdf is also the cdf with the best chance of producing your sample.




If you want something other than the ecdf you need to restrict things a bit: there will be an infinite number of cdfs within an epsilon distance of the ecdf for example. 

What's the basis to choose one?


Perhaps more importantly -- what do need one for?",2,wrt2lm,"Hello everyone!

I'm working on a project in which I estimate different quantiles of a distribution. But I'd like to obtain a definition on the distribution that could have a cdf as close as the one I estimated. 

Do you have any recommendation for that?",AskStatistics,2022-08-18 12:55:44,2
Start by getting the proportions of successes and confidence intervals for each state and plot it. That will tell you something.,2,wrn2ae,"My dataset explores call center data. My predictive field “successes” is determined by a variety of factors like state, frequency of calls, strategy types, etc. 

My first priority is to determine if the state a call is heading to in any way impacts the result. 

To do this, I built a table of successes and fails for each state, then ran a chi squared analysis on it. The p-value was incredibly low (<2.2 e-16), so I’m thinking I might’ve done something wrong here.

Any advice?",AskStatistics,2022-08-18 08:53:04,4
"> How do I know if I should include interactions?

From expert matter knowledge or literature. But be aware: You need a [much larger](https://statmodeling.stat.columbia.edu/2018/03/15/need-16-times-sample-size-estimate-interaction-estimate-main-effect/) sample size to estimate interaction effects compared to main effects. This may prove to be prohibitive in your case.

> Can I even do that with such a small sample?

There is no statistical police coming to arrest you, but you may have abysmal power for any interaction effect, as explained in the blog I linked to above.

> How many predictors is too many?

It's difficult to say much in much generality (the absolute maximum is 30 independent and 1 dependent variable). But see the dicsussion [here](https://stats.stackexchange.com/questions/12854/maximum-number-of-independent-variables-that-can-be-entered-into-a-multiple-regr). A very general rule of thumb says you need about 15 observations for every independent variable. If you include interactions, you'll need much, much more.",4,wrfges,I have a small sample (N = 31) and want to calculate a multiple regression. I have three independent variables as predictors. How do I know if I should include interactions? Can I even do that with such a small sample? How many predictors is too many?,AskStatistics,2022-08-18 02:58:43,3
"Let x and y be the leftmost position of the two respective people. Then all possible combinations of their positions is the square A={(x,y) : 0≤x,y≤49}. The two people collide if |x-y|≤1. Assuming the two people are dropped _uniformly_ and independently along the track, the probability of colliding will be the area of the region {(x,y) in A: |x-y|≤1} as a proportion of the area of A",5,wrlyln,"Let's say there is 50m of train tracks and there are two people standing on either side of the tracks. The people are both 1m wide. If you randomly drop these two people anywhere along the tracks, what is the probability that they will collide when crossing the tracks?",AskStatistics,2022-08-18 08:07:28,5
Inverse,1,wr3jwu,"Hi Everyone,

 I hope you are doing fine. I am currently in the process of designing a comparative analysis that shall contrast the differences between various forecasting methods. I am trying to predict stock returns by running a cross-sectional regression using various factor inputs. However, no matter what I do, each and every regression receives a negative out-of-sample r2 score.

I checked everything and could not find any error in my code. I tried manipulating the data but even that did not help improve the r2 values.

Here is my code to run the regression:

    from sklearn.linear_model import Lasso
    from sklearn.linear_model import Ridge
    from sklearn.linear_model import ElasticNet
    
    
    def prediction_function(df, regmodel, window, n_factors, alpha):
    
        factor_list = df.columns.values[3:].tolist()
        predicted_df = pd.DataFrame()
        date_df = df['date'].iloc[window:].copy()
        date_df.reset_index(inplace=True, drop=True)
        if df.shape[0]<=window:
                return None
        else:
            for i in range(window, (len(df))):
                Y_train = df['fwd_daily_exret'].iloc[i-window:i].copy()
                X_train = df[factor_list].iloc[i-window:i].copy()       
                X_test = df[factor_list].iloc[i].copy()
                X_test = np.array(X_test)
    
                X_test = np.reshape(X_test, (1,-1))
                X_test = pd.DataFrame(X_test)
                if regmodel=='OLS':
    
                    lm = LinearRegression()
                    ols = lm.fit(X_train,Y_train)
                    predictions = ols.predict(X_test)
                    predictions = pd.DataFrame(predictions)
                    predicted_df = pd.concat([predicted_df,predictions],axis=0)
                elif regmodel == 'Lasso':
    
                    model = Lasso(alpha = alpha)
                    model = model.fit(X_train,Y_train)
                    predictions = model.predict(X_test)
                    predictions = pd.DataFrame(predictions)
    
                    predicted_df = pd.concat([predicted_df,predictions],axis=0)
    
                elif regmodel=='Ridge':
    
                    model = Ridge(alpha = alpha)
                    model = model.fit(X_train,Y_train)
                    predictions = model.predict(X_test)
                    predictions = pd.DataFrame(predictions)
                    predicted_df = pd.concat([predicted_df,predictions],axis=0)
    
                elif regmodel=='EN':
                    model = ElasticNet(alpha = alpha, l1_ratio =0.3)
                    model = model.fit(X_train,Y_train)
                    predictions = model.predict(X_test)
                    predictions = pd.DataFrame(predictions)
                    predicted_df = pd.concat([predicted_df,predictions],axis=0)
    
            predicted_df.reset_index(inplace=True, drop=True)
            predicted_df = pd.concat([date_df,predicted_df],axis=1, ignore_index=True)
            return predicted_df

and here I apply the function:

    from tqdm import tqdm
    tqdm.pandas()
    
    OLS_predictions = regression_df.groupby('permno').progress_apply(lambda x: prediction_function(x,'OLS',window, n_factors, None))
    
    OLS_predictions.reset_index(inplace=True)
    OLS_predictions = OLS_predictions.drop(OLS_predictions.columns[1],axis=1)
    OLS_predictions.columns.values[1] = 'date'
    OLS_predictions.columns.values[2] = 'OLS_pred'
    OLS_predictions.reset_index(inplace=True)
    
    stock_data = regression_df.groupby('permno').apply(lambda x: x.iloc[window:])
    stock_data = stock_data.reset_index(drop=True)
    OLS_results = stock_data.loc[:,['permno','date', 'fwd_daily_exret']]
    OLS_results
    
    OLS_results = pd.merge(OLS_results,OLS_predictions, how='left', on=['permno','date']) 
    display(OLS_results)
    
    OLS_metrics = calculate_metrics_linear(OLS_results, 'fwd_daily_exret','OLS_pred', 'OLS_metrics', n_factors)
    display(OLS_metrics)

the calculation for the r2 is done using:

    def calculate_metrics_linear(df, true_variable_name, predicted_variable_name, dataframe_name, len_factors):
        r2= r2_score(df[true_variable_name],df[predicted_variable_name])
        r2_adj = 1-(1-r2)*(len(df[predicted_variable_name])-1)/((len(df[predicted_variable_name])-len_factors-1))
        MSE = mean_squared_error(df[true_variable_name],df[predicted_variable_name])
        rss = np.sum((df[predicted_variable_name]-df[true_variable_name])**2)
    
        df_metrics =  pd.DataFrame(data=[r2,r2_adj,MSE, rss])
        df_metrics=df_metrics.rename(index={0:'r2',1:'r2_adj', 2:'MSE', 3:'RSS'},columns={0:dataframe_name})
        return df_metrics
    

The r2 score I receive are:

OLS: -0.14

Lasso: -0.23

Ridge: -0.24

&#x200B;

But I always end up having negative r2 scores and I can not find the error. Did someone else ever experience something similar? Or am I having a mistake somewhere? 

I highly appreciate any help or advice! Thank you very much! And have an awesome day! 

Cheers,",AskStatistics,2022-08-17 16:28:31,6
"> do I use a one tailed test as I am testing for a decrease in counts, and not both an increase and decrease? 


You *can* use such a test -- there's no statistical problem with it. It's more a question of what would be acceptable to your colleagues, reviewers and journal editors, and I can't tell you what their expectations will be. Some areas of research have no problem when there's a clearly directional hypothesis and others seem to harbor a strong resistance to it (which might relate to a past history of abuse of it, or it might not).

I would suggest that if you have any way to pre-register your analysis before collecting the data, to attempt to do so, because it will protect you from accusations of p-hacking if someone suspects you chose the direction *after* seeing data.

I can't tell you what the reception to a one tailed test would be like in your area; only that statistically it's not problematic if (as it sounds) your hypothesis preceded the data

>  how do I set out ... what population group we are looking to have a lower count than?

It depends on the software you're using, which you don't mention.

> comparing oocyst counts 

Out of curiosity, is there any reason you're not using typical count models (e.g. negative binomial regression)? It would seem the more obvious thing to do.",5,wqvcqo,"I am comparing oocyst counts (malaria parasite eggs essentially) in two strains of mosquito and seeing if there is a statistical significance between the two data sets. 

In theory, we would expect strain x to have a lower count than strain y, so do I use a one tailed test as I am testing for a decrease in counts, and not both an increase and decrease? 

Furthermore, if it is one tailed, how do I set out what my “baseline” is, i.e what population group we are looking to have a lower count than?",AskStatistics,2022-08-17 10:50:07,15
"This is a bit confusing. You say correlation analysis but then also say you’re using regression analysis.

Correlation doesn’t have an IV/DV relationship, given its nature (e.g., lack of temporal precedence). You’re just measuring the association between variables. In theory you can consider one as ‘y’ and one as ‘x’, but the actual analysis makes no distinction. 

For regression, predictor (IV) and outcome (DV) are fine. Also saw someone mention predictor/criterion, which would work too.

You’re totally right to avoid IV/DV terminology. That’s typically used in experimental contexts where the IV is something you manipulate and the DV is an experimental variable of interest.",3,wqyzog,"I'm using regression analyses to explore the relationships between parental and peer attachment, resilience and academic motivation. 

I'm treating attachment as IV and resilience or motivation as DV, but I've been told that using IV and DV is inappropriate in the context of correlational analysis. 

How should I label them?

Thank you!",AskStatistics,2022-08-17 13:18:43,13
"""given that the distribution is binomial"" is not at all a clear statement, but I'll say that you think that each child chooses independently a Binomial(25,p) for some fixed p.

then ""what is the degree of freedom"" is not at all clear. There are no degrees of freedom in the Binomial distribution. Unless you're referring to trials, in which case you're sort of saying they flip 25 ""biased coins"" and take only those grapes corresponding to the heads on the coinflips... 25 trials.

perhaps you can reword your question to be more understandable?",7,wqx0nf,,AskStatistics,2022-08-17 11:58:27,9
"[This diagram shows a schematic.](https://imgur.com/F4zf1jI)  If I misunderstood the problem statement, you can make your own graph.


Here's a discussion of the possible effects.  There could be a difference between locations within a building; this is a fixed effect because it's simply the difference between rooms, and it's fixed because you cold go back to the same room and expect the same result.  The mean of building 1 could be different from the mean of building 2; it's another fixed effect.  You have a full factorial experiment within each location, meaning that you can determine the seven effects and interactions for distance, day and time of day.  You don't measure multiple replications at each time of day, so we don't know the random error of measurement.


[I built the model in JMP](https://imgur.com/cVuU9dt), I assumed that there are six rooms in building 1 and five rooms in building 2.  This is simply to generate random data in a file so JMP generates an ANOVA table showing the terms in the model.  That gives 264 readings for X.


I'm not modelling the location as a random effect.  If so, it would mean that the rooms within a building were chosen randomly, and a subsequent experiment to see the difference between buildings might choose different rooms.  


If the levels for distance, day and time of day are the same for all locations (and hence both buildings), then I see that the only nesting is location with building.  However, interactions with location are also nested within the buildings.


The model assumes you want to treat the time of day as representing random error.


[Here are the terms you would see in the model.](https://imgur.com/2HFFRHL)",1,wqfolk,"Hi all,

I have been given some data from an experiment and I am trying to figure out how to test several hypotheses.

The data are results for an assay for some substance, X, where samples were collected from a number of locations with a known source of X. The samples are collected at 3 distances from the source, with repeated sampling twice daily at 0900 and 1700 for 1-4 days. The locations (rooms) are nested within two buildings. The test quantifies X, but we are mostly interested in presence/absence for application of the test as a diagnostic, so I have been treating this DV as binary.

The hypotheses that we want to test are (1) test sensitivity to X changes over distance and (2) test sensitivity to X increases over time.

I think the assumptions of linear models are invalid here: non-independence of observations since we have repeated measures. I believe I would need to use a mixed-model of some sort to comment on the effects of time and distance on the test sensitivity but I'm not very familiar with how to execute this properly.

Can anyone please indicate how I would specify / interpret a mixed-model that accounted for the repeated measures on the same locations? Thanks!",AskStatistics,2022-08-16 21:40:24,4
I wonder if a multivariate T test might help with both aims at the same time.,3,wqe0rx,"Hi there,

I've just done a project measuring the respiration rate of some tomatoes (one batch of truss, one batch of cherries) in terms of carbon dioxide production (RR(CO2)) and oxygen consumption (RR(O2)), ethylene production (respiratory quotient (RQ), and R(C2H4)

I have all the results and am unsure how I want to do a data analysis. I'm thinking of t-testing two sample assuming equal variance of RR(CO2), RR(O2), RQ and ethylene production (R(C2H4) of truss tomatoes and cherries. 

I'm just not great at imagining data analysis in my head - what do people think? My main questions I'm trying to answer are a. If the 2 cultivars are physiologically different by any of the physiological measures (RR(CO2), RR(O2), R(C2H4), or RQ)

b. If any of the physiology indicators are related (correlated) to eachother.",AskStatistics,2022-08-16 20:16:29,2
"Depends on what you are comparing. You said you want to compare offices, so I would think it would be something like summing the hours spent on a type of task per office and dividing by the tasks of that type done at that office so you would have an average hrs/task for each type for each office. If you want something else then you would need to compare other things.

But this is more theoretical. I would be wary of doing any kind of calc like this on such a small sample size of 2-5 employees per office. At least do it over a longer term instead of one week maybe.",1,wqf5rq," 

1. Objective: Gain insight on how much manhour is allocated for each task across the different staff sections.
2. I got raw data of employees' manhours for a task at work with rows or lists of tasks per pay period (40 hours) and the columns have the employees with estimated hours that work on that tasks. For each office (there are five offices and each office ranges from 2-5 employees) Do I take the average hours of these employees for each task and total it up to analyze to compare the offices or do I take the average of these employees hours for each task and average it? I am trying to see what insight I can get for each of the offices of that particular tasks and if manhours are allocated accordingly",AskStatistics,2022-08-16 21:12:55,2
"Sometimes you’ll have an overfitted model where train loss is much lower than test loss. But as long as test loss of overfitted model is lesser than the test loss of another less overfitted model or an optimal one, it should still be okay.",3,wq8ocm,"Specifically in missing data imputation and propensity score weighting, a lot of the literature suggests creating large complex models as it will tend to minimize bias, ignoring the risk of overfitting. 

But what confuses me is how overfitting does not harm the quality of imputation or propensity adjustments. I assume the ideal situation is to produce a model that is most generalizable as that would give you the best chance of calculating the true value of what you are modeling? Is this not that case?

Can anyone explain the intuition behind this specific situations?",AskStatistics,2022-08-16 16:12:03,4
Fivethirtyeight.com has explored this idea. You might dig through their archives and see their sources. Look for multiple sources and multiple formulas.,2,wqd8vw,"I have all SCOTUS votes since 1946. And I know, for any 2 justices, how many times they either voted the same or voted differently. I also have the votes grouped by “court”. (EX: Second Rehnquist Court.) I have herd of a formula to line them up by who they vote like and ideological leanings. I know the formula has many other uses. What’s that formula?",AskStatistics,2022-08-16 19:39:45,1
"I doubt it would make any difference that could be discerned. 

I expect it's a standard pseudo-random number generator. Good ones are carefully checked for differences of this kind -- as well as for many other aspects of their behavior; there's batteries of tests that are performed on very long sequences of numbers from those kinds of things. I'm quite confident that the google one will at least meet that standard (e.g. that a relabeling of the outcomes would make no detectable difference to the properties you might care about).

So there should be no difference (or at the worst, no practical difference, you might get rounding-error type effects that could make a difference that you wouldn't detect in a million lifetimes of daily dinner choices).

Any difference you do feel you see in the characteristics of the performance one way or the other will almost certainly be perfectly explainable by the kind of difference you'd see with random variation.",2,wqcmgc,"My wife and I were discussing what to have for dinner and we decided on two separate options. We couldn’t agree on which option we wanted so we decided to select by random number generator. We have a number set of 1–10, but I was wondering what would give a more “random” answer.

1 –5 for option one and 6–10 for option two.

OR

Even numbers for option one and odd numbers for option two.

We’re just using the generic random number generator from Google. I’m inclined to say they are both just as random but I’m pretty sure I’m wrong. Thanks!",AskStatistics,2022-08-16 19:10:38,2
"> how did X-bar/R charts become more prevalent than BH&H's example?

Mainly, control charts are about statistical control of manufactured product and BHH is about hypothesis test of experimental effects.  In manufacturing, keeping things in a state of control is good for financial performance, so every process needs to be kept in control.  Some processes may not be in control, or may have poor capability, in which case experimental improvement would be appropriate.  There are fewer processes that need to be improved than there are that just need to be maintained.


Control charts aren't a hypothesis test, they exist to give indication of a process that is out of control.  More precisely, they aim to achieve an economical rate of error of over-reacting or under-reacting to variation in series production.


BHH also works in the context of series production, so to that extent they have they same audience as control charts.  But, BHH wants a hypothesis test giving statistical significance to the magnitude of an effect.  Control charts only need to use statistical ideas to decide if the process has changed, but not how much it changed and with what significance.


BHH's reference distribution idea is interesting, it really helps put practical thinking into engineers' understanding of experimentation without resort to inferential statistics.  


> How would BH&H's example in section ""A significance test using an external reference distribution"" different from X-bar/R-charts


For those that are following along, we're looking at page 34 of the first edition of 1978.


The example states that there are two groups of data where an experimental treatment was changed between samples, each sample size 10, and that there is a mean difference between the groups.  The goal is to test the hypothesis that the mean difference is statistically significant.


There are 210 readings available from series production just before the experiment.  This gives the opportunity to see how common it is for that observed difference to occur in a distribution from ""a standard process, no modifications having been made over that period."" BHH calculates subgroup averages from data that was gathered before the change.  However, the subgroups are moving.  210 readings give 201 subgroup averages.  


Differences between adjacent subgroup averages are calculated and tabulated, there are 191 of these.  Then, without resort to a distribution model, BHH counts the proportion of differences that exceed the observed difference.  


We could analyze the BHH data with Xbar-R.  Combine, in time order, all 210 readings, plus the 10 new readings at the baseline condition and the 10 new reading at the test condition.  Construct Xbar-R with subgroup size 10.  This leads to 23 subgroups.  The R chart can tell us if the process was stable, meaning the variation was in control.  The Xbar chart would tell us if any of the subgroups means seems to be much different from the grand mean.


If the last subgroup mean is out of control, then we'd ask what might cause it.  However, we know that an experimental factor changed at that point in the process, so we'd be inclined to conclude that the factor had some effect.",2,wqc5vy,"This may be a question for those that are older that would have used the Box Hunter Hunter ""Statistics for Experimenters"" from the 1980s.  I've been reading it because it was my mentor's go-to text back in the 90's.  How would BH&H's example in section ""A significance test using an external reference distribution"" different from X-bar/R-charts? More so...how did X-bar/R charts become more prevalent than BH&H's example?",AskStatistics,2022-08-16 18:49:13,3
"It's not what you're asking about but I'm concerned about the thinking that lies behind such framing of the question. Specifically this:

> Given an infinite population size 'N', previous studies have shown that a random sample taken from this population accurately represents the entire population 80% of the time.

> The other 20% of the time, this sample taken is incorrect.

I don't follow what either arm of this quote actually means. what does ""accurately represents the population"" mean, operationally? What does it mean for a sample to be ""incorrect""?

Note that if you're randomly sampling a population, most samples will be unlike the population *in some aspect*, to *some degree*, that's the nature of random sampling. If you remove/ignore/separate off the ones that look unlike the population on some criterion, you no longer have random sampling, and if the properties of whatever you're doing rely on random sampling any consideration of representativeness will screw up the properties. e.g. if you're doing inference, the frequentist properties of that inference won't work as they were designed to.

[in short, if someone's trying to do something like this, they're almost certainly going to be doing it wrong.]


---

Can you clarify what ""accurate representation of the entire population"" means?",2,wqbs4d,"Hey r/AskStatistics,

Just wanted to know if I could apply Slovin's Formula to this issue I'm attempting to solve:

Given an infinite population size 'N', previous studies have shown that a random sample taken from this population accurately represents the entire population 80% of the time.

The other 20% of the time, this sample taken is incorrect.

&#x200B;

Could Slovin's formula be applied in this scenario to determine the minimum sample size (n) needed to have accurate representation of the entire population 95% of the time? (95% confidence interval).

I am coming up with n=25 but am unsure if my logic is correct in this scenario.

&#x200B;

&#x200B;

https://preview.redd.it/exr05jvfm6i91.jpg?width=500&format=pjpg&auto=webp&s=e7c9939093e19c2db3d90899d44fd3a6a53069de",AskStatistics,2022-08-16 18:31:15,5
The first thing that occurs to me: people who are at higher risk to die for any reason are also probably more likely to get vaccinated (because they are at higher risk to have bad outcomes from COVID).,29,wpt21z,"May 2022, all-cause mortality 40-49

Unvaccinated: 64.1/100K
Vaccinated 2 dose: 106.4/100K
Vaccinated 3 dose: 83.7/100K

May 2022, all-cause 50-59

Unvaccinated: 290.6/100K
Vaccinated 2 dose: 420.7/100K
Vaccinated 3 dose: 332/100K

Data source:

“Deaths occurring between 1 January 2021 and 31 May 2022 edition of this dataset”

https://www.ons.gov.uk/peoplepopulationandcommunity/birthsdeathsandmarriages/deaths/datasets/deathsbyvaccinationstatusengland",AskStatistics,2022-08-16 05:28:09,14
"Its just fancy counting. It's just a matter of counting all combinations and then which ones contribute to a win 

Note that there are 36 possible values for each member of the pair so for each of the (36 possible)  first draws there are 36 possible second draws. 

There are two different  probabilities for scenario 2, since choosing two distinct numbers now covers twice as many combinations as under scenario 2 but choosing a repeated number still only covers one",2,wq2djy,"Scenario 1: 

There is a pool of 36 numbers (1-36). To play you picks two numbers from the pool. you can pick the same number twice, eg. (2,2) is a valid pick. during the draw two numbers are selected from the same pool. the same number can be drawn twice, eg (5,5) is a valid draw. To win, your numbers must match the drawn numbers in the same order, eg. if (1,2) is drawn then (2,1) is not a winner. 

&#x200B;

Scenario 2: 

There is a pool of 36 numbers (1-36). To play you picks two numbers from the pool. you can pick the same number twice, eg. (2,2) is a valid pick. during the draw two numbers are selected from the same pool. the same number can be drawn twice, eg (5,5) is a valid draw. To win, your numbers must match the numbers draw however the order is not important, eg. if (1,2) is drawn then (2,1) is a winner.",AskStatistics,2022-08-16 11:55:01,1
"Do you have a typo in the title? I'm pretty sure the equation is not true, unless you meant E(X|X+Y=z)+E(Y|X+Y=z)=z",3,wq0cbt,"I'm not sure how to even go about deriving this.

I'm assuming it relates somehow to the fact that f(Z=z) = f(X=x)*f(Y=y) but I don't really even know how to interpret the conditional here.

Is it just saying that E(X)+E(Y) = E(X+Y)?

What exactly would the algebra look like to simplify E(X|X+Y=z) to this look like?",AskStatistics,2022-08-16 10:33:24,9
">  wondering wether a pearson or spearman correlation is appropriate

If you're interested in monotonic association, Spearman (or a number of other rank-based correlations) would generally be better. If you're interested in linear association, or in measuring the linear part of some association, Pearson would be better since that's what it measures (though if you add suitable assumptions - such that the association is specifically linear rather than nonlinear but monotonic - Spearman may also be suitable to measure it)

Many relationships are not linear, particularly (as with some subscales of some particular questionnaires) where the values tend to accumulate more toward one end of the scale.

Note that for questionnaires which simply add Likert-like items, the individual items are effectively assumed to be interval-scale when you add them.

If you're interested in *testing* linear correlation (you didn't mention it), there may be some additional considerations for the Pearson, but they should be surmountable.",2,wpt7p3,"Hi, completing my masters in sport and exercise psychology and currently writing up my dissertation. 

I'm investigating the relationship between perceived coach leadership behaviour and coach-athlete relationship quality and athlete satisfaction. 

I've used three questionnaires, CART-Q, LSS and ASQ, all three of which have multiple sub-scales. I'm just wondering wether a pearson or spearman correlation is appropriate? Similar studies to mine have used both and I'm unsure why.

Thanks",AskStatistics,2022-08-16 05:35:43,2
"The conditioning is different in your two scenarios, and therefore you'd also be answering different questions.

You need to unambiguously define the question you're trying to answer. Either scenario (or neither!) might be appropriate, depending on what you're trying to achieve.",1,wpq64m,"Hi!
Stuck on thinking through a work problem and can't find anything to answer this question.
All help is appreciated.
  
I have a group of customers who have placed 1 - 2 orders. They'll be split by if they brought product A or B in each order.  
If I want to calculate lift of B in the second order following A in the first order, is there any positive or negative to including the customers who have placed only one order, that order containing A?  
  
Effectively should my universe be everyone who bought A in their first order, or only those who bought A in their first order and have placed a second order?",AskStatistics,2022-08-16 02:54:10,3
"If their choices are independent, the number of players doing x is binomial(n,p).

If you don't know the value of K, you need to at least know its distribution (and whether it's also independent of the players choices about x vs y)",1,wpn6uf,"Hey this might be a weird question. I have N players that choose to do either x with probability p or y with probability (1-p).

There is a random number K which is lower than N. 

Is there a way to calculate the probability that more than K players do x?",AskStatistics,2022-08-15 23:48:23,4
"I don't think *balance* is particularly a problem here. In particular, I don't think it leads to bias in estimates. It's possible I've misunderstood something about your setup where I've missed something that would cause a bias, but you don't typically have balance over predictors in regression; some combinations of variables will happen much more often than others quite naturally. This isn't of itself a problem.

If you had a controlled experiment where you were allocating a fixed number of observations to different combinations of conditions, you seek balance then (for a couple of reasons, including efficiency), but with observational data, you *don't* benefit if you throw away data to obtain balance.",5,wp6wgo,"Hello all!, 

I am working on a project to create some models to discover the relationship between two quantitative variables: visibility and precipitation. So I have a massive dataset (over 37 million observations) similar to this:

| visibility | precipitation | temperature | wind_speed | sky_coverage |
|------------|---------------|-------------|------------|--------------|
| 10         | 0             | 65          | 18         | clear        |
| 10         | 0             | 63          | 18         | clear        |
| 10         | 0             | 64          | 8          | few          |
| 10         | 0             | 65          | 8          | clear        |
| 9          | 0             | 65          | 7          | scattered    |

When inspecting the histograms of the two main variables (visibility and precipitation) I discovered that they are really unbalanced. In visibility, the 85% of the data have the maximum visibility value which is 10. And in precipitation around 90% of values are 0.

**How do I deal with such an unbalanced dataset in regression?** 

As far as I know, leaving the dataset unbalanced would lead to models that have poor precision in classification problems. But what about regression? I am not sure wether over-sampling would be appropriate as I would need to add a massive amount of observations to balance the dataset. I also thought about under-sampling but am afraid of losing important insight when doing it randomly, as I would be dropping millions and millions of observations.

Apologies if the answer seems obvious, I am pretty new to the statistics world and could not find many resources about unbalanced datasets in regression. Thanks in advance for your help!",AskStatistics,2022-08-15 11:24:22,3
"You need a probability of getting a rejection at that effect size (your desired power).

This will be a two tailed test?


I should warn you - the required sample size for this effect size will be high (even if the power you want is modest)",1,wpdasv,"I'm about to start doing a research study on gastrointestinal surgical site infection (SSI) rates comparing a standard laparotomy to the use of a wound retractor device. Before I start collecting data I need to know if this study will be feasible to do in the timeframe I have. 

If the SSI rate in my hospital is 6%, and the reported SSI rate of the wound retractor device in question is 4%, what is the minimum sample size I need for a p < 0.05? 

Please help, I suck at stats!",AskStatistics,2022-08-15 15:42:32,15
All of the comparisons you list are independent. Nothing there is paired.,3,wp9kui,"So Im unsure of which tests to run in these scenarios:

So Im working with mice. There are male and female mice that are untreated (control), and male and female mice that are treated. There are also age groups

**1)** Compare all treated mice and all untreated mice (regardless of gender)

**2)** Compare treated male-mice vs treated female-mice

**3)** Compare by age:

Embryonic treated mice vs embryonic untreated mice

Adult treated mice vs adult untreated mice 

&#x200B;

When would I use a paired t-test vs an independent t-test?",AskStatistics,2022-08-15 13:12:00,7
"I think you'll need the correlation between time point 1 and 2 as well. I think the formula is this:

> SD(2) = SD(1)\*rho + sqrt(SD(diff)^(2) + SD(1)^(2)\*(rho^(2) - 1))

where rho is the correlation coefficient and sqrt the square root.",2,wp3v5s,Hey all - I have a mean (SD) of test scores at time point 1 and mean but not SD of test scores at time point 2. I also have the mean and standard change in test scores between time point 1 and time point 2. Can this information be used to find the SD of test scores at time point 2?,AskStatistics,2022-08-15 09:23:57,2
"You can only work out how much the percentage would change if you knew what the percentage chance would be under the other condition (not having a sibling die by suicide). 

However, if that percentage chance was really small then an odds ratio of 3 would roughly triple the probability (e.g. if it was 1% without sibling suicide the probability would go to just under 3% with it). If it was not very small in that other condition, then it would do somewhat less than triple (and the larger it was in the other condition, the less the increase would be to change  into the condition).

e.g. if the percentage chance was say 10% without having a sibling die by suicide then the odds of that case are 0.1/(1-0.1) = .1/.9 = 0.1111 and the odds ratio takes the odds with sibling-suicide to 0.1111*3 = 0.3333 = p/(1-p). So p=.3333/(1+.3333) = 0.25, or 25%, which would only be 2.5 times as big

If the percentage without  was 20% then the odds without  are 0.25 and the odds with would be 0.75, so the probability would be 0.75/(1.75) = 3/7 = 42%, or 2.1 times as high a chance. 

and so on; if you know the chance without, you can compute the odds without, and hence the odds with (by tripling) and hence the chance with.

Or if you prefer you can combine it all into one calculation:

p2 = o2/(1+o2) = 3 o1/(1+3 o1) = 3 (p1/(1-p1))/[1 + 3 (p1/(1-p1))] =  3p1/[1 - p1 + 3p1] = 3p1/[1+2p1]

or even slightly simpler, p2 = 3/[2 + (1/p1) ]

You can check for yourself that if you take p1 = 0.1 that this gives p2 = 3/12 = 0.25 just as we calculated before.",4,wpcker,"It's giving odds (3x) but I don't really understand how that impacts to an individual person. In this case, what i need to know is: ""A woman whose sibling died by suicide is x% more likely to die by suicide."" is that even possible with the data in here?

[study](https://bmjopen.bmj.com/content/3/4/e002618)",AskStatistics,2022-08-15 15:11:56,1
"I suspect that it may be a question about multiple testing correction, specifically Bonferroni.",3,woxdqk,"[https://ibb.co/rtrYckM](https://ibb.co/rtrYckM)

I'm going through a problem sheet I found online and it's going well except for this question, as far as I know this has nothing to do with the significance level... 

I'm trying to think of it in terms of balancing type I and type II errors. It is essential that the xbar\_i are at least as high as xbar\_1 so you want to avoid making a type II error so that you don't accidentally make the conclusion that the xbar\_i are equal when they're not, this would correspond to a relatively high alpha. 

It still provides no indication as to which alpha should be chosen specifically, and to be honest I would just stick with 0.05 if I ran into this problem in practice. Thoughts?",AskStatistics,2022-08-15 04:38:15,1
"Why would you assume 5?

df = (# rows - 1) x (# columns - 1)

Here, that’s 2 x 2 = 4. 

Given the marginal distributions, it’s the number of cells that are free to vary.",7,wolugx,"Hello,

I've conducted a chi-square test, but I'm struggling to understand the results. I have three language groups, and three phrasal verb groups so I would assume the degrees of freedom would be 5, not 4... why is it 4?

https://preview.redd.it/xlmc09jzzrh91.png?width=890&format=png&auto=webp&s=844eca2f2bc6dafcb5237347e2599c9a4ea525aa",AskStatistics,2022-08-14 17:58:23,5
"You would have 2 for A and 2 for B. they could be 1,0,0 and 0,1,0. Contrast coding could be -1,0,1 and 1, -2, 1 or 2,-1,-1 and 0, 1,-1. 
Modern stat programs let you specify that your variables as factors (or nominal, or class variables) and do the coding implicitly. Specific comparisons or range tests such as Tukey hsd can be done on marginal means.",2,wols95,"Hi!   
I was wondering if anyone can give me a quick hand to refresh my memory. How would a regression model design using indicator variables write out (factor A has three levels, and so does Factor B), with no interactions.",AskStatistics,2022-08-14 17:55:24,1
"Computing Pearson's correlation coefficient doesn’t require normally distributed data. The classic test for the correlation does (IIRC it actually assumes multivariate normality), but you can use a different one, e.g. permutation test or just bootstrap it.


> I tried using spearman's correlation, which doesn't assume normal distribution, but the significance level of two of the six correlations was over 0.4 (thus not usable for research).

You shouldn’t choose tests base on how favorable the results are. What you are doing is textbook p hacking.",22,wo8xau," I have 4 variables I want to create a correlation matrix with. The problem is that two of these variables have non-normally distributed data, one of the assumptions of pearson's correlation. If I run the correlation matrix either way (assuming they are normally distributed, when they aren't), pearson's correlation is significant on a 0.08 level across all correlations. Can I use the pearson correlation either way, given that I have a 0.08 level of significance?

I tried using spearman's correlation, which doesn't assume normal distribution, but the significance level of two of the six correlations was over 0.4 (thus not usable for research).",AskStatistics,2022-08-14 08:24:25,10
"Check out this: https://marlab.org/power_mediation/

They have an R Shiny app that can be run online or locally.",1,woiq46,"Hello, I’m trying to run power analysis and write a paragraph about it for a grant application. 

I have a small dataset <300 individuals. The DV is continuous fitted in regression model using SEM. So my question is:

Do we just need to run post-how F test on multiple regression to get power value based on the model r2, number of predictors, alpha value and the existing sample size? 

I have searched for sample grants, but it seems what I found is people either have big sample size or people just do the priori test to get the sample size required given the .8 power and commonly found effect size.",AskStatistics,2022-08-14 15:32:37,4
"Depends on the research Question and the data on Hand. If u want to analyse the effect of a independend variable (e.g. Tv time) on a dependend variable (e.g Aggression), than a regression model would be suited. If u want to compare means, than a t test would be best. 
There Are different regression Models (simple, multiple, logistic, hierarical etc.) as well as different t tests (one sample, unpaired two sample etc.). Each analysis need a specific Type of data, e.g a one sample t test need two means per participant like the dopamin level for two different Times. A unpaired two sample t test on the other side needs two seperat groups e.g. The dopamine Level of Group A (Women) and Group B (Men). 

So ask yourself, what do u have, what do you want to find out, and which analysis allows this.",1,wocm7p,"My course is on ANOVA and regression model.
It seems that I mostly use Univariate/ Multivariate/ Repeated Measures/ T tests… (along the lines of ‘compare means’ and ‘general linear model’, I suppose.)
But I’m not getting which one to use in which cases.

I only know which variable is fixed/ random and repeated/ non-repeated measures…

Please help me! 🙏",AskStatistics,2022-08-14 11:05:38,2
"No.  Bayes makes sense whatever the prior probability is, regardless of whether it is based on evidence.  But even if it is based only on evidence (the result of a previous Bayesian calculation), different people can have different evidence available.  And all Bayesian calculation has to start somewhere.  Or were you thinking of an infinite sequence of evidence going back into the past?   Even if you had that, it would not be clear how to apply Bayes rule to that.  So to be a Bayesian you have to come up with a prior to use when you obtain your first piece of evidence (data).  Then after that, you use the posterior from your last calculation as the prior when you obtain more data.",5,woai14,"I've just picked up a Bayesian statistics book (Bayesian Methods for Hackers) and I'm admittedly a beginner in this. 

&#x200B;

In the first chapter, the author differentiates between *P(A)* and *P(A | X)* by saying that the latter is ""interpreted as the probability of A given the evidence X"" ([link to Ch1](https://nbviewer.org/github/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/blob/master/Chapter1_Introduction/Ch1_Introduction_PyMC3.ipynb)). And checking out Wikipedia, it seems to agree with him: ""*In Bayesian statistical inference, a prior probability distribution, often simply called the prior, of an uncertain quantity is the probability distribution that would express one's beliefs about this quantity before some evidence is taken into account*."" ([link to wikipedia](https://en.wikipedia.org/wiki/Prior_probability#:~:text=In%20Bayesian%20statistical%20inference,%20a,evidence%20is%20taken%20into%20account)). 

&#x200B;

But this doesn't make sense to me. Doesn't everyone base their prior probability on evidence?",AskStatistics,2022-08-14 09:33:43,7
"Yes and no. The exact percentage depends on your distribution. In particular, yours are for normally distributed random variables. 

On the other hand, you can derive lower bounds left outside the range that are linked with various higher moments of ANY distribution (assuming they exist). The most famous are Markov inequalities and it's variant proved for the second moments has a name chebyshev inequality. According to it, within k standard deviations around the mean you expect to find at least 1-1/k^2 of the observations.",9,wo3zl1,"I’ve been diving into this today and I find it very fascinating. But I can’t help but have some doubt as to whether this can be relied upon as some sort of constant. Like pi which can be derived regardless of scale/arbitrary measurements. So anyways, given a set of numbers, say 50 numbers in a set of data, will ~68% of that data always fall into +/- 1 standard deviation from the mean of the 50 numbers? And then the ~95% for 2 standard deviations and so on? Thanks.",AskStatistics,2022-08-14 04:16:15,6
I write a short article on that. It is open access and available [here.](https://www.tqmp.org/RegularArticles/vol12-2/p114/p114.pdf),0,woci5b,"In what case should I use Huynh-Feldt? I'm not understanding the question asking ""if you need to
make a correction for departure""…",AskStatistics,2022-08-14 11:00:48,1
"It will depend of many factors, including your undergrad and whether you already have work experience and what kind

For people new to the field yes a degree will be very useful. Yes you can learn stuff online but how will you put this on your resume for recruiters to see? 

Many recruiters are very afraid of hiring the wrong person, and a degree makes many recruiters much more comfortable hiring you.",3,woa687,"I win a scholarship, I have the opportunity to begin a master  degree in data science, but I don't know if this master degree  is good because a lot of things in this area you can find in internet or I can make internet course and learn almost the same in less time, another thing that I see is that a lot of companies don't see if you have a master degree or not only look your experiences, one thing good that I see if I do this is a big step in my personal career to be a better professional, I am a statistician.",AskStatistics,2022-08-14 09:19:21,4
"Sounds a lot like a 1-dimensional random walk. I would start by googling that to get you pointed in the right direction. Actually, I think there’s a special kind of random walk called the “gambler’s ruin.” You can try googling that, too.",1,wo8yuv,"I am playing a game. I am generating random numbers 1 to 1000. If the number is 1 to 495 that is considered a win of $1. If the number is 496-1000 that is considered a loss of $1. I am done generating numbers either when I finish 17 games OR as soon as I enter a net loss of $1 (this can be as early as the first game). What is the probability that I will complete the 17 games with a profit of money? Bonus points for the probabilities of me finishing the 17 games with a profit of each amount (+$1, +$3, +$5, +$7, +$9, +$11, +$13, +$15, +$17).",AskStatistics,2022-08-14 08:26:26,5
"Consider the set {-1, 1}",4,wo8ref,"[Solved] I just started to learn statistics and wondering about standard normal distribution. Standard normal distribution has 0 mean and 1 std. How is it possible? If a dataset has 0 mean, that means every data just 0 and the std surely also 0 right? Can somebody help me with this?

Thank you guys for your answers, I got it now",AskStatistics,2022-08-14 08:17:13,4
"Tukey’s outside values were never intended to be considered outliers to be removed from the analysis. Criteria based on the median absolute deviation are generally better. I’m not a financial analyst, but my naive sense is that the most money is made and lost on outliers. I would remove them with caution.",1,wo8bpz,"So I am currently having the problem that I need to filter out outliers based on their market returns. The problem is that I can identify outliers on the upper bound (using the IQR method), but due to the formula, (price(t+1) - price(t))/price(t), market losses are capped at -1, which leads to me not being able to identify outliers on the lower bound because of the cap and those values not being able to be smaller than the needed Q2 - 1.5IQR. 

Has anybody an idea how to solve this?",AskStatistics,2022-08-14 07:58:27,3
"The df would be 13 for group, 4 for time point, and 52 for the interaction. Total df = N-1. Dfe = N-70.",2,wo8axk,"Hi Reddit,

I'm a little confused regarding an a priori analysis for a tumor mouse model.

We will have 14 groups of mice, each with a different kind of tumor implant (control included). The mice are going to be sacrificed at 5 different time points and different gene expression levels in endothelium measured by RNA seq. The goal obviously is to find common upregulated genes between the tumors.

* **My test choice:** ANOVA fixed effects, special, main effect and interactions (because we have one dependent variable (upregulated genes) and two independent variables influencing each other (tumor and time point))
* **Groups:** 70
* **df:** This is where I'm stuck. Would it be 69 (total groups - 1)? Or 13 (tumor groups - 1)?",AskStatistics,2022-08-14 07:57:27,1
"See rule 2 (https://old.reddit.com/r/AskStatistics/about/rules/ if you're on a desktop/laptop), in particular this part:

> This subreddit is to answer questions asked here about the theory or practice of statistics, such as questions about statistical analysis. It is not a place to get/give random demographic, economic etc facts,

If you only use new reddit you'll need to expand (clicking the arrows) from just the short form name of the rule.

In any case I strongly doubt anyone has worldwide data on all forms of trauma on the same people.",1,wocvey,"The ICD-11 lists 1.6 million terms in reference to disease or trauma that required professional treatment (accidents, poisoning, etc.). Naively it appears to me that no one could expect to live a year without being subject to disease or other serious trauma. Has there been any work done on these ""simple"" statistics?",AskStatistics,2022-08-14 11:16:41,1
"Hello! Before you start considering more complicated alternatives, I would recommend taking a step back. In your summary for part 1 you state the following:

    Data do not follow a Poisson distribution (underdispersion). Poisson regression cannot be used as it requires equidispersion (mean and variance are equal), and negative binomial regression is only appropriate for overdispersion.

What exactly do you mean by ""the data do not follow a Poisson distribution"". I suspect you are just looking at the mean and variance of your dependent variable Y, simply based on it's marginal distribution. However, the assumption for Poisson regression is that the ***conditional*** mean, E(Y|X), is equal to the ***conditional*** variance (i.e., mean/variance of Y conditional on the predictors, X). To start, you could conduct a Pearson goodness of fit test to determine whether there is evidence of overdispersion/underdispersion. If you find there is a statistically significant lack of fit due to underdispersion/overdispersion, then a quasi-Poisson model is one alternative you could consider.",2,wnvse2,"Research Help! Underdispersion Issue.

 Data Question - Underdispersion - Research

 

Hello everyone, I'm sorry if this is not the right place for this question. I have been looking for the best subreddit to post this under in hopes of some help, so please point me in the right direction if this isn't the appropriate place.

I'm a PhD candidate working on my dissertation. I've been stuck for about a month on a particular data problem, and I cannot figure it out on my own. I've been trying to contact the statistical support team available to graduate students at my university, and I keep getting ignored (currently CCing everyone in the department to see if it helps). I've found some answers through academic articles and some youtube videos, but it leads me to a second problem.

Part 1. I work with SPSS. I'm working with count data that is under-dispersed. The mean is larger than the variance (M 1.64 & V 1.46). Data do not follow a Poisson distribution (underdispersion). Poisson regression cannot be used as it requires equidispersion (mean and variance are equal), and negative binomial regression is only appropriate for overdispersion.

Part 2. Academic articles tell me that the Conway-Maxwell-Poisson distribution and Generalized Poisson regression can be used with under-dispersed count data, but I cannot find information anywhere about how to do this using SPSS.

Does anyone know (or can point me to any sources) how to address underdispersion using SPSS?

Is there a cutoff point for underdispersion? The difference between my mean and variance is very small (M 1.64 & V 1.46). Would it be appropriate to use Poisson regression with these data?

Thank you for taking the time to read this and for any information you can provide!",AskStatistics,2022-08-13 19:58:51,1
"How big the data are? Because if it’s just a vector of integers, even with 50 million values, both R and Python can run parametric tests in a fraction of second. No need to dive into C++ yourself.


A better question is, what exactly do you mean by ""test their randomness""?",3,wo249p,"I looked into multiple things :

 

* L'Ecuyer's [TestU01](http://simul.iro.umontreal.ca/testu01/tu01.html) (with SmallCrush, Crush, BigCrush),
* Doty-Humphrey's [pracrand](http://pracrand.sourceforge.net/) with its PractRand suite,
* NIST 800-22 A.

but I have no experience with C or C++ so I don't know what to do with this. also, I read the user guide or TestU01 and I still have no clue what to do. what should I do? are there any good guides? 

should I spend a week understanding the basics of C and C++ . I do know how to program in other languages. So, I think I would be okay investing sometime into it if that will help.",AskStatistics,2022-08-14 02:18:01,9
"(Partial) eta squared is actually just (partial) R squared, i.e. what most people know as the coefficient of determination. It’s a proportion of variance of the dependent variable that can be predicted by the independent ones. Presumably, if two scales measure the same construct, they should be highly correlated and the eta squared should be closed to one. So in that sense, it works as a validity measure. But I don’t know if there are any agreed upon thresholds of which values are sufficiently high.",1,wnxbtv,"Hi I’m trying to determine the construct validity of a measure and have found a study which when comparing the measure to related and unrelated measures used partial eta squared as their measure of construct validity. 

I’ve not seen this done before so am wondering if that’s conventional and ok to do? And if so how do I interpret it when trying to determine if it shows that say convergent validity is say poor, good, excellent ? 

If you’d know a reference for that too would be great. Thanks!",AskStatistics,2022-08-13 21:22:20,1
"The usual tests would be a two sample proportions test (two sample z-test) or a 2x2 chi-squared test (homgeneity of proportion, equivalently independence test); these two tests are equivalent. There are other tests that might be done instead.",1,wnx9f1,"Hey everyone! Just discovered this subreddit. It's been a while since I have taken statistics, but I think I have a pretty straightforward one that I have no idea what test to use to compare. I am trying to compare 24 week retention rate between two groups. Retention rate of group 1 was 115/286 (40%). Retention rate of group 2 was 96/283 (34%). Is there significant difference between the two? Thank you so much!",AskStatistics,2022-08-13 21:18:41,1
"This does not make any sense because you have not named a random variable or a probability distribution.  Those are the things that have expectation.

For example if X is a random variable having the [first of two possible definitions](https://en.wikipedia.org/wiki/Geometric_distribution), then it does have expectation 1 / p.  But for the other definition, it does not.

Was that what you were talking about?

The part about logs I really did not get.  You want the expectation of what function of X?

Edit: meant to say X has geometric distribution (which is where the link goes).",1,wntfn3,"I have a personal project I'm working on, the purpose being to do some statistical calcs for determining average time for a result to happen etc. But in my own mind I'm getting confused about some things. I did some searching online, but I'm not quite sure of what terms I need to search for to get the info so I can learn.

Given a success chance of P (between but not equal to 0 and 100%), I assume the expected value (average?) of the number of attempts in order to reach success would be 1/P.

However, what about log(X,1-P)? (i.e. 1-P is the base, with X being some number between 0 and 1) Is this something similar to calculating the number of attempts that 1-X percentile of the sample size population will go through? Like saying the 99th percentile (very poor luck) would achieve success in log(1-0.99,1-P) attempts on average?

Also, not sure if this affects anything or not, but only the first success matters. The success rate would change and the calculation is then repeated with a new number etc.

I had thought about both of these for quite some time and thoroughly confused myself (thinking that the 50th percentile should be equal to the expected value etc. when it doesn't equal it in reality). Would it be correct to say that the expected value is the overall average, where the latter is the average for specific subsets of the population? And is there a particular percentile where it should equal the expected value?",AskStatistics,2022-08-13 17:57:29,4
"> My question is: should I be including the model failures in my denominator when I calculate power? 

It doesn't make much sense to include them, in my view. By including them in the denominator, you're counting them as de facto not significant. This may be viewed as a very conservative approach. But you have the ability to just run more simulations so I don't really see any benefit to including them.",1,wnzt7j,"Hi everyone,

I'm trying to run a power calculation by simulation on a set of exponential decay datasets using the nlme package in R. Here's the process:

\- Simulate a bunch of exponentials, using some conservative parameters with a small effect size and some noise

\- Run a nonlinear mixed effects model 1000 times each at different sample sizes, and record p-value

\- Calculate the fraction of models that have a significant decay coefficient at significance level of 0.05

However, one small issue: sometimes the model fails to run in R, generally due to convergence issues. There's a few different errors that I'm encountering. Every time the model fails I simply rerun it until the model runs correctly.

My question is: should I be including the model failures in my denominator when I calculate power? 

Option 1: power = significant/(significant + not\_significant)

Option 2: power = significant/(significant + not\_significant + model\_failures)

Thanks a lot!",AskStatistics,2022-08-13 23:50:43,1
"I think that Fundamentals of Data Visualization is a great resource.

https://clauswilke.com/dataviz/",13,wni86u,"Hi all, I wanted to know if anyone knows of any websites / books / etc. that I can consult to improve my data visualizations, making them more aesthetic, thinking mostly about publishing those visualizations on social networks. I can use R to produce most graphs and visualizations, or even Photoshop or other design tools if needed, my problem is more the ""inspiration"" (if for example I see a chart I like or some well presented data I am able to copy 1:1 the original, my problem is to have that idea). In case there is no resource like the one I am looking for, websites with examples of visualizations would also be good to have a base to start from.

Thank you very much in advance!",AskStatistics,2022-08-13 09:15:19,10
"Sounds like you are thinking in terms of rates (goals per unit time). Sounds like a perfect application for a Poison distribution where you would want to compare the rate parameters.

There is probably an out-of-the-box procedure that I'm sure you could Google in just a couple of minutes.

How I would do it without Google is with a likelihood ratio test. Null hypothesis is that the rates are the same and the pooled rate is just the sum of goals divided by the sum of times. For the alternative hypothesis, the rates are assumed independent and the MLE for each is just goals / time for each player.

You reject the null hypothesis when this ratio is bigger than some constant and you work out the math so the probability of that happening is your desired alpha level

EDIT : perhaps I misread your question. In fact your question seems very simple since you know both rates are 0.8. So just multiply that by 3000.

If you want to consider a range of values, note that the variance of a Poison variable is the rate paragraph  (same as the mean), so you could work with that. i.e for 10,000 time units you'd have expected mean and variance of 8000. You could provide a range of  mean +\- 3 sigma, which is 8000 +\- 3  * 89",2,wno3z8,"Player A and Player B both score a goal on average 0.80 per 90 minutes, but player A has played 7,000 minutes and player B has played 11,000 minutes.

How can I work out what Player A would score if he were to play an extra 3,000 minutes?",AskStatistics,2022-08-13 13:41:41,2
"About 10 years removed from learning about this but to the best of my recollection.

L1 regularization is used to reduce model complexity and multicollinearity. Just like you mentioned with PCA it can reduce number of variables in a model to explain nearly the same amount of accuracy. In the ML space you may have 10000 variables and in DL billions/trillions of parameters so data reduction is good especially for real world performance. Multicollinearity is also a much larger issue in regression space so it's important to account for it.

Criteria is what you set the penalty term (alpha) to. At 0 it is equal to a linear regression. The larger the alpha, the larger the elipses (loosened constraints) which means more variables will hit your diamond and be eliminated from the model.",6,wnct9u,"

I've read ALOT of papers and questions here as well, on other  websites, but I didn't get the point behind the LASSO. my background is  in economics (not an expert in math).

sorry for my bad english...

so, broadly speaking our desired  total cost function by which we want to balance between

&#x200B;

1. how well function fits data. (the RSS)
2. Magnitude of coefficients. ( by forcing some restriction either l2 or l1( here comes the main problem)

my first question is why would we want to reduce the magnitude of  coefficients? what is the idea behind it? for example (arbitrary one),  if I want to explain (or whatsoever) the DV through some explanatory  variables why would I prefer the estimated coefficients to be less in  terms of their magnitude ( I know they should be standardized).

my second question is, what is the criteria behind the lasso optimization? in other words when the lasso omits some variables from  the model how did he ( lasso) remove them? (I know it's due to the diamond shape of the constraints).

I understand that the lasso constraint has corners and so the ellipse will often intersect the constraint region and when this occurs, one of the coefficients will equal zero.

for example, the criteria behind stepwise or best subset are well  understood ( e.g some factors are less correlated than others with the  DV, so they wipe off from the model. (I just explain the very big idea  so you can understand me well). another example, when we use PCA the  general procedure is easy to understand, we project each data point onto  only the first few principal components to obtain lower-dimensional  data and so on.. so again the logic behind it the easy to understand.

**in a few words, I understand how stepwise omits some  variables based on some criteria. in contrast, I can't understand why some variables will be omitted from the model (they touch that diamond  but why would they in the first place touch it? based on what?**  so, for example, if I have 10 independent variables, and the lasso omits  4 of them, are they insignificant in terms of p-values? are they making  MSE larger? what is the basic explanation if I want to interpret why  they dropped off from the model?",AskStatistics,2022-08-13 04:57:15,17
'Compare' is really vague. Most *suitable* analyses for these kinds of data already deal with different sample sizes. What are you trying to find out?,1,wnng9o,"I have a dataset with compromised of 649 cases, unevenly split between 10 respondents. It's 10 different politicians and their respective social media posts. Though, if I were to simply start my analysis with the uneven spread cases my results would be skewed by the online activity of my overrepresented politicians. How do I weigh their cases so each one only makes up 10%, or is this unnecessary?",AskStatistics,2022-08-13 13:10:14,2
"That’s uh…a major undertaking. Computer assisted diagnosis is already a difficult problem, and a focus of ongoing research. You’re asking for something that is likely to require a large team of experts in multiple fields several years of work, depending on what exactly you’re trying to predict.",8,wnrwoc,"Hello there, I was wondering if it is possible to pool a large set of medical data and feed it into a code that can determine the likelihood of a certain outcome. Basically, an app that a medical practitioner can plug data in and gets a probability according to their input.

I found a mobile app called BayesMobile but it requires a certain software in which the code has to be written in C++ language. Not a great interface though.

So, I was wondering if it is possible to produce such an app with the available data and where can I find help including possibly commissioning a programmer and an app designer?",AskStatistics,2022-08-13 16:40:20,1
"Double check that you are using the right value for degrees of freedom (it should be n-1, per the question text). If that's right, it may just be a rounding issue. You can verify these numbers are correct in R:

    > pt(-2.3, 7)
    [1] 0.02749555
    > pt(-2.3, 8)
    [1] 0.02523542
    > pt(-2.3, 9)
    [1] 0.02349969",2,wnk7ai,"**Here is an answer I found. My question is, how did they arrive at those specific values for n=8, n=9, n=10 (eg.  if n=10, P(...)=0.02350)?** Because the t-distribution chart I'm looking at gives (slightly) different values.

***Solution (in question):***

https://preview.redd.it/ef0xr9o5rih91.png?width=726&format=png&auto=webp&s=491ff2e0c4df1cb8039d3d4f821a897ce7f52bd3",AskStatistics,2022-08-13 10:41:50,4
Most of the caveats of statistics are overruled by simply having large enough sample sizes.,6,wn4pnw,I took an intro stats course a semester ago and I feel that I don't really recall a lot of the important assumptions and other details. I'm looking for maybe a textbook or something I can read a little of every day to jog my memory so at the very least I know what the things I've forgotten are. A website or workbook that has a list of problems I could work on every now and then could also work (maybe something like LeetCode?).,AskStatistics,2022-08-12 20:53:49,7
"Not really an answer, but you could mail the author or submit an issue on [github](https://github.com/strengejacke/sjPlot/issues).",1,wncgpc,"  

i need to plot selected terms from zero inflated negative binomial regression. i am referrring [https://cran.r-project.org/web/packages/sjPlot/vignettes/plot\_model\_estimates.html](https://cran.r-project.org/web/packages/sjPlot/vignettes/plot_model_estimates.html) . I attempted to utilise the pick or remove specific terms from plot (terms/rm.terms) function, but it was unsuccessful for me. I just want the age variable from the count and zeroinflated models to be displayed in the plot. Any assistance would be greatly valued.

library(sjPlot)

library(sjlabelled)

library(sjmisc)

library(ggplot2)

library(MASS)

library(pscl)

library(boot)

zinb <- zeroinfl(ivdays\~age+sex+edu,

link=""logit"",

dist = ""negbin"",

data=caterpillor)

summary(zinb)

plot\_model(zinb)

\# i get an error in following r code

plot\_model(zinb, terms = c(""age \[b\]"", ""age \[c\]""))

Error in .axisPars(usr, log = log, nintLog = nint) : 

 non-finite axis extents \[GEPretty(inf,nan, n=5)\]

In addition: Warning messages:

1: In min(new\_value, na.rm = TRUE) :

 no non-missing arguments to min; returning Inf

2: In min(dat$conf.low) : no non-missing arguments to min; returning Inf

3: In min(dat$estimate) : no non-missing arguments to min; returning Inf

4: In max(dat$conf.high) : no non-missing arguments to max; returning -Inf

5: In max(dat$estimate) : no non-missing arguments to max; returning -Inf

6: In axis\_limits\_and\_ticks(axis.lim = axis.lim, min.val = min(dat$conf.low),  :

 NaNs produced

\## following r code do not work. it do not exclude these variable from the plot

plot\_model(zinb, rm.terms = c(""sex \[M\]"", ""edu \[y\]"", ""edu \[z\]""))

here is dummy data

structure(list(id = 1:100, age = structure(c(1L, 1L, 2L, 1L, 2L, 2L, 2L, 3L, 3L, 3L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 3L, 3L, 3L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 3L, 3L, 3L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 3L, 3L, 3L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 3L, 3L, 3L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 3L, 3L, 3L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 3L, 3L, 3L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 3L, 3L, 3L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 3L, 3L, 3L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 3L, 3L, 3L ), levels = c(""a"", ""b"", ""c""), class = ""factor""), sex = structure(c(2L, 1L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 2L, 2L, 1L, 1L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 2L, 2L, 1L, 1L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 1L, 1L), levels = c(""F"", ""M""), class = ""factor""), country = structure(c(1L, 1L, 1L, 1L, 3L, 3L, 3L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 3L, 3L, 3L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 3L, 3L, 3L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 3L, 3L, 3L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 3L, 3L, 3L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 3L, 3L, 3L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 3L, 3L, 3L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 3L, 3L, 3L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 3L, 3L, 3L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 3L, 3L, 3L, 2L, 2L, 2L), levels = c(""eng"", ""scot"", ""wale""), class = ""factor""), edu = structure(c(1L, 1L, 1L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 2L, 2L, 2L, 3L, 3L, 3L, 3L ), levels = c(""x"", ""y"", ""z""), class = ""factor""), lungfunction = c(45L, 23L, 25L, 45L, 70L, 69L, 90L, 50L, 62L, 45L, 23L, 25L, 45L, 70L, 69L, 90L, 50L, 62L, 45L, 23L, 25L, 45L, 70L, 69L, 90L, 50L, 62L, 45L, 23L, 25L, 45L, 70L, 69L, 90L, 50L, 62L, 45L, 23L, 25L, 45L, 70L, 69L, 90L, 50L, 62L, 45L, 23L, 25L, 45L, 70L, 69L, 90L, 50L, 62L, 45L, 23L, 25L, 45L, 70L, 69L, 90L, 50L, 62L, 45L, 23L, 25L, 45L, 70L, 69L, 90L, 50L, 62L, 45L, 23L, 25L, 45L, 70L, 69L, 90L, 50L, 62L, 25L, 45L, 70L, 69L, 90L, 50L, 62L, 25L, 45L, 70L, 69L, 90L, 50L, 62L, 25L, 45L, 70L, 69L, 90L), ivdays = c(15L, 26L, 36L, 34L, 2L, 4L, 5L, 8L, 9L, 15L, 26L, 36L, 34L, 2L, 4L, 5L, 8L, 9L, 15L, 26L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 5L, 8L, 9L, 36L, 34L, 2L, 4L, 5L, 8L, 9L, 36L, 34L, 2L, 4L, 5L), no2\_quintile = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L), levels = c(""q1"", ""q2"", ""q3"", ""q4"", ""q5""), class = ""factor"")), class = ""data.frame"", row.names = c(NA, -100L))",AskStatistics,2022-08-13 04:36:41,1
"It seems pretty straightforward even without context. Changing the order slightly:

Firstly they explain why you can change the limits on the sum for D (but not the number of terms!). 

They define a new function G. 

They explain why everything involving n except terms-involving-*a* come outside of g(). 

They use basic properties of the expectation operator


The result follows immediately by putting those steps together.",1,wn76v9,"&#x200B;

https://preview.redd.it/uvwtcvqx9fh91.png?width=650&format=png&auto=webp&s=a52fd8bec4eb625cbef5276ece5b43c42396a159",AskStatistics,2022-08-12 23:11:09,4
"You're using calculations that assume at least interval  (possibly higher, I haven't thought my way right through your proposed measure). On a Likert item?

You really have to be clearer about what you mean by ""similarity"" and ""closeness"" keeping in mind that Likert items are not necessarily treated as having equal intervals. (Why does it make sense to do so in this case? What consideration led you to the particular statistic you chose?)",1,wn7ze5,"Say I have a likert-style test that results in a number on a scale from 1-5.

Two people take the test.

I would like to compare the scores of the two people to see how ""similar"" they are.

This is how I am doing it:

* highest possible value/(highest result/lowest result)
* So if Person A scores a 3. And Person B scores a 4 with a likert scale of 1-5
* 4/3 =1.33
* 5/1.33=3.76
* So on a scale from 1-5, the similarity falls at 3.76, or 75%.

Is this the correct way to go about this or is there a more standard way. I am not a statistician and looking for the simplest way to say how two scores compare in terms of closeness.",AskStatistics,2022-08-12 23:59:46,3
"It's hard to give an answer without knowing what you understand about those terms, and what you don't. Do you have any background in probability and statistics?",4,wmsbno,,AskStatistics,2022-08-12 11:26:53,5
Working out the probability of getting a 7-letter word at the beginning of the game would be doable. But middle game? How are we supposed to know what letters remained in the bag? Same goes for your second question.,3,wn05br,"Ive never played scrabble, but friend told me something happened to them that was nearly impossible.  Maybe someone who knows scrabble can explain.

The game was already underway, my friend was loosing.  My friend gave up all their letters for a full new hand of 7 letters.  The seven letters spelled a word.  They put the words on the board that provided triple word score.  They won the game.  What are the chances of pulling 7 letters that spell a word, and then what are the chances the open spot on the board is a 3x multiplier of the points for that word.  And what are the chances they already had letters but decided to throw them all out for an entire new hand, and that that hand was seven letters that spell a word. And they went from loosing the game to winning the game.",AskStatistics,2022-08-12 17:10:40,1
"[prophet](https://facebook.github.io/prophet/) will be able to pick up on the multiple seasonalities and generate nice predictions, I use it in R but that is purely personal preference.",1,wmotbt," Hello, I am working with a time series (mostly in Python, tinkering with R) that contains hourly data over the course of about 5 years. I'm mostly doing this for a learning experience to teach myself both Python, machine learning, and statistics at the same time. My goal is to use this data to predict certain future values (out to at least 6 months, ideally a year or more) like monthly averages and the peak (hourly max) values in a month. The time series has 3 levels of seasonality, daily, weekly, and yearly, with the yearly pattern being by far the strongest, though the weekly pattern is still pretty evident. I've been messing around with a few different forecasting techniques like Holt-Winters, ARIMA (and its variants), and STL decomposition methods to see which method tends to give better results. So far I've been able to get pretty good results when trying to predict the monthly averages but have been utterly unsuccessful trying to predict the monthly peak values and I'm curious if there's some different technique, method, or consideration I should be doing to predict these values.

One thing I've tried doing is just taking the historical monthly peak values, creating a time series from those and using that time series to predict the peaks, but as I mentioned, this hasn't been performing very well. Another thing I've tried is to create forecasts for the hourly values and just pick the peak values from the hourly forecasts, again, not too successful yet. So I was wondering if there is anything else that I should be looking into to help predict peak monthly values from the hourly data.

One thing I've thought of doing is taking the upper bounds of my confidence interval on my monthly average forecasts as the peak forecasts, but that seems ""hand wavy"" and not really based on any actual statistics, but if that seems like a reasonable approach let me know.",AskStatistics,2022-08-12 08:58:11,1
These are Likert items?,1,wmy1vv,"I want to see if the mean between two different answers on a questionnaire is significantly different (same population). Answers range from 1-5, which test should I use?",AskStatistics,2022-08-12 15:34:43,8
"They're correct as far as intent goes but technically they misspoke slightly. It has no free (i.e. *unspecified*) parameters. If you specify all the parameters, you completely specify the distribution curve -- e.g. it has no ability to change to model what you find in data.",2,wmxl6u,"I came across a Quora link where one of the answer says that the Standard Normal Distribution has 'No Parameters'. But what about the definition of Standard Distribution is a distribution with mean =0 and SD= 1. Aren't they the parameters in this case?

I somehow find the sentence ""Standard Normal Distribution has 'No parameters'"" and ""Standard Distribution is a distribution with mean =0 and SD= 1"" , opposing. Aren't mean and SD the parameters of Normal Distribution and even for the Standard Normal Distribution ?

Am I right ? What am I missing here.

Link of Quora answer -> [https://qr.ae/pv5WPw](https://qr.ae/pv5WPw)",AskStatistics,2022-08-12 15:14:52,2
"There's nothing in your results that give any indication of a problem.

1. You can't use the same data to suggest a specific hypothesis\* and to test that same hypothesis. If you want a data-generated hypothesis (like that one about 8 and 13) you need to generate a new set of data to test it.

2. How many rolls you should do depends on how much power you want (how much chance of finding some difference from fairness) at some specific set of unfair probabilities, using some test (specified before you roll the die), at some specified significance level. 

  Without a specific alternative stated up front, about as good as you can do with these data would be a chi-squared test. 

  If you want to pick up a pair of population proportions like 3% and 7% with the rest being 5% on a chi-squared test, n=400 only gives you about a 25% chance of picking this up at the 5% significance level. If you wanted a 90% chance of  you'd need about 1600 rolls.

A slightly different way to look at it: if you roll the die 400 times, the standard error of the sample proportion on one face for a fair die is about 1.09%; as a result you could easily see values like 3% or 7% with a fair die, and you'd see a couple of each across the set of results fairly often. If you want a narrower range of results you need substantially more rolls. To halve the range of variation away from 5% you're seeing there would require 4 times as many rolls.

This is of course a lot unless you have an automated die rolling and recording rig set up (simple setups have been built that can do it using a lego setup to roll the die and a phone to get images and some AI to read the numbers in the image).

However, if you're using this d20 in D&D (or something with a similar core mechanic) then there's a pretty good argument for testing the average instead. This more specifically focused test will tend to be a little more sensitive to changes that would make the die tend to roll high or low, and so would not need quite as large a sample size to pick up issues likely to affect play. You'd still probably want more than 400 rolls though.

On the other hand, if you can't see a clear difference from what you'd easily get with a fair die in 400 rolls, you could play quite a few sessions to get to 400 rolls and still not be able to claim it affected gameplay. So maybe that's another way to look at it.

---

\*(that faces 8 and 13 are unusual, something you didn't specify before rolling but only noticed after the fact)",9,wmil47,"Am I unlucky with these rolls or is my dice unbalanced ? How many rolls should I do to perceive an unbalance ? How can I perceive an unbalance ?  
For example in this table after 400 rolls, I would think the difference between 8 and 13 as opposed face is a clue.

Thanks for your help !  


[Paranoia mode activated](https://preview.redd.it/nmietwxkl9h91.png?width=118&format=png&auto=webp&s=663a758164fc4fcc7c3eb1e877ea76005f6c7f44)",AskStatistics,2022-08-12 04:09:21,3
"1. > A confidence interval is an interval for a parameter / function of parameters.

   What quantity are you seeking an interval for?

    The thing you link to is talking about the mean of the lognormal but you don't mention being interested in the mean anywhere.

2.  > I have normalised the data between 0 and 1

     > The distribution of data follows a lognormal distribution

   -- well, no, it can't, not if you normalized it to lie between 0 and 1

3. When you day ""didn't work"" -- you're going to have to explain more about *exactly how*. What the problem was.",4,wmq3vy,"I have some time data (measuring duration taken to walk between two locations) for about 70 different people. There are 20,000 observations in total.  I have normalised the data between 0 and 1 as each person has a different distance they walk between two locations. The distribution of data follows a lognormal distribution. How do I calculate confidence intervals for this population? 

I have tried following the following on stack exchange but didn't work for me. 

[https://stats.stackexchange.com/questions/33382/how-do-i-calculate-a-confidence-interval-for-the-mean-of-a-log-normal-data-set](https://stats.stackexchange.com/questions/33382/how-do-i-calculate-a-confidence-interval-for-the-mean-of-a-log-normal-data-set)

I'm working in Python but can move to R if that is easier to do this. 

I appreciate any advice.",AskStatistics,2022-08-12 09:52:05,3
"You’re kinda stuck here — you can’t control for things you don’t have data on. Putting zeroes in for missing values isn’t a good option either — sure, those cases will then be included in the analysis, but what does that zero actually mean? Also, you’d be saying that two neighborhoods that have in reality have different values on one or more missing control variable(s) are actually the same (since you assigned the same value to both). 

The best option, which may or may not be feasible, is to collect more data.",1,wmjgkm,"I am investigating the effect of football stadiums on surrounding house prices. I have a list of neighbourhoods with average house prices and other characteristics, and in my regression I would like to use both stadium capacity and the amount of negative newspaper articles as control variables. However, both control variables thus only include data for the neighbourhoods that include a stadium, and are empty for the neighbouroods without a stadium. For now, I have placed a ""."" for the empty values, but then Stata does not take these into account in the regression.

My question is, how should I go about this in order to include all neighbourhoods in my regression. Should I place a ""0"" instead of a ""."", or does this make the data incorrect (as the other neighbourhoods for instance have '0' capacity, but that is due to the fact that they have no stadium). Or are there other ways in which I could solve my problem? Thank you all in advance.",AskStatistics,2022-08-12 04:57:18,7
"I wouldn’t be too concerned about the scales but I would suggest your analysis treat “video” as a random effect so that you can generalize your results beyond the 20 videos used in the experiment. In other words, a mixed model in which “subjects” and “video” are both random effects and “scale reversal” is a fixed effect.",2,wmtart," Hi guys! I have two independent groups that I am trying to compare. Each group consists of people's ratings (1-9) for 20 videos based on how much the video is liked. The difference between the groups is that they have flipped/reversed scale labels. I would like to see whether there is a difference in mean ratings between the groups (both for all videos, and for each video separately). Which statistical test(s) should I use?

I was thinking doing an ANOVA-type test but I am not sure if this will work since the scale is made up of integers, so I'm not sure if it fulfills the continuous dependent variable requirement? Would the necessary test be parametric or non-parametric in this scenario?",AskStatistics,2022-08-12 12:09:34,5
"E~N(0, sigma) i. e. the random term follows a normal distribution with mean 0 im a rw. But you're using the previous values average.",2,wmsdsi,"Hello everyone, I am studying on my own, also following courses on youtube, the time series analysis.

I arrived at the lesson where the Random Walk is explained ([https://www.youtube.com/watch?v=l\_EDA7rDH3s&list=PLtIY5kwXKny91\_IbkqcIXuv6t1prQwFhO&index=5](https://www.youtube.com/watch?v=l_EDA7rDH3s&list=PLtIY5kwXKny91_IbkqcIXuv6t1prQwFhO&index=5)): unfortunately after the theory part, in the practice part with python the file ( RandomWalk.csv) is not provided (the file where the random walk was built on the historical series of interest taken from this dataset kaggle [https://www.kaggle.com/datasets/regaipkurt/financial-markets](https://www.kaggle.com/datasets/regaipkurt/financial-markets)).

So I'm trying to build it myself, however having no time series experience and having just started studying I'm not sure if what I'm doing is actually right.

&#x200B;

I leave the code I used to calculate the white noise and the random walk for the time series of the S & P500 price: i 'm using this formula 

 P(t) = P(t-1) + E(t) 

I am computing the random walk in the following way: the value of P today = P yesterday + White Noise today

    df2 = df.copy()
    df2['rw'] = np.nan
    
    # white noise
    wn= np.random.normal(loc = df2.market_value.mean(), scale=df2.market_value.std(), size = len(df2))
    df2['wn'] = wn
    
    #random walk
    i = 0
    for i in range(len(df2)):
      if i == 0:
        df2['rw'][i] = df2['market_value'][i]
      else:
        df2['rw'][i] = df2['market_value'][i-1] + df2['wn'][i]

On a theoretical and practical level, is what I am doing right? In case you suggest me what am I wrong?

&#x200B;

Thanks to everyone for any form of help or suggestions.",AskStatistics,2022-08-12 11:29:37,4
"I wouldn’t say the prior is “usually” based on data from previous experiments. Prior knowledge can be… quite nebulous. And some priors are priors of convenience or structure.

I also wouldn’t call the posterior the “updated prior.” The posterior is the thing we want, the way we get to make statements about the model conditioned on the data. Bayesian updating is great but not all Bayesian statistics is really about updating priors.

Now, the key difference in both cases is what the “predictive” distribution is a distribution *on*. When you look at your notes, or your text, you should see Pr(someVariable | things). What is that someVariable? How is it different from the variable of interest in the posterior or prior? Think about this for a minute before skipping to the answer below. 

The answer: predictive distributions are about *data*. Observations. The things we see, not the parameters that generate them.",3,wmo12s,"I am learning about Baysian statistics and I am really struggling to understand what is meant by the prior and posterior predictive distributions.

I would prefer an approximate commonsense definition to a mathematically vigorous definition, What I really need is a basic understanding so I can move on in my course.

First let me explain what I *think* the prior and posterior distribution means, so if I'm way off there someone can set me straight on that before I try to further understand what the respective predictive distributions are.

**Prior distribution:** The distribution of possible values for some random variable θ in the population in question before your experiment is done.  This is usually based on data from a previous experiment or based on a hypothesis about the population.

**Posterior distribution:** the updated prior distribution of θ after doing an experiment.  This will be somewhere between the prior distribution and the distribution of the data from the experiment.

**Example:** So if we are trying to find the probability of getting heads when flipping a specific coin, we might use a prior distribution of `θ~Beta(100,100)` to represent our prior beliefs about likelihood of the coin being unbiased.  Here we picked a sample size of 200 because most coins are unbiased, so we want an informative prior.  After flipping the coin 50 times we get 48 heads.  Now our prior distribution will reflect the new information from the experiment.  This will help us predict our probability of getting heads after one flip of this coin.

So how would the prior predictive distribution and posterior predictive distribution play a role here?  How are the predictive distributions different from the respective prior and posterior distributions?",AskStatistics,2022-08-12 08:25:54,4
It is better in outlier detection because extreme scores can increase the sd so much that they are not outliers using an sd based criterion. The median rather than the mean absolute deviation is usually used. [Here is one example](https://psycnet.apa.org/record/2003-09632-002).,1,wmnueu,"In what situations, if any, is it practical to use a Mean Absolute Deviation as a measure of dispersion instead of Standard Deviation? Just asking for general knowledge and learning.",AskStatistics,2022-08-12 08:18:19,2
You have less than 2 results in one of the age groups. You can’t compare across groups with that few results. Can you reasonably roll that group into another group (regrouping other bins/buckets as necessary)? Otherwise I’d compare without that small group as it’s too small to allow comparison.,1,wml1xk,"Hi all, PLEASE RESPOND.

I have a university project due Monday and of course I just get loads of errors for ANOVA.

Can somebody please explain to me, like I'm 5, what the hell I'm supposed to do if JASP tells me:

""Number of observations is < 2 in Q1 after grouping on Age G""

I just want to compare how different age groups responded to different questions, and now I can't.

PLEASE HELP.",AskStatistics,2022-08-12 06:16:48,3
"Good question! I have a possible answer.

They only mention in briefly in the paper: *""Since each child can occur several times in the calculations and since siblings and cousins can be assumed to be correlated, we estimated the CIs using bootstrap techniques""*, but there's a better explanation in the model assumptions section of the supplement.


They analyse survival of ""pairs"" rather than individual children, the 1.5% refers to the individual children, but the inverse K-M curves in Figure 2 are (presumably) from the pairs.


The difference is, given a set of children in a family `{C_1, C_2, ..., C_n}` and a set of uncles `{U_1, U_2, ..., U_n}` then you have only `C_n` children, but `U_n * C_n` pairs.


I don't know without the raw data, but it seems plausible that the per-child incidence and the per-pair incidence could differ by this sort of a factor if ASD families tend to have more children or more uncles/aunts, or both.",3,wmapfx,"Apologies if this is not an appropriate post for r/AskStatistics

I’m trying to understand the incidence of autism (ASD) in your child if you are a mother who has a sibling with autism, but am confused by why some numbers in the study differ from each other, due to my lack of understanding of how the statistics works. This freely available study from Sweden appears to have the appropriate data: DOI 10.1016/j.biopsych.2020.03.013.  [Inherited Risk for Autism Through Maternal and Paternal Lineage (biologicalpsychiatryjournal.com)](https://www.biologicalpsychiatryjournal.com/article/S0006-3223(20)31384-6/pdf) 

I am confused by how to interpret Figure 2, which is described as “Age-specific ASD prevalence for offspring with uncles and aunts diagnosed with ASD from maternal lineage and paternal lineage”. This figure shows a cumulative probability (?) at age 13 of approximately 0.09 (9%) for “maternal lineage (ASD-affected uncle)”, which appears to be the number I am looking for.

However, I’m confused by why the other, dashed, line on this figure, described in the caption as “unexposed groups (participants with aunt/uncle free from ASD diagnosis)”, is 0.03 (3%), despite the paper itself clearly describing that the overall incidence of ASD in the study was 13,103 of 847,732 (1.55%) - shouldn’t the latter number (overall incidence of ASD = 1.55%) be higher than the former (probability of ASD without hypothesised risk factor = 3%), given that the risk is higher when you have an aunt/uncle with ASD?

Am I missing something obvious about the statistics here?",AskStatistics,2022-08-11 20:27:14,2
"No software distinguishes between independent variables and control variables, because there is no mathematical difference. In both cases the effect of all other variables are partialed out from the effect of every single variable.",6,wmh4yr,"Hey there, 

I have a linear regression model with a lot of controls and I want to interpret the f test. As the program I use (stata) does not distinguish between independent variable and control variable wouldnt the f test be heavily inflated by the controls?",AskStatistics,2022-08-12 02:42:13,3
"Lot of ways to try and explain this, but for ELI5...

Imagine I have 5 cards with numbers on them, face down. There are 5 points of uncertainty there - 5 things you don't know. If you flip over one card, there are 4 remaining unknown values, etc.

Now, if I tell you the mean, I've given you information about the set of 5 cards. Flip 4 over and you already know the 5th b/c there is only one number in existence that can be combined with the four known values to give that mean. 

So, by giving you information about the set as a whole, you can combine that with other information to reduce total uncertainty... in a set of 5 values, when I give you the mean, there are only 4 remaining points of uncertainty you need to resolve before you know everything. 

Degrees of freedom always made more sense to me when conceived of as an information/uncertainty measure.

EDIT: Thanks for the award!",85,wlvg5n,,AskStatistics,2022-08-11 09:20:03,10
"Real Analysis. You will be taught the material in ""introduction to mathematical stats"" during your PhD. You will be expected to already know the important parts of the material in real analysis coming in to the program (some programs won't even accept you without a real analysis course(",1,wmfhag,"Looking back at your first and second year of PhD coursework: If you could only take one of these courses before starting PhD, which one would you take, Real Analysis or Introduction to Mathematical Statistics?",AskStatistics,2022-08-12 00:57:12,2
"Complex question that we likely won’t have an answer to! It could be that the variance of the individual scales, when combined, effectively “negate” the total combination score.

The question really is: are the sub scales meant to be totaled into a combined composite or are they meant to be used as individual sub scales?",2,wmb49b,"Hi. I have a DV called BL, an IV called CBBE with 6 subscales under it, and 2 mediators Trust and Love. I ran a mediation analysis using model 4 on SPSS through PROCESS.

My finding:

Trust partially mediates CBBE and BL, while Love does not mediate it at all.

I also conducted the analysis using every subscale under CBBE as IV, while Trust and Love mediate. With *each* subscale, I found that Love partially or fully mediates the relationship between the IV and DV. 

My question is: Why does Love not mediate the complete scale and DV but mediates every subscale under the IV?

Note: Keep in mind that CBBE does affect Love so path a is fulfilled. However, path b (ie Love doesn't significantly affect BL whenever CBBE is involved but in all the other subscale mediation analysis DOES affect BL) doesn't exist.",AskStatistics,2022-08-11 20:47:28,4
"Programming it from scratch will be a pain in the ass; I recommend using one of the existing libraries for it, [e.g. here](https://rosettacode.org/wiki/Welch%27s_t-test#Python/).",4,wm1nxr,"At the moment, I am recreating statistical analyses in Python (for practice purposes in both programming and statistics). However, for the t-test, I am unable to find a formula for calculating the p-value based on the t-statistic. The google results point me to the t-statistic table. Would anyone know the formula, or know where to find it?",AskStatistics,2022-08-11 13:40:34,7
"This is unanswerable without knowing what you're measuring, and what you're actually trying to accomplish.",1,wmb342,"I am looking for more creative (or lesser known) statistics to measure a performance metric over time that I have stored in a simple vector. (Something simple like push ups completed daily). Basically other ideas besides things like: Cumulative average, moving window average, moving window variance, average change, average change of lagged measurement, etc ...etc. Are there any other creative ways to monitor changes in a vector? I am open to any suggestions.",AskStatistics,2022-08-11 20:45:46,1
This response is a Likert item?,1,wlzq0l,"I conducted a questionnaire and want to look at if there’s a significant difference in how to different age groups answered a question. The answer is ranged from 1 to 5 and 86 respondents answered. I’m not sure how to go about this, any help would be greatly appreciated. Thanks!",AskStatistics,2022-08-11 12:19:11,2
I would start by graphing the Medication x Time interaction separately for each sex. The interpretation of the interaction may be clear without doing additional tests. Keep in mind that testing simple effects or simple slopes don’t help under the interaction (unless you are willing to accept the null hypothesis) although they can be informative in their own right.,2,wlr64r,"Hi,

Let say I conducted an experiment with a 2 x 2 between-subjects design. Let say sex (male vs. female) and medication (given vs. not given medication). I measured their self-reported stress (continuous dependent variable) weekly for 4 weeks (i.e., measured 4 times for each participant). Thus, there are two between-subjects variables and one within-subject variable. 

A mixed repeated-measure seems to fit the set up. I found a significant 3-way interaction. How do I do a simple slope analysis that is normally done with 2x2 between-subjects? For instance, for those who were given medication (within one level of the between-subjects variable), does gender (the other between-subjects variable) affect their stress level for the 4 weeks (within-subject variable; DV)?

I cannot find a tutorial online for this question. I can only find those that has one between-subjects variable.

I also have a second related question. For such set up, would increasing the number of levels for the within-subject variable artificially led to significant difference? Thus, does measuring stress level for 20 weeks instead of 4 increase type I error?

Thanks.",AskStatistics,2022-08-11 06:16:26,1
"There's many papers, blogs, webpages etc that discuss them. Besides the references and other reading in wikipedia, you might look at (for example):

https://www.cambridge.org/core/journals/economics-and-philosophy/article/abs/comparing-rubin-and-pearls-causal-modelling-frameworks-a-commentary-on-markus-2021/49C42C4D0793F09AA321F4E460A83A13

or

http://philsci-archive.pitt.edu/19773/1/Markus_Commentary_Revisions_Unblinded.pdf

(etc) and in turn the references that they mention.

Also see --

https://erikgahner.dk/2021/causality-models-campbell-rubin-and-pearl/


Gelman talks about it a number of times -- e.g. see the multiple links at the top of this post:

https://statmodeling.stat.columbia.edu/2009/07/09/more_on_pearlru/

There's a lot of discussions out there.",2,wm4uyg,"Where can I read about the relationship between the Rubin causal model and causality as framed by Pearl? I read somewhere, although I can't find the reference now, that Pearl's framework stands in opposition to Rubin's model. I'd like to know more about the relationship and contrasts between those two frameworks. Can anyone provide any references?",AskStatistics,2022-08-11 15:55:54,2
"Yes, your assumption is correct.

Regarding your second question: Those variables are covariables, they can be assumed to influence either the IV and be therefore need to be controlled for.",3,wlu1r1,"Hi all - I'm having trouble identifying the IV and DV in my study:

I'm conducting a study where we are allocating participants into two conditions (Easy vs Difficult set of anagrams) to look at how aggressively they respond when they receive either positive or negative performance feedback. We test for aggressive response by asking their opinion of the anagram tests they just completed. We then test for narcissism, self-esteem, self-estimated IQ and, anti-social personality traits through a series of questionnaires - this is to see if these are large contributing factors in their aggressive response. 

Am I correct in thinking the IV is therefore the conditions (Easy vs Difficult anagram conditions) and the DV is the aggressive response?

I am just confused about whether narcissism, self-esteem, self-estimated IQ and, anti-social personality traits aspects fall into the IV or DV. 

Thank you!:)",AskStatistics,2022-08-11 08:22:12,8
"Are you choosing to treat your Likert item as merely ordinal? 

If no, you could perhaps consider the Wilcoxon signed rank test (because you end up assuming an interval scale in computing the pair differences\*) or otherwise perhaps a sign test (if you were specifically interested in conclusions about the median of the pair differences)


There's an infinite number of other possibilities, however


\* a *lot* of books get that wrong! So many directly  say you  use it when your response is ordinal.",1,wm4e38,Looking to see if there is a significant difference between how respondents answer how they view the NBA versus the NFL on a questionnaire. I’m comparing the values (answer range from 1-5) and the NBA has one question and the NFL has another. Which test is appropriate?,AskStatistics,2022-08-11 15:35:17,4
"Interesting problem! Half baked thoughts follow:

If DV3 and DV4 are measures of different strategies then why are they dependent variables? For me at least, unless you instructed the subject to use a particular strategy, the outcome is how accurate and fast they were and the strategy is an individual difference/independent variable - just an uncontrolled one as they were not told to use a particular strategy.

Then the analysis boils down to looking at the interaction between the strategy usage and it's relationship to group as IVs to predict differences in speed|accuracy. You could use an efficiency measure or a MANOVA with group and the two types of strategy. Of course if group is strongly related to strategy, and strategy usage is strongly related to outcome then there may be colinearity problems.

Other than the above I like the sound of the clustering approach - you could just run k-means or k-nn clustering on DV1...4.",1,wlmlo5,"The experiment summarises each subject's performance into four DVs, none of which are on similar scales. In theory there will be tradeoffs from one DV to another, but this is not guaranteed. Across these four dimensions, then, each subject's behaviour across the whole experiment can be located at a particular location in the response space.

To illustrate further, two of the dimensions are speed (average completion time) and accuracy (proportion correct trials). Usually as accuracy is improved as response speed goes down. A particular subject may be both more accurate and faster than another subject.

Attempts have been made in the psychophysics literature to combine speed and accuracy into things like an ""efficiency index"", however this experiment can measure *how* participants choose to optimise their performance. Furthermore, the psychophysics literature often needs to assume that participants find a task equally difficult. The other two DVs are also classically imagined as trading off against each other.

In our theory a subject could achieve equivalent speed *and* accuracy to another subject by scoring high on DV 3 and low on DV 4, while the other scores low on DV 3 and high on DV 4.

A subject could be very high on DVs 3 and 4, and they will also score high on speed and accuracy. It's useful to think of DVs 3 and 4 as using strategy A and strategy B, respectively.

There are two kinds of subject, so ultimate analysis is between groups. The scientific question is whether or not there's a consistent group difference in the relative use of strategies A and B to achieve equivalent performance/efficiency. However some participants could be good at everything or bad at everything.

My initial idea would be to PCA the 4D data, and should one component capture most of the variance, a t-test on the eigenvalues would do.

If two or more linear components describe most of the variance then I would have a multivariate comparison to make, right? A two component solution could then imply that efficiency and strategy balance are independent, if each eigenvector of the PCs load mainly onto each pair of DVs. If they don't conveniently map onto the DVs, then there's a more complex relationship between the DVs.

Unfortunately I can easily imagine the data showing that at least some participants from both groups will be good at everything, and the interesting group difference will be in the subjects who find the task sufficiently difficult that they need to trade off between strategies A and B. In this case the group difference would be in the *shape* of a hyperplane for each group. Participants in group 1 may prefer strategy A as their performance improves to ceiling accuracy, while participants in group 2 prefer strategy B.

So I am left with several options:

PCA the data and hope that there is a readily interpretable linear  subspace. This is something I have done before in class.

Skip dimension reduction and test whether one or two 4D clusters are better descriptors of the data. I have never done this before but seems straightforward, however I think it ignores the inevitable non-independence of the data.

Something I have discovered today called a Kernel PCA, which I honestly don't understand (but would be willing to get to grips with.

Or something I haven't thought of. Happy to answer any further questions.",AskStatistics,2022-08-11 01:54:08,3
"Answered in r/spss.

Next time please crosspost or link to the original post instead of making completely separate posts in multiple subs. That way anyone wanting to help can easily tell if the question has been answered already in another sub.",1,wlvnmz,,AskStatistics,2022-08-11 09:28:40,1
Wait why is outliers on the list?,2,wlos6e,"Can u guys suggest me some applied statistic books like 

\- Outliers - Malcolm Gladwell

\-The Book of Why: The New Science of Cause and Effect  - Judea Pearl and writer Dana Mackenzie

\- Everybody lies- Seth Stephens Davidowitz 

Thankyou",AskStatistics,2022-08-11 04:10:32,2
"It’s just an interaction effect. Run a single model, Interact your grouping variable with the time effect.",3,wld3p9,"I have some cellular growth curves I want to compare.

In total I have 6 curves: half receive treatment A and half receive treatment B. On its own treatment A and B should have no real difference on the cells, and isn't a particularly interesting question as resting state.

I then have three other treatments (1, 2, 3). One of those three treatments (#1) is a control which I use for normalization between A and B.

That is normally I visually display #2 and #3 as values relative to #1 within both A/B treatment groups. In practice this ends up looking like a survival curve that is not necessarily strictly decreasing at all points.

I want to compare
1) whether treatments #2 and #3 each differ from treatment #1 within each group.
2) whether the relative effect (normalized to #1) of treatment #2 differs between A and B, and vice versa rod the relative effect of treatment #3.

What would be a good approach to utilize? I currently have a sample size of 5 with multiple timepoints for the 6 groups described above. It's worth noting that while values are fairly nicely centered around their mean, some of these cells also die and dying cells show way way less variance and drastically lower neans, so equal variance is not a good assumption. Treatments #2 and #3 potentially have this lethal affect, although they can differ between A and B.",AskStatistics,2022-08-10 17:13:30,7
"Happens all the time on Mac. There will usually be a recovery file if you force quit, so you should be ok. I’ve never lost data. When you open SPSS again it should ask you if you want to open the recovery files.",1,wlkqsj,,AskStatistics,2022-08-10 23:53:27,1
">  how can I determine sufficient # of 'tests' to capture the failure rate with certain level of confidence?

If you have little prior idea of a likely range on the proportions, use standard Margin of Error calculations, suited to the particular coverage level you wish.

https://en.wikipedia.org/wiki/Margin_of_error

If you do have a pretty good prior idea of a likely range on the proportions, choose the end of that range closest to 1/2 (if you're interested in absolute error)\* and use a confidence interval width calculation here:

https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval

\* for relative error you choose the other end and use a different calculation",2,wl9x3s,"Hello-

I have a statistic question on the following scenario.

Imagine we have 1 physical sample. And there is a 'test' we can perform on this sample which will have pass or fail outcome. And the outcome has randomness to it. Meaning if you test 50 times, we could see 4-5 times fail, for example.

My question is, how can I determine sufficient # of 'tests' to capture the failure rate with certain level of confidence?

Any statistically theorem or formula I can look into would be really appreciated-

Thanks-",AskStatistics,2022-08-10 14:52:41,2
"Absolutely not an expert, but I have a four year old Lenovo with about half those specs (Ideapad 730 i think?) and can run all the models I need to no problem. That includes random forest, shallow neural networks, and computationally intensive mixture modeling (e.g., LCA with BLRT). 

If efficiency is of the essence, then might be worth looking into further. But I at least haven't had problems.",1,wlg70l,"Is Lenovo L14 Gen 2 (16gb ram, 11th gen core i5, 512gb ssd) good at run most statistical softwares/packages, coding and programming ? 

Any suggestions or recommendations are welcomed.",AskStatistics,2022-08-10 19:41:28,1
"It all comes back to what the model is for. The term for what's ""outside the bounds of data"" is extrapolation - and it's an important consideration when modeling.

You're right, a polynomial term can improve the fit of a regression model to a dataset, but why do you have the model in the first place? Is it to understand some underlying dynamic in your data? Absolutely it doesn't make sense all the time that if you had data on the far end of your x variable that your y variable would go down. That said, does it make sense for you to collect data that far? Are they outliers, or is your x variable maybe censored or capped somehow?.

Take an example from physics - if my dataset is how high a ball is at different points in time when I toss a ball up in the air, the polynomial fit is going to be pretty good when predicting height from time. Some physical relationships are actually quadratic, or very close to it, because there are two conflicting forces (e.g. gravity, and me throwing the ball). This ball example isn't perfect either (I'm missing wind resistance, some tiny tiny changes in gravity from the height of the ball, etc). 

So for your dataset, you can technically fit the data ""better,"" with a polynomial fit. It doesn't mean you should, because it might not extrapolate well. There are many different ways to compare models, and while a polynomial fit might be ""better"" in terms of lower error to the data used to create the model, the linear model might be ""better"" in extrapolating to new data outside of the bounds of X.",3,wla3zj,"I've read that you can add a polynomial term to your linear regression in order to get a better fit for a curvy looking relationship. I've always wondered about what happens after the bounds of the X axis, e.g.  when you fit a second order polynomial, it all looks great for bounds of your data.. but from my understanding about a second order polynomial, is that it is a quadratic curve, which means it goes up and then back down again.

But I cant imagine that for many use cases, the relationship being modeled is expected to drop back down if you extend the X axis with more values... I have included an example of my data. As you can see, the polynomial curve would suggest that the positive relationship drops back down to become negative. However, when I fit a LOESS curve to the same scatterplot (loess in blue and linear line of best fit in red) it doesnt suggest that it would drop back down, and the latter seems like the sensible conclusion.

I have a feeling this is just down to my inexperience with statistics and I have likely not grasped the concept of it correctly. So I'd appreciate any insight on this.

https://preview.redd.it/orrq30amxyg91.jpg?width=1137&format=pjpg&auto=webp&s=ffe599d78ad32a7f5ac4e0a67be8a5cff658a77b

https://preview.redd.it/3nuk6uqzjyg91.jpg?width=1204&format=pjpg&auto=webp&s=38f715844c0f2efe91fd9761c471be563b8a8b6c",AskStatistics,2022-08-10 15:00:30,2
"I would say the title matters much less than the specific training, classes, internships, etc. I would look more at what kinds of research the professors are doing than the title. 

That said, if the entry requirements are lower, is there more emphasis on data science vs. pure statistics, for example? More or leas programming in one program? Are either funded?",1,wl6a5x,"Hey everyone,

I wanted to get some feedback on the quality/benefits of the applied statistics masters program at Loyola U. Chicago? I graduated college a few months ago and currently am working to regulate derivatives in Chicago. I graduated with a bachelor's in Finance, so my math-oriented experience in undergrad is limited. I am absolutely willing to go ahead and take more courses before applying, but I wanted to ascertain if this is a good program or is ""applied statistics"" not worth the time? I know that is phrased vaguely, but I would love to get more experience in statistics to strengthen my resume for the future in finance. I definitely noticed that this program's pre-requisite requirements are far lower then UIC's masters in statistics program. So what are the pro's and con's to applied statistics vs statistics master's programs? And has anyone had experience with this program in particular?

Thanks so much for any feedback, it is much appreciated. A link to the curriculum for the program is below:

[https://www.luc.edu/math/msappliedstat/curriculum/](https://www.luc.edu/math/msappliedstat/curriculum/)",AskStatistics,2022-08-10 12:24:21,1
"Your loss function doesn't change based on type of predictor. If this is a classification tree you still use gini etc.

Your potential cut points could be the avg between the values of your quantitative variable. Pick a cut point, calculate the impurity, repeat and then find the cutpoint with the lowest impurity.",2,wl80h7,"I know we can use gini index, entropy, or information gain to find the best node (ie root node) in a decision tree if predictor variable is qualitative (categorical), but what do we use to find the best root node when predictor variable is quantitative? ( I am aware how to make the best split of the variable but I am not sure of the criteria to find the best variable to split on?)",AskStatistics,2022-08-10 13:33:32,2
"[Penn State](https://online.stat.psu.edu/statprogram/) has the course notes for their undergrad and graduate courses online. You won't get a degree out of it, but is a fantastic resource.

Click on the 'degree level' and then the course number. Click on 'Course notes' tab at the top of the screen to go through each of the lessons. I have noticed that you still need to read from the recommended textbook for some information, but its a start.",5,wl5z8k,"Hi! I'm looking for a good (possibly free) online course or book on statistics.
I am looking for something that starts from the basics but goes deep, is heavy on the math and explains things.
I am an engineer, so I have some calculus / linear algebra / probability background (I already hear the mathematicians laugh here, as they should).
I'm NOT looking for things like ""the hypotheses of this test are the following, and the reason will stay a mystery. Use the test and good luck"".
Moreover, I'm also looking for a resource with the option to actually know if your understanding of concepts is right. I never self-studied math without teachers or fellow students, so I do not know if the best way is to do exercises, precipitate in forums, join study groups that you know of or what. 

Any recommendation is appreciated.",AskStatistics,2022-08-10 12:12:34,2
"I figured out that Prior 2 should be P(E) but not sure what the question in everyday words is. What do you believe the probability is that the evidence, E, is ... ?",1,wlblfq,"Hi r/AskStatistics, I'm trying to ensure I understand Bayes Theorem. Can you tell me if this explanation is correct or if it has mistakes? Also, I'm  not sure how to write out in words a generalized input question for Prior 2 and for H on E. Here goes:

Imagine reasoners hope to ascertain the probability that a hypothesis, *H*, is true in light of new belief-altering evidence *E*. Bayes theorem asks them, before they acquire or consider the new credence-altering evidence *E*, to assign the probability of *H* conditional on *E*. To do so, the reasoners need to first assign unconditional probability to *H* (one ""prior probability"" or ""prior""), unconditional probability to *E* (the other ""prior probability"" or ""prior""), and finally a probability to *E* given *H* (the ""likelihood"").

Inputs for Bayesian theorem written in words:

Prior 1 or P(H): What do you believe the probability is that the hypothesis, *H*, is true?

Prior 2

Likelihood or P(E | H): Given *H* is true, what's the likelihood/probability of *E* obtaining?

Probability of H conditional on E or P(H | E):",AskStatistics,2022-08-10 16:03:16,3
"Depends on what you mean by ""introduced"".  It's called Brownian motion because Brown first called attention to it.  It's also called a Wiener process because Wiener published the first rigorous treatment (for certain notions of rigorous).  But that is just one kind of Gaussian process (independent increments).  For general Gaussian processes, I don't know.  History is complicated.  Thinking this kind of question has a simple answer is always wrong.",2,wkxsua,"Who was it that first introduced the notion of a Gaussian Process ? Was it Gauss himself ?

https://distill.pub/2019/visual-exploration-gaussian-processes",AskStatistics,2022-08-10 06:42:47,5
"I am really interested in the reddit statistics of users; female/male percentage, age, demographic etc. Where can one find these?",1,wlcm5b,,AskStatistics,2022-08-10 16:50:37,1
"I'm not sure quite what you mean by randomness. Typically, probability is the field of mathematics dealing with randomness. At the college level, Ross's *A First Course in Probability* is great.

An almost philosophical book on Baysian probability is ET Jaynes's *Probability Theory: The Logic of Science.*",3,wkw82o,"Hi Team,

do you have recommendations for deep dives into Randomness? Bayesian approach would be great.

**EDIT**: Comments below make it clear, I should be more specific. After some research I found the following article on nature.com, which comes quite close to what I am interested in.

Ref: [nature.com: Improving randomness characterization through Bayesian model selection](https://www.nature.com/articles/s41598-017-03185-y)

Cheers",AskStatistics,2022-08-10 05:30:56,10
"I'm not sure how specialised your effect size calculation needs to be, but this StatQuest video has good general discussion on the topic: https://www.youtube.com/watch?v=VX\_M3tIyiYk",1,wklc9x,Any advice is appreciated! Need to calculate effect size for a group’s pre-post changes in mean. N=26. Data is from an instrument scored on a scale of 1-5. I’ve calculated significance using Wilcoxon signed-rank test. What method would be the best? Thanks for your input and any clarifying questions that will help educate me!,AskStatistics,2022-08-09 19:08:37,2
"Yes, failing to reject is not evidence in favor of the null, and certainly provides no confidence level in it.

However, 

>they cover how to test a data set for normality, before doing any other analysis or transformations on it (a good sound best practice, IMO).

Treating ""Step 1"" as **always** checking to see if data are normally distributed seems unnecessary, unless step 3 nearly always some test that specifically requires normally distributed *data*. Few tests actually require this, including many, many that people commonly think do require it. 

Even when tests do, there are a lot of problems with normality testing, in general. Almost no data are really normally distributed, and what most often matters is how badly different they are from normal, and in what way(s). You will (almost) never reject normality with a small sample size, and (almost) always reject it with a large one (unless you are using a very low powered test).  So, these tests become a test of sample size, more than anything else.",7,wkbh0w,"I work in medical devices, and we have standard procedures (SOPs) we must follow for all sorts of things.  How we do statistical analysis of test data is one of those things.  I studied stats a bit in grad school (15+ years ago), just enough to be able to defend and publish my own research, but am far from a stats ""expert.""  Anyway, in my work's stats SOP, they cover how to test a data set for normality, before doing any other analysis or transformations on it (a good sound best practice, IMO).  One sentence though, stood out to me as poorly phrased, and I just want to see if actual statisticians (which I am not) agree with me before I suggest we change it.  Here's the quote:

""If the p-value is >.05, the data can be assumed to follow a normal distribution at a confidence level of 95%. ""

My thoughts on this are that normality testing has a NULL hypothesis that the data set follows a normal distribution.  If p<.05 one would reject the null hypothesis, indicating that we can say with 95% (or greater) confidence, the data set under test is NOT normally distributed.

If however, p>.05, then you ""fail to reject"" the null hypothesis.  That's all.  This does not mean you have 95% confidence that the data are normal, only that you don't KNOW that they aren't normal.  So yes you go forth and assume the data follow a normal distribution, but I think saying you have 95% confidence that the data are normal is incorrect.

Am I picking nits here, or is this an important distinction?  Or am I actually wrong?  Thanks for any guidance and/or explanations.",AskStatistics,2022-08-09 11:55:34,13
"> Do I need Bonferroni (or similar) in these cases?? 

If you want to control for *overall* type I error, yes, that or something like it. If you don't want to control for overall type I error, then no.

It's not really a statistical question, IMO. You can readily compute bounds on type I error under either scheme (that's essentially a stats question), but whether that's important to you is another issue, which has strong elements outside of statistical considerations themselves.

Why and how you chose your type I error rate(s) in the first place, and what your intended audience would find convincing, for example.",2,wkhgri,"Do you need to do multiple comparisons correction (i.e. Bonferroni OR any other) in the following cases (the cases are not related):

- I am testing a few hypotheses grounded in theory only.  I heard multiple comparisons corrections are really relevant most for very large comparisons I.e. DNA. 

- I have 2 measures of symptom A and 1 measure of symptom B.  These conditions are not linked.  If I test 2 hypothesis of symptom A, and 1 hypothesis of symptom B do I need to divide alpha by 3?  Or can I divide alpha by 2 for symptom A and use 0.05 for symptom B?

- If I have two separate participant cohorts and I want to test them both using two hypothesis.  Do I divide alpha by 2 or 4?  

- I have one cohort but two treatment phases.  I want to test 2 hypothesis during each phase.  Do I divide alpha by 2 or 4? 

I know Bonferr. Is too conservative for most things, no need to mention that. 

Thank you!",AskStatistics,2022-08-09 16:04:26,2
"That there would be heteroskedasticity when you approach a boundary (what I assume to be the case) is a no-brainer.

Presumably you also have nonlinearity as you approach the boundary (a line will be sure to cross a boundary that the data cannot), and likely changing skewness as well. The nonlinearity would likely be the biggest issue, then the heteroskedasticity.

Don't focus only on one aspect of the problem; a good solution will probably consider all of these aspects at once.",5,wkhphy,"Hi all, I running multiple regression with the response variable being the sum of scores on a well-known psychological scale; the model shows clear heteroskedasticity - funnel-shaped with very low variance for low fitted values and higher variance for higher fitted values. Is there a standard approach for addressing this?",AskStatistics,2022-08-09 16:15:36,4
"You might be able to test for differences in means through more standard statistical methods instead of through a model using contrasts. But yes, the third contrast is not independent from the first two. I’m not sure what the solution is but you might be able to come up with different contrasts that answer your question",1,wkcdfu,"Hi all, I have one categorical factor that has four levels: A, B, C, and D. I need to compare them in a generalised linear mixed effect model. I have three hypotheses, and their null version is:

1. A - B = C - D
2. A = B
3. C = D

So a contrast matrix should look like

||Contrast 1|Contrast 2|Contrast 3|
|:-|:-|:-|:-|
|A|1|1|0|
|B|\-1|\-1|0|
|C|1|0|1|
|D|\-1|0|\-1|

However, the model says rank deficient, so I think this means that contrast 2 and 3 are not independent from contrast 1. (contrasts need to be independent from each other in contrast setting).

So I think my coding scheme doesn't work. Is there anything else I can do to test my hypotheses.

I have already referred to Schad et al. (2020) in JML, but can't quite figure out :(",AskStatistics,2022-08-09 12:30:22,2
"I don't know anything about fantasy baseball, but I'd approach this as points above replacement (like the WAR stat).  I think you want to measure the difference between the player's points and some floor, like an average.

    =(player points - average points) / price",3,wk2j8o,"Apologies, this is a noob question.

I'm trying to create a score for the value of players in fantasy baseball.

Currently I use a simple points per value ( Total points / price ).

However, I would like to somehow weight it so that higher points score more highly in general. When doing a simple points/price the low scoring players come out too highly, when in reality these players are not as useful as the budget available to spend is well above their price point.

Something like... ( Total points \* weighting factor ) / price

Where the weighting factor needs to be variable, based on points.

Any ideas what I could do? Also, I'm using excel so it'd be fantastic if you know a function that can help!?

&#x200B;

Edit: Here's an example to explain

Player A has 100 points and costs $10

Player B has 50 points and costs $5

Both have a value of 10 Points per $. However, in reality Player A is much more valuable to my team. Is there any way to manipulate the data to represent this?",AskStatistics,2022-08-09 05:51:47,3
Regression or ANOVA,1,wjwayr,"Hello
I was hoping I could understand the best statistical analysis to use or if it is simply multiple individual paired T-tests. 
I will use a made up example involving cars where I predominantly want to see if a car tune up impacts the speed of the car before and after the tune-up

So say my dataset includes:

- One main independent variable which can be further stratified - eg: Car tune Up which can be stratified by the mechanic (two of them) AND also whether this was the car's first or second tune up. 

- All cars have received atleast 2 tune-ups

- Multiple dependent quantitative variables which are pre and post- eg: average speed, average RPM, average fuel consumption all before and after each ""tune-up"" - assume all are normally distributed

I understand that to just to look at car speed before and after the tune up would be a paired T-test but what about if I wanted to:

- See if there is a difference in speed improvement between mechanic 1 vs mechanic 2 

- See if a tune up made a difference on the other recorded variables (RPM, fuel consumption etc)

- See if there is a difference in speed improvement between tune up 1 vs tune up 2

I'm not sure I quite understand what test to do when I have paired samples but multiple dependent and independent variables like this. Is it just multiple T-tests?

Thanks for all help in advance.

Edit: formatting and clarification",AskStatistics,2022-08-08 23:47:37,1
">  I was reading about how normality doesn't matter when there are a large number of data points. 

This is not really true. For some things it doesn't matter, for some things it may matter quite a bit; for many it's more like ""it depends"". Additionally, there's the problem of 'how large does 'large'?"" need to be, and again, it depends -- on things you generally won't know.

Your biggest issue with time series is usually not so much normality as dependence and not being identically distributed over time (time trends, for example)

Not also that ""60"" is not necessarily ""large""

> when I break up a sjngle time series data into two with 30 points each, the kurtisis reduces to ~3,

This is a very clear indicator of exactly the problems I pointed to above; you apparently don't have independent, identically distributed data. You can't just ignore *all* of the assumptions.

What are your response variable(s)? What values can they take?",1,wjvz0b,"Yesterday, I was reading about how normality doesn't matter when there are a large number of data points. Coincidentally, there has been a post in this sub just a few hours ago in which people discussed the same. Now, I have 18 time series with 60 data points each and I need to assess their homogeneity. I have used non-parametric tests and now want to use parametric tests (cumulative deviations test). The time series have a kurtosis varying from 9 to 25. However, when I break up a sjngle time series data into two with 30 points each, the kurtisis reduces to ~3, probably. I was wondering if it is okay to apply the parametric tests for the larger samples if normality doesn't matter in their case?",AskStatistics,2022-08-08 23:26:28,1
"Do you know the name of the distribution that H has? Do you know anything about its mean and variance, and do you know about formulas for expressing those?",2,wjmwa3,,AskStatistics,2022-08-08 15:58:07,8
50 is an awfully large number of outcomes for a logistic model to handle. It sounds like the outcome may be a count variable-- have you looked into zero-inflated poisson regression? Or perhaps negative binomial regression?,3,wjleqq,"Hi all:

What kind of regression model should I use?

My outcome variable of interest are real numbers between 0 and 50 or so max. Approximately 95% of the sample are zeros. I also have a set of explanatory variables. 

I have been leaning towards an ordinal logistic regression, which seems best suited to handle the preponderance of zeros in the outcomes. 

However I would very much welcome your feedback and thoughts.

Thanks!",AskStatistics,2022-08-08 14:56:01,5
"Residuals are calculated by (y - y-hat) where y-hat is the predicted value of y for each observation given the model. Basically it takes all the points it used to calculate the regression coefficients and plugs them back into the model to spit out the predicted y.

Also this is the residuals of the full model (unless I totally misunderstood your questions). If you calculated a new model with additional variables or different variables that model would provide different residuals.",2,wj9wbf,"I am doing some assumption checking for a multiple regression and I was wondering how the residuals are calculated. Are they based on all of the IVs included in the model, and if so, how? Also, am I correct in assuming that it would be wrong to only test the primary IV's residuals for normality?",AskStatistics,2022-08-08 07:13:26,2
"It sort of sounds like you're maybe falling into this error: 

http://www.stat.columbia.edu/~gelman/research/published/signif4.pdf

(But also it's not clear to me what the need for that particular kind of shuffling/randomization is but perhaps there's something about your setup I didn't follow.)",1,wjko0e,"Hi all.  


* So I have being examining the  cross correlation of a distribution of calls of our animals (periodicity of calling) across synchronized recordings (so two mics, synched up in time).  

* We have six measures of this output, which we normalized. For these six measures, only the centre magnitude was significantly different between treatment and control in a t-test, with treatment producing a value for this \~ twice as large as control.   

* Next, we compared the control to randomized data, and the treatment data to randomized data (by shuffling each subject's data 100x and taking the mean of the 100 shuffles).   

* Then we performed t-tests comparing the actual control data against the shuffled data, and the treatment data against the shuffled data. For control data, all 6 measures were significantly different than the randomized data (with most t-tests revealing a p value <.001). For the treatment data, the only measure that was significantly different from the randomized data was centre magnitude, with all other measures revealing p values that were quite far from our alpha of .05 

&#x200B;

So what can we really draw from this?  Merely that treatment appears to be more variable? Are there more appropriate tests that should be conducted?  


Any advice here would be much appreciated.",AskStatistics,2022-08-08 14:25:25,1
"I’m slightly confused by your explanation of the dice roll. I ran some quick simulations. Choosing a random number between zero and 1. If it is less than .0001, use T5, if between .0001 and .0011, use T4, if between .0011 and  .0111 use T3, if between .0111 and .1111 use T2, else T1. Then chose a random integer between 1 and 7 to use for the A-G.

The odds of receiving at least one T3A and at least one T4A are about 2/1000. So if you had 1000 account playing the game, you could expect two of them to get a T3A and a T4A within 100 days. Since the odds for B-G are the same, you would expect about 14 accounts to get a T3X and it’s matching T4X. (Roughly)

I just simulated in Excel using 27000 columns of 100 rows, each with T#X randomly chosen, and counted how many columns had at least one T3A and at least one T4A. Any refinement is left up to the reader.",1,wjjs1k,"Hey! I'm trying to work out the odds of something occurring and running into issues trying to figure the odds correctly, to give a bit of background this relates to a game and it's more of a discussion point trying to figure out the odds of this situation happening, additionally, we are looking to find the odds of this occurring over a period of 100 days

Every day this event can happen once, we are looking to measure over 100 days.

Every day you have the ability to spawn 5 different levels of items with the following odds:

 
- T2 - 10% chance
- T3 - 1 % chance
- T4 - 0.1 % chance
- T5 - 0.01 % chance

- T1 - if T2-5 fails then it will 100% become a T1 level

What's also important to note is there are 7 different kinds of items that spawn:

A/B/C/D/E/F/G

They have equal odds of happening, there isn't one that is rarer or more common.

What I'm trying to work out is the chance of, over 100 days, having a T3 and T4 of the same kind e.g T3A and T4A, actually occurring. 


I was reading [here](https://oldschoolrunescape.fandom.com/wiki/Drop_rate#:~:text=When%20calculating%20a%20drop%20rate,75%25%20drop%20rate%20from%20chickens) about how just because you have a 1:100 chance of something happening that doing it 100 times only gives you a 63% chance of it actually occurring. When applied here it seems to become very complex, is there a simple way to figure this out?

edit:

Some additional rules

- If you roll a T3A then you are still able to roll another T3A the next day
- it rolls in order e.g T2 has 10% chance, this is checked first before checking T3",AskStatistics,2022-08-08 13:49:38,3
"I don't follow this, sorry. Why are you using a normal distribution for something that can't be normal?",1,wjgoc8,"I need some help with statistics. I haven't taken the class since college (13 yrs ago).  


I have 1 set of values I used to calculate the distribution data series in Excel using the ""NORM.DIST()"" function. This gave me a bell curve chart in Excel. The call data is in seconds and you can't have negative seconds in the metric I am using. The data crosses 0 between the Mean and -1SD.   


With the normal distribution calculations, there is 15.79% probability the call data value will be greater than 146s. This is the probability area under the curve to the right of the +1SD. How do I convert this to cumulative seconds? And is the Normal Distribution curve format the best solution, or should I try something else (e.g. t-test)?  


Also, i'm not sure if this is accurate or not, but I added a second set of values to each section label for an adjusted %. The adjusted % are the normal distribution %'s proportionally increased by the cumulative probability to the left of 0.  


Any help is greatly appreciated!

https://preview.redd.it/euums5qnbjg91.png?width=1001&format=png&auto=webp&s=264337d20a68e6e8f18156e6b5a4f09e0bd07205",AskStatistics,2022-08-08 11:45:03,4
"It is very unfortunate, but it still is a fact that most ""validated"" psychometric tests do not remain, or even internally consistent when you use it for something slightly different than what it was originally designed for. Even if you test a different age group, or different socioeconomic group, you can end up measuring something completely different, or measuring nothing. This is because most of these scales were validated decades ago, and the people validating them did not believe it necessary to perform any type of multigroup analysis for factorial/measurement invariance. It is ALWAYS a good idea to verify your measurement instrument's validity and reliability on your sample. It is good for you, and it is good for the authors of the instrument.",2,wjadiy,"Hello

I've looked through a few stats and research sites, but can't seem to find a clear answer here. Is it necessary to find internal consistency reliability (such as Cronbach's alpha - although I am aware of the issues with Cronbachs) of already validated psychometric tests? Such as a Big 5 test, or the Becks Depression Inventory etc? I know these established tests have published norms and psychometric properties already, can I use these and state that the test has good internal consistency, or is it better to recalculate based on my sample (general population, not some out of the ordinary group)?

All answers great appreciated - thanks!",AskStatistics,2022-08-08 07:33:34,4
Okay in what sense? I don't understand why you want to do t-tests or chi-squared tests first.,2,wj9t41,"I have a small set of predictors (7) but will probably have a small sample size (\~80). Would it be okay to fit logistic regression models that include age, sex and a single predictor as covariates? I would do this for each predictor that is significant following statistical testing (t test or chi squared test). 

My objective is to identify significant effects, adjusting for age and sex.

Thanks.",AskStatistics,2022-08-08 07:09:40,3
"1. Your description of how the combat works in Risk looks incomplete. If I remember right (it's been a long time) the defender may be rolling two dice or one die while at the same time the attacker is rolling at least as many dice, dropping their lowest dice - they compare the same number of dice - but the defender wins ties. (This is still somewhat incomplete but it does indicate you seem to have left stuff out.)

2. 3 isn't the average on a single die. It's 3.5. So for example when the defender rolls two dice their average is 7 not 6.


The thing with substituting another mechanic for the one currently in the game is that you want that the chances of the attacker winning in each scenario (2-vs-1, 3-vs-1, 3-vs-2 etc) to be about the same. 

In the actual game mechanics, for 2-vs-1 the attacker wins about 41% of the time. If you have a single defender and call their roll a '3' while the attacker's still rolling as before and taking the best die, the attacker would win 75% of the time. This is a big difference.

Similarly 3-vs-1 the attacker should win about 75% of the time but with your proposal the attacker would win 7/8  (87.5%) of the time.

You'd actually get considerably closer to the right chances if you called the defender's single die a '4' but it's still a bit out in both cases. We also haven't looked at the case where there's 2 defenders, but I believe that using 4 for that case won't be close enough.

---

(btw -- the plural of die is *dice*)",2,wj8tch,"Hello! This weekend, after a couple drinks, my friends and I decided to play the famous game of global conquest, Risk. 

After a couple hours, it was already 1:00am and I was not only exhausted; I was losing. I was controlling Africa, and was almost certain that I was going to get steamrolled, which would result in the game becoming a boring repetitive process of rolling defense dice against attackers. 

For those unfamiliar, **the combat in Risk is as follows:** The attackers and defenders roll dice, and they are put head to head, with defenders winning on ties. Attackers, if able, can use up to three dice, and will take the top 1 or 2 results out of 3 based on how many defending troops there are.  

I told my friends I was going to bed, and that they should take the **average dice roll, a ""3""**. This began a LONG argument, mostly out of curiosity, about whether or not that is fair. My friends argue that this would make the game less random. My theory is that it would be equivalent for the reason that the attacker is still rolling, so it's still entirely up to chance.

For example, if you are familiar with Dungeons and Dragons, attacking in that game requires to roll a twenty-sided die, where you ""critical hit"" only if your Hit Die returns a 20. My argument is that, hypothetically, one could instead of capping it at ""20"" fixed, first roll an auxiliary die which would ""decide"" what specific number would need to be rolled on the hit die in order to critical hit.  This would create more perceived excitement and randomness, but the odds are still exactly the same. (1/20 = 5%). 

Therefore, in the Risk scenario, the defender's dies act as either a fixed number, or one which changes based on the defender rolling it, but it's only a **perceived** change and the actual statistics are exactly the same. 

&#x200B;

What do you think? Help with settling this silly argument would be great, none of us can think of a great way to prove our side.",AskStatistics,2022-08-08 06:25:10,2
"Can you quickly test for group differences between physician/specialist in your main outcome variable(s)? The argument ""there are no significant differences in sleep quality between these groups"" could be a reply to the juror's claim that you need to stratify. If there are differences (or if you can't test them), you'd do best to acknowledge that in your discussion/limitation section. ""Due to unavailability of data, we could not exclude possible confounding influences such as ..."". 

In both cases, politely thank the juror for their helpful comments in improving your research while forcing a smile.",7,wixqs2,"Good evening friends, I tell you that I am doing my thesis, I am investigating whether there is a relationship between burnout syndrome and sleep quality in the medical staff of 3 health centers in my city. For burnout syndrome I am using the MBI scale (Maslach Burnout Inventory) and for sleep quality the Pittsburgh questionnaire  

One of the jury's corrections was the following: ""you have a bias because you cannot include a physician (a non-specialist physician) and a specialist, because the specialist does not do night shifts (work at night), and also the specialists may work in other places apart from the hospital. So you should have segmented them or asked that in your demographic file to exclude or include in your sample and make it more homogeneous""  

Well, I was dismayed at this, since I only need to make minimal corrections to support my thesis, all the work has already been done and I had not thought of it that way. Actually, the objectives of my research are: to determine the relationship between burnout and quality of sleep, describe burnout in my sample and finally describe the quality of sleep in my sample. What should I do, or what do you recommend me to read? I'm thinking that the juror who told me that doesn't really know much about statistics and only told me that because she's mean (she has a reputation as a mean person at my university). I will be grateful to you if you can guide me, thanks.   

&#x200B;

PS: It should be said that she (the jury) began by telling me that she had done her thesis on: quality of sleep in doctors who only did night shifts.",AskStatistics,2022-08-07 20:00:28,11
"> Can the results of a particular Mann-Whitney U-test be generalized to the population the sample(s) came from?

Sure, if there was a suitable basis to do such a generalization, just as with any other test. The usual such basis is random sampling from the population of interest, which works perfectly well for this test without any need for adaptation of the test. 

> So if the comparison is between men and women’s response to a tv show in Canada, can the results of the test be generalized to all men and women who watched the show in the US?

No. Or rather there's no basis to do so in what you have mentioned. You'd have to provide an argument that would say that the results would generalize from one country to another like that. I don't presently see any  suitable basis to do so.

This is no different for any other test.",3,wix1o4,"Was reading an interesting study today and they were evaluating results with a Mann-Whitney U-test. I’ve only taken AP stats in high school lol so I know nothing about this. I looked around online and couldn’t really find an answer. 

Can the results of a particular Mann-Whitney U-test be generalized to the population the sample(s) came from? So if the comparison is between men and women’s response to a tv show in Canada, can the results of the test be generalized to all men and women who watched the show in the US?

Idk if this is obvious and just means random sampling had to be used to produce the sample or if I’m thinking about this all wrong.",AskStatistics,2022-08-07 19:26:03,7
"Testing for normality is useless. Large enough data and you will always reject normality because nothing is perfectly normally distributed, and smaller datasets you will fail to detect that they aren't normal because the tests lack power.",23,widn41,"hi, I hope its not too stupid of question

but we learned about godness of fit test, and we can use it to check if a sample comes from normal distribution

so isnt it much better than using qqplot to check if data is from normal distribution?

since qqplot is pretty much more of ""guessing"" game if we think the line and dots look good enough

and using the godness of fit test give us a more accurate / concreate way of deciding if its actually noraml",AskStatistics,2022-08-07 04:33:14,7
"Statistically, you are probably looking for a one sample ttest comparing the average rating with zero.

Scientifically, you are trying to prove that photos are inherently linked to emotions, which is probably controversial. References in image processing and in psychology/neuroscience disagree a lot in this field.",1,wipzby,"I'm doing an experiment using a set of pictures that have previously been rated for their emotional content. I want to have my participants rate the pictures as positive, neutral or negative so I can check that these pictures actually have the emotional valence I'm claiming they do, but I'm not sure what test I should use to do this, as I don't have the raw data these standardised ratings are based on? Any help would be appreciated, also please talk down to me I am very dumb and need all the help I can get.",AskStatistics,2022-08-07 14:00:49,5
"I'm not particularly knowledgeable about this one but I believe it's appropriate for this situation; indeed it appears to be cognate with Armitage's (1955) gastric ulcer example. 

There's other possible tests (or as Agresti puts it, ""many""), but this should be fine if you're happy with the specification of linear trend in the probabilities -- it will still be sensitive to monotonic trends but does best on the specified linear ones; indeed if the overall trend is strong it will generate significant results even when the trend doesn't follow a monotonic ordering; on the other hand, so could most other suitable tests)",1,wiuocp,"Doing an unmatched case-control retrospective study. Controls and cases are patients were successfully cleared of infection vs. those who had recurrent infection within 2 years of treatment (all patients received the same treatment). Looking at factors associated with success vs. failure. One of the variables the patients' pre-treatment health grade from a validated classification system that stratifies patients by overall health into grades 1, 2, or 3 based on comorbidities and other clinical/infectious data, with grade 3 being the least ""healthy."" 

I have a 2x3 table with the 3 grades vs. the 2 groups (cases and controls). Can someone confirm for me whether a chi-squared for trend (aka Cochran-armitage) is appropriate to see if health grade is significantly associated with treatment outcome? I believe so since the grading variable is ordinal, but I've never used this test before so I'd appreciate another opinion!",AskStatistics,2022-08-07 17:31:12,2
"> What does the Y axis tell us?

The same thing it always does, it's still a histogram.

> Are the bins weighted, or are the values put into them weighted?

Each observation gets a weight. A regular histogram is equivalent to a weighted histogram with all weights set to 1.

> How are these weights chosen?

That depends on the reason that one is using a weighted approach.

> Why do we construct weighted histograms, surely unweighted histograms give the best picture of reality?

Again, that depends. 

Here's a very cartoony example. Imagine you polled 1000 people on the phone, but you realized the demographics of your respondents are not representative of the broader population. In particular, your question of interest was a 1-100 rating, and you also asked about age and sex of the respondent. You get the right age distribution, but your respondent pool is is 25% men and 75% women while your population is 50-50. In this case, the *combined* histogram of men and women won't be a particularly great representation of the overall combined male+female population. You could try to fix that by giving each male response a weight of 3 and each female response a weight of 1. The weighted histogram will then hopefully better-reflect the population. The weights don't add up to the number of samples anymore, so perhaps normalizing it to a density makes a bit more sense.

Someone can correct me if that's a bad example, I don't do social statistics or work with sampling like that. The sort of places I've needed weighted histograms is when dealing with techniques like importance (re)sampling. If you're trying to make statements about a distribution *f*, but you can only draw from some approximation *g*, then you can re-weight your samples according to the importance weights (proportional to *f*/*g*) and a weighted histogram will show you the distribution you're after (if your approximation isn't too awful).",1,wist2a,"Lots of the information online is very abstract and doesn't explain this in an easy to understand way. I (think I) understand regular histograms: you simply define some bins, take some measurements and count the number  of times a measurement falls within a certain bin. This results in a  graph where the X axis is the values, and the Y axis is the number of  times a measurement gave a value which fell in that bin. I know that the  Y axis of histograms can be scaled such that the Y axis gives the  proportion of measurements which fell in that bin, and the sum of the  heights of all bins should be 1. AFAIK this is different to normalising the histogram, where the *area* of  the histogram is 1. I am not entirely sure why weighted histograms confuse me, so I would appreciate if someone could kind of ELI5 them. A few things I immediately don't understand about them:

* What does the Y axis tell us?
* Are the bins weighted, or are the values put into them weighted?
* How are these weights chosen?
* Why do we construct weighted histograms, surely unweighted histograms give the best picture of reality?",AskStatistics,2022-08-07 16:03:40,1
Why don't you want to write the GRE?,1,wiqfp8,"I am building my list of graduate schools I want to apply to for next year. I am looking for an applied statistics MS, biostatistics MS, or something similar. I am specifically looking for universities that don’t require GRE or GMAT scores. I have a near 4.0 gpa in statistics at my current university with 9 credits left to graduate if that information is helpful. What universities would Reddit recommended?",AskStatistics,2022-08-07 14:20:19,3
">  1) Days per week wearing any brand sports bra and 2) Days per week wearing Brand X sports bra. Both variables range from 0-7 days.

okay

> want to see if the percentages for each day

According to your previous statement you don't have this information. You said you have *two* variables, each of which is a count of days per week, but not which days they were.  To do this you'd need 14 variables one for each day for X and overall.

> are significantly different between the two variables. 

This is the biggest issue. Brand X is a proper subset of ""any brand""  so you're testing the hypothesis ""is the proportion of times wearing anything *but* brand X exactly zero on each of these days"". i.e. you're testing whether *nobody ever wears non-brand X sports bras* on each of the days.

This is not tenable. A single person in the population -- just one out of millions -- wearing any brand but X on a given day of the week -- *ever* -- makes H0 false.",2,wijfl1,"I have two variables: 1) Days per week wearing any brand sports bra and 2) Days per week wearing Brand X sports bra. Both variables range from 0-7 days. I've created a visual chart comparing the two and want to see if the percentages for each day (not the overall mean) are significantly different between the two variables. For example, is there a significant difference b/w those who wear any sports bra 1 day per week and those who wear Brand X sports bra 1 day per week. Again, I don't want to see if the means for the two variables are the same, but rather for each number of days within the two variables. Is this possible, and if so, how can I perform this type of test in SPSS?",AskStatistics,2022-08-07 09:16:04,1
"Alpha needs to be calculated from your sample, using a dedicated statistical suite that can calculate it (SPSS, R, SAS, STATA, Minitab, whatever you prefer), but alpha is emphatically NOT a good measure, and people have been recognizing it more and more. The original authors probably have an alpha value from their own original study somewhere in the article as well.",1,wim5mn,"This is probably a silly question so I apologise in advance!

For my dissertation I used the BFI-44 and the DASS. My question is how do I get Cronbach's Alpha? Do Is there a published figure for each scale or is it something I calculate myself for my own sample? I don't have access to either manual, so can't refer to them unfortunately.",AskStatistics,2022-08-07 11:14:52,1
Looks like it's as a percentage of respondents.,2,wil6h9,,AskStatistics,2022-08-07 10:32:26,11
"There are a variety of model comparison statistics you could use. Adjusted R-squared, AIC, BIC, or Bayes Factors would all work, even on non-nested models. 

Typically people use more than one of these, to get converging evidence (and they have subtly different conceptual meanings!).",5,wiih2q,"Hi guys,

I have two regression models (done on stata) and have been asked to choose which one is better, I was just wandering how I would go about doing this and what statistics I should be looking for to choose one over the other? 

&#x200B;

Thanks guys!",AskStatistics,2022-08-07 08:34:05,6
"roughly exponential growth plus slowly changing seasonality, yes, combined with an almighty covid shock.

If you *only* fit seasonality you're going to have a bad time. Also, most of your regression output is going to be nonsensical because of the nonstationarity; even after you fix that you'll still have serial dependence.


If you were just modelling pre-covid, taking logs and seasonal differences would be my first thought before even really thinking about an actual model.",5,wi4u44,"https://prnt.sc/Eej_TPPfZjKy

Hello, im not sure if this shows seasonality? there it looks like there are regular ups and downs with a overall increase of sales.

I thought it had seasonality till i saw my R2

https://prnt.sc/R4HFjPp-WA7J

What am i missing here lol is it cyclical?",AskStatistics,2022-08-06 19:33:06,2
"There is no concern about multiple comparisons if you do ANOVA comparing ~ Sunlight + Fertilizer (alternative hypothesis) to ~ Sunlight (null hypothesis).

But there may be some concern to combining the data sets from two different experiments.  That, however, is a scientific question rather than a statistical one.  If both were controlled experiments, the controls may go out the window when the experiments are combined.",2,whqg42,"\*Details/variables changed to preserve anonymity.\*

Factorial ANOVA was used to create two models: 

PlantGrowth \~ Sunlight (Low, High) + Fertilizer (two levels: A, B). //Study 1 

PlantGrowth \~ Sunlight (Low, High) + Fertilizer (two levels: C, D). //Study 2

&#x200B;

These two models form two hypothesis. 

H1: Plantgrowth is affected by sunlight and fertilizer //Study 1

H2: Plantgrowth is affected by sunlight and fertilizer //Study 2

\*The two studies have different fertilizers\*

Study 1 found that fertilizer did not play a role in PlantGrowth. Study 2 found that fertilizer played a role in PlantGrowth. 

The paper then lists a third hypothesis: 

H3 (paraphrasing): Plantgrowth is affected by sunlight as well as all variations of fertilizer. 

However, authors don't create a new model (which they should IMO):

PlantGrowth \~ Sunlight (Low, High) + Fertilizer (A, B, C, D). 

They use the results of the previous hypothesis to answer H3. I feel like the hypothesis is about considering the fertilizers \*overall\* and it's inappropriate to analyze them separately. Also, is there a concern for multiple comparisons here?",AskStatistics,2022-08-06 08:06:03,1
Can you provide some explanation about what the first picture is showing?,3,whwikg,"Hello! Till recently I had no idea about p-value but now one part of my thesis had some statistical stuff and I have no idea how to convert X symbols into letters that show significance like in the second table. How do I do this? 

https://preview.redd.it/75iqkbrxb5g91.png?width=143&format=png&auto=webp&s=a1584a1c1ec5e5045ca9a0f9dbf7a4487610a989

https://preview.redd.it/86xf1k0zb5g91.png?width=176&format=png&auto=webp&s=907a38a42518a7b94acfb16540e83ed42df01b25",AskStatistics,2022-08-06 12:44:41,4
"Hey, it was not clear how different questions can lead to the same variable.
And also, what are you measuring? What is the scale?",2,wi1j91,"Hello all,

I'm helping a friend with data analysis but I got a little stuck at one point and I need a recommendation.

So there are 3 groups in its data. Each group was subjected to different questions that would create the same variable. However, the second group was asked 2 questions about this variable, and the third group was asked 1 question. I thought I could solve this problem by centering the total scores, but I noticed another problem. The common question asked to the third and second group has multiple answer option. No one in the second group gave multiple answers, but the third group did. Now, according to my research, I can reduce the answers to a single column by doing Count Values within Cases in SPSS, but this time we lose the information on which answer they gave.

What would you suggest about this? If I apply this procedure to both groups, the answers of the second group to that question will always appear as 1. In this case, I cannot sum and center to create second groups score.

Thank you",AskStatistics,2022-08-06 16:40:43,2
"What do you mean by ""we roll a 20-sided die and add up the results to a perfect 100""?  You rolled a 20 sided die until the sum of rolls equaled exactly 100 or else you started over from 0?",2,whq20l,"Yesterday my friend and I played this game where we roll a 20-sided die and add up the results to a perfect 100, then we have to flip a nickel and get heads three times in a row. I thought this was impossible or at least extremely unlikely as rolling for a perfect 100 would be hard and then having to restart if you don’t get three heads in a row seemed to add tit he impossibleness but low and behold I did it on my first try. How lucky was I?",AskStatistics,2022-08-06 07:48:11,8
You can use a q-q plot for any data to assess normality. I wouldn’t necessarily call it a “test.”,3,whuwa6,"Can a q-q plot be used to test the normality of residuals in multiple linear regression, or is it only suitable for simple regression?",AskStatistics,2022-08-06 11:28:41,3
Levene's is a test for equality of variance. Why would that suggest Mann-Whitney?,1,whze5g,"Hello!  I ran an independent t-test and Levene's test came back significant  (<0.05). When I checked for outliers, I found a few in my data. I'm  looking at the difference in waist circumference and BMI in control and  experimental group after following a Mediterranean diet.

I'm wondering if I should remove the outliers and try the independent's t-test again.  
Or,  keep the outliers and do a Mann-Whitney test instead. My professor told  me that if Levene's test is significant to use Mann-Whitney.

I'm not sure what instances should I keep or remove outliers. I have access to Laerd but I can't seem to find information on outliers there. How do you typically go about outliers in your data?",AskStatistics,2022-08-06 14:58:41,6
Yes you can. It would just be a one-tailed test where H1 posits that the effect is > 0.,2,whxwcr,"Hello, I am currently formulating  hypotheses for an experimental study. Can I e.g. predict a ""positive main effect"" ? Or are main effects just main effects and we can only say there is a main effect for iv1 and the manifestation of the iv1 has a positive/negative effect on the dv?

What do you think? I'm not sure..

Thanks for your opinions:)",AskStatistics,2022-08-06 13:49:32,1
It's not clear what you're talking about. Can you give an example?,1,whxa63,"This is probably super easy, but my statistics education is somewhat poor.

Let's say you have combinations of 10, in groups of 3 (10 combinations of 3), and you will make them sequentially.

How many tries will you need for a particular number (let's say ""1"") to appear in over 90% of the times.

Thank you in advance, and if there's a formula for this, please share, so I don't feel ignorant.",AskStatistics,2022-08-06 13:20:33,5
"Back up a minute. What are you actually trying to do here? You’ve given a joint density and said you want to integrate something, but what thing and why?",1,whovus,"I'm super blanking on how to even think about this.

f(x,y) = 1 for 0<|y|<x<1

I know that f(x) = integral of f(x,y) evaluated over the range of possible values for y and vice versa.. But I'm confused on how to arrive that those. Initially I thought that this would be -1 to 1 for the range of y values and 0 to 1 for the range of x values. But I think it needs to ultimately involve x and/or y since those boudn each other as well?

Can somebody walk me through this?",AskStatistics,2022-08-06 06:54:51,3
"> but if r=1

> does it mean that x and y must be linear?

If exactly 1 (rather than say being displayed as 1 when 0.996392763 is rounded off to 2dp), yes. Otherwise you could see something displayed as 1.00 while the relationship is not linear (in some cases quite clearly not)",2,whorxq,"hi

I learned that r signify the strength of the linear correlation betwen x and y

&#x200B;

so while |r| might be for example equal to 0.9

while it does indicate that there is strong linear correlation, it dosent necessary mean that the correlation between x and y is actually linear

&#x200B;

but if r=1

does it mean that x and y must be linear?",AskStatistics,2022-08-06 06:49:39,4
There are many ways this can happen but the most general statement is that the slope of the relationship between A and O is not the same as the slope of the relationship between the residuals in A after being predicted by B and O.,1,whva3k,"Let's say I did a simple linear regression and found that predictor A had a significant effect on outcome O, with the beta coefficient being -0.481. Then, I did a multiple linear regression and after I adjusted for another predictor B, I found that the effect of A on O was still significant, but beta coefficient was now -0.213. 

How can I explain this change in concise terms? Could I say that the coefficient of A decreased because predictor B also had a significant effect on the outcome?",AskStatistics,2022-08-06 11:46:19,2
"I presume you're doing this because you want to use a test that assumes normality. 

1. Don't do two tests. Pick one *before* you see the p-value. 

2. Failure to reject means didn't identify that it was not normal, it doesn't mean the null was true. 

  *Of course* the population distribution isn't actually normal (indeed provably so in this instance; I just had to see the name of the variable to know that). That's not even a useful question to answer (whether or not the population distribution is exactly a normal distribution). 

   You really need the answer to a different question. One that's not best answered by hypothesis testing; the question is how much impact the non-normality in the population would impact the properties of your inference.

As to why the K-S would tend to reject and the A-D would not, when the A-D has a reputation for good power:

 The Anderson-Darling is a biased test (all goodness of fit tests are); in its case, at small to moderate sample sizes it's *extremely* unlikely to reject close-to-symmetric lighter tailed than normal distributions. (This is also true of the Kolmogorov-Smirnov but to a smaller degree in that situation.)

Is this particularly consequential? Hardly at all for most tests people want to assume normality for; why the heck would you worry about slightly lighter tails (though it's hard to tell for sure with just a histogram to go by) in a small sample?

But in any case, the sample may not be especially relevant to the question of the assumption. About the population. Under H0.",6,whk924,"&#x200B;

https://preview.redd.it/vnm4bkkn92g91.png?width=1016&format=png&auto=webp&s=a228676cda04a5335b2ead43fd75eab124ddb95e

https://preview.redd.it/twc25gsua2g91.png?width=1016&format=png&auto=webp&s=b3cfcb758afebfc0e715d50c164d81353517ba87

So I did a normality test with 2 different methods, KS-test and AD test, then it shows a different result. KS-test shows these data is not normally distributed (p-value < 0.05) and AD-test shows these data is normally distributed (p-value > 0.05). Now I'm confused, which one should I believe?",AskStatistics,2022-08-06 02:28:11,1
"**R^2 is not about ""effect sizes""**: An effect size is a statement about how large of an effect a change in x has on y. There is only one way to correctly explain R^2 that is correct, and one can do it with simple words that are ""correct enough"" to convey most of it.

Let's say that **variation** means ""ups and downs""- a measurement of how far some people are above and some are below average. Then an R^2 =0.67 means that:

""67% of the variation of brain function recovery can be predicted using age"".

Once again, this is not related to ""effect size"", just as |correlation| tells you nothing about how steep the slope of a relationship is.",3,whltmr,"For instance, let's say that we found R-squared = 67% for the effect of Age on Brain Function Recovery following an accident. How would you go on to explain the concept of effect sizes to a child with no statistical knowledge?",AskStatistics,2022-08-06 04:09:34,2
"Not if you're using the same data to select the variables and estimate their values. Biased coefficients in included variables (away from 0) and biased standard errors (mostly toward 0 on included variables) do not make for reliable predictions in general, and tend to yield poor coverage for prediction intervals.

If you use data splitting, *maybe*; you at least avoid some of the issues that way, though you still may not have such great MSPE. Personally I'd still want better regularization.",7,whfj0x,,AskStatistics,2022-08-05 21:33:57,2
I think this probably breaks rule 2 ... but I'm going to leave it for now,1,whcxsm,"Hello,   
While looking for a hobby project, I stumbled upon Gnu Dap, which is an open source implementation of SAS. From what I can tell there are a few pieces missing such as macros & datalines, and the project itself has been dormant since 2014. Would there be an interest in revitalizing the project to be an open source drop-in replacement for SAS?

Thank you!",AskStatistics,2022-08-05 19:15:04,1
"Offhand I don't know of any specific name for such a ratio; it's a ratio of a pair of symmetrically-spaced quantile intervals. You could no doubt come up with one. 

I have seen formulas somewhat like it a couple of times before. If I happen to remember where I'll give some pointers. Does seem reminiscent of some of Blum's formulas, but that's not where I'm remembering it from.

What area are you working in? Presumably the motivation for such a formula was not really based on some theoretically optimal estimator. 


>  For random values, it is approximately 1

You're supposing some distribution when you say ""random values"" but you don't say *which* one. Presumably a symmetric one given the quantiles involved.",1,whf817,"I came across some code doing automatic data editing, using various summary statistics. It has a formula

(P75-P60)/(P40-P25)

Is there a name for something like this? For data in question, it gives a value of approximately 2 for good data. For random values, it is approximately 1. A high value can indicate other anomalies.",AskStatistics,2022-08-05 21:17:12,1
"If you have suitable summary information - typically mean, SD and sample size - you can do t tests and anova

If you're prepared to assume equal population variances (as you have done already if you were prepared to use the ordinary two sample t-test) then its possible to estimate that common variance from just one of the samples but it modifies the test slightly (the df are based on your sample alone). In that case just the original  sample mean and same size would suffice. 

If you are trying to show a difference from some other study's population mean, you could use a reasonable lower bound - if one can be worked out in some way - on sample size in place of knowing the exact value. (Alternatively if you were doing something like an equivalence test to show *equivalence*, then an upper bound would need to be used instead)

Lastly, if you didn't know the standard deviation and didn't want to assume population variances were equal you'd need some kind of bound or estimate on the ratio of population variances. There may be some circumstances where this is feasible. 

For example, if you have other sample information -- if you have the range you can bound the standard deviation, if you have the interquartile range it's possible to construct a test\* for this case.

\* I strongly advise against just scaling for population ratio of IQR to standard deviation unless the sample size is huge. Nevertheless a suitable approximate t-test can be obtained, but you need several different formulas for how the d.f. relates to specific sample sizes (depending on exactly how the quartiles are being computed; typically you'll need four formulas), and a scaling constant.",1,wh0n4n,"I.E. if a scientist had access to their own dataset on, example, heart rates in such and such an intervention group, can they test for significant difference against previous published work, or would they need the *whole* data set from previous studies for inferential analysis? It must be possible *somehow* or else meta-analysis wouldn't exist but it's the one thing that never came up in my stats modules. I assumed meta-analysis always compared mean 1 vs mean 2 vs mean 3 for difference, but maybe I've got the wrong end of the stick?",AskStatistics,2022-08-05 10:03:01,1
[deleted],2,wh0908,"I’m trying to understand statistics at a “real world” level. Let’s say, for example, that I want to know the probability that I will randomly run into someone who also voted in the last election, and here‘a where I got stuck:

Number of people in total population: variable A

Number of people who voted: variable B

Me: I voted (100% chance)

Random person X: chances they voted?

So my thought was to take the rate that the population voted (B/A) and apply the rate evenly to me and the other person (X). But, if I voted, wouldn’t it change the probability of the other random person? And is this how they calculate this in general or is there a more specific way to think of this? So here’s where I got stuck, and I was trying to conceptualize this. Can someone explain this to me like I’m a 5 year old",AskStatistics,2022-08-05 09:47:02,8
"In my opinion they look at historical data - eg how many people got struck by lighting per year over the last 20 years. 

For those type of event, it seems the only realistic way to do it.",8,wh007u,"I always wondered what the concept and approach is to calculate it. What numbers do I need, and how do I set up the problem? This might be a very basic stat question, and I’m a total novice, so please be forgiving if this is a stupid question.",AskStatistics,2022-08-05 09:36:47,8
"> You have a bin of N balls. You take a simple random sample of n balls from the bin. Each of the n balls are of a unique color. What is the probability that all N balls are of a unique color?


As framed that's a Bayesian question -- to answer it you would need some form of prior information, such as a prior distribution over the possible ball colour distributions  (and yes the word distribution is deliberately used twice there)",1,wh6afi,"Problem statement:

Goal: Conclude with high confidence that each observation in a population is unique with respect to a certain trait, without examining each element of the population. 

We can model this with balls in a bin. In this case, the goal is to conclude with high confidence that each ball in a bin is a unique color, without checking every ball.

Thinking of specific questions one can pose for this situation. One way to pose this may be the following:

You have a bin of N balls. You take a simple random sample of n balls from the bin. Each of the n balls are of a unique color. What is the probability that all N balls are of a unique color?

Or perhaps another good question for this practical scenario:

What is the likelihood that the n balls drawn are of a unique color if there exists duplicate colored balls among the population of N balls?

Open to solutions to different specific mathematical questions(probabilities, likelihood, confidence intervals, etc..), but the main point is that the goal is met.",AskStatistics,2022-08-05 14:02:19,7
This is Markov chain simulation and what you are looking for are the steady states of the chains.,2,wgvcfh,"Features/Experiments: 

1. I would be working with data that is generated in 3 ways : recursively like one observation leading to other upto say n length. Collection of these chains would be the dataset. 
Secondly, just have a map/space of items and do random walks to choose the observations and hence collect such long chains as samples. 
Thirdly fix the possible options (the observation space can be infinite, words for example) and just assume that they are independent and have no relation amongst them. (Not the best option imo)

2. I would be looking for some sort of ""convergence patterns"". Does a pattern emerge as I reach the n th observation I converge to a common point or some selected patterns for related chains. Can I model this probabilistically?

Is there a name for such studies? Or some concept from clinical trials? How do I look around? Any keywords that could help? Causal inference is probably not the way to go right?",AskStatistics,2022-08-05 06:20:47,4
It boils down to the fact that gradient of expected value of a function of z cannot be computed in this case as expected value of a gradient of function of z with respect to pdf of z.,1,wgqxp3,"hi guys, sorry if it's a stupid question but i am kind of new to this stuff and i am learning.  
 i would like to know if it's possible to numerically approximate the gradient of a distribution. i ask you this because i was reading this blog post on  stochastic backpropagation and the reparameterization trick [http://gregorygundersen.com/blog/2018/04/29/reparameterization/](http://gregorygundersen.com/blog/2018/04/29/reparameterization/)

and i noticed that in the section "" undifferentiable expectations"" the author says  
"" The first term of the last equation is not guaranteed to be an expectation. Monte Carlo methods require that we can sample from *pθ*​(*z*), but not that we can take its gradient. This is not a problem if we have an analytic solution to ∇*θ*​*pθ*​(*z*), but this is not true in general. ""

&#x200B;

It is not clear to me why  the presence of that term that is not an expectation value is a problem from a computational point of view",AskStatistics,2022-08-05 02:30:15,8
"male=1 female=0, compute and subtract the mean like usual",3,wgqdqg,"Hi, [here](https://cdn.vanderbilt.edu/vu-my/wp-content/uploads/sites/2149/2016/07/29193724/RightsSterbaMBR_2019.pdf) is a very nice paper about delta *R\*\*2* in linear mixed models. The authors illustrated used an illustrative example with students nested in class-rooms. One predictor was sex. On page 18 they mentioned that the predictor sex was group mean centered:

""Model B adds to Model A a fixed component for both the within-class and between-class parts of sex, namely the slope of the level-1 predictor class-mean-centered sex and the level-2 predictor class-mean sex.""

However, I'm not aware how group mean centering should work for categorical factors...",AskStatistics,2022-08-05 01:56:05,5
"Just to get you started with statistics terminology that will help you search and communicate better, if you have *data* (your two datasets of 4 years of observations) and a *model* (a birth-death model of some form) then what you're talking about is *parameter estimation* (estimating the birth and death rates). It's called estimation because you'll never know what the actual birth or death rates were, but you can use the data to inform plausible values.

Now, practically speaking, birth-death models can be a bit gnarly to work with depending on what your data look like and what you're assuming about the birth and death processes. Are you working with a constant-rate linear birth-death model (assuming that when there are *n* particles in the process the birth rate is lambda \* n and the death rate is mu \* n)? Or something else?",2,wgz5rt,"I have two sets of data of 4 years each, I have data for each month, and I need to compare both of them by knowing the birth rate and death rate of both of them. How do I calculate it?
Sorry if my question isn't well phrased, English isn't my mother tongue, and I just recently got into statistics and frankly speaking I'm quite lost.",AskStatistics,2022-08-05 09:01:16,3
"Are you sure that `tree1` is a numerical variable and not a factor? I don't think R will convert the 1 in the name to `TRUE` just by itself. I tried replicating your situation and found no problem (I might miss something):

    x1 <- runif(100, 0, 10)
    x <- rnorm(100, 0, 5)
    y <- 0.5 - 2.5*x1 + 1*x + rnorm(100, 0, 4)
    
    dat <- data.frame(x1, x, x)
    
    mod <- lm(y~x1, data = dat)
    
    variable.names(mod)
    mod$coefficients",10,wguskm,"I worked with R for a long time, but today R surprised me. This surprise caused a lot of problems though. 

Mainly, I had a simple lm() object. Let's say that in the training data set I have two variables: tree and tree1. In the formula, I use only tree1, eg. H ~ tree1. 

I have other functions for model analysis written by me, which often utilize variable.names(). And here is the surprise: when I called model$coefficients, I received treeTRUE instead of tree1. 

I never encountered this, I guess it is to differentiate between variables more easily. Did you know why this happen? And if there is a way to turn this off?",AskStatistics,2022-08-05 05:56:11,4
"Actual scaled-4th moment kurtosis or excess kurtosis? Oh, wait, no, it's below 1 so it can only be excess kurtosis.

Either way, kurtosis values are not especially easy to interpret; just about any interpretation you give will have exceptions, but broadly speaking you can say that higher kurtosis tends to go with heavier tails.

Okay if the population distribution had an  excess kurtosis of 0.6-ish,  it's fairly likely that the population distribution is somewhat more heavy tailed than say a normal distribution (which has excess kurtosis 0), though if this is a sample kurtosis, then with a sample you could perhaps see a value like 0.63 from a normal distribution (it looks like the probability of a value at least that large if the population is normal is below 8% at any sample size, though it does depend somewhat on the exact definition of sample kurtosis that you're using; I've seen at least three different ones)

>  Is it normal? 


That you get a kurtosis that's not very inconsistent with the population having a normal distribution is not itself evidence for normality.

There's no reason to think the population distribution is actually normal (and to be honest if we were face to face  I'd bet a dollar that it **isn't**; not that this is likely to be the least bit important for anything I'd expect you'd care about). However, I would say that *looking at the sample* to choose a model for the population that the data came from, so you can perform *inference using that exact same sample* is somewhat problematic.

Why do you think you need to find out if the population has a normal distribution?",4,wgoo1h,"What does a kurtosis of 0.63 mean please?  Is it normal?  

I would also appreciate any material I can read about it.

Thanks!",AskStatistics,2022-08-05 00:07:43,4
[deleted],2,wgh6m9,"I have an assignment for uni and we have been asked to determime the difference between two groups. The data set has three groups though

What is generally seen as best practice?

A) drop the third group from the dataset and proceed with a t-test, or

B) Use an ANOVA with a post-hoc test with all three groups but only looking at the groups in question?

Note: the data set is the PlantGrowth data set in R",AskStatistics,2022-08-04 17:42:16,2
"Hallo, you can use the VLOOKUP function to do this automatically. Just google that term and youll find many tutorials on how to do it. Good luck!",3,wgq507,"I have a two wave study. I need to get the results from the second study on the same line as the first study. 

However... at Wave 1 I had 600+ participants and Wave 2 260 participants.

The only thing that I have to match them up is a code that they created themselves. 

Is the only way to manually search matching codes or is there another (simpler) way to do it?

Hopefully yours",AskStatistics,2022-08-05 01:40:43,2
"You have to make a move if possible.  So 1-4, 6-4 for example would also get you out in two turns.",2,wghqcn,"If you don’t know how to play, I’ll explain.

In this position, black has to roll the dice combination 6-1 or 1-6 to escape his checker, a probability of  2/36 (or 0.0556). So the first row of the table means that 5.56% of the time black will have successfully made this escape by roll one. What I don’t understand is how the subsequent values for the rows are obtained. My instinct to obtain row two is (1 - 0.0556)\*0.0556, the probability of a failure on the first roll times the probability of a success on the second roll. But that’s incorrect.

https://preview.redd.it/aunsq349nsf91.jpg?width=1869&format=pjpg&auto=webp&s=2181b0298355014e1c5833e1f9978c902cc1ae12",AskStatistics,2022-08-04 18:08:04,2
"Let me guess, interview question?",6,wgaqn8,,AskStatistics,2022-08-04 13:01:45,13
Is this a binary variable? What are the options?,1,wgb2ev,"I conducted multiple regression and found the relationship between one independent variable and the dependent one. The relationship is strong - p-value is 0.001. However, I am interested in finding out more about it. the independent variable is a political party. I want to know which party's preference has the highest impact on the dependent variable. How to do that?  Thankssss.",AskStatistics,2022-08-04 13:15:28,8
Yes go for it. I am a data Enthusiast myself,1,wg4a5j,"Hello all, as I'm data addicted I'm creating videos of data vizualization. I would like to know if I can share here and also if you guys can provide feedback. Thanks",AskStatistics,2022-08-04 08:35:34,2
">I have an index score for a total score of participation in an activity

Is there a *reason* you are being so vague about the outcome variable in your analysis?  I ask because I often see people compute index variables out of some perceived need to ""standardize"" their outcome variable when in fact it is almost always preferable to do inference on the *raw* outcome (and perhaps use an *offset* variable if appropriate).

>The sample size is about (n=100) so not high statistical power.

You're right, that's not a huge sample...but power depends not only on the sample size but also on the *expected effect size*.  Do you have a goal in mind for the minimum acceptable power you'd like to have when you do inference?  If not, just do what you can and then report your power afterward, so your audience can tell if your study had enough power to convincingly arrive at your conclusion 

>I am perceiving this dependent variable as ordinal... I asked another personal about it and think I should treat it as a linear/scale data

Ok, perceptions aside, *what is it actually*?  You haven't told us what it is so I have no idea what exactly you mean when you say it's measured ""0-5"".  Are saying that it can only take on integer values (0,1,2,3,4,5), or is it continuous on the range [0,5], or what?


>think I should treat it as a linear/scale data and conduct a linear regression, which would allow for the inclusion of other variables as IVs. I can't really find any good resources to support/cite either argument

Have you collected data regarding any additional variables that influence the outcome of interest?  Are you interested in arriving at a result that ""controls"" for those other variables?  If so, then simply producing bivariate correlations will get you nowhere and a model is the more appropriate choice. This isn't really the type of question that you will find ""support"" for in the literature (unless your outcome has been very extensively studied already) because the answer is based on *your* goals and the specific nature of the data *you* collected.",1,wgfis8,"Hi there,

Analyzing an index score: I have an index score for a total score of participation in an activity, which is the dependent variable. The sample size is about (n=100) so not high statistical power. In addition, the dependent variable is not normally distributed. I am perceiving this dependent variable as ordinal, as it is measured from 0-5 recommended using Spearman's correlations coefficients or use of Kendall's tau to see how other variables individually correlate with the dependent variable. I asked another personal about it and think I should treat it as a linear/scale data and conduct a linear regression, which would allow for the inclusion of other variables as IVs. I can't really find any good resources to support/cite either argument to be honest. Any advice!",AskStatistics,2022-08-04 16:25:47,1
"Start with a list of questions that might seem interesting.  Then, rank the questions by priority.  Priority means that answering the question would compel some action to take place in the real world.  Then, delete the questions that are interesting but not compelling.


For each compelling question, draw a graph of how the result might be presented.  It's just a sketch to frame the analysis.  Would it be a bar chart, a scatter chart, a pie chart?  Maybe a run chart of something versus time?  Maybe a plot of some response variable by some group variable?


At this point you're ready to try to torture the data file to make it reveal what you need.  Also at this point, you might find that the data is not formatted in a way that the question can be answered.  Or maybe the file doesn't even contain the necessary information.


If the data file has information that answers a compelling question, then come back here, one question at a time, and ask for guidance.",5,wg4h1e,"&#x200B;

https://preview.redd.it/6zo9zg1atpf91.png?width=830&format=png&auto=webp&s=7ed696f013c2519829f2de5809056b42a0c22ee2

I have years of daily data in this data set. So far, I have summarized each of the variables by month and by year. For the monthly summary, I have each of the months from January 2012 through June 2022 summed up (similar to the table below). 

&#x200B;

https://preview.redd.it/cfa6mfq8upf91.png?width=786&format=png&auto=webp&s=8ec6f8b5873444e373b8a4f1c3dbeab94a10292d

There are no variables that would lend themselves to identifying anyone, just the total number of meetings, persons served, etc. Are there any types of statistical analysis that would be appropriate to run a set of data like this? To maybe compare how the data has changed over over the course of months, quarters, or years or any other suggestions that you might have?

Thanks!",AskStatistics,2022-08-04 08:43:32,1
"put a '\\' in front of your '\*' symbols so reddit doesn't read it as markdown for *italics*

Your simulation is wrong.

    > mean(replicate(1000000,length(unique(sample(c(1:5,5),5,replace=TRUE)))==5))
    [1] 0.030739
    > 5/162
    [1] 0.0308642

Looks pretty consistent to me

The direct calculation is a straight multinomial probability

           5!
     -------------- (1/6)^1 (1/6)^1 (1/6)^1 (1/6)^1 (2/6)^1 
     1! 1! 1! 1! 1!

      = 240/6^5 =  5/162

Check in R:

     > dmultinom(c(1,1,1,1,1),5,c(1,1,1,1,2)/6)
     [1] 0.0308642

It all looks right.",5,wgen6n,"I'm self studying through the Wackerly textbook, and I don't understand how the textbook arrived at its answer.

A balanced die is tossed six times, and the number on the uppermost face is recorded each time. Suppose that the die has been altered so that the faces are 1, 2, 3, 4, 5, and 5. If the die is tossed five times, what is the probability that the numbers recorded are 1, 2, 3, 4, and 5 in any order?

The textbook answer is 5/162.

Calculations: 2C1(2 ways to get 5)\*4C1(4 ways to get 1 to 4)\*3C1\*2C1\*1C1 / (5C1)^5 = 0.015

I tried simulating this through R, and I'm getting about 0.09 with 100,000 simulations, which is different from both answers. Is there something I'm missing?",AskStatistics,2022-08-04 15:47:45,5
"One assumption here is that the sampling distribution of xbar is approximately normal. So, for the 95% CI, 95% of the means are expected to fall in the interval defined by xbar +/- (1.96 * SE). That means 2.5% of the means fall below the lower limit and 2.5% fall above the upper limit. So, one could say that the probability of falling above the upper limit in a 95% CI is 0.025. 

Remember that here 1-alpha is 0.95 and what’s left in each tail is alpha/2, or 0.025.",1,wgbyqy,"I had the following question:

prove right / wrong:

if we build a lot of CI with alpah = 0.05, for the mean (mu) of normal distribution, then we can expect that 2.5% of the CI the mean (mu) will be bigger than the upper limit of the CI

now my attempt:

the statment above is true, the  limits of CI is : X\_bar +/- Z\_0.975 \* SE

in praticular we want to check  P\[ mu > upper limit of CI = X\_bar +Z\_0.975 \* SE\] 

so we get P\[ (X\_bar - mu)/SE < - Z\_0.975 \]

but Im not really sure how to show that the above is actually 0.025

another idea I had (but Im not sure if that is ""right"")

the chance of mu not being in the CI is 5 %, and the chance of mu being outside the CI is uniformly {bigger than upper limit, smaller than lower limit}

&#x200B;

if someone could help me understand here, much appreciated

thank you all",AskStatistics,2022-08-04 13:54:53,2
"Here is a similar post by a person who wants to use PCA to extract factors that can be used as independent variables in multiple linear regression. Is that a cleaner approach or is it fine to stick with factor analysis?

https://www.researchgate.net/post/Multiple-linear-regression-after-Principal-Component-Analysis",1,wg5cal,"I am analyzing the results of a survey, where the answer to each question is rated on a Lichert scale, and I have used factor analysis (exploratory and confirmatory) to find 10 factors that can explain answers to the questions in the survey. (Actually more than half of the questions in the survey does not depend significantly on a factor, but I guess that is OK)

Now I would like to pick one of the remaining questions in the survey and use something like multilinear regression to examine how the answers to the chosen question depend on the 10 factors from the factor analysis. But I am not sure if this is meaningful.

Ideally the factor analysis should capture all the variance in the dataset so is it even meaningful to look for further correlation after the factor analysis model has been tested?

I am planning to calculate the expected value of each factor for each respondent. Then I can use these values as explanatory variables for multilinear regression. Is this a good idea, or is there a better approach? Should I look for something like an structural equation model? Did someone invent an algorithm for clustering the confounding variables in a multilinear regresion model? 

My basic goal is to understand the dataset, so it would be great have an approach that allows me to create some simple plots such as a pie dragram or a series of scatter plots.

This is a hobby project, but I would like to use it as an excuse to learn some statistics. So I would be grateful if someone could recommend a good reference and some words that I can google for. So far I have been using factor_analyzer and semopy to create models in python, but I think that I need some direction before I can succeed.",AskStatistics,2022-08-04 09:18:30,1
"If the p value is low, reject the ho 

Joking. I’d teach them Bayesian first and the bayes factor and then maybe teach them about the frequentist world. Could also lie to this 10 year old and tell him fiducial is where it’s at 

But yes to answer the question, very hard to explain p value to anyone without the usual def",60,wfohz2,"This was an interview question btw. I’m not looking to actually explain it to a 10y/o 😂 
I know what it is, but realized that I don’t know the “science” behind it.",AskStatistics,2022-08-03 18:46:07,32
Principal component analysis and then k-means clustering maybe,1,wfuyjl,"I've got a questionnaire in which I'm asking participants to rate 17 countries on eight different rating scales each (scale 1-7). I'm now interested in finding out which countries are similar to each other in their rating profiles and ideally find groups of countries that are perceived similarly in some way.

If every country had one rating each, I think that would a case for factor analysis. However, I've got multiple ratings per country and the rating profiles might differ between countries, which is why I want to avoid collapsing all the ratings into one index per country. 

Visually, I think I could find groups by comparing distributions of ratings for each country to the others (e.g., with a graph of multiple boxplots for each country), but I'd prefer if there was some way to identify these patterns statistically.

Would anyone have an idea of how to best do this? Thanks so much!",AskStatistics,2022-08-04 00:36:15,6
"Put a \\ in front of the # in item 5 so Reddit doesn't read it as Markdown for ""make  this a  top level heading""",2,wg35nw,"Hello, I was looking for help with a research study and how to approach an analysis

I'm working with a dataset of 255 stem cell transplant patients to see if those that received a Prophylaxis medication, developed a medical condition called CDI during hospitalization. Also looking to see if they developed CDI 180 days post discharge.

My questions for analysis are:

1. Did the use of Prophylaxis help prevent the development of CDI?

2. Did the use of Prophylaxis assist in preventing CDI for patients 180 days post hospital discharge?

3. Does a patients diagnosis (Lymphoma or Multi Myeloma) affect the development of CDI in those that received a prophylaxis and developed CDI?

4. Does the type of chemotherapy a patient received affect the development of CDI in those that developed CDI and received a prophylaxis?

The variables I have are:

1. Date they started stem cell treatment

2. Disease (Multi Myeloma or Lymphoma)

3. Chemotherapy Regimen

4. Did the patient receive a prophylaxis (Y/N)

5. Number of prophylaxis doses

6. Length of Stay in hospital (in days)

7. Last date of prophylaxis medication

8. Did they develop CDI during hospitalization (Y/N)

9. Patient discharged on prophylaxis (Y/N)

10. Follow up date

11. Did the patient develop CDI during follow up period (Y/N)

12. Is patient alive or dead by 180 day follow up date (A/D)

13. If dead within 180 days, death in days from day 0

My question for this sub, are really what statistical methods I should consider using to answer the questions above? My thoughts are to use binary logistic regression to answer the first question. But the other questions, I'm not sure how to approach. 

Thank you for any help in advance!",AskStatistics,2022-08-04 07:48:52,2
"Why not put the interaction as an additional term directly into the logistic regression?

The direct interpretation is in terms of the model for log-odds, naturally",3,wg2scg," I use STATA.

I'm doing a study on types of abuse and race/socioeconomic status. I'm using a logistic regression as all of my variables are dichotomous except for age and the index developed for socioeconomic status (which is also developed from exclusively dichtomous variables).

I'm trying to determine if there is an interaction effect/moderating effect between race and socioeconomic status on abuse. Is it more appropriate to use the logit function to run the analysis again with a c.socioeconomicindex##racevariable or to run a separate, linear regression with a separate variable (newvar = socioeconomicindex \* racevariable). Or am I completely off base with both?",AskStatistics,2022-08-04 07:33:18,11
"Liner regressions, whether using or not FEs, will drop incomplete observations. This means that it is not a problem with the year, but a problem with the controls themselves. Try running the regressions with only the year effects, are all years represented in the output? now try running the regression with only the controls and see which one (s) are causing the missingness issue. Also, if your controls are not time-variant they will be dropped from the analysis (this shouldn't affect the years thought).",2,wfzce8,"Hi there,

I am doing a linear regression with control variables and time fixed effects in stata. When Im including the controls the time fixed effects only include the years back to 2001 eventhough my data goes further back in time. The controls are only available from 2001 onwards. Does this mean the data from before 2001 is not included in the regression as long as I use controls?",AskStatistics,2022-08-04 04:56:58,1
"https://towardsdatascience.com/inference-vs-prediction-b719da908000

I haven't read that, but it should get you  to a point where you can ask a more nuanced question.",3,wflwh9,,AskStatistics,2022-08-03 16:46:38,2
"The alternate in this kind of case is ""what you'd like to see IF it is true"". In other words, the burden of proof is on you to find evidence that this treatment increases scores. 

So, the Null says otherwise, and only if the data are convincing that μ>45 do you reject Ho. Then you are going with the Ha: μ>45.",2,wfm1ff,"A population is known to have a mean of μ = 45. A treatment is expected to *increase* scores for individuals in this population. If the treatment is evaluated using a one-tailed hypothesis, then which of the following is the correct statement of the null hypothesis?

Comment: Shouldn't the μ go up, if scores are expected to increase?",AskStatistics,2022-08-03 16:52:43,5
"Are you looking at the standard error of the estimate (probably the most common) or the se of the coefficients (often reported by stats packages)?

If it's the observation, it is the uncertainty around the mean prediction, in units of the response variable.

The coefficients are the uncertainty of the estimate, and they have the same units as the coefficient (eg response variable over predictor variable).",1,wfiqyw,"I've done a regression on a data set that compares the distance (miles) a student lives from a college and the number of years they have completed of education. I've been asked what the standard error signifies in this regression and what units the standard error is in ($, grams, tons, miles etc). I'm really not too sure and could do with some help if anyone knows how to figure it out :)",AskStatistics,2022-08-03 14:33:49,6
You have no way to tell from this information how the two variables interact. That male-female difference is unlikely to be constant across all ages,3,wfgc0k,"Hi all, trying to figure out how i'd calculate the below example:

""Customers between 70 and 80 years old have a 10% chance of buying a product on my website (when looking at all Genders).

Male customers have a 12% chance and Female customers have a 6% chance of buying a product (when looking at all age brackets)

How would i calculate/estimate the chance a Female Customer (6%) who is between 70 and 80 years old (10%) has for buying a product? ""

My first thoughts was to just find the mean between the female customer % and the 70-80 age bracket %, but i feel like there must be a more accurate way? 

Thanks",AskStatistics,2022-08-03 12:56:41,7
"Plot your data, compare it to the regression results. Does a straight line make the most sense, or would a different function (e.g. adding a squared term) work better?",2,wfh3v5,"I'm trying to fit a multiple linear regression model where most dependent variables are between 1-3, but I'm more interested in the values that are greater than 3.

My model now is still positive and has a good correlation, but predictions that should be around 15 are dragged down to less than half of that. About 5% of my data's dependent variables are >3.

I've considered logistic regression calculating probability of >3, but I need to get predicted values in the end.

Any suggestions? I'm only familiar with linear regression and have just recently started learning about logistic and LASSO/RIDGE.",AskStatistics,2022-08-03 13:27:45,1
Please see rule 5 https://www.reddit.com/r/AskStatistics/about/rules,1,wfk6lo,"There are 5 people to make a decision after me. Person 1,2,3,4, and 5. They have 5% of making decision 1, 34% chance of making decision 2, and 61% chance of making decison 3. What is the probability that 1 person makes decision 1, three people make decision 3, and 1 person makes decision 2. 

My logic to solve this is as follows: There are 20 different combinations where the question is satisfied, so the probability of this situation occurring = (0.61^(3))(0.05)(0.34)(20).

The reason I am asking is that when I do this method for solving all possible 3^5 possibilities, I come to an answer that is more than 100% which obviously is not correct. I double and triple checked my work to make sure that the error is in my method of solving it and not a mistake with my math or calculator.

Thank you very much for any help and your ideas. I appreciate it.",AskStatistics,2022-08-03 15:33:02,3
"You are confusing the regression coeffiecient with the correlation coefficient. A slope of 0.1 means that when X increases by 1 unit, Y increases by 0.1 units. Depending on the metrics of X and Y this can be a lot, or practically nothing. The correlation coefficient in a simple linear regression is the standardized regression coefficient which expresses the same idea in standard deviation units. You absolutely cannot have a correlation coefficient of 0.1 and have the points clustered tightly around the line around the low slope, it is not mathematically possible. You will never get a perfect relationship in practice either.",13,wf8s9q,"The confusion is coming from the fact that a slope of zero means that there is no correlation between two variables. That part makes sense, because as one variable increases, the other doesn't react in any way. But what about a slope of 0.1? If the points are very strongly correlated along a line of slope 0.1, does that make it seem to have a strong relationship? What about 0.001? This causes me to think that even if the points are perfectly aligned, if the slope isn't high enough then the correlation is still not there. If taken to an extreme, I would think the slope necessarily has to be exactly 1 in order for the correlation to be perfect.

Possibly, I am confusing ""correlation"" with ""correlation coefficient"" as well? Does anyone have any insight on this?",AskStatistics,2022-08-03 07:58:48,8
"You *can* chain analyses of different types together, and sometimes it works great. But just because you can doesn’t mean you should, you can end up with an eldritch abomination.

Empirical Bayes is one place you see them combined. There’s also bagging (bootstrap aggregating) which is sometimes done with Bayesian analyses. And of course, MCMC convergence diagnostics are basically all frequentist approaches (because that’s the right framework for asking about how well your posterior samples approximate the unknown posterior). These are what I can think of off the top of my head.

This is a very crude generality, but I’d say that non-Bayesian approaches do a lot more conditioning, where Bayesian approaches do a lot more marginalizing (e.g. imputation versus treating the missing data as a parameter and sampling it). But this means that Bayesian approaches will play nicely with Bayesian approaches (just marginalize parameters when needed and add new layers to the model hierarchy as appropriate), where frequentist approaches will play nicely with frequentist. Putting them together requires a point of contact that doesn’t create conditioning-vs-marginalizing friction. The majority of places I see multiple types of inference combined it’s a shortcut to save time on Bayesian analyses by fixing something to a particular value. But then again, I mainly work on Bayesian statistics so I mostly read Bayesian papers, so I could easily be missing things.",3,wevzpp,"Like, in popular presentations of applied probability theory, there's often a lot of talk about ""A Bayesian would do this, while a Frequentist would do it this way"", but I've got to figure that most actual statisticians/informed users applied statistics aren't going to artificially limit themselves to only using one framework, but would instead use whichever they think is most appropriate for the problem at hand.  That got me thinking: are there times you'd use both frameworks together, like break a problem up into sub problems, some of which are better solved with frequentist methods and the others with Bayesian methods?  If so, what are some examples?  

To perhaps reframe the question a bit, I just came across a comment on a Quora question that said frequentist probability is a measure of indeterminsm, while Bayesian probability is a measure of uncertainty.  That made a light go on in my head about why we can't talk about the probability of a constant but unknown parameter in frequentist theory.  It also reminded me of the Lindley paradox, which is usually resolved by saying that the two methods simply answer different, but related questions.  That made me wonder what real world problems might involve both types of questions.",AskStatistics,2022-08-02 20:43:53,5
"Hi I love this sub but think I am a green belt. Black belts may arrive and correct me. 

Here is my attempt at an answer, it may at least give you a starting point. 

The first bit is that you need to use the correct test and that depends on the comparisons you make. 

If you are comparing week x with week y, as in week i with week i+1, like last week vs this week, then it’s a comparison between two ratios and you could try using the binomial test. Wikipedia or any decent YouTube channel on stats explains it. 

If, on the other hand, you have many weeks of prior data, you could probably plot its distribution and say how this this week fares in comparison to the distribution of previous weeks. If the distribution of previous ratios looks like it’s a normal distribution you could use tests that are based on that. If not, you can place your new ratio in a quantile of the previous ones - like “this week’s answer is more extreme than 90% of previous answers”. 

Once you know what test you are using you can plug values in and see what sorts of values appear significant.

One thing you have to do is consider whether you go with the normal default “alpha”, which is the level of improbability that will disprove your null.  It’s usually 0.05, i.e. 1/20, i.e. an answer which is as improbable as a 1/20 event will be taken to be different for reasons other than noise.

Also, it sounds like you are the most statistically informed person in the room, so you’ll have a duty to explain to the directors in words of one syllable what a quantile means.",2,wexamr,"Hi AskStatistics! 

I've been asked to calculate how many people need respond to a weekly survey out to a random sampling of customers to make valid comparisons between customer groups. The groups are delineated by product ownership among the companies product suites.

The data is typically presented as ""X % of customers in product suite y feel they'd move to a competitor if they lost feature z."" Management likes to watch this rate week over week very closely but I worry that no one has determined what sample sizes are needed to reliably test if this rate is changing from week to week or if it is a product of random chance given different respondents each week.

The survey audience changes each week in an effort to not ask the same customer to complete it multiple times within a short window of time and the questions use a 1-5 style, linkert scale.

Is there some sort of power analysis or sample size literature that I can read up on? I'm not having much luck finding similar examples or resources so far.

Thanks for any help you can offer!",AskStatistics,2022-08-02 21:50:48,1
"I think Poisson regression is a reasonable approach, since you’re predicting a strictly nonnegative value. Also, since I think there’s a fair bit of autocorrelation here (one week is going to be similar to the last) you might look at like an AR model. 

What is your math/stats background?",3,wetxij,"Hi all, I am currently trying to forecast the amount of sacks TJ Watt (player for the Steelers) and the amount of yards Derrick Henry (player for the Titans) would have gotten if they played all 17 games this season (in which they played 15 and 8 games respectively). I’m a bit new to forecasting, so if someone could explain how I might go about this or point me to a particular resource, it would be greatly appreciated!",AskStatistics,2022-08-02 19:04:37,5
"Are you after videos, books, courses, ...?

Anything in particular you want to learn?

Do you have calculus, linear algebra or neither?",4,wf1qg0,"hey all

newbie here

i d like to learn statistics

where do i start?",AskStatistics,2022-08-03 02:12:27,5
"It does sound a bit rude, doesn't it?",39,wejnrp,,AskStatistics,2022-08-02 11:46:53,10
"> but also is there a way to predict how large a chunk you might be able to form?

This part I'd probably try do by simulation to start with; I expect it would be pretty right skewed.",1,wezbhv,"This is a question inspired by jigsaw puzzles. So I know that with a standard puzzle based on a grid, most pieces will have 4 surrounding pieces, so piece 2 will have a 4/(x-1) probability of connecting to piece 1 where x is the number of pieces in the puzzle. 

My question is, if you took a subset of pieces, say 100 out of a 1000 piece puzzle, how many pieces would you expect to connect? I'm curious about both the number of discrete connections, but also is there a way to predict how large a chunk you might be able to form?

Separately, I'd be interested to read any other puzzle related stats or observations anyone has.",AskStatistics,2022-08-02 23:45:24,7
"Then together they are significant in the explanation of the dependent variable. Even when each independent variable alone is insignificant.

You can make significant noise by clapping your two hands. Even when doing the same movement with one hand alone makes no noise at all, no matter which of the two hands you use.",6,wepx3p,,AskStatistics,2022-08-02 16:01:08,5
"Personally, I would forego SEM for the time being, and use PROCESS in either R or SPSS to run a Conditional Process Model in which the grouping variable acts as a moderator for all 3 paths of the theorized causal chain. It will give you explicit hypothesis tests for the difference in the specific indirect effects. Multigroup Analysis works, but unless your primary goal is to ensure the measurement/structural/factorial invariance of the model, PROCESS will give you more intuitive results.   


Also note that ""complete mediation"" is not qualitatively better than ""partial mediation"" (I'm using quotes because both concepts hail back to the Baron and Kenny approach, which has been argued by many to be unsatisfactory).",1,wesh43," We collected data from two different samples (e.g., one clinical group and one healthy group). We conducted mediation analysis with a serial mediation model (i.e., 1 IV->1 Mediator-> 1 Mediator->1 DV) and the model fit was good for the clinical group with no direct effect from the IVs to DV (complete mediation). However, for the healthy group the model still has the direct effects from the IVs to DV. We then decided to use multigroup analysis to see if the differences between models is significant. (Spoiler: It wasn't :(  ). 

What I want to ask is is this a good method to tackle the data? THanks.",AskStatistics,2022-08-02 17:55:55,4
"Most of them. Three times I started to list a couple of things that I *didn't* use, but in each case I realized I had used them each  for something.

For example I thought I was pretty safe listing experimental design as something that doesn't come up at all, but no, I use that sometimes in simulations.

I give up; some of it doesn't come up very often but eventually I end up using most of what I learned; at the least it *informs*  what I do.",6,weaova,,AskStatistics,2022-08-02 05:42:05,6
"The answer is entirely up to you and what you're trying to measure. The problem is that the measure ""number of cards drawn until the pattern is matched"" is only defined in situations where the pattern does eventually match, so it has no meaningful value in situations where the match doesn't come up.

That means your options are, broadly:

1. Redefine your measure so it always has a meaningful value (e.g. count drawing the deck with no match as having drawn 52 cards, or keep count of the number of no matches and remove them from the number of trials so that you're measuring the *conditional* average).
2. Redefine your process so that it always has a meaningful value (e.g. shuffle the deck like you're doing).

Whichever option you take, you just need to be clear that's what you've done when you make use of the results.",1,weq52u,"I want to know the average amount of cards which are drawn from a deck before the drawn cards match a certain pattern. For instance, what is the average amount of cards you need to draw before you have drawn three hearts in a row? While this question is probably answerable using math, chances are I wouldn't understand the math. So I wrote a program which shuffles a standard 52-card deck and draws cards until the pattern is matched. I loop over this a million times and then take the average number of cards that were drawn. The average of a million samples is accurate enough for my purposes.

My problem is that there will be times when you can deal out the whole deck and not match the pattern. The only solution I have for this is to reshuffle the deck and start drawing again, and now the first card that is dealt is considered to be the 53rd card instead of the first card dealt. Is this the right way to find ""the number of cards dealt to get a match"" for an iteration of a loop where the first 52 cards didn't provide a match?",AskStatistics,2022-08-02 16:10:52,1
What do you mean by product data?,1,weos29,"I am looking for the best way to determine the amount of inconsitency in our shop product data. ie. Grab 100 products, test if the stock on hand number is equal to the number of stock on the shelf. Basically, pass/fail condition. I am thinking of checking 100 products that will either pass or fail the test. The total population is 28000 products. This test is to measure the accuracy of our product data. What would be the best method/statistical application? Once I have the numbers on the sample set, how do I use it to estimate for the population? I used to have a grasp on this stuff and am looking to come back to speed. Cheers!",AskStatistics,2022-08-02 15:12:49,1
"Yes, start simple and then expand. What you will want to consider is the Intersection over Union (IoU) metric (aka the [Jaccard Index](https://en.wikipedia.org/wiki/Jaccard_index)).",1,weiogh," 

I have a dataset of 66 individuals that were asked about their medications. I have another dataset from medical orders that includes the medicines physicians prescribed for the same individuals.

I want to check how how often those two datasets agree, meaning that a medicine prescribed (in orders data) is also present in the self-reported data.

So there will be three scenarios for each medicine:

1. Prescribed medicine and self-reported
2. Prescribed medicine and not self-reported
3. Non-prescribed medicine yet self-reported

How do I go about this? Would a simple percent agreement be enough (% of present in both)?",AskStatistics,2022-08-02 11:07:22,1
Use %Tol if you are assessing how well the measurement tool can evaluate an acceptance criteria (ie can my tool accurately decide pass/fail). Use %Study as a baseline for process improvement (so you can benchmark against your improved process) and for process validation.,1,wed96e,"I see a lot of conflicting opinions on whether to use %StudyVar vs %Tolerance when analyzing a gage R&R. I know %StudyVar is looking at your gage’s variance as a percentage of the total variance & %Tol is looking at your gage’s variance as a percentage of the tolerance limits you’ve defined on the test. 

I just can’t figure out which is appropriate for my studies / when to use which result.",AskStatistics,2022-08-02 07:33:16,7
"just an intermezzo on the use of words:   
It is Events (not Probabilities) that can be exclusive or non-exclusive.",2,weinin,,AskStatistics,2022-08-02 11:06:16,1
You would normally round *off*.,4,weic66,"I've done some regressions in Stata and have had to put the coefficient and standard error into a table to three decimal places. However, the answers are much longer, should I round up/down or just leave it? For example, the answer could be 0.0237834343 should this be 0.023 or 0.024?

Thanks guys!",AskStatistics,2022-08-02 10:54:03,2
"1/200, or 0.5%. Assuming the events are independent (i.e. whether the equipment shows up or not doesn't affect whether it has the bonus).",1,weibx2,"What are the chances of winning a 1/10 and then immediately winning a 1/20?
In summary, playing a game and grinding for equipment. The chance of the particular equipment showing up is 1/10 and then it has a 5% chance to have the specific bonus on it. Any help would be appreciated! Ty",AskStatistics,2022-08-02 10:53:45,4
"> Why can't we assign a probability to the unknown parameter, thinking about the confidence interval in Bayesian terms without using the full, complicated Bayesian framework? 

Fundamentally because of Bayes' rule. Set aside the issue of what a probability is, or how to interpret it. If you want to make statements of the form Pr(parameters | data), that's hard when all we're armed with are things that let us make statements about Pr(data | parameters). Bayes' rule lets us flip that around, but the *cost* is that you need to make statements about Pr(parameters) to do so.

There *is* an asterisk to all this, which is the ever-obscure [fiducial inference](https://en.wikipedia.org/wiki/Fiducial_inference) which tries to do the Bayesian thing without actually doing the Bayesian thing. It's... weird. It's also pretty limited in what you can do with it, and not really studied by more than one or two research groups, judging by the citations you find when you try to investigate.

I'm also going to take issue with calling Bayesian statistics ""complicated."" All statistical models are complicated. Bayesian models catch more attention to this because frequentist stats is the default. But if you start diving into a t-test, you'll find way more complexity than you bargained for on something seemingly ""simple."" Degrees of freedom show up, for one. And it turns out that the things we actually do are approximations. And that's one of the most commonly used tests out there.

> Why can't we do the same thing with applied statistics? Like, does insisting that confidence ≠ probability, as textbooks often do, actually have any benefit? Or, to be more precise, would interpreting an n% confidence interval as ""There's an n% chance this interval contains the true value of the parameter"" cause any actual problems in applying the interval to a real-life situation? 

Is it going to matter if Tom in marketing runs an A/B test and concludes that since the 95% CI is [1,2] there's a 95% probability that the true value is in [1,2]? Probably not. It wouldn't change the practical implication, and it's probably not the worst mistake he made in that analysis, either.

Is it okay if the entire scientific community starts believing that the p-value is the probability that the null hypothesis is true? I'm going to say ""no."" Because it isn't, and even in the best-case scenario, a p-value is 2 pieces of information short of actually providing the answer: namely, the prior probability of the hypothesis and the probability under the alternative. And that's when you're in a neatly-structured binary situation, ""the coin is fair or it's not."" Beyond that, there are a lot of other bits of information we'd need to know to actually say what the probability of one particular hypothesis is given the data. And it's important to realize that, because it's a reminder of the fundamental limitations of what we can do and say.

Why did I switch from confidence intervals to p-values? Because it turns out [they're connected](https://stats.stackexchange.com/questions/16467/what-is-the-relationship-between-a-p-value-and-a-confidence-interval).

> I guess what I'm asking is, is the difference between confidence and probability merely a technicality or is it actually meaningful in applied probability?

On one hand, maybe it isn't that big a deal compared to the multitude of statistical sins that get committed commonly. The average quality of data analysis might be better improved if we paid more attention to the various ways the data are fucked, violations of big assumptions like independence, and the like.

On the other hand, there are a lot of subtleties in statistics. Some of them are very important. Some of them are less so. But many of them are deeply connected (such as confidence intervals and p-values). Getting sloppy in one place where maybe it's okay can lead to getting sloppy somewhere it's not. It's better to grapple with these subtleties rather than try to pave them over. Because understanding that frequentist statistics sees parameters as fixed and data as noisy is kind of useful if you're going to be doing frequentist statistics.

On a perhaps vaguely more practical hand, consider this. Confidence intervals are frequentist concepts, and good confidence intervals give you the appropriate coverage probability. There's no force on heaven or Earth that guarantees that an 89% posterior credible interval has 89% coverage because they are *fundamentally different things*. You should use the one that corresponds to the answer you want for the situation you have. And you can't know which is which if you're trying to pretend they're the same.",16,wdxoe7,"Like, why are they seen/presented as two totally different ways of thinking about/formalizing probability?  The way I think about it, if we have a constant but unknown value (which a strict frequentist interpretation says can only have a probability of 0 or 1), all the unknown information about it corresponds to other unknown values (e.g. if we knew the exact initial conditions of some previous coin flip with an unknown value, we could, in principle, determine the result without ever looking at the coin) and we can imagine simulating the situation, varying the values of the unknown parameters, and hence construct a probability distribution.  

I'm not suggesting that reframing the unknown information as a list of parameters, coming up with a set deterministic equations to plug them into, and running such simulations is necessarily _practical_, but just that, as an abstract idea, it seems to me that it relates frequentist probability to Bayesian probability.  

I can't be the first one to think of this idea, so I assume that either 

1. It's actually well-known among statisticians that this is how it works

or, more likely,

2. There's a flaw in my reasoning and/or something I've not considered.

Which is it?


On a related note, why can't we informally incorporate the idea of knowledge into probability without using the full Bayesian framework?  Like, why do classical confidence intervals have to be understood through the strict frequentist lense in the first place?  Why _can't_ we assign a probability to the unknown parameter, thinking about the confidence interval in Bayesian terms without using the full, complicated Bayesian framework? 

 I mean, I get that pure mathematicians would want things to be rigorous and that the frequentist and Bayesian versions of probability theory start from different axioms, but physicists do stuff all the time that's probably kind of sloppy from the perspective of the pure mathematician (like treating Dirac Delta as an actual function, even though it technically isn't, without justifying that it still has the relevant properties they're relying on).  They do it because it works, and doesn't cause any problems.  Why can't we do the same thing with applied statistics?  Like, does insisting that confidence ≠ probability, as textbooks often do, actually have any benefit?  Or, to be more precise, would interpreting an n% confidence interval as ""There's an n% chance this interval contains the true value of the parameter"" cause any actual problems in applying the interval to a real-life situation?  

I guess what I'm asking is, is the difference between confidence and probability merely a technicality or is it actually meaningful in applied probability?",AskStatistics,2022-08-01 17:52:06,21
You certainly can compare the two models. The most common way to make this comparison is through ANOVA on the residuals.,2,weafsp,"Hey there,

Im working on a linear regression with an interaction effect. I want to know if I can compare the model with interaction effect to a model without. Does this give me information about the quality of the interaction effect (comparing the Rsquared) ?",AskStatistics,2022-08-02 05:30:46,6
"Haven't tried to verify with the actual numbers, but I'd guess these p values are probably just a 2 sample t test between the male and female groups.",3,we80wk,"I'm doing an analysis of the following data table. Is it possible to figure out which Test score was used to get the P-value? (Also which data would I use to find the P value (Mean, SD, both?) Thanks so much. I already had one other question answered and I greatly appreciate this Reddit page. I hope to give back to this page one day:)

[This is the overall sample size in each group](https://preview.redd.it/3e7lng8e0af91.png?width=1332&format=png&auto=webp&s=e3787d345dcc445b1fbcbce62b5bb957a2f0cc44)

[This is the data that I want to investigate](https://preview.redd.it/ctxs10t40af91.png?width=1254&format=png&auto=webp&s=d5e49d2c4f23ed4f421ad44f0e1f0b84cb2d6df9)",AskStatistics,2022-08-02 03:21:33,4
"R^2 is more common ""because it penalizes errors according to how bad they are."" Really bad errors are penalized more than small errors. Really though I think it is mostly historical as R^2 makes some of the math easier when you are fitting the model.

As the other commenter said, when it comes to evaluating the model it often makes sense to look at R^2 as that is what the model fitting is trying to minimize.",2,we3vcl,"I understand that R values near -1 are strongly correlated as much as R values near 1 are, and R^2 removes the negative, but if you want to get a standard for correlation, wouldn’t R^2 make the correlation seem less than it actually is since |R^2|<|R| for -1<R<1?",AskStatistics,2022-08-01 23:04:07,3
"Since the same subjects were measured, you should use paired t-tests if you are only comparing one parameter.

If you are comparing more than one parameter (joint angles and something else) you could use a two-factor ANOVA without replication followed by Tukey's HSD.",1,we7c4z,"Currently working on a small project where we are comparing two different types of push-ups done by the same subject. The data we have includes, for example, joint angles with measurements consistently over time. Can I use a t-test to compare the joint angles for the different types of push-ups? If not, what kind of analysis should I use to compare them? Any help is appreciated. Thank you.",AskStatistics,2022-08-02 02:40:53,6
"First of all, most rules of thumb are wrong. Modern methods are robust to heavy missingness patterns (i.e., way more than 10%) and listwise deletion is **never** the best option. From such methods, the easiest to implement is Full-Information-Maximum-Likelihood. This is a method which, without imputing any data, uses all available observations for the inference of the variance-covariance matrix later used for inferential analyses. If you use R, this is available in lavaan and it's pretty easy to implement with the argument ""missing = fiml"". Otherwise I believe MPlus has it as a default option. Here more on this:

Newman, D. A. (2014). Missing data: Five practical guidelines. Organizational Research Methods, 17(4), 372-411.",2,wdytt6,"I am currently doing my honours thesis project. It has a longitudinal quantitative design and uses a number of scales across two time points.

The current sample is 102 participants, yet I have some MCAR data (Little's test is non-sig) and I am not sure how to deal with it. The missing data makes up 1-4% of a number of different subscales and measures. But if I use listwise deletion it will remove 13 cases out of the whole sample (I only need 36 participants for power of .9).

I am not sure how to proceed on this one. I've been told by stats teachers that <5% of missing data can be ignored. Others have said listwise deletion is appropriate or even imputation. Any and all suggestions/guidance is appreciated.

&#x200B;

Edit: I use SPSS for analyses ",AskStatistics,2022-08-01 18:44:38,1
"As far as I remember, PROCESS only dummy-codes multicategorical X, M, Z and W variables behind the scenes (in fact, there is a ""multicategorical"" menu button for this express purpose), covariates need to be coded manually, unless you want them to be treated as metric.",1,we52c3,"Hi there,

I'm running a simple mediation model using Hayes's PROCESS macro in SPSS.

I know that if I'd like to controlling marital status (categorical variable, 3 levels), then I have to add the variable in the covariate(s) box.

My question is that do I need to do dummy coding and then put all dummy variables (i.e., 2=3-1 in this case) in the covariates(s) box? Or I just have to put the categorical variable marital status in the covariates(s) box and PROCESS will do it for me?",AskStatistics,2022-08-02 00:17:26,3
"Rather than continuous or categorical, I would maybe consider age an “ordinal” variable—essentially categories that have natural ordering. A lot of the time, ordinal variables can be treated like continuous variables, and personally I think age is one of those cases (think about it, being 28 and a half years old makes perfect sense), but if you want to read more about ordinal variables here’s a link: 

https://en.m.wikipedia.org/wiki/Ordinal_regression

A case where ordinal variables might be useful is when you have qualitative descriptors that have natural ordering (like dark, dim, bright, very bright)

ETA: if you are bucketing age groups then the ordinal approach is more relevant, otherwise maybe just keep it continuous",6,wdr439,"Hi all!

I’m working on a moderation analysis and had a question about “age” as a continuous or categorical variable. 

The main question is “does trauma impact anxiety”, perhaps differently depending on age of a youth sample. 

Originally, I was thinking of comparing children and adolescents as two categories to see if different relationships emerged based on developmental stage for our outcome variable (anxiety). When I do the moderation analysis using age as categorical (child*trauma, adolescent*trauma), I find that trauma increased anxiety for the child group, but not the adolescent group.

However, when I treat age as continuous and use age*trauma as the interaction term, there is no significant relationship with anxiety.

So, is there still a relevant moderation effect here? Or, should significance be showing up in both analyses in order to consider this relevant?

Thank you!",AskStatistics,2022-08-01 13:09:05,10
"I would say definitely statistical inference, linear algebra and matrix concepts. You should be able to find a Jacobean, Hessian, etc, and be confident with eigendecompositions and projection matrices, etc. 

There’s a question regarding whether you need measure theory or not - I would say not?",3,wdl60b,I just want to know which statistical concepts or mathematical concepts I should really learn/know as bare minimum before I approach this subject. Thanks!,AskStatistics,2022-08-01 09:06:24,1
"The first question you need to ask yourself when doing any type of 'significance testing' is: ""what am I testing for?"". What does statistical significance even mean in this context and what is the null hypothesis the p value is referring to? 

&#x200B;

In this case, the p value of a pearson correlation commonly refers to the null hypothesis that the correlation is exactly equal to zero. So your p value is the probability of observing your data, given the null hypothesis is true. If the p value is low, it means your correlation is not as likely to be zero. 

The other thing you need to do is actually look at the data. Some types of correlations have assumptions about the data. Does your data fulfill this assumption? (see ascomb's quartet).",4,wdi9hj,"Hello! I would appreciate some help with how to test if my correlation coefficient (0.71) is statistically significant. When I calculated a 95% confidence interval I got [0.41, 0.86]. Does this mean I am 95% confident my correlation coefficient is between 0.41 and 0.86? Is there any other more useful way I can test for significance?",AskStatistics,2022-08-01 07:02:25,9
"To calculate the expected future life span under each scenario you would calculate the integral of the survival function from now until some maximum lifespan or until the effect is no longer present, whichever comes first. This requires information outside just the hazard ratio itself.",1,wdk6io,"I don’t know much about statistical methods, but I’m trying to translate the hazard ratios in this study to life expectancy or loss of life years : https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6926145/

Specifically this part :
> After controlling for 6 individual-level factors and fixed effects of survey year, structural stigma was associated with mortality among individuals who reported past-year same-sex sexual partners (HR=1.95, 95% CI: 1.14, 3.31). Further, there was a dose-response relationship with mortality in this group, such that those residing in communities in the highest quartile of structural stigma had the greatest mortality risk, controlling for these same factors (HR=2.12, 95% CI: 1.03, 4.38). In sensitivity analyses, the effect size for structural stigma ranged from 1.54 to 2.30, indicating a consistent, but small-to-moderate, effect.

Any help is appreciated !",AskStatistics,2022-08-01 08:25:14,1
"I learned from the classic texts by Joe and by Nelsen (and from several papers), but I don't suppose they're good introductory texts. I am curious to see what might be suggested; maybe I'll learn something.",2,wdkvuf,,AskStatistics,2022-08-01 08:55:18,2
"This is hard because we don’t know what you know or what “real life” domain you work in, if you have one. And, respondents may estimate “the basics” differently from you and each other. 

_but_ 

These are interesting: 

Mackay
https://www.inference.org.uk/itprnn/book.pdf

Jaynes
https://bayes.wustl.edu/etj/prob/book.pdf

I personally like Knuth “the art of computer programming” but it’s about what it says. Teaches me good algorithmic thinking. But then I write stats algos. Loads of it is about n choose k and things like that though.",1,wdiic5,,AskStatistics,2022-08-01 07:13:17,2
[removed],1,wdhme0,"Hi r/AskStatistics brains, I have a question that probably has pretty easy solution, but I'm not sure how to go about it.

I helped organize three events at work that were all quite different and the resulting scores are wildly dissimilar:

1. The first was academic, a quiz, that resulted in scores from 43 to 136 points out of a possible 145. There were 17 participants.
2. The second was athletic, a search for things hidden around the office. Scores between 50 and 760 points, 10 participants. (Not sure of the total possible points, but I suppose I could add everyone's scores together for this if needed)
3. The third was creative, make a video, and votes on the best video ranged from 2 to 14 points, also 10 participants.

How do I calculate an overall winner in a way that's fair? Obviously, the person who received 760 points in the second event would win if I just added the scores together. *But I also want to make sure they're rewarded for scoring so highly (second place had 470 points)*. It's early and I'm not fully awake yet, but **how do I weight the individual events so that the winner is fair?** I can provide raw data if needed, but I'd like to learn this as well.",AskStatistics,2022-08-01 06:33:13,7
It looks like you have a multicollinearity problem since the value of your binary covariate “Pa” is determined by the value of the home and this is the major problem you need to asses.,1,wdkvfv,"I'll make up a scenario that's similar to what I'm doing, but hopefully easier to understand.

Let's say I have data for three variables: the price (premium) of homeowner's insurance (Ph) as the dependent variable y, and then the independent x variables are the value of the home (Vh) and the presence of bundled auto insurance (Pa) for those customers who decide to purchase it alongside the homeowner's. I need to see the effect of Vh and specially Pa on Ph. Pa is either 0 or 1, either you bundle or you don't.

The problem is, it turns out that customers who have expensive homes tend to self-insure their homes and NOT bundle auto insurance, meaning that customers who do bundle their auto insurance have cheaper homes and thus lower premium, and so the regression outputs an absurd negative coefficient for auto. Which makes no sense of course - no insurance company will give you auto insurance for free, much less pay you to take it.

So, my question is:

Is there a way for the linear regression to control for the value of the home and eliminate this confounding factor, such that it tells me, if the value of two homes are exactly equal, how much premium would an auto insurance bundle add?",AskStatistics,2022-08-01 08:54:52,5
"Yes, the betting companies do recruit mathematicians and statisticians. So in a sense yes.",67,wcyshl,honest question,AskStatistics,2022-07-31 13:58:24,37
"It’s a very counter-intuitive notion. One critical issue is the probability of .95 cannot be deduced formally from the axioms of probability theory even though intuitively it seems as if it should be.95. Another issue is that there are various methods that will contain the parameter 95% of the time yet sometimes the intervals do not even overlap. For some purposes, probability is best thought of as subjective without adopting the whole of the Bayesian approach. It is reasonable, although not required by the laws of probability, that one adopt a subjective probability of 0.95 that a 95% confidence interval, as typically computed, contains the parameter in question. You would not be susceptible to a Dutch Book.  At a practical level, Bayesian credible intervals based on flat priors and confidence intervals are in the majority of cases very, very similar.",2,wd6luh,"Wikipedia says that two valid interpretations of confidence intervals are

>1. The confidence interval can be expressed in terms of a [long-run frequency](https://en.wikipedia.org/wiki/Long-run_frequency) in [repeated samples](https://en.wikipedia.org/wiki/Replication_(statistics)) (or in [resampling](https://en.wikipedia.org/wiki/Resampling_(statistics))): ""*Were this procedure to be repeated on numerous samples, the proportion of calculated 95% confidence intervals that encompassed the true value of the population parameter would tend toward 95%.""*[\[10\]](https://en.wikipedia.org/wiki/Confidence_interval#cite_note-CH-10)  
>  
>2. The confidence interval can be expressed in terms of probability with respect to a single theoretical (yet to be realized) sample: ""*There is a 95%* [*probability*](https://en.wikipedia.org/wiki/Coverage_probability) *that the 95% confidence interval calculated from a given future sample will cover the true value of the population parameter.""* [\[11\]](https://en.wikipedia.org/wiki/Confidence_interval#cite_note-Neyman-11) This essentially reframes the ""repeated samples"" interpretation as a probability rather than a frequency.

but also says 

> A 95% confidence level does not mean that for a given realized interval there is a 95% probability that the population parameter lies within the interval (i.e., a 95% probability that the interval covers the population parameter). 

This makes no sense to me because the latter statement seems to me to be saying *the exact same thing* as the two former statements, just more concisely.  

The article goes on to say that 

> According to the strict frequentist interpretation, once an interval is calculated, **this interval either covers the parameter value or it does not;** it is **no longer** a matter of probability. The 95% probability relates to the reliability of the estimation procedure, not to a specific calculated interval. \[Emphasis added\]

What they seem to be getting at is that the parameter to be estimated is a constant, not a variable, so it doesn't make sense to talk about it's probability being anything other than 0 or 1.  I've heard this before and, while I very strongly disagree with defining probability that way (though I don't object to the frequentist interpretation of probability overall), it at least makes sense.  But it says it's ""no longer"" a matter of probability after the interval is calculated.  That doesn't make any sense to me.  Like, if we're talking about the frequentist interpretation and so strongly avoiding the Bayesian interpretation (which is the impression I've gotten when I've asked about this in the past), why should it matter whether we're asking about the probability before or after calculating the interval?  It's not like calculating the interval magically changes the parameter somehow, so why can we say that ""There is a \[n\]% probability that the \[n\]% confidence interval calculated from a given future sample will cover the true value of the population parameter"" but once we actually take a sample and do the calculation, we can't say ""There is a n% probability that the n% confidence interval calculated from the given sample actually covers the true value of the population parameter""?  How are those two statements different in a meaningful way?

Like, it might make more sense if it just said not to talk about the probability of the *parameter* having a given value, but why not talk about the probability of the *interval* actually containing the true value?

To reframe my question: say I've taken a very large number of samples from some population and I compute an n% confidence interval using each one.  I should expect that approximately n% of those intervals will actually contain the population parameter, right?  Equivalently, were I to start randomly sampling from the set of confidence intervals, I should expect that approximately n% of the intervals I choose will actually contain the population parameter, right?  So, I have an n% chance of randomly selecting an interval that actually contains the parameter, yes?  So why can't I then say that there's an n% chance an given interval actually contains the population parameter?  Like, how is it any different than if I have a standard deck of 52 cards, with a 1:1 black to red ratio and I can just say that there's a 50% chance of choosing red and a 50% of choosing black, if I were to randomly choose a card?",AskStatistics,2022-07-31 20:04:37,18
"Here is the important pre-question: Why are you considering log transforming at all? Many people (perhaps including your stats profs) seem to think that this is ""good"" or ""necessary"" for right skewed data, but a very quick review of the assumptions shows that usually it is not. In short, there is no assumption in linear models that says that your dependent variable needs to be symmetric, or normally distributed.",2,wd3gl5,"Hi all,

I have a DV that is right-skewed, plot attached. I'll be defining a linear mixed model, and I'm trying to intuitively understand the pros/cons of log transforming the DV then running a LMM vs. defining a GLMM with a log-link function and random effects.

https://preview.redd.it/l7m4net0yze91.png?width=1200&format=png&auto=webp&s=b0b2ee51a1c6353462f7af7a4eae0c83430cb243

I saw [this useful post](https://stats.stackexchange.com/questions/47840/linear-model-with-log-transformed-response-vs-generalized-linear-model-with-log), and while I sort of understand that that log-link is the better choice, I'm not sure I understood it well enough to then justify it to my stats-illiterate colleagues.

Thanks ya'll!",AskStatistics,2022-07-31 17:31:03,7
"I don't see how you're going to get 3 variables from your three scales - taking the median is going to reduce your three scales to 1 variable.

`
[1,1,2] -> 1
[4,5,5] -> 5
[5, 2, 1] -> 2
`

Setting aside that you haven't said what tests you're running, I can't recommend doing this because it reduces the information content of your scales. Considering all the potential 125 ways someone could answer your three questions (assuming a 5pt scale), you would be reducing the space of responses to a set of discrete values. Unless your hypothesis is very clearly about the median of responses to very related questions, I wouldn't do this.

More commonly I see people adding Likert scale responses, or taking the sum of binarized versions (e.g. # of top-2 responses). If I had 3 questions related to a specific construct, adding them retains more information for analysis than taking the median. 

Someone that answers highly on 2/3 questions, like [5,5,1] would lie in the upper end of the scale for the new variable, instead of the top end.",1,wd1exl,I have collected data using three likert type scales. I've been recommended to transform the responses from each scale into one variable which will be the median of all the responses. So I will have three 'new' variables which I can use to conduct statistical tests. Is this correct? What are the drawbacks to doing this?,AskStatistics,2022-07-31 15:55:13,2
Consider using a [polychoric correlation](https://en.m.wikipedia.org/wiki/Polychoric_correlation). It's appropriate for ordinal variables that are hypothesized to reflect underlying normal distributions. The R package `psych` has a function to compute polychoric correlations.,1,wcv2de,"Hi, 

I'm trying to find a correlation between two likert scales of a sample size of 20 and I wanted to know if I would need to use the median or the total score to carry a spearman's test

&#x200B;

Thank you beforehand",AskStatistics,2022-07-31 11:13:45,2
"This sounds like something specific to your discipline and your study, not general statistics. I would talk to your advisor, or ask in a discipline-specific forum.",1,wczce0,"Hello everybody.

I wonder what is the suitable method of sensitivity analysis to check the input variables of a fuzzy cognitive map for a decision support system? The fuzzy cognitive map has 21 parameters with a value range between -1 - 1 and only one output value.

Which one of the methods from the python's library named [SAlib](https://salib.readthedocs.io/en/latest/) would you choose for it?",AskStatistics,2022-07-31 14:22:24,1
It depends on your role not the industry? Statistical analysis can be applied to almost anything that generates data.,1,wcxef3,"Which industries allow data analyst job positions to use statistical analysis, rather than just reporting and visualizations/basic descriptive statistics? Finance? Tech? Insurance ?",AskStatistics,2022-07-31 12:57:10,3
"Age was probabably a bad example.

Most questions are e.g.,: How do you rate your sleep quality : Extremely bad to Extremely good  
How competitive are you in-game: Not competitive at all to Extremely competitive  
etc.  
So I cannot do e.g., a table like this because the selectable answers are always different

||Extremely bad|somewhat bad|Neither good nor bad|.....|
|:-|:-|:-|:-|:-|
|Variable|%|%|%|%|",2,wcn76s,"Hi, I have 30 Likert Scales (And 10 nominal frequencies), and I want to present the results.

I only want to show which answers were taken the most (Which category was selected with the highest frequency). I also thought about showing the second most selected category (See picture below).

Unfortunately, I don't find a proper way how to report the Likert scale frequencies in APA. I'm discouraged from putting in every graph of my scales to save space and move them to the appendix. So only showing like, ""Look here, this is where most people categorise themselves"".  


Any recommendations?

  


https://preview.redd.it/mtfannrd9we91.png?width=1288&format=png&auto=webp&s=269636bc236497696f98013641b7ca349187023e",AskStatistics,2022-07-31 05:06:10,3
"Not without more information, no. Is there some specific reason you want to use CCA? It's hard to for us to say anything specific without knowing anything about your data.",1,wcut9d,I am working on a capstone assessing staff shortage and satisfaction scores related to visitation restrictions. Any suggestions on how to utilize canonical correlation to show this?,AskStatistics,2022-07-31 11:02:25,2
What sort of work will you be doing? Different jobs require very different sorts of things to be done.,2,wcge9z,"I was offered a 6 month detail at my current workplace to serve as a statistician. If they don't like my work, they will hire a statistician at the end of my detail. I only have 3 classes in statistics, so I'm likely not the best candidate long term. However, I do want to do quality work and recognize that much of my work will be passed on to a future statistician who may be unfamiliar with the industry. I am writing to ask if anyone can outline what their work products look like? I work as an engineer, much of my work follows a specific format. For those of you who work as a statistician, do your work products follow the standard introduction, methods, results, discussion and references format or are your work products different? I welcome any links to examples or tips. I do work with R and SQL, just have not aggressively approached the data with a statistical lens before. Thank you!",AskStatistics,2022-07-30 21:43:36,3
"What you are suggesting is a very complex model, likely to either have issues with convergence or a high risk of bias and misspecification. You will also likely need a large sample size, given the high number of parameters you seem to be suggesting. 

My experience with statistical consultants has never been that helpful. Mainly because it is really something that you have to work out yourself and by the time you get to modelling stage, you are the expert on your data and your hypotheses. I have found they generally point you toward the statistical approach they are most comfortable with. 

I don't know what your level of experience with SEM is but I suggest looking into Dr Erin Buchanan's materials on YouTube and her website. She has a grad level course that she provides all of the materials for, including lectures and labs. Another good resource for understanding how to fit difficult models is Dan Bauer and Patrick Curran's YouTube and website. There are lots of key books, including Kline, which are also linked with these resources. 

Good luck, I chose to do a complex SEM for my PhD thesis and the self-directed learning was probably the most enjoyable part of it.",3,wcizr1,"I have developed a conceptual model containing six variables for my dissertation proposal in the social sciences. There is a central behavioral variable related to human agency with three affective/attitude-related variables adjacent to it, and also one ""outcome"" variable. The attitude variables have reciprocal relationships with the behavioral variable, but also each other. The relationships directed from the behavioral variable to the attitudes is moderated by a personality-related variable. As such, the model contains several feedback loops. My research advisor suggested employing a nonrecursive model in SEM to analyze the conceptual model. But I'm unsure if this approach is appropriate for my model in its current form, if I have to modify my model, or even if it's possible to analyze these relationships at all. I know non-recursive models usually require multiple instrumental variables for each endogenous variable but it could be difficult to identify these when there are so many closely related variables in the model.

I haven't collected any data yet. I just want to make sure that it's possible to analyze these relationships effectively before I commit fully to this model for my proposal. As I am not a statistician, I have also been looking at several websites providing statistical consultation but am wary about the quality of the service provided and the risk of being scammed. Can anyone provide me with some feedback or, failing that, a reputable reasonably-priced consultancy site for small scale projects such as mine. Please bear in mind I'm a poor student! :)",AskStatistics,2022-07-31 00:25:33,6
"> I honestly believe it's too overwhelming or rather uneven to analyze all those 30 versus the 10 samples we collected in previous collections.

I don't see how either comment is justified. The change in sample size shouldn't present problems that I can think of, and I am not at all sure what would be overwhelming. Indeed 10 data points seems ... extremely low, unless you really don't  care about power.

> I selected the 10 samples from each of those 30 by random selection.

Don't throw away information without an extremely good reason. I don't see any good reasons here.

> Is my random selection of 10 samples out of those 30 completely fine or rather harmless? Kinda a little paranoid but my intention was to at least make the comparison evenly to some other collection date points. Is this a valid thing to do?

Why not ask before you did it, rather than after?


> group pairwise comparison

I'm missing something. What kind of pairwise association survives sub-sampling your data?",0,wch9in,"Hi. I wanna ask something about the research im working on.

Basically, our research has set of data that I decided to analyse for my own research paper (thesis). It is a sample collection by date collection point and initially was supposed to observe 10 samples each collection point. However, the last two date collection points, they (the team) decided to increase the observation to 30 samples. This was not initially planned so I am clueless of such move.

I honestly believe it's too overwhelming or rather uneven to analyze all those 30 versus the 10 samples we collected in previous collections. We also have collection dates with less than 10 samples which would make comparison highly uneven. Increasing the last two collection dates to further widens the gap.

So for my own thesis analysis, I decided to analyze only 10 out of 30 samples of the last two collection points and leave the overall analysis to the assigned staff for the project (because I honestly don't know about their last minutes plans, not too privy abt the later changes).

I selected the 10 samples from each of those 30 by random selection. I used a random number generator from samples 1 to 30 and picked the first 10 unique numbers which represent the samples.

The study would involve group pairwise comparison of these observations from other collection date points. Our standard collection was 10 per date but we previously collected as low as 4 samples in some dates due to loss beyond our control (e.g. death of observered animal). Those with low sample size will be kept intact but those 30 will be randomized and select 10 from them.

Is my random selection of 10 samples out of those 30 completely fine or rather harmless?  Kinda a little paranoid but my intention was to at least make the comparison evenly to some other collection date points. Is this a valid thing to do?

Thanks for reading and sorry for long rumbles.",AskStatistics,2022-07-30 22:34:28,8
"If you need something specific, you could also make your own using Shiny and Plotly. I have created illustrations for the Central Limit Theorem and Cronbach alpha's sensitivity to the number of items in the past and they worked great",3,wcib2f,"Is there a free repo for interactive web apps illustrating statistical concepts? Something like the Wolfram Demonstrations Project, but made with no proprietary software that can run directly from the browser.  
For example, I’m looking for simple apps like this:[https://demonstrations.wolfram.com/TheNormalDistribution/](https://demonstrations.wolfram.com/TheNormalDistribution/)",AskStatistics,2022-07-30 23:41:03,1
"Oh man this seems very complicated LOL. But it's interesting! There's no non-parametric equivalent for a regression. Multiple regression seems like the best option for this if your DV has a lot of ordinal levels (thus treating it as continuous) and different participants in each predictor category because you have predictor variables. There's wiggle room for parametric assumptions for multiple regression because oftentimes it doesn't meet assumptions. I'm using a hierarchical multiple regression analysis for my dissertation and this is great to have more control over nesting the data.

If your participants are the same in each category, then you could use a factorial repeated measures ANOVA. You could also use an independent factorial ANOVA to test the different IV levels.",5,wc89zn,"Hello everyone, I’m not really sure if this is the place to ask for advice, but I’ll go for it anyway. I am a masters student working on a research proposal for one of my classes. I’m not super familiar with non-parametric designs so I could use some advice on the proper analyses for my proposal. I am proposing a study to test for differential pain ratings in those exposed to physician gaslighting. My IV has 2 levels and is nominal. I have a mediator with 2 levels that is also nominal. I also have a moderator to test which has 6 levels with potential overlap between levels (eg, I’m testing to see if demographic factors influence rating outcomes) which is also nominal. Finally, my DV will be measured using an ordinal scale. I expect to use some type of regression analysis to test the mediator, although I’m not really sure which one would work best for this study. I also might use a Chi Square to test the moderator, but I’m not really sure if that is the correct approach. Any advice would be greatly appreciated. Thanks!",AskStatistics,2022-07-30 14:46:18,7
"Have you read Casella and Berger already?

If not, I'd suggest that.

Also Cox and Hinkley (*Theoretical Statistics*)",6,wc5bjw,"Hey guys I am a post grad statistics student, I would love to get deeper in theoretical probability and estimation theory please suggest some books!
Thanks anyway",AskStatistics,2022-07-30 12:28:14,3
"Run a single model instead of 4, and add an interaction term between tenure length and TSR",5,wc3mae,"Hi everyone, 

For my master thesis I'm looking at the pay-for-performance sensitivity (pfp) of 1500 S&P CEOs and if this sensitivity changes as CEO tenure increases. In my table underneath you can see my main results.  What is missing from the table are only the control variables. T1 are the CEOs with the shortest tenure and T3 are the CEOs with the longest tenure.  So now my question is how do I test whether the pfp is significantly different between T1 and T3? In this case it does not seem like the pfp is any different between T1 and T3, but if I replace TSR with ROA this seems to be the case. 

I assume that ANOVA will not work as my independent variable is not categorical. 

And I am btw using R. 

&#x200B;

https://preview.redd.it/81xw2gzyvqe91.png?width=624&format=png&auto=webp&s=06bb34354e6d8ce51b434c65fe618452ac11c70b

Thanks!",AskStatistics,2022-07-30 11:09:54,2
"I'm not a statistician. If I were, I'd probably buy
fewer lottery tickets. Or maybe more? In any case,
the recent high megamillions jackpot reminded of
a question that's popped into my head in the past. For how long could the jackpot accumulate, theoretically? Like, as jackpot-winning tickets
continue not to circulate over the course of n-
number of drawings, what are the odds that no
one hits the jackpot and it grows to some top 5
richest-person in the world amount like 50+ billion dollars? And how do the odds of this happening (no one winning) change with each successive drawing? I know as the pot grows, it brings the non-problem gamblers out of the woodwork, and the more people buying the less likely it is that no one wins. But I've always been curious about the numbers and theory behind it.",2,wccv7p,,AskStatistics,2022-07-30 18:32:09,7
That sample size is fine for ANCOVA results to be valid unless your distributions are extremely non-normal in weird ways. The purpose of a covariate is to control between-subject variance but the two-way repeated-measures design you propose does that already (even without the covariate). An ANCOVA that you could do is a one-way with pretest scores as the covariate. Typically this is more powerful than the two-way repeated-measures ANOVA.,3,wbxxwl,"I am completing work on my proposal for my dissertation on mathematics anxiety. I will pretest and posttest math anxiety and provide an intervention to a group of participants while also having a control group that won't receive the intervention.

I will only have about 60 participants (about 30 in each group).

I had originally planned a 2-way ANCOVA.  It is has been a bit for me with statistics.  I am being told my sample size is too small and to select a different statistic. How can I still control for a covariate? What is my new statistical design?",AskStatistics,2022-07-30 06:47:47,3
"There's no 'tradeoff'; it's straight mathematics. 

There's a number of potential confusions here, but it's not clear which ones you might be hitting. So I may tell you some things that are obvious to you.

1. Before we start on the main issue, 
it's possible that in part you're confused about what a sampling distribution is.

  A single sample mean is a random variable and has a sampling distribution. (Of course once you observe it - obtain a realization of that random variable - it's just a number.)

2.  > understand the sampling distribution of mean (probability distribution of all possible means) of sample size n follows a normal distribution

  Not unless the original population is normally distributed. Otherwise normality of the sample mean is at best an approximation (whether it's a good approximation depends on the sample size and the characteristics of the original distribution)


---

Note that if |μ - xbar| is less than some distance d, then not only is xbar within that distance of μ  but μ is also within that distance of xbar.

[ Framed another way (a more general approach to confidence intervals), Q=[xbar - μ]/(σ/√n) is approximately pivotal. Which is to say that its distribution doesn't depend on μ or σ; whatever their values, you get the same distribution for Q. You can use that distribution for Q to produce an interval within which Q will lie with know probability (given the population distribution of xbar). You can then back out an interval for μ with the desired coverage.]


>  confidence interval seems to be telling me there is a 95% chance for the population mean μ to be within the range of x±2(σ/√n)

This is not quite a correct interpretation of a confidence interval (well, not unless you're very careful to be clear about what you mean by 'probability'; it doesn't apply to the single sample)

Note that the interval is random and mu is fixed. Once you observe the data, the interval will either overlap or not overlap mu. Over the long run of many, many such samplings, about 95% of the intervals they generate will overlap the (unchanging) population mean.


\*(±2sd is not exactly a 95% interval, even if you know the population s.d.)",3,wc4jqa,"I've been reading statistical estimation and so far understand the sampling distribution of mean (probability distribution of all possible means) of sample size n follows a normal distribution with mean μ (population mean) and standard deviation σ/√n (population standard deviation σ). From normal distribution's property I can easily understand therefore the probability of getting a mean value for a single sample of size n within the range  μ±2(σ/√n) is roughly 95%. However, I don't understand the reverse process which seems to be at the heart of confidence interval calculation. For a single random sample of size n, if its mean is x, confidence interval seems to be telling me there is a 95% chance for the population mean μ to be within the range of x±2(σ/√n). Wouldn't this require the normal sampling distribution of mean to have a mean at x, which is clearly not the case as x is the mean of only one particular sample? Is this an acceptable tradeoff during estimation or am I missing something big here?",AskStatistics,2022-07-30 11:52:44,2
Idk something with poetry would be pretty cool though.,1,wbzizb,"Hi everyone. I’m completing a master in statistics, but I already have a phd in language sciences. I’m currently looking for a topic for my master thesis and I was hoping to find something that combines statistics with linguistics or natural language processing. Does anybody have any recommendations? Thank you very much!",AskStatistics,2022-07-30 08:04:24,1
"Looks like homework, see rule 1


https://www.reddit.com/r/AskStatistics/about/rules


Not going to read your code.

 Note that type I error is under H0 while type Ii error is under H1. Beta is a function of 'how false' H1 is.  It will be different at each effect size (for a given sample size). 



Which is to say to compute it at a specific effect size (equivalently, a set of population means and sds) you must specify that exact alternative.

If you want a power curve (power function) you need to evaluate it at many such points and interpolate/curve fit (thoughtful approaches can cut the simulation effort way down though, albeit at the expense of some additional work getting to a function that's easy to interpolate and then transform back)",2,wc4flp,"    s_s = 100000
    counter = 0
    p = []
    
    for i in range(1,s_s+1):
         while counter < s_s:
            g1 = np.random.chisquare(1,2)
            g2 = np.random.chisquare(1,2)
            g3 = np.random.chisquare(1,2)
        
            g1t = np.sqrt(g1)
            g2t = np.sqrt(g2)
            g3t = np.sqrt(g3)
        
            levene = sp.levene(g1t,g2t,g3t, center = 'mean')
        
            if levene[1] > 0.05:
                anova = sp.f_oneway(g1t,g2t,g3t)
            
                p.append(anova[1])
            
                counter +=1
    
    hesap = sum(map(lambda x: x<=0.05, p))
    print(hesap/s_s)

I calculate type 1 error rate with this code lines. But I want to calculate type 2 error rate. How can I do it ?",AskStatistics,2022-07-30 11:47:27,2
"You're right to suspect that choosing which variables to put into a multivariate analysis based on a bunch of univariate analyses is in general a bad idea. You could easily discard a variable that does matter (but this only shows up when others are properly controlled for) while including one that doesn't (because it is correlated with a variable that does matter). It's not really about the significance threshold chosen specifically, you'd have the same problem with alpha = 0.1 or alpha = 0.01. You'd also have the same sorts of problems doing stepwise variable selection approaches where you sequentially add or don't add variables to the analysis, even if that looks more multivariate (though in that case you get added issues with order of addition). Bad practices die hard, though, and you'll find a lot of people doing stepwise regression or univariate->multivariate pipelines. Possibly, (probably?) essentially because ""that's what everyone else does.""",1,wc2vzv,"Hello. I am a medical student conducting research and learning statistical analysis/proper study design. In a lot of the papers I come across ([https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4195194/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4195194/) for instance) will perform a univariate analysis on their variables that may/may not predict prognosis using Chi-square/Fisher/T-test/etc and then insert significant variables into a Cox or stepwise logistic regression. My question is, if using .05 for significance throughout all univariate analyses, does this introduce experimentwise error and therefore bias/alter the multivariate analysis in some way? What's wrong with just doing just a multivariate analysis instead of multiple univariate analyses then a multivariate analysis (I'm assuming it just reduces # of variables/DF in model?)",AskStatistics,2022-07-30 10:36:04,3
"If you have the population of interest, why sample it?",7,wc2vr3,"I'm looking for advice / validation on my approach to a data analytics project. I'm using a data set which has \~100K records of loans from the government. I'm considering this a ""population"" since it's all the loans which were granted during the lifetime of a particular government program. My goal is to identify patterns / correlations related to my variable of interest, the loan amount. I looked at the loan amount and it is  right-skewed. From my intro knowledge of statistics, I believe I can take a random sample of the 100K records, hopefully ending up with a bell-shaped sample distribution which I can then use to calculate confidence intervals, hypothesis tests, etc. The results of my calculations on my sample data would then be able to be generalized to the population with a certain level of confidence. Does this sound like a reasonable approach? Any other suggestions?

If I don't do the sampling, it seems like I wouldn't be able to say whether differences in the loan amount between population subgroups are significantly different or not.",AskStatistics,2022-07-30 10:35:45,7
"They are nothing like each other. 

Rethinking is a great introduction to practical Bayesian data analysis and thinking about statistical ideas, with heaps of worked examples and a sense of humour. 

BDA is a dry math-intensive reference that goes deep into the details. Very thorough and rigorous, but IMO only something to consider if you’re maths inclined and really want to get deep into Bayesian stats. 

Get both!",21,wbk882, A very basic question. Can any Bayesian Statisticians summarize the difference between McElreath's Statistical Rethinking and Gelman's Bayesian Data Analysis?,AskStatistics,2022-07-29 17:35:06,5
"Correct. But only if you want exactly 13 correct answers and the questions are independent of each other. If you want at least 13 answers correct, you’d have to take the sum of the blue term starting from x=13 and ending in x=20. Maybe that’s what you want?",11,wb3m7x,,AskStatistics,2022-07-29 05:32:42,7
"Yes, it does. The overall chance of success (S) is S= X% * Y%/100

So you want to increase whichever one (X or Y) will lead to the highest S. 

For example, let's say we have X=20 and Y=40. So our chance of success is 20*40 /100=8%.

If we increased X by 20%, we have S=40*40/100=16%.

If we increased Y by 20%, we have S=20*60=12%.

So in this case we are better off increasing X (and a general rule of thumb is to increase whichever one has a smaller chance of success)",1,wbfalm,"Lets say I have a scenario where if I have to win, I first have to pass A (which is X% chance).

and IF I pass A, I have to then pass B (which is Y% chance). 

If this game, if I get to choose +20% in either A or B. Does it matter which one I boost the chance?",AskStatistics,2022-07-29 13:47:05,5
"MANCOVA (and any kind of regression) requires complete data, so you will need to do some kind of imputation, if you are not comfortable with dropping cases.",2,wbg7em,"Is there a way to do a MANCOVA (or another analysis) in SPSS or R that does not delete cases that have missing data?

I'm trying to compare mean differences on several dependent variables with one covariate. I have one predictor (or independent variable) that has two levels. I feel really stuck with the missing data aspect and would appreciate any help. I cannot figure out how to do multiple imputation, but maybe that's the only solution? Thanks!",AskStatistics,2022-07-29 14:27:04,3
"If you have a 1/33 chance of pulling any good card, and an independent 1/5 chance that, if found, the good card will be the best card, then you have a 1/165 chance of pulling the best card.",2,wbdyza,"Am I wrong? 

I am opening packs of cards, 1:33 packs has a good card. 

There are 5 good cards in the pool of possible good cards. 

So I have a 1:5 chance to pull any one of the good cards. 

Does this mean I have a 1:165 chance to pull the best card out of the 5 good cards? 

Side notes: you can pull multiple of the same good card, they all have the same odds of being pulled. 

So just because I pull one, does not mean it’s removed from the pool of available good cards.",AskStatistics,2022-07-29 12:48:48,3
"There are so many answers to this question that nobody can possibly describe a solution for you based on the information you’ve shared. 

My recommendation would be to ask around in your company and find someone with a strong knowledge of statistics and seek their help.",9,wb2lyy,"We want to compare the global average of a sample to the average of each sub-group to see if there is a notable difference. We are writing a  rapport for high up's and for archiving our process. From our knowledge of statistics, to compare 2 averages or an average to a constant, you require to use either a Shapiro test or a t.test but both of them need to follow the normal distribution, which isn't the case with our irl data. Sadly, our knowledge of stats is based on classes we had in which most stuff was arranged to fit with a certain narrative, we were never really thought how to deal with that kind of data.

We did cover a bit of  log transformation in my class,  except that multiple sources say that the results aren't reliable for further comparison, so this is why we are  asking what is the procedure for such a situation.

&#x200B;

edit more context of  info 

 

The  index is the iqbr index of quality of a bande riveraine. It's  whole  formula that gives a value in between 17 and 100. We have a territory we  sampled from the towns that recquired the study. We did 24% of all the  streams and rivers in the territory.

We  want to know to compare the average of the 4 watershed on the  territory. We want to know if one of them distinguish themselves with  their IQBR compared to the rest. There is not previous data at all, so  we are here to do a global review of the entire territory so that the  towns can know how they will apply the reglematation  that was installed  by the ministry. The same of the program is the PRMHH.

Like, having multiple box plot, one for each watershed and one of the whole thing and like we get the average and tells us yes there is a significant difference between watershed A and the global one. It's  hard for me to properly translate terms du to the language barrier,  alot of the terms I only know in french and I am trying to avoid giving  confusing and useful information.

So like in my labs and classes  it was always the first thing  gotta check the normality and oh would you look at that it fits then  lets proceed to the next step, but like what are we supposed to do when  the  stuff isn't staged and doesn't follow that kind of stuff is all  the knowledge I have from that class useless.

This  whole thing honestly just shows me how little those 2 classes actually  thought me on stats and how much of it was just vomit everything on the  exam and instant forget the rest.

I have a visual idea of what I want but idk how to get there, but there are pieces of the map missing.",AskStatistics,2022-07-29 04:43:11,17
"Hey OP! I am a statistician and came from a biomedical background. You can still work in insurance/finance without the actuarial science major. I am in Australia and worked in an actuarial consulting firm as a statistician. I mostly did modelling of claims data (frequency and severity) and didn't have any actuarial background prior to doing this. Data science and statistics in these areas are rising considerably in recent years. In saying that, applied statistics or computer science cut across all industries so yes you can 100% find a job that isn't actuary related.

Start looking at some Data Analyst internships while you're still at uni to get a taste of different industries. Jobs that you can get from applied statistics range anywhere from statistician, data scientist, to data analyst/business analyst. If you have CS minor as well, you could look at more specific jobs like machine learning engineer, data engineer etc... It's really quite a broad discipline and has the ability to cater to your personal interests!

Best of luck!",2,wbfrdp,"I am currently an Actuarial Science major going into my fourth year at Purdue University, but am coming to the realization that I do not want to spend the rest of my young adulthood studying for exams when I can barely stand studying for exams now. I can transfer to Applied Statistics major and be done quicker, so I was thinking of adding a Computer Science minor just to diversify my profile. If I made this switch, would I be able to find a job that isn't actuary related and what jobs are they?",AskStatistics,2022-07-29 14:07:16,1
"Schools often have their own academic computing clusters. Maybe see if there is an internal resource before you go and pay for your own cloud service.

Also look into Microsoft Azure, the user interface is more transparent (though still a bit murky). Also, consider learning a bit of R, many ML platforms natively support R so you don't have to fuss with the Stata install.",9,wb0npw,"Hi there. I am a PhD student and I would like to run a statistical simulation using Stata. The good thing is that the task can be run in parallel and I will need as many cores as possible. The problem is it will take weeks or longer on my personal computer and block it. I wonder how difficult it is to rent some computing power online. To start with, my budget is about 100-200€ (for 500€ I can buy a 24 core Xeon workstation so yeah... trade offs). I looked into AWS in some detail but found the entire thing to be so complicated and nontransparent. Since AWS uses your credit card and does not enable to set a hard money limit, I am very reluctant to use it since I have heard of people who lost quite some money due to wrong configs. Is there a beginner friendly service that you can recommend? Target: 20 to 48 cores, total memory of about 32GB, space < 20GB, Ubuntu or similar system. Any ideas are welcome, thanks. BTW if this is the wrong place to ask I am happy for guidance.",AskStatistics,2022-07-29 02:55:26,12
"Depends on a few things;

1. *exactly* what you mean by 'a trend'. There's a host of possible definitions, and they don't necessarily agree about what the trend might be. 

  There's also what you're normalizing by. Is this pens per thousand people, for example, or are you just pretending the population is constant (i.e. that you'd regard people buying 1% fewer pens per year but the population increasing by 1.8% per year as actually being an increasing trend? What happens when the popularity of pens doesn't change but the population no longer increases? You're hit by a sudden surprise when in fact the decreasing trend in pens per head of population may have been there for decades.)

2. Issues of models/assumptions; a typical method for finding *some* kind of time trend (like, say, a Mann-Kendall approach) relies on assumptions (such as independence) that are unlikely to be reasonable in practice. 

 (In case the mere mention of a specific method might be enough to entice you to jump on that, be warned; that method could identify an increasing trend - because the year-on-year numbers tend to go up more often than down, while the mean is decreasing over time - again, it's important to be clear about what you mean by 'trend')

3. An equality null of 'no trend' is quite implausible in this instance. Are you looking for a one-sided test (so the alternative would be increasing and the null would be 'no trend or decreasing trend')?

4. If the data contains a mix of increasing and decreasing trends over time (e.g. decreased during the pandemic, increasing now), would you want to identify that?",1,wbd6hg," Hi, I have the data of number of pens sold (yearly) from 2014 to 2021 and want to analyse if the increasing yearly trend is stastically significant or not. I have SPSS v26. Can you please tell me which statistical test to use and how?

I'm a beginner. Any help is greatly appreciated. Thank you so much.",AskStatistics,2022-07-29 12:14:01,1
"Small differences make a large difference in the tails of a distribution. If one group is 5 points higher on IQ and you are selecting only people in the 99th percentile,  then the 5 points would make a big difference in the proportion selected from each group. Effect size interpretation always depends on the context.",1,wbd3zc,"I've been struggling to wrap my head around this for years. Again, let us assume that two different groups of people are on average only slightly different, and their scores follow a normal distribution. I could imagine two general opinions about this:

1. Small between-group differences don't matter, because the distributions are so closely overlapping. The only ""real"" differences emerge at the extreme tail ends, e.g., most high-security prison inmates are men because only the most physically aggressive people make it into such prisons - and men are slightly more aggressive than women on average. 
2. Even small between-group differences do matter because in some cases even small differences make a ""meaningful"" - if not large - impact. E.g., in medicine where a very costly and INeffective drug is still being used because it saves human lives (even if just very few). In addition, even though the different distributions largely overlap, the majority of one distribution will still be measurably higher or lower in some aspect than the majority of the other distribution. In other words, most people in group A will be measurably higher/lower than most people in group B, e.g., East-Asians having on average 5 more points of IQ than Caucasians (I'm not sure how up-to-date this research is, but let's take it for granted just in this case for the sake of the argument). 

How do I reconcile these two opposing views? Take the IQ example. If a stranger on the street were to tell you East-Asians are on average smarter by a mere 5 IQ points and wants to argue that this is a big deal, how would you respond?",AskStatistics,2022-07-29 12:11:08,3
"First off, I have to ask, why not just do the proper Bayesian thing? Posterior predictive distributions are very little extra work on top of sampling from the posterior, and plenty of packages automate the process for you.

Secondly, I have to ask, were you not satisfied by our answers to your previous question, [""Can we treat a (frequentist) sampling distribution as a (Bayesian) posterior distribution under certain assumptions?""](https://www.reddit.com/r/AskStatistics/comments/w4izlh/can_we_treat_a_frequentist_sampling_distribution/)? Because the answers to that, such as what [I said previously](https://www.reddit.com/r/AskStatistics/comments/w4izlh/can_we_treat_a_frequentist_sampling_distribution/ih2lmva/) are pretty much plug-ins for the answer here. If you cannot treat a sampling distribution as an approximate posterior (and in generally you cannot), then any down-stream analysis based on a posterior cannot properly be approximated by using something that does not properly approximate the posterior. And if you *can* treat the sampling distribution as an approximate posterior, then using that for an approximate posterior predictive distribution follows straightforwardly. So, the crux is whether or not the approximation of the posterior is valid.

What you're describing here is, I think, a way to produce frequentist [prediction intervals](https://en.wikipedia.org/wiki/Prediction_interval).",1,wbbpvj,"Suppose I have a sample of Y and X, and I estimate the following model (maybe by maximum likelihood):

Y = b0 + b1X + e, where e \~ N(0,s)

I get estimates for b0, b1, and s. I can also get the sampling distribution of my b0, b1, and s estimates (either from a theoretical result or by bootstrapping).

Suppose someone asks me to predict Y when X = a. Can I use the sampling distribution to form something similar to a posterior predictive distribution from a Bayesian analysis?

For instance, I could run the following sampling procedure 1,000 times:

1. Draw values b0, b1, s from their joint sampling distribution.
2. Using this drawn values, draw a Y from N(b0 + b1X, s)

At the end of this procedure I should have a distribution of possible Ys that, my intuition says, should represent our uncertainty about what Y will be given our sample.

Is this a valid procedure? If so, is this equivalent to some Bayesian analysis with a particular choice of prior? If not, what does this procedure represent?",AskStatistics,2022-07-29 11:10:46,8
"Q10 is fine. If the interval for mu was 13 to 16 you would indeed reject the hypothesis that mu was 12 (at alpha equal to 1 minus the coverage)

Q11 looks like their answer is wrong. You need an interval that *includes* the hypothesized value since it asks for *not reject*. Your answer is right.",41,wakbf0,,AskStatistics,2022-07-28 13:25:49,10
"Wrong sub, go to the SPSS sub.",2,wb13am,"Hi everyone,

I am trying to compare means from a factorial anova (and get p-values) between Warm/Spain vs Competence/Germany and Warm/Germany vs Competence/Spain

I am attaching how I did the syntax for comparing Warm/Spain vs Warm/Germany, Competence/Spain vs Competence/Germany, Warm/Spain vs Competence/Spain and Warm/Germany vs Competence/Germany.

If someone could yield some insight into how I could do these: Warm/Spain vs Competence/Germany and Warm/Germany vs Competence/Spain in syntaxe it would be much appreciated :)

&#x200B;

DATASET ACTIVATE DataSet25.

UNIANOVA VAR00001 BY Dimension COO

  /METHOD=SSTYPE(3)

  /INTERCEPT=INCLUDE

  /PLOT=PROFILE(Dimension\*COO) TYPE=LINE ERRORBAR=NO MEANREFERENCE=NO YAXIS=AUTO

  /EMMEANS=TABLES(Dimension) 

  /EMMEANS=TABLES(COO) 

  /EMMEANS=TABLES(Dimension\*COO) COMPARE(Dimension) ADJ(LSD)

  /EMMEANS=TABLES(Dimension\*COO) COMPARE(COO) ADJ(LSD) 

  /PRINT ETASQ DESCRIPTIVE HOMOGENEITY

  /CRITERIA=ALPHA(.05)

  /DESIGN=Dimension COO Dimension\*COO.

&#x200B;

[Results](https://preview.redd.it/gogeze61hhe91.png?width=899&format=png&auto=webp&s=3459b1865983ce3ace64f55327a32e6304344f10)",AskStatistics,2022-07-29 03:20:28,5
"Clearly different measurements on the same subject/unit are dependent.

Depending on the setup (and perhaps on your own choices about how you want to treat things), you might use the subject as a block, or perhaps in many cases it would be better regarded as a random effect (particularly if subjects are randomly sampled from some population of interest), though some other approaches might be used.",2,waphe8,"I want to compare multiple different measurement techniques of the same characteristic. I performed the four different approaches on a sample, and want to compare these approaches to see if they are significantly different. I had originally considered ANOVA, but I think this violates the independence assumption. Is that true? What might my backup option be?",AskStatistics,2022-07-28 17:01:03,4
The probability is computed from the F ratio. The two numbers in parentheses are the df for the numerator and the df for the denominator.,2,wat00s,"I'm a medical student tasked with data analysis research and quite a bit lost on where to start with interpreting the data I was given. I've looked into F-ratio and how to interpret it but getting more lost and confused. I have a general understanding that the F test analyzes group data and how well the independent variables fit into the regression model but unsure how to make sense of the numbers I was given. 

In the linear regression results, one of the line has F(9, 335) = 3.82; Prob > F = 0.0001. 

I think I understand that Prob > F = 0.0001 indicates statistical significance since it's less than 0.05 but mostly confused on the F(9, 335) = 3.82 portion. Unfortunately I was not the one who collected the data but am responsible for writing a paper on the results. As a student with very hazy background in statistics, any help is appreciated.",AskStatistics,2022-07-28 19:43:02,4
Your question is very unclear,4,watyoe,"My friends were having a heated discussion about the probability of pulling a MTG card. 

You have a 3% chance to pull the rare card. There are 5 rare cards in the set “A,B,C,D,E” all with equal odds of being pulled. 

Each print run contains 34 packs with a rare card guaranteed. 

What is the probability of pulling rare card E in a print run of 17,000,000?",AskStatistics,2022-07-28 20:30:17,5
"> However, I would feel a lot better if there was some sort of numerical metric, that way I could create a program to test and reject certain parameters automatically.

One metric that's sometimes used would be mean squared one-step-ahead prediction error; leave out the most recent (say) half of the data and project one time step ahead each time and average the squares of those prediction errors. It's not suitable in every situation, though. You might prefer absolute errors or some form of relative error.

Some people go further back -- to leaving out nearly all the data, but I think this has some problems in that it tends to overvalue fit to the earliest data. Another approach applies weights by recency (such as exponentially decreasing weights so the most recent predictions get more weight).

If you are using methods that cope with missingness well, you can leave out random data and predict it, but that tends to underestimate the problem of predicting with actual time series (in that a missing value in the past has values in its future as well as past values in the available data, which you won't have in the real prediction problem)

I can't tell you that a MSPE is ideal for your circumstances; in some situations other things will be more suitable. But it might at least get you thinking about some useful possibilities.",1,wao7zd,"I'm doing a multiple season trend decomposition of a time series (in python) and testing out different parameters such as the seasonal periods, smoothing window, etc to see which gives the ""best"" decomposition. However, in that regard, I'm at a loss for how I would determine which settings are giving the best results and I can't seem to find any resources that explain it. So far I've just been eye-balling what looks good and what doesn't based on my knowledge of the original dataset. I.e., I look at the resulting trend and seasonal components and see if they are behaving in the way I would expect them to.

However, I would feel a lot better if there was some sort of numerical metric, that way I could create a program to test and reject certain parameters automatically. The only idea I've had is to compare the ratio of the variances (or standard deviation) of the residuals to the original data and try and minimize that ratio. The idea would be that the more data that is explained by the trend and seasonal components (and consequently the less residuals there are) the more accurate the decomposition likely is. However, I don't even know if that is a meaningful metric so I was hoping someone had any other ideas as to how to determine a numerical metric that can be used to judge the accuracy of a time series decomposition.",AskStatistics,2022-07-28 16:05:21,1
"1. looking at differences in 'demographics' and 'comobidities' 'between groups' doesn't sound like it fits either of the bases for inference (random sampling of a population of interest, randomization to treatment), though perhaps you do have that but didn't describe how one of those entered into it.

2. proportions *are* means (code being in the condition of interest as '1' and not being in it as '0' and the mean of those 1's and 0's is the proportion in the condition).

3. However, having means does not on its own automatically mean that ANOVA is suitable. For example, the distribution of small proportions will tend to be quite skew and may be heavily discrete. If sample sizes are large (and no, n>30 is not sufficient to count as large if  you have  small proportions but may be okay if the proportions are say within the middle half of the the range of possible proportions), then ANOVA may do okay.

4. More typical analyses would be chi-squared test (homogeneity of proportions) or a binomial regression model such as logistic regression or probit regression. If the conditions for ANOVA are suitable all the approaches should give you about the same p-values.",2,walxmh,"I am a clinician and I am looking at at differences in demographics and comorbidities between groups. I took all patients with stroke and separated them out by stroke location so there are 5 groups and I want to compare them. I used an ANOVA to compare the mean ages but I was wondering the best test to compare other characteristics such as the proportion of HTN, DM, drug use, etc. Is this still an ANOVA if Im not dealing with means?

Thanks!",AskStatistics,2022-07-28 14:30:02,3
"I think I can provide a tentative answer to my own question:

I realized that the variance of \\epsilon is independent of time, thus E\[\\epsilon \* \\epsilon\_{t-1}\] is equivalent to E\[\\epsilon\^2\]=Var(\\epsilon)=\\sigma\^2. Then:

Cov(N\_t, N\_{t-1})-E\[N\_t\*N\_{t-2}\]+Cov(N\_{t-1},N\_{t-2})- \\sigma\^2

can be re-written as:

\\sigma\^2-\\sigma\^2+\\sigma\^2-\\sigma\^2= 0

Is that the correct assumption to make?",1,wakyms,"Hello all,

I am interested in finding the autocovariance of the following function:

G\_t= \\epsilon\_t-\\epsilon\_{t-1}, where \\epsilon\~N(0,\\sigma\^2)

I define N\_t= \\epsilon\_t

I have the following:

Cov(G\_t, G\_{t-1})= E\[G\_t \* G\_{t-1}\]-E\[G\_t\]\*E\[G\_{t-1}\]=E\[G\_t\*G\_{t-1}\]-0

Then,

E\[G\_t\*G\_{t-1}\]=E\[(\\epsilon\_t-\\epsilon\_{t-1})\*(\\epsilon\_{t-1}-\\epsilon\_{t-2})\]

which is equivalent to,

E\[N\_t \* G\_{t-1}\]+E\[N\_{t-1}\*N\_{t-2}\]-Var(N\_{t-1}) = E\[N\_t \* G\_{t-1}\]+Cov(N\_{t-1},N\_{t-2})- \\sigma\^2

Or

Cov(N\_t, N\_{t-1})-E\[N\_t\*N\_{t-2}\]+Cov(N\_{t-1},N\_{t-2})- \\sigma\^2

I want to make the following assumptions, though I am not sure if I am correct:

Cov(N\_t, N\_{t-1})= E\[N\_t\*N\_{t-1}\]=N\_{t-1}E\[\\epsilon\_t\]=0. The expectation taken at time t, thus we already know what happened in the past, we just need to know. The same argument for E\[N\_t\*N\_{t-2}\]=N\_{t-2}\*E\[N\_t\]=0. However, I am not sure if I am allowed to do so.

At this point I am not sure how to proceed to further simplify this equation.

Any help is appreciated,

Thank you!",AskStatistics,2022-07-28 13:51:48,1
"First of all this is an actuarial topic. 

Life tables represent the probability of a person aged exactly x to die in a one year interval. As such you have more than enough equalities with which to solve for the parameters of gompertz",1,wakdxv,"As part of a model I am building I need to simulate mortality for individual patients using random deviates from a time-to-event distribution. A paper I identified describes the following the approach for achieving this: “Background mortality was obtained through age- and sex- specific 2017 US life tables which were converted into corresponding age- and sex-specific Gompertz time-to-event distributions”. 

The problem I am having is that I have no idea how to perform this conversion and the generate the Gompertz shape and rate parameters from a life table. I have been stuck on this for weeks so any guidance anyone can provide on how to achieve the conversion described would be hugely appreciated. 

Thanks",AskStatistics,2022-07-28 13:28:39,1
"Expand the square, apply linearity of expectation (E[X+Y] = E[X] + E[Y]) and see what you get.",3,wajeoa,"Im reading this paper and they used this formula ""because of the general rule"" without going any further 

E(X - a)^2 = Var(X) + (E(X) - a)^2

Thanks!",AskStatistics,2022-07-28 12:49:59,11
">what is the magical ""sample size number"" that suddenly turns out sample from ""small enough that we can't reject any null hypothesis"" to ""big enough that we could reject a null hypothesis""?

This also depends on effect size, and can be answered with a power/sample size calculation

>We see how, if I didn't make any mistake in my reasoning, a very small sample size would cause you to remain agnostic towards the null hypothesis in one of the outcomes, whereas a huge enough sample size would cause you to draw definite conclusions in both scenarios. If this is true, where is that barrier that turns a sample size from ""small enough that I can only draw conclusions in one of the possible outcomes"" to ""big enough that you can draw conclusions in both outcomes""?

Your logic isn't wrong, but goes awry when you talk about *accepting* the null hypothesis, which is not something you can do in the usual framework of null-hypothesis significance testing. By definition, you can only draw conclusions when you do reject your null hypothesis.

What you *can* do to show no relevant effect (n.b. not no effect) is to conduct equivalence or non-inferiority tests. Here your null hypothesis is that the weight loss pill *does* have an effect of size ∆, and you can reject that null to show the effect is at least smaller than ∆. Obviously, you would need to determine what you think is the smallest relevant effect ∆ ahead of time for anyone to believe your conclusions.

That also answers your question of when the sample size is large enough to do this, that's the sample size at which the equivalence test has enough statistical power. What constitutes enough power is a judgement call though that hugely depends on circumstances. But as you correctly reasoned, a big enough sample size will usually give you enough precision to be able to conclude an intervention doesn't have much effect. 

There's also a fun possibility with very large sample sizes where you simultaneously detect an effect significantly above or below 0, that is also significantly below ∆. In this case you conclude that, yes, there is some difference, but also that it is smaller than we consider relevant. [Scenario D in this image](http://www.annalsofian.org/articles/2014/17/4/images/AnnIndianAcadNeurol_2014_17_4_365_143984_f1.JPG).

Smart questions btw, keep asking yourself and others these!",5,wahxn8,"Let's say that I want to test whether variable X has a statistically significant impact on variable Y in an experiment, like it is usually done in medicine or psychology. I have an experimental group and a control group in my sample. The null hypothesis is that Y_experimental and Y_control are equal in the population (in other words, that X has no significant effect upon Y). We'll assume in the entirety of this post that our sample is randomly selected.

In order to have statistical significance, we can have a small effect size as long as this compensates with a big sample size, or we can also have a very low sample size as long as it compensates with a huge effect size. 

Obviously, having both a ""big"" effect size and a ""big"" sample size will make us reject the null hypothesis. However, I am confused about the way we position ourselves to the null hypothesis in the other three scenarios:

1. Small effect size, small sample size

2. Small effect size, big sample size

3. Big effect size, small sample size

For the sake of argument, let's loosely define the ""small"" and ""big"" from above as ""infinitesimally small"" and ""astronomically big"", in order to think of the most extreme/ideal cases, where those numbers approach the limit, if you understand what I am trying to say (sorry that this sounds like the opposite of rigorous but I don't know of any better way to explain my question). 

From my logic, it would naturally make sense for our relationship to the null hypothesis to actually be different in those three scenarios. In scenario 1 and 3 (small enough sample size, any effect size), it makes sense for me to remain *agnostic* towards the null hypothesis (""our sample is small enough that we don't reject or accept the null hypothesis""). However, in scenario 2, it makes more sense for me to *confidently reject* the null hypothesis (""our sample is big enough that we can say with more than a certain percentage of certainty (which is the confidence level) that our null hypothesis is surely FALSE""). Is this true? The reason I am saying this is that when you have a small enough sample size, you can't draw any conclusions about the lack of effect of variable X upon Y, since maybe you just chose the minority of cases in which it doesn't. But if you have a huge sample size, you can be confident enough that X  has no effect upon Y, since your sample would now be representative of the population. Therefore, according to my logic, when you have a small sample size, you can only draw inferences about what null hypotheses are false in the population but never which are true.

If I am wrong, where is the mistake, and if I am correct, what is the magical ""sample size number"" that suddenly turns out sample from ""small enough that we can't reject any null hypothesis"" to ""big enough that we could reject a null hypothesis""?

Example: 

Let's say that I develop a new pill and I want to test whether it has a statistically significant effect upon weight loss in the adult population.

In my first hypothetical experiment, I randomly select 20 million participants of all age groups, genders, etc. and I split them into 10 million in the experimental group, and 10 million in the control group. I notice that my pill has no significant effect upon weight loss. Therefore, my logic tells me that you should *confidently accept the null hypothesis*: there is no effect. Alternatively, if there was a significant effect, I would confidently reject the null hypothesis: there is an effect.

In my second hypothetical experiment, I randomly select 10 participants of different age groups, genders, etc. and I split them into 5 in the experimental group and 5 in the control group. I notice that my pill again has no significant effect upon weight loss. Therefore, my logic tells me that you should *remain agnostic towards the null hypothesis*: maybe my pill has an effect *or* not. However, if my pill has a significant effect in a sample size of 10 (because of a huge effect size), then I don't remain agnostic towards the null hypothesis, I confidently reject it.

We see how, if I didn't make any mistake in my reasoning, a very small sample size would cause you to remain agnostic towards the null hypothesis in one of the outcomes, whereas a huge enough sample size would cause you to draw definite conclusions in both scenarios. If this is true, where is that barrier that turns a sample size from ""small enough that I can only draw conclusions in one of the possible outcomes"" to ""big enough that you can draw conclusions in both outcomes""?",AskStatistics,2022-07-28 11:51:31,1
"Depends? Population registers might have info on the the age of residents, then you can exclude everyone who is younger and do random sampling. That's what studies on older people do here",2,wab600,,AskStatistics,2022-07-28 07:17:47,3
"> I assume that the posterior P'(A,B|C_data) is similarly structured as the prior, i.e.

Pretty sure that's wrong. I mean, it might be a fair approximation, but then again it could be an awful one. The actual posterior distribution is, up to a normalizing constant, `P(C_data|A,B) \* (w1\*p1(B) + w2\*p2(A,B) + w3\*p3(A,B))`.

> How do I efficiently find the posterior distribution P'(A,B|C_data), and the corresponding w' and p'?

Why would p1'() be different from p1()?

Let's back up. You have a mixture prior on A and B, which decomposes into three weights (which must sum to 1), and three distinct distributions. I'm going to assume for the minute that these distributions don't have hyperparameters, that is, they're fixed.

There is an easy solution to a *simpler* but related problem. Which is the univariate prior mixture Pr(x) that has some probability p of x being constant (at a fixed predefined value) and 1 - p of x coming from some continuous prior distribution. This can be handled by [reversible-jump MCMC](https://en.wikipedia.org/wiki/Reversible-jump_Markov_chain_Monte_Carlo) in which you basically hop between the two models ""x fixed"" and ""x free"" in the posterior. You fix p and 1-p, and you get obtain the posterior probabilities of being in different models by counting samples. The proportion of samples with x == x_constant is the posterior estimate of the probability of being in that model, the proportion with x != x_constant the probability of being in the free model.

Now, your model is a bit more complex because you've got a joint structure, but it seems to me that the same general approach applies. Lay out fixed prior weights w_1, w_2, and w_3, and jump between the models. I think that since we've basically put the whole thing into one model the only annoying bit is getting good moves between them. That is, I don't even think there's any Jacobian involving the marginal likelihoods under different mixture components.

Alternately, depending on how many components your mixture has, it seems to me you could consider fitting each separately, estimating the marginal likelihood, and then combining the results post-hoc as if you had done RJ-MCMC in the first place. That is, if you fit all 3 models, and get marginal likelihoods L_1, L_2, and L_3, then the posterior estimates of the weights can be gotten as w_i' = L_i \* w_i / (\sum_j L_j \* w_j). Then you can use those weights to combine your posterior samples from each model (either by downsampling or by doing weighted sample summarizing for means/quantiles/etc).

Am I missing something that makes one of these approaches impossible?",2,wa670d,"Hey all, I'm not sure if I have phrased the title right, but maybe you can help a non-statistician (me) with what I think is a weird problem.

To give a minimum working example, assume we have parameters A and B, and a forward model that predicts a variable C:

    f(A,B) -> C

Now, assume that we also have a prior, but it is composed of three sub-populations: 

    1. The first has a fixed A=0, but some continuous probability density on B i.e., p1(A=0, B)
    2. The second has a combined continuous probability density p2(A,B), with A&B potentially correlated (i.e., it is not easily written in the form p(A)*p(B) )
    3. The third has fixed B=0, but similarly a probability density p3(A,B=0)

When combined, these will obviously form a mixture distribution, where the constant parameters in populations 1&3 are deltafunctions (written as d), and the occurence rate of each population are regarded as w:

    P(A,B) = w1*p1(B)*d(A) + w2*p2(A,B) + w3*p3(A,B)

Now suppose we have measured C, called C_data (with associated uncertainty), and I would like to infer which combinations of A's and B's most likely describe the observation. To do so, I invoke Bayes theorem:

    P'(A,B|C_data) = P(A,B) * P(C_data|A,B) / Z

The Likelihood P(C_data|A,B) is tractable, but computationally expensive to evaluate (due to the forward model).

I assume that the posterior P'(A,B|C_data) is similarly structured as the prior, i.e.

    P'(A,B) = w1'*p1'(B)*d(A) + w2'*p2'(A,B) + w3'*p3'(A,B)

How do I efficiently find the posterior distribution P'(A,B|C_data), and the corresponding w' and p'? If the A-B-space would not be split into the weird subpopulations, I would probably use MCMC or Nested Sampling, but here they do not apply due to the restrictions A=0 or B=0 for the respective population. (Why MCMC or Nested Sampling? Because in my real example, we actually deal with a lot more parameters than just A and B).

I would appreciate if somebody can tell me if they have seen a similar problem before, or if I'm missing some completely obvious solution (again, I'm not a statistician by training).

Thank you so much in advance!

Edit: formatting",AskStatistics,2022-07-28 03:24:08,1
"The order. A lot of people read this as P(4 tails and 6 heads) = P(10 heads) which is not true. You gave a specific order of H,T tosses and that specific order of 4 tails and 6 heads tosses has the same probability of 10 heads in a row.",11,wa0d3z,,AskStatistics,2022-07-27 21:49:53,10
Looks like a form of kernel smoothing.,17,w9lrwq,"so i do an exercise of programming R, and i found this equations, idk what kind of topic it all, but when i find it on google, it seems like clustering (mean-shift clustering (?)), is it true ? can anyone give me a source link too if u know it ? thanks",AskStatistics,2022-07-27 11:12:02,6
"If by some miracle you haven't already been told how lambda relates to E(X) and you don't know how to compute it, look up the exponential distribution on wikipedia to see the relationship between the rate parameter and the mean. It's right there in the section on Properties.",9,w9ta8d,I.e. A device fails after an amount of time modeled by an exponential distribution where lamda = 0.02. what are the odds the device lasts at least 10000 hours?,AskStatistics,2022-07-27 16:19:51,4
"The reviewer probably is unaware that repeated-measures ANOVA is a mixed-model analysis with “participants” as a random effect. Basically, using time as another variable is equivalent to analyzing the means of the three occurrences. However, you may prefer to use the median if some values are very influential.",1,w9oyud,"I have completely within-subjects 3x3x2 experimental design. Each participant completed three trials for every variable combination. The ""time"" variable is not one of the three variables in the 3x3x2 model.

*Time* in this study is not actually time per se; it's whether a trial was the first, second, or third occurrence of each variable combination. I have three trials for each condition to eliminate variability, and not because I expect it to be a meaningful variable.

I used a mixed effects ANOVA model with Participant as a random effect. A reviewer balked at this, saying I should use a Repeated Measures ANOVA instead. But it seems to me that one would only use RMANOVA when expecting ""time"" to be a meaningful variable.

Is the reviewer correct here, or did I just not explain my reasoning to them clearly enough?",AskStatistics,2022-07-27 13:23:39,3
"> the only thing where power law and lognormal differ is the size of the tail

This is generally pretty important. 

> (looking up lognormal mentioned something called ""beta distributions"")

These are *completely different*.

We need to know what you're trying to do. What kind of distribution you work with is going to be completely determined by what you're actually trying to accomplish.",8,w9mavh,"I am using some scientific research as basis for my programming project. They use a power law distribution. From my brief googling, it seems that the only thing where power law and lognormal differ is the size of the tail. Lognormal looks easier to implement, just multiply a couple of random numbers (which are trivial for a computer and implemented in pretty much any language I know of). Is there something else I could use (looking up lognormal mentioned something called ""beta distributions"").

This is for a game, so I do not care if the exact probabilities get changed a little, just that the general shape (the big ""hump"" at the beginning and then the tail) remains.

&#x200B;

Unfortunately I'm using a game engine with proprietary scripting engine, so I can't just plug in Numpy or Scipy or something like that.",AskStatistics,2022-07-27 11:33:56,13
"You can get a reasonably close reproduction of the Chi-squared test statistic and the degrees of freedom from the info in the table

https://www.socscistatistics.com/tests/chisquare2/default2.aspx

Differences between what you see and what you get may depend on whether they had used a continuity correction or not

https://www.sciencedirect.com/topics/medicine-and-dentistry/yates-continuity-correction",2,w9kytk,"I'm doing an analysis of the following data table. Is it possible to figure out which Test score was used to get the P-value? TIA!

https://preview.redd.it/fl6nuxvbd5e91.png?width=1088&format=png&auto=webp&s=8da3c503ecb01965ab04041797a83fe1d41a23f3",AskStatistics,2022-07-27 10:39:00,1
"I've found another helpful quote:  


>""*This λ3 is better known as Cronbach’s alpha. Guttman (1945) remarked “λ3 is easier to compute than λ2, since only the total variance and the item covariances are required. If the covariances are all positive and homogeneous, then λ3 will not be much less than λ2 and may be an adequate lower bound. If the covariances are heterogeneous, and in particular, if some are negative, then λ2 will be definitely superior to λ3. λ2 can be positive and useful when λ3 is negative and useless” (pp. 274-275). In brief, λ1 ≤ λ3 ≤ λ2. Therefore, with modern computational facilities, λ2 should always be preferred over λ3*."" (p.96)  


(Bendermacher, 2010)  


Does this mean that in the case of heterogeneous covariances, as they usually occur in the field, λ2 is to be preferred?",1,w9ap1j,"Hi Everyone, 

I am writing my thesis and have used several surveys. To estimate the reliability of the scales, I used Guttman's Lambda-2. Now, I am supposed to describe why I used Lambda-2 instead of Cronbach's Alpha (in the best case with the use of a source). 

So far I have written that it is better than Cronbach's Alpha under certain circumstances, based on Osburn's article (2000; ""Coefficient Alpha and related internal consistency reliability coefficients"").   


>*„Lambda2 is interesting because it always gives a lower bound that is as good as coefficient alpha but in some circumstances may be considerably better.“ (p.346)*  
>  
>*„Lambda2 was a slightly better estimator than coefficient alpha, but the differences were not large.“ (p.350)*

(Osburn, 2000)

Unfortunately, it's not enough to write that Lambda-2 is better than Alpha but I need to describe **WHY** it is better.   
Can anybody help me with this? Thank you so much in advance!",AskStatistics,2022-07-27 03:02:42,1
"I'm not sure how much is openly discussed on this topic, this is sort of a super niche concept, but the double counting in a scientific survey is covered under the concept of ""multiplicity"" in sampling (not to be confused with multiplicity in clinical trial analysis). It is a form of sampling frame error. Essentially everyone should have a known chance to be selected for a study, if they accidentally select a person twice beyond their known probability of selection, it can throw off the estimation process, because the real chance of selection is much higher than was originally planned. 

**However!** It only applies to the **specific** study population for a **specific** time frame. So in some situations, for example if the goal is to measure total number of ""experiences"" you could sample the same person for each experience they had, because the ""experience"" is the sampling unit, not the person. There are other situations but it boils down to what concept you are exactly studying and if it makes sense for you to show up multiple times or not.

From a quick google, this seems to be related: https://olc.worldbank.org/system/files/Video%2015_Multiplicity%20adjustment.pdf",1,w9hvdl,"Hi today someone asked some questions for a Survey.

I told them “no thanks, I’ve done the same survey other times.”

The person told me “ you don’t know what you are talking about, every day is different”

I replied that if I answered, my answer would be “Double- Counted “ 

I left, I understood that I was selected randomly but my answers wouldn’t be “ random” since the questions don’t change and are not the type “ How was your experience today” I tried to google things but haven’t found the Scientific Concept that talks about this and how to deal with it. can someone point me what topic or concepts talk about this. Thank you",AskStatistics,2022-07-27 08:35:19,2
"From what I understand, if you want to build a predictive model you can just use as many features as you want.  If you want to build an effect size model (eg you want to see specifically how each feature effects your outcome) you need to choose features that do not have collinearity, and are not dependent/similar measurements.  You’ll also need to consider your sample size in either model type.  I often hear about greens rule of thumb, which implies you should have 10-15 samples per variable.",2,w99kj0,"Hello Everyone,   


I am using the Cox Proportional Hazards model. But I am very confident that this question is not limited to that model only - Thank you for reading! 

I am currently doing a study, where I have extracted 100 features from a dataset. I want to investigate all of them to see which features are significant.   


My understanding is, that when doing this kind of analysis, I would go iteratively through the individual features (including confounders), and extract the impact of each feature by itself.   


I also want to make a model that takes into account the interaction between a set of features. However, these features are highly dependent on each other. I am not sure if this is allowed, as when that is done, the variance of the confounders is split onto all of the features, deeming the confounders ineffective.   


I have performed this analysis, and I do get significantly different results when doing this. Let's say that I have 3 features (Theta, Alpha, Beta), and 2 confounders (Age, Sex).  


Doing the 'single effect' analysis, Theta and Alpha are not significant, but Beta is.   


When doing the 'multi effect' analysis, all become significant. I am hoping this means that by themselves, Theta and Alpha do not have an effect, but when all of them are interacting with each other it does have an effect.   


So the question is. Is this allowed? If it is, do I need to be wary of something else? 

If anyone has any understanding of this field, I would be very very grateful!

Thank you so much in advance!",AskStatistics,2022-07-27 01:54:30,2
"I'm not clear on whether your goals are *overall* goals or *per bet* goals. For example:

I assume that with each bet, the goal is to recover your *cumulative* losses rather than the previous bet's losses only. Is that correct?

Also, I assume that the goal is to make 2.4 units of profit on any given bet, rather than to merely ""end up"" with 102.4 units in your bankroll, and then stop betting. Is that correct?",1,w9gmr7,"hey all new member here and noob question

 

i want to ask how to calculate the number of bets i can make with a given starting bankroll

if after any loss i increase the size of the bet to cover the loss and make a target profit

i.e.

bankroll : 100 units

odds : 3.40

profit target : 2.40

i begin with betting 1u and lossing

now i have 99u and looking to recover that 1u and also make 2.40u proftis and so on

&#x200B;

\*i only use Excel",AskStatistics,2022-07-27 07:45:21,10
[deleted],1,w9gm3u,"I have a 1D problem i.e. a continuous variable x relates to a continuous variable y. I want to measure the correlation between the two variables. However, I have domain knowledge that tells me the tail ends of the distribution of x (i.e. very small x and very high x) correlate a lot more strongly with y.

It is theoretically reasonable to chop up my data into 5 quantiles wrt x and report the spearman's rank correlation to y for each quantile? More concretely, I want to show that the 0%-20% quantile and the 80%-100% quantile correlate strongly with y, while the middle quantiles do not have a strong corelation. Thanks!",AskStatistics,2022-07-27 07:44:37,9
"What are you practicing, and at what step are you getting confused?We aren't going to do it for you, but can you explain a step or two...",3,w8tq3q,"Consider two lottery players, Abdul and Gaju, with different strategies. The lottery has numbers from 00-99 (total 100 numbers). One winning number is selected randomly from that 100 numbers. Each number costs 70 bucks and winning number price is 2000. 

Abdul buys 50 numbers independently and the numbers are selected from uniform probability. (Duplicate numbers may exist). Guji buys 50 lotteries of same exact number.

a) Find the expected value of Guji profit/loss (winning - cost)

b) same for abdul

I am really confused on this practice question. Can someone help, thankyou.",AskStatistics,2022-07-26 13:10:46,14
"1. You *have* to explain the variables. This is essential

2. It would be important to include a link to your previous post; there were several relevant comments made by people there that would be useful context but which shouldn't need to be repeated.",15,w8os3t,,AskStatistics,2022-07-26 09:52:40,15
Following.,1,w8mdk2,"I am currently writing a paper for uni and stumbled across the following problem:

I want to use the 10 Fold Cross Validation method to validate the results of a logistic regression, but I am unsure when to use methods like the Likelihood Ratio Test and Wald Statistic in order to also validate the function and coefficients.

Since the coefficients are going to be created through Gradient Descent based on the data and Cross Validation leads to different sub-sets of data for testing and training, the coefficients and results of the Likelihood Ratio Test and Wald Statistic should differ.

So would you normally have an individual approach for each of sub sets of the Cross Validation or would you rather build one model in advance and use it on each of the sub sets?

As you see I am having difficulties getting a hang on the relation between Gradient Descent, Cross Validation and the Likelihood Ratio Test/Wald Statistic, if someone could explain it to me I would be very thankful.",AskStatistics,2022-07-26 08:15:50,2
"> The thing is, I noticed that for every sector, the standard deviation of SalesRevenue was either slightly greater or considerably greater than the mean. Does that automatically mean the data is skewed?

No. A Normal(0,1) has SD > mean but has no skew.

> Was that a good idea though? Or do I need to check that the resulting output approximates a Normal distribution before proceeding (in which created we've used a Lognormal distribution to model our data)? And what happens if it doesn't?

While it's always good to think about checking distributional assumptions, you're barking up the wrong tree here. For one, KNN isn't model-based clustering. Beyond that, not all model-based clustering uses Normal distributions and even for versions that *do*, you need to be very careful about *what* is assumed to be Normal (related: people checking marginal distributions in classical regression models which only assume Normality of the residuals).

> And do we need to take the log (natural log base e) of both variables, or only the one that's skewed?

KNN is just saying with math, ""find the k nearest neighbors, and your prediction is whatever the majority of them are."" I would argue on that basis that you should choose transformations (or not to transform) on the basis of what similarity makes the most sense.

If you log things, you're making it about multiplicative differences. If you think something being twice as big in that dimension matters more than the absolute difference, logs make sense.

In other words, leave things alone if you want 1 to be closer to 0.1 than to 10.

> The way my coworker has been using the K Nearest Neighbors method is simply to compute a distance metric from one client to the next. This is simply the square root of the sum (z-score of SalesRevenue + z-score of EmployeeCount).

One can use many distance metrics with KNN. I'm not enough of an expert to know which will be most useful.",3,w8vjnu,"I started working as a data scientist less than a month ago.

A few days ago I had what sounded like a good idea, because my coworker who has quite a bit more experience than me was all for it. But I wanna check with you to make sure it is indeed a good idea.

Basically, the goal of what we're doing is to select a client, and have the model quickly pick out 3-5 clients that are most similar to the client selected. This is based on two variables, let's say SalesRevenue and EmployeeCount, and split up by sectors.

The thing is, I noticed that for every sector, the standard deviation of SalesRevenue was either slightly greater or considerably greater than the mean. Does that automatically mean the data is skewed?

Either way, I suggested we take the log of both variables, and so we did, and got the model to work and to give us the 3-5 nearest neighbors to each selected client.

Was that a good idea though? Or do I need to check that the resulting output approximates a Normal distribution before proceeding (in which created we've used a Lognormal distribution to model our data)? And what happens if it doesn't?

And do we need to take the log (natural log base e) of both variables, or only the one that's skewed?

The way my coworker has been using the K Nearest Neighbors method is simply to compute a distance metric from one client to the next. This is simply the square root of the sum (z-score of SalesRevenue + z-score of EmployeeCount).",AskStatistics,2022-07-26 14:23:44,7
"If you have specific statistics for both age **and** sex, then you can weight each point proportional to the ratio of the expected count to actual count.

For example, with 

Age|Sex|Count|Expected %|Actual %|Weight (see equations below)
:--|:--|:--|:--|:--|:--
<=17|F|30|20%|18.75%|1.067
18+|F|60|35%|37.5%|0.933
<=17|M|50|20%|31.25%|0.640
18+|M|20|25%|12.5%|2.000

In the above sample, the different female groups are somewhat close to the true proportions, but males under 18 are overrepresented and males 18 and over are underrepresented.

To set the weights to the ""correct"" values, you want

    0.1875 * w_fu = 0.2
    0.375 * w_fo = 0.35
    0.3125 * w_mu = 0.2
    0.125 * w_mo = 0.25
    0.1875  * w_fu + 0.375 * w_fo + 0.3125 * w_mu + 0.125 * w_mo = 1

You can see that the counts times the new weights add up to 160, although that's not strictly necessary in cases where you are just using weights for sampling, like in bagging/bootstrapping.
    
## Raking

A common method for calculating survey weights with more than one variable (e.g., age and sex) that you don't have specific subgroups for is [raking](https://en.wikipedia.org/wiki/Iterative_proportional_fitting). It's an iterative process that involves:

1. Setting all weights to 1 initially.
2. Loop through each categorical variable, and adjust the weights proportionally so that the proportions of all the weights in each group add up to the proportion you'd expect from the overall population.
3. Repeat (2) several times until weights converge.

For example, if you had a table that looks like this

Age|Sex|Count|Raked Weight
:--|:--|:--|:--
<=17|F|30|0.383
18+|F|60|1.142
<=17|M|50|0.730
18+|M|20|2.174

and you had a population that was 50% male, 50% female, 30% under 18, 70% over 18, the weights would be as calculated above (doing the math for it would be a bit complicated, but it is just a series of calculations similar to the ones for the simpler scenario above).

Also, in practice, you might want to limit weights in extreme cases. I've used

    weight_max = median(weights) + 6 * IQR(weights)

Where `IQR(weights)` is the inter-quartile range of the weights (difference between 75th and 25th percentile).

# Weight Caveats

Note that when you do use a weight, in most models, it is identical to ""this row appears $<weight> times"". So if you have 10 rows with a weight of 2, it acts as if the row appears twice, including for calculations involving statistical significance, and you effectively have 20 rows from that data.",2,w8pwqz,"Hi,

&#x200B;

Firstly you will have to excuse my ignorance or naivety.

&#x200B;

I have been given a task to create anaylsis for a survey conducted, but it isn't something I have done before. I have tried to have a look around online and it seems creating survey weights is the common way to go in order to produce analysis of the survey. I am struggling in understanding how the weights are calculated though, and I can't seem to find much through google searches. Plenty of stuff relating how to use weights.

&#x200B;

The survey was sent out to individual households within the local area. Initially, we had a list of all households within the local area. The local area is split into smaller areas (of around 1500 residents) and in total there are 163 of these smaller areas. Using the household list a survey was sent out to around 6.77% of the households within each small area. We asked for one adult to complete the survey to each household. To help keep it random, we asked for it to be the person who's birthday was next. In total we have around 3600 reponses. 

&#x200B;

Now the survey reponses are all back I think we want to create weights based on the population. Using population estimates, we have the population for each of the 163 small areas by age and sex. The survey also contains age and sex. There are problems where some of the areas are poorly represented, for example one of the 163 areas only has a single response for males aged 55+. 

&#x200B;

We also asked the household size in the survey so that information is available too.

&#x200B;

Could someone lend some advise on how to go about creating the weights? I use R quite a lot and understand I can use the survey package. But within the package I am not sure whether to use calibrate or rake. I am also not very comofortable explaining the methodology behind how this works.

&#x200B;

So far I have created a total population weighting by dividing the percentage of population by the percentage of responses for each of the 163 areas. I have then creating a household weight, which is essentially the number of adults in the household. I then multiplied these together but have read that this might not be the best way to do this? I have then tried creating a weight by age band and sex. the age bands are very wide and the geographic area has been scaled up to (to 58 areas). I have calculated this in a similar fashion to the total population weight by have the percentage of population for each group and dividing that by the percentage of respondents but using the weighting created by multiplying the total population weight and household selection weight.

&#x200B;

Apologies for the wall of text.

&#x200B;

Thanks for any help.",AskStatistics,2022-07-26 10:37:49,5
You could calculate the relative survival - essentially the ratio of the proportion of observed survivors in your cohort compared to the proportion of expected survivors in the total population cohort. Most relative survival measures I use match to life tables that are matched by age and sex.,1,w8vcxo,"Hello

What is the best statistical test if I want to compare the survival rate of my patient sample to the known the survival rate in the total population",AskStatistics,2022-07-26 14:16:18,1
"I believe you're after this:

https://en.wikipedia.org/wiki/Concordance_correlation_coefficient",1,w8v58r,Does anyone know? Is this presented with Bland Altman plots? How is concordance different from correlation? Thanks,AskStatistics,2022-07-26 14:07:28,3
"What do you mean by ""run""? I will assume that you mean ""sets of 120 that contain 1 head"". There is another common meaning of ""run"" in this case that I will ignore for the moment.

The Binomial distribution formula: 

[n C x]•p^x •(1-p)^n-x

n= 120, x= number of heads, p=0.05. The ""n C x"" part is

n!/(x!•(n-x)!), or nCr, or nCx on a calculator.

In your case this boils down to 120C1 = 120, so

120•.05^1 • .95^119 = 0.013405. Multiply times 10000 and you get 134.05.

Here is a video of mine on the formula if you want to be walked through it.
https://www.youtube.com/watch?v=GGNWtz5B4bk",3,w8o3nq,"Example: If I have 120 coins with p = 0.05 of heads. And I flip those each of these coins ten thousand times, how do I estimate the number of runs that will have show one head?

I've already have a simulation answer (using Python), but I wanted to check if there's a formula for that.

I just want to extrapolate the ideas [discussed here](https://www.reddit.com/r/askgaybros/comments/w8c0jh/why_is_everyone_straight/ihp469q/)",AskStatistics,2022-07-26 09:24:57,3
"It sounds like things are getting a bit complicated. How many 'steps' in each personality trait? In other words, are they 1-10? 0-100? If they're a small number (<10), take a look at the distributions of your outcome(s) by personality trait level. If it's a hazy random pattern, you either don't have enough data or there's nothing there. If there are real trends, but different shapes for each one, think about creating a categorical version of each one. You might get away with simple, binary versions (e.g. X1 > 5 vs. X1 < 5). Or you may need three groups within each one (low, medium, high). 

Seriously - using binary versions will seriously make your discussion section much easier to write, e.g. ""Subjects with more personality traits above the thresholds had worse outcomes.""

Going back to your question, centering the variables should have no effect on how the model behaves. It should only change the intercept. While there is no specific reason not to include C3 in a model with other variables, it is unlikely to model the actual curve well. You'd need C, C\^2, and C\^3 to really model the relationship with a polynomial, and that's going to be very difficult to describe in your discussion section.",1,w8sxaj,"Dear readers,

I am currently performing a multiple regression analysis for my Master's thesis, using the Big Five personality traits. I had to do loads of transformations in order to maybe get some linearity on them. Since that didn't do the job, I decided to try and create polynomials. Conscientiousness, 1 of the 5 traits, was centered into ConscientiousnessC, ConscientiousnessC2 and ConscientiousnessC3. ConscientiousnessC3 was the only linear variable of all 5 centered variables in total.

My questions is: Can I run a regression analysis with the 4 initial variables, and add ConscientiousnessC3 without any problems? Or does the model only accurately run on either 5 normal or 5 centered variables?

It would really help me if someone could confirm this.

Thanks in advance!",AskStatistics,2022-07-26 12:38:36,2
"Your question appears to be programming-specific rather than about statistics (it's not about how to do statistics, it's about how to code an `if`)

>  if given a value is smaller than 0.05.

Then why does it say "">"" before those lines of code?",4,w8y0h8,"I've a loop whic is conducting some calculations. In my codes there is a if else statement. This statement's job must be go to some codelines if given a value is smaller than 0.05.

How can ı handle it ?  At else I want to go belong to levene varible again. How can I do it ?

    num_runs = 1000
    p_degerleri = []
    
    
    
    for i in range(1,num_runs+1):
       
            g1 = np.random.chisquare(3,20)
            g2 = np.random.chisquare(3,20)
            g3 = np.random.chisquare(20,20)
            
            g1t = np.sqrt(g1)
            g2t = np.sqrt(g2)
            g3t = np.sqrt(g3)
            
            levene = sp.levene(g1t,g2t,g3t, center='mean')
            
            if levene[1] > 0.05:
                anova = sp.f_oneway(g1t,g2t,g3t)
                p_degerleri.append(anova[1])
        
            else:
                break
                
                
                
    hesap =  sum(map(lambda x: x<=0.05, p_degerleri))
    print(hesap/num_runs)",AskStatistics,2022-07-26 16:05:19,4
"Looks like it's probably a case of complete separation or perhaps quasi-complete separation (though I think the former is probably the situation). You should learn about it if you're using logistic regression or other binomial regression models

https://en.wikipedia.org/wiki/Separation_(statistics)

https://stats.oarc.ucla.edu/other/mult-pkg/faq/general/faqwhat-is-complete-or-quasi-complete-separation-in-logisticprobit-regression-and-how-do-we-deal-with-them/",14,w8d22w,,AskStatistics,2022-07-26 00:13:18,11
The sum of the weights that you used.,1,w8nxl4,,AskStatistics,2022-07-26 09:18:11,2
"If you *know* the population, and you *know* that the sample is from the population, then what are you testing? The purpose of a test is to do inference on on the population using the observed sample. You have the population, so what are you trying to infer?",1,w8nm3b,"Can a t test be used to test a sample versus the whole population or would that be a z statistic? I have the means, variance, SD, error, N of both population and sample. N=20 for sample, population N=400",AskStatistics,2022-07-26 09:05:46,1
More info is needed. What cells are you talking about? Are the sites success rates physically correlated with one another? What do you mean by in between successful sites? Spatially in between?,1,w8iujj,"Smart statistics-angels, I need some help :/ 

I have a measurement plate for cells, 6 columns and 8 rows. Now my success rate on this plate is around 30%. Now I look at every site if it's successful or not (I go through all of them top to bottom). I count how many unsuccessful sites there are in between the successful sites. I expect that after counting this way for 3 plates, there would be a bell-shaped distribution on how many unsuccessful sites there are in between the successful sites, with a peak at 2 sites in between because of my success rate of 30%. Is this assumed correctly? Because in reality I have a large peak at 0. Or have I just not counted enough plates?

I hope this makes sense.",AskStatistics,2022-07-26 05:44:55,5
"I don't understand what you mean and what the problem is. Do you mean to say that masculine people tend to have more positive self-image, and the two independent variables are highly correlated? Can you clarify the problem?",1,w8hylw,"General Information: 
I am currently analyzing a questionnaire in SPSS . I have chosen multiple linear regression as my analysis method. As dependent variable I have the profit in € and as my independent variables the gender, whether one sees oneself as masculine and whether the negotiation was same-sex. In addition, I have taken as control variables the age and whether the person ever worked in sales. I am quite a beginner in the matter, but have read up a bit in recent days and had to find that the variable gender, masculine self-image has a very high correlation (.873) according to Pearson, which is not a good indicatior which Leads to significances being p= .896 and p= .608.
( My Alpha is <.05) 

Question: 
My question now is whether I can still use the model with specification of the extremely high significances or should I run a separate test for each of the two variables. (Again, the significance is > .05, but lower with a value of .265). So the general question is whether a very high significance has to be interpreted in a special way.",AskStatistics,2022-07-26 05:01:33,5
"If the positive/negative aspect is the only thing you are interested in, create a new column `positive` which equals one when IHC is 5, and 0 otherwise. Then compare the rate of positivity by type of lesion with something like a Binomial or (Pearson's) Chi Square test.",2,w8hk9s,"I have “skin lesions” that I have stained with IHC (immunohistochemistry) that have a grading scale (ie 0-5). I am trying to compare within a certain type of lesion, the significance (ie p value) between the lesion having a IHC scale of 0-4 and 5.   Reason why I am doing this is because 5 is the criteria of it being a “positive” stain.",AskStatistics,2022-07-26 04:41:21,11
One approach would be to look at the distribution of the conditions by age group and adjust the proportions in each age group to some typical age distribution (like the population proportions).,2,w8cwpw,"

Hello,

Our recent 2021 census in Australia collected information about long term health conditions, which is great, but I know that age standardisation will be needed for some conditions such as arthritis, dementia, etc. especially if I am to map this data. At the moment, mapping rates of dementia by suburb will unsurprisingly highlight suburbs with an older population. 

What information and standardisation methods do I need to employ in order to account for age with these data? I can obtain total number of people with certain long term health conditions by age as well as total population by age. 

Thank you in advance for any guidance or direction.",AskStatistics,2022-07-26 00:04:38,4
">  How do I first test for normal distribution? 

I would not test it at all; it won't give you a useful answer to the question you need to consider. The question is not ""are these variables normally distributed"" -- they literally *cannot* be! It's a complete waste of time to test it. What would a non-rejection tell you other than you didn't have the power to tell that it wasn't normal *in spite of the fact that it cannot be, for a complete certainty*. 

The real question is how much this non-normality matters for the properties of your test, specifically the significance level and power (probably not a great deal - depending on your precise hypotheses and whether you assume your scales are interval).

Given the small sample size, *if* you are assuming your scales are interval and *if* your hypothesis is about population means I might be inclined to use a permutation test based on means rather than a straight t-test to make sure of the significance level.

If your hypothesis is *not* specifically about means, I might consider some other test statistic.",1,w86woe,"Hello, 2 questions 

1. I have a set of 30 data points that's going to be split roughly 23-7 that I would like to run some tests on to see if these 2 defined groups have different significant means. How do I first test for normal distribution? There's 15 different variables im looking at, but for example one variable would be ""trust in science"" rated on a scale of 1-100 and I want to see if the 2 groups have significantly differently levels of trust. 

2. If found normal, what's the right test? If found not normal, what's the right test? 

Thanks!",AskStatistics,2022-07-25 18:55:47,4
"> I'm curious about the assumption of normality for parametric tests in general,

There's no normality assumption for parametric tests in general; since ""parametric"" doesn't imply normality at all (it encompasses tests that assume normality but is not in any sense limited to them). It simply means that there's a distributional or model assumption that's completely defined aside from a fixed, finite number of unspecified parameters. For example, if you have a test of H0: the population being standard uniform, U(0,1), against an alternative. H1 of the population being U(0,θ) for θ<1, that's a parametric test. (A good test statistic for that case would reject H0 when the largest observation was ≤ some critical value  - fairly easy to calculate - which depends on the sample size n and the significance level.)

see https://en.wikipedia.org/wiki/Parametric_statistics

Each parametric test has some parametric distributional assumption. Some tests' significance levels are fairly sensitive to their distributional assumption and some are not so sensitive. Typically a test will be somewhat sensitive to some kinds of deviation from the assumed distribution and insensitive to  some other kinds of deviation.


> I'd like to use the t-test as an example because it feels intuitive to me. When conducting a t-test, is the assumption of normality referring to normality of the sample data, normality of the population, or normality of the sampling distribution of the mean?

1. The word 'assumption' arises because of what is assumed when deriving the distribution of the test statistic under H0. Specifically, that then guarantees you maintain (don't exceed anywhere under the null) the desired type I error rate. The significance level may be somewhat robust to that distributional assumption, in that you can diverge from it in some ways and not impact the significance level a great deal. 

2. As just mentioned, some tests are fairly robust to this assumption under some conditions, which - when that happens - means you can still get close to the desired type I error rate somewhat outside that particular mathematical assumption. Some people loosely call those weaker conditions ""the assumptions"" but they're much nearer to rules of thumb.

  [To clarify - the amount of impact depends on the significance level as well as the sample size and the kind of deviation from normality and the particular test; it's no good being convinced that your 5% test is going to be fairly close to 5% if you then turn around and do a Bonferroni correction for multiple-comparisons and you end up doing say 20 tests at the 0.0025 level, where the effect on the actual significance level may be relatively large.]

  Note that there was *no mention whatever of power* there; we therefore don't know if the test is *any use* when we say 'robust' in that sense, only that the significance level is probably about what we asked for.

  Let's consider the one-sample t-test, then (which is also the test applied to pair-differences when doing a paired t-test). It assumes (at least when H0 is true), that the *population* distribution is normal (it also assumes that the variables X1, X2, ..., Xn are independent and have the same population mean and variance). Given those assumptions, you can show that the t-statistic has a t-distribution when H0 is true. You can use this fact to make sure that the test doesn't exceed the selected significance level (type I error rate), alpha. 

  [If you further assume that under the alternative the only change is to the common population mean, then the distribution of the test statistic under the alternative will then be non-central t, but this is not necessary for the test to 'work'; you could have the variance change slowly as the mean changed away from the null value -- or even the shape change slowly as the mean changed -- and the test would still work perfectly well. This means that the shape of the sample is not particularly relevant to the assumption since you don't know if H0 is true in the sample. It may be that the assumption is perfectly reasonable when H0 is true.]

3. Of course, no such simple assumption will be true in practice. It's not practical to think that a population distribution will adhere exactly to such a simple assumption. This is not of itself consequential. The issue is rather whether it's close enough for your purposes (which will not generally be the same from situation to situation and person to person); if the properties of your test are close to what you need them to be (e.g. your type I error rate is quite close and the power is not badly affected) then all may be well.

  With the one-sample t-test, if the distribution is not very skew nor heavy tailed, typically the significance level is only moderately affected unless the sample size is pretty small, and improves as the sample size gets larger. (Power is a somewhat different matter, but I don't want to go on a long digression on that, suffice to say that large samples don't 'rescue' you in the sense that power for small effect sizes doesn't necessarily get close to what you'd have had under that normality assumption; the relative efficiency may be quite low).

  Broadly speaking, while the formal assumption is indeed that the population is normal (you don't actually derive the t distribution for the test statistic without that and the other assumptions) often the significance level is reasonable under considerably milder conditions.

  For the ordinary two-sample t-test the situation is perhaps slightly better still -- it's less sensitive to moderate skewness than the one-sample test is (particularly a one-sided one sample test). However, it is sensitive to the assumption of equal variances when the sample sizes differ.


4. This particular kind of level-robustness to moderate violations of the formal assumption of normality is not always the case. So let's consider another test where the formal assumption is normality: the F test for equality of two population variances. There the assumption is again of common normal population distribution within each group (and independence within and across groups), but the test is considerably more sensitive to that normality assumption (indeed, it's *particularly* sensitive to different kurtosis than that of the normal).

5. What we have not yet addressed is the very practical question of how to decide when your significance level should be okay. This is a much more complicated question, keeping in mind that it's a question about the behaviour of the population *when H0 is true*.

6. Incidentally - while you didn't ask this - if what you're really interested in is guaranteeing the significance level under non-normality - this is *trivial* to achieve in the simplest cases where you would use the t-test (one-sample, paired or two-sample equal variance). I don't know why people make such a fuss about the middling level-robustness of the t-test to non-normality when you can absolutely get it whenever you want with very little additional effort. (However, if the sample sizes are *really* small or the population distribution is very heavily discrete - mostly only taking a few values - then there are a number of issues that crop up, but I won't extend this answer further by addressing them.)

---

> I would love to follow up with some questions after getting a straightforward response to the question above, because the more I look into this the less I understand it.

It's probably not as straightforward as you might hope, but that's because the real situation is not quite as simple as most people try to make out.

If that came close enough for you then sure, fire away.",15,w7nfjj,"Hello everyone,

I'm curious about the assumption of normality for parametric tests in general, but I'd like to use the t-test as an example because it feels intuitive to me. When conducting a t-test, is the assumption of normality referring to normality of the sample data, normality of the population, or normality of the sampling distribution of the mean?

I would love to follow up with some questions after getting a straightforward response to the question above, because the more I look into this the less I understand it.

Thank you",AskStatistics,2022-07-25 05:15:33,33
"When your textbook writes ""standard deviation of the sample means"" it means ""The standard error of the mean is the standard deviation of the sample means of all possible samples of size n of the sampling distribution""

Look at that sentence for a while and hopefully it will start making sense.

You're right in your second half - considering each unit to be a sample makes no sense whatsoever.

edit: check this PDF https://faculty.ncc.edu/LinkClick.aspx?fileticket=KFfch0hkBL0%3D&tabid=3329&mid=4267",1,w8af9x," 

Hey all

I'm very confused about the sampling distribution idea. I understand CLT and that sample means are normally distributed, no worries. But the questions in my textbook keep taking one sample of say 40 people, and then tell me to find the SD of the sample means (plural, sample meanS). And looking at it, you have one sample, which means you have one sample mean, which means SD(Xbar) = s/root(n), n = 1, SD(Xbar) = s

For example, one question says there is a singular sample taken, then asks me to find the standard deviation of the distribution of means. In this case, there's only one sample mean or point estimate, but it treats the number of people used to get the single sample mean as the number of sample means. Very strange.

I thought maybe they were saying that each unit can be considered a sample, with a sample size of one, and thus each unit's value, or observation would be equal to a sample mean. But then I realised, say you're looking at heights of everyone in the world, if you just take a census, the distribution of all the observations will have a sd of say sigma, and then according to this method, the standard deviation of the point estimates would be sigma on root n, in this case root of 8 billion. But obviously, if your sample means all come from a sample size of one, then your sample mean distribution and just your population distribution are the exact same and therefore have the same sd. But according to this formula, they'd be different by a factor of 1/root(8 billion). Hence, you can't consider one unit to be a whole point estimate/sample mean. This made me very confused, and my instant reaction was ""textbook has made an error"" but it's done like this in every single question. I feel like I'm missng something, I'm prepared to facepalm when I realise.

Thanks in advance for any help.",AskStatistics,2022-07-25 21:44:29,4
"Not in general. With small *integer* parameters yes, but it involves solving a polynomial equation, so that generally only works for polynomials up to quartics. There may be some special cases where you can go higher (some particular p's might leave you able to find a factor and solve a lower order polynomial)

I think the polynomial is of degree a +b-1

Also if either a or b is 1 you can do it even if the other is not integer. There's probably some more cases if I think about it.

The required function (the inverse regularized incomplete beta) is commonly available in mathematics libraries and also comes up in other cases - e.g. you can use it for the t distibution with a little transformation

The cdf (the inverse of the quantile function) can be done for integer parameters but can soon become unwieldy, so you probably wouldn't want to go to very high parameter values for it in most cases.",3,w7zqvj,"The title pretty much says it all. For a beta distribution with parameters a and b, is there a closed-form solution for the p-th percentile?",AskStatistics,2022-07-25 13:44:09,3
"The volume of blood they claimed they could work with was too small to yield reliable samples, particular as they claimed they could perform multiple tests on one drop

Not really a statistical issue, though.",6,w7z403,"I read that there is some sampling problem related to statistics that played the biggest role in Theranos' failure, but can't find any references to it now. Can anybody help?",AskStatistics,2022-07-25 13:18:22,1
"What do you mean by ""normalizing"" a dummy variable? Also, can you clarify what you mean by ""the trap""? I've seen that term being thrown around the past few weeks by various users, so I assume it is an idiosyncratic term a random instructor uses somewhere, but I'm not sure I get what you mean by it.",4,w7y5bj,"Hello everyone,

I would like to  know how I can normalize a dummy variable in Spss. It seems to me that it doesn't really make sense. But if so, how can I get way of the trap?
The dummy variables are my main dependent variable.",AskStatistics,2022-07-25 12:39:59,10
"If you're looking for a more comprehensive and rigorous approach to mediation, I'd recommend Vanderweele. S[ee p. 2 of this paper for a quick summary.](http://web.pdx.edu/~newsomj/cdaclass/ho_mediation.pdf)",2,w7u0et,"Y’all have been so helpful to me, I am so grateful. I have one more unrelated question. 

I have read a bit about mediation in logistic regression, but I have never done this kind of analysis myself. I’m reading a paper that says they assessed mediation by:

1. Fitting a logistic model with only the potential mediator and outcome 

2. Fitting a logistic regression model with multiple predictors and no mediator

3. Adding the mediator to the model in 2

They include no further details and do not really discuss it in the results either. My best guess is they are asking “does adding the mediator change the other coefficients in the model?” Which doesn’t sound right to me. Any insights?",AskStatistics,2022-07-25 09:55:14,4
"You need to **look at your data**.  You have discretized data, and you are treating it as if it was continuous.  There are only four observed levels, {1,2,3,4}. There is no fix for this in SPSS or in any other package, the data is what it is.",59,w79we2,,AskStatistics,2022-07-24 17:02:56,22
"> I have a histogram where the X axis has a range below 1 so when normalised the Y axis is greater than 1.

This is fine. A normalized histogram is an estimate of the density of X, and the density is not a probability. It is not bounded between zero and one.

> I am trying to read off the likelihood which I believe for a normalised histogram should usually be the corresponding point on the Y axis. 

Are you trying to read off the *probability* (not likelihood) of a particular value of X? Then it is not given by the value of the density (nor the corresponding point on the histogram). 

If you want to calculate the probability that X lies in a narrow interval around a value x (the only thing that makes sense for a continuous variable, since the probability associated with any single value is zero), then the probability is approximately the area of the rectangle with width equal to the interval and height equal to the point on the Y axis corresponding to x. For a normalized histogram, the area of an individual bar (the width times the height) is the (approximate) probability of an observation lying in the range of that particular bar. Note that it's perfectly fine for the height of the bar to be greater than one, as long as its *width* is less than one.",3,w7t6iw,I have a histogram where the X axis has a range below 1 so when normalised the Y axis is greater than 1. I am trying to read off the likelihood which I believe for a normalised histogram should usually be the corresponding point on the Y axis. Obviously as my Y axis is greater than 1 this isn't working - how do I resolve this issue?,AskStatistics,2022-07-25 09:21:51,2
"> P(W) = 0.2

> P(W) + P(W) + P(W) + P(W) + P(W) = 1

This means after 5 attempts, on average you will have 1 win. It does not guarantee a win, but you can get more than 1 win, which is why the average can be higher than 1 even if you aren't guaranteed a win. Think about how 2 coin flips can be both heads or both tails even though it's 50% chance for either on a given flip.

> P(L) = 1 - P(W)

> P(L) = 0.8

> P(L) * P(L) * P(L) * P(L) * P(L) = 0.8^5 = ~0.33

This is the *correct* way to calculate the chances of losing all/winning any of them. There is only one way to lose all of them, and that requires 5 successive losses, which you calculated there. Every other possibility that isn't that involves winning at least one time. Those two probabilities must add to one, since those are the only possibilities.

For more general cases, I'd recommend reading about the [Binomial Theorem](https://en.wikipedia.org/wiki/Binomial_theorem)/[Pascal's Triangle](https://en.wikipedia.org/wiki/Pascal%27s_triangle)",1,w7qr9o,"Please help me understand the following problems that randomly came to my mind.

# 1. Scratch-off lottery ticket

A scratch-off lottery ticket has probability of winning 20%. So if I buy 5 tickets, I have 100% probability that I win. 

    P(W) = 0.2
    P(W) + P(W) + P(W) + P(W) + P(W) = 1

Although my common sense says to me that it is not 100%, because I already tried to buy 5 such tickets and I did not win :) So I try to multiply complements. 

    P(L) = 1 - P(W)
    P(L) = 0.8
    P(L) * P(L) * P(L) * P(L) * P(L) = 0.8^5 = ~0.33

So probability that I **do not win** when buying 5 scratch-off tickets is 33%, so probability that I win when buying 5 tickets is \~67%. That explains why I did not win :) But honestly I have no idea what I have done, I just tried this and it made a sense. 

Is it a correct answer? if yes, why?

# 2. Poll for elections

An agency is doing a poll by a telephone. They ask 1000 respondents. My country has 5 mil people. Probability that my phone will ring is 0.02%. What is the probability that my phone will ring if they make 10 or 1000 polls by a telephone?

I do not know how to approach this example. Is is the same thing as with the tickets?",AskStatistics,2022-07-25 07:44:43,5
"The important part of propensity score matching is trimming the tails; i.e., eliminating the patients who were not considered eligible for one or more of the treatments. After you calculate everyone's propensity scores, do this step first. Show histograms of the propensity scores within each of the three groups (line them up). Be ruthless in your trimming. If treatment A+standard has three patients hanging out below a certain threshold, while the remaining 200 A+standard treatment patients are all well above that threshold, ignore those three outlier A+standard patients when you create your low cutoff for the mutual propensity score range. 

Once you've done this, any propensity score matching method you use should get answers very similar to the answers you get with simple covariate adjustment. Use this fact to evaluate how well you've implemented the analysis. Here's an article making the comparison, and also describing a few ways to use propensity scores poorly so you can avoid them. 

[https://www.sciencedirect.com/science/article/pii/S073510971637036X](https://www.sciencedirect.com/science/article/pii/S073510971637036X)",1,w7p5oi,"Hi everyone I'm working on a retrospective cohort study that compares survival time for two treatments (e.g. A+standard treatment & B+standard treatment) and there is also a third control group (standard treatment only). The reviewers commented that the control group is very different from the two groups and we should do matching. However, after searching, I found these two articles:

[https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4285179/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4285179/)

[https://journals.lww.com/epidem/Fulltext/2013/05000/Matching\_by\_Propensity\_Score\_in\_Cohort\_Studies.12.aspx](https://journals.lww.com/epidem/Fulltext/2013/05000/Matching_by_Propensity_Score_in_Cohort_Studies.12.aspx)

The first one talks about how to use propensity score matching in a survival analysis, and the second one talks about how to use propensity score matching in three groups. Can anyone give a simple explanation on how I should combine these two articles to match my two cohorts with the controls in order to conduct a survival analysis?

Thank you in advance",AskStatistics,2022-07-25 06:35:49,1
"The strategy fails because there is a limit on how much you start with. If you start with $32.00 there is still about a 3/400 chance that you lose everything, and you’ll only have gained 25¢ if successful. This is a martingale betting strategy https://en.m.wikipedia.org/wiki/Martingale_(betting_system)",4,w7o7ho,"So, I’ve had this idea for a while that could theoretically be used to make money at casinos with any game that offers 50/50 odds with a 1:1 payout. (picking odd/even or black/red on roulette as examples) 

Hypothetically, you start with $32.
You put a quarter on black. $0.25.
You lose.
You then double your bet. $0.50.
You lose again.
You double again. $1.00.
You lose again.
You double a final time. $2.00.
You win, getting your $2.00 plus an additional $2.00. 
With a loss of .25, .50, 1, and 2, and  again of 4, no matter how many times you lose and double your bet you will always make a profit equal to your starting bet.

Now, $32.00 is an arbitrary amount of money that I decided because starting at a quarter you can double it a total of 7 times. .25, .5, 1, 2, 4, 8, 16. Adding those up you get $31.75 so you won’t run out of money unless you lose more than 7 times.

The odds of losing, however, are 50/50 7 times or .5^7.  A 0.78125% chance. 

In addition, every time this cycle is completed, you win $0.25, which is 0.78125% of your starting amount of money.

Is my math faulty, or is there some other reason that no one does this on a large scale in casinos?",AskStatistics,2022-07-25 05:52:46,7
VIF might tell you what you need. Correlation is fine to.,1,w7ilpr,"I'm running a logistic regression model and checking the collinearity. My explanatory variables include both categorical and continuous data. 
I wonder if I should use :
 (proc reg data=dataset ; model y=x1 x2 x3  / tol vif collin) 
or
 (proc logistic descending data=dataset ; model y=x1 x2 x3 /corrb )

Thanks for helping ！",AskStatistics,2022-07-25 00:37:07,2
"Yes, it's possible. There's an explicit formula in terms of quantities you have.

Please note rule 1

https://www.reddit.com/r/AskStatistics/about/rules/",1,w7m1l6, hello guys i really need help in a statistic excersise using SPSS [https://imgur.com/a/E9w0lga](https://imgur.com/a/E9w0lga) is there a way to solve the d) question with the info of the tables in the second photo or it is more complicated than that?,AskStatistics,2022-07-25 04:05:24,2
one tail,1,w7ld28,"Hi,
If a problem has the wording “at least”, does it mean I have to do a two tailed test or a one tail test?

For example:
A professor claims that the average score for his class is at least a 75.",AskStatistics,2022-07-25 03:26:56,2
"Strictly speaking, neither is the case. Covariance stationarity is linked to *unconditional variance/covariance* of *dependent variable*. Say, AR(1) with phi = 1 is covariance-nonstationary but error terms can be homoscedastic.

On the contrary, say ARMA(0,0)-GARCH(1,1) with alpha+beta<1 is stationary as long as distribution of the error term has a finite second moment. From the point of view of OLS, you have to use heteroscedasticity-corrected covariance, as the *errors here are conditionally heteroscedastic*.",1,w7kwyu,"Hey guys,    
(1) Homoskedasticity is the condition where the variance of the regression residuals is constant for all observations.    
(2) Constant variance of a time series is one of the properties for a time series to be considered covariance stationary.

Are (1) and (2) the same thing by deduction? If my observed time series values have a constant variance (2), that mean they diverge from my regression line by quite the same amount across all observations, which logically means that my errors are the same across all observations (1).

Can someone confirm we're talking about the same thing?",AskStatistics,2022-07-25 03:01:02,1
"There is no clear ""elbow"" here, but the elbow method is not a particularly good method anyway. What are you clustering, how are you clustering it, and what are you trying to do with your clusters?",16,w73rtn,,AskStatistics,2022-07-24 12:32:09,11
"Even for an independent samples test, checking for overlap between between CIs is overly conservative (i.e. it is possible for an independent samples test to reject even when there is overlap). In the case of a paired sample test, comparing the CIs is completely misleading because the relevant uncertainty is the uncertainty in the mean of the *difference* scores, not the individual sample means. If you want a CI, then you should be computing a CI for the mean difference.",2,w7cohx,"So I'm trying to learn some basic statistics and I'm following some videos on how to use Jamovi. I just learned about the different t-tests and comparing the means. 

So I'm trying to compare the mean to see if there's a statistically significant difference between a group of people before and after a treatment. The table says that the mean difference is 8.25, with p<0.05, but the plot shows that the CI for the pre and post treatment are overlapping. I thought if they over lapped it's statistically not significant. So why is the p value <0.05? 

&#x200B;

Can anyone help me understand this?

https://preview.redd.it/usikyklqhmd91.png?width=1073&format=png&auto=webp&s=63353a2d4923383d334451277686ebd8f95b578f",AskStatistics,2022-07-24 19:16:29,5
"Unless there was something that drove different people to a town center that was potentially highly correlated with any of those questions on those different days (e.g., basketball tournament, jockey convention), the best you can do is analyze each variable separately, and maybe take note if the distributions are significantly different between days, which could be indicative of an issue with sampling or some fundamental difference between the populations each day for a given gender.

With the histogram data, otherwise, you could do something like a [Naive Bayes Classifier](https://en.wikipedia.org/wiki/Naive_Bayes_classifier), which would look something like this:

    P(male|height,color,shoe) = P(male) * (
     P(height|male) * P(color|male) * P(shoe|male)
    ) / (P(color) * P(shoe) * P(height)) 

    P(female) = 1 - P(male)

Note that if you sampled only one gender on a particular day, you would probably want to use general population statistics to estimate `P(male)` or `P(female)`, or just assume 50/50 or 51/49 for Male:Female.

In this case, you are simply trying to optimize

    P(gender) * P(height|gender) * P(color|gender) * P(shoe|gender)

for each subject you are applying the predictor to.

Note that for shoe size and height, you might want to use a [Normal Distribution](https://en.wikipedia.org/wiki/Normal_distribution) for the probability terms, estimating the mean and variance. For color, you would create a probability table by taking the proportion of counts for each color.

Naive Bayes isn't perfect, but it often does surprisingly well, especially for a smaller number of variables. The worst-case scenario I see here is height and shoe size being correlated, so it might over-value those statistics.",1,w7avns,"So I have multiple datasets which are all histograms and cannot be linked. The topic of the data is quite complex so for example imagine I was surveying different qualities between men and women. The data I have is equivalent to me spending a day running around the town centre surveying men, and the next day running around the town centre surveying women. I ask them only 1 question, such as their height, shoe size or favourite colour. I don't record who gave which answer - so I cannot say their height AND shoe size. Just gender and answer to one question. Thus I only have 6 datasets, 3 for each gender. Each dataset has three columns and the gender, their answer and number of times that answer was given. I.e. histogram data.

I am trying to classify people as male or female based on these three questions, but again can only ask any new people only 1 question. I currently have three histograms, one for each question, and each histogram has two plots - one for men, one for women. All I can think to do so far is make a cut where the two histograms intercept and classify any new people as male or female based on which side they are more likely to be in (so if this new person says their height is 5 foot 2, they would be classified as a woman because it is more likely). Is there any other classification techniques I can use on a dataset like this, or is this really the only method?",AskStatistics,2022-07-24 17:50:15,1
"As typically used it's *really* not good. 

Here's a few discussions (of many): https://stats.stackexchange.com/questions/20836/algorithms-for-automatic-model-selection

http://denversug.org/presentations/2010CODay/StopStepPresntn.pdf

https://www.stata.com/support/faqs/statistics/stepwise-regression-problems/



I'd also suggest chapter 4 of Harrell's *Regression Modeling Strategies*.

There are more suitable ways to use stepwise (e.g. if you're splitting off data for separating model identification from any estimation, testing, CIs, predictions etc, which at least stops it screwing with the properties of the inference), though it may still not be the best use of the data (there's better ways to identify models).",7,w77g9o,"I rarely see papers (I’m epi/public health) that say they did stepwise regression, but is it considered an appropriate method? My professors only had negative things to say about it.",AskStatistics,2022-07-24 15:11:46,5
"Not directly statistics-related, but some Python programming tips:

1. Do not name variables the same as basic functions/types. `list` is already one of these, and it can lead to a lot of confusion. `number_list`, or even `x` in the case of a basic example is a better name. Although you won't necessarily encounter errors, it is confusing for others to read and makes creating bugs much more likely.

2. You need to call `list.sort()` (or `x.sort()` if you renamed your variable to `x`), since `sort` is a method, not an attribute. Also, `list.sort()` is a class method, so you could call `list.sort(x)` if you didn't overwrite `list`, but most of the time you don't do that. `y=sorted(x)` is another way of doing that without changing the original object.

3. You can make your initial data-generation statement a bit more quick/efficient with `x = [random.uniform(0,1) for _ in range(N)]`

4. Although you can't do this with (3) above, you do not want to sort the list after appending each value. This is somewhat inefficient, especially depending on the sorting algorithm and data. A single sort at the end is all that you need.",4,w75kfj,"I am trying to automatically find clusters in heatmaps by comparing the values in the matrices the heatmap was made from. If you know any better way to do it that what I am trying to do here then please let me know!

To simplify the quest for the right way to deal with this I am doing this:

I made the following code:

    import random
    N = random.randint(10,100)
    list = []
    for number in range(N): 
        list.append(random.uniform(0,1)) 
        list.sort

This will create a list of random length, consisting of random floats.

Such as this:

    >>> list
    [0.021263396654293998, 0.03529789771538705, 0.04941283716007161, 0.0978048115260517, 0.1820585617547711, 0.1830721362829114, 0.19380450825617357, 0.32216434123018545, 0.3258980098817065, 0.32625306772508755, 0.35535228866825364, 0.4497179746309843, 0.4674667236388331, 0.5001723371033484, 0.5099819534410736, 0.5132612305912886, 0.5937894235873404, 0.6174029377390258, 0.6186119995392255, 0.6190422339751034, 0.6439727855585428, 0.6462866261059345, 0.6669652292671929, 0.7952694822956691, 0.8300339384749739, 0.840006865012779, 0.8715172776315]

&#x200B;

The next step I need to do is to find if any of the floats in the list could be considered closer together than the rest. E.g :

    0.1820585617547711, 0.1830721362829114, 0.19380450825617357
    or 
    0.5937894235873404, 0.6174029377390258, 0.6186119995392255, 0.6190422339751034

are apparently closer than the rest of the floats (there could be others). I have to find floats like these, a minimum of 3 floats, and add these to a list.I am thus far using variance. I am checking the variance of every triplet of floats. If a triplet has a low variance (around 0.5 \* variance for whole list), I assume I can keep checking the next float. If the variance doesnt increase significantly I assume it can be added to a list where the triplet already is in. Then continue on with the next float. But this does not really work.

Does anyone have any suggestions on how I can find floats that have more similar values compared to the rest? Or a better way to identify blocks in a heatmap that seem to ""belong together""?

Any help is greatly appreciated!",AskStatistics,2022-07-24 13:50:08,6
"1. Linear algebra first and foremost

   Depending on what is covered, possibly also ...

2. multivariate calculus

3. multivariate probability (joint, conditional and marginal distributions)


Edit: it looks like they do some coverage of multivariate distributions in that video series, but you may want more basic material first.",1,w6o3ie,I found a [yt playlist on multivariate analysis](https://www.youtube.com/playlist?list=PL3DFCC23FCE3C7EFB). Which lectures should I watch before watching CCA?,AskStatistics,2022-07-23 22:28:42,6
"A few montha ago I did some analysis on Risk game strategies. What I did was a monte carlo simulation, which was straightforward to implement and took a few minutea to run, but gave consistent results within a reasonable margin of error.",2,w6ostx,"I hope this is an okay place to ask this question. I am currently designing a board game and I would like the be able to calculate how much of an impact luck has on the result of a match.

Players fill a bag with a amount of X various tokens, and then pull an amount Y of tokens from that bag without replacing. They get 1 Victory Point for each of their tokens that is pulled.

Over the course of the game, that ""pulling session"" outlined above happens Z times (the bag is fully emptied after each pulling session), with varying amounts of X and Y for each session Z.

My assumption would be that I can decrease the impact luck has on the outcome of the game by changing variables X, Y and Z (treating them as averages). For one, I'd like to know which of these variables has the highest impact (or if maybe one of them doesn't have any impact). I would further like to be able to answer hypotheses like this:

*""If I want the impact of luck on the outcome of the game to be less than P (for example with P = 5%), what sets of average X, Y and Z would be able to accomplish that?""*

To make it clearer what I mean with an example: Say over the course of the entire game, player A has put in 30 tokens, player B has put in 40 tokens, and 90 neutral tokens were put in. The expectation would be that player B wins the game. However, player A could get lucky and win by getting better pulls by chance. I'd like that ""chance"" to be less than an arbitrary probability that I decide on.

I have a rough understanding of statistics and combinatorics, but it's a bit hard to put it together to solve this issue. Can anyone give me any pointers for concepts/formulas or specific topics I could read up on to solve that kind of problem?",AskStatistics,2022-07-23 23:13:44,2
"> I want to be able to write a function in python which takes a cut off point and plots a ROC curve for that cut off

You don't plot ROC based off of a single threshold / cutoff. ROC is made by interacting across all possible cutoffs and adding a point on the curve for each one.",1,w6ibvq,"I  have a fairly niche situation: I have two datasets and both are for  histograms, i.e. the X column is values and the Y column is frequency.  For an easy example lets say one dataset is for men's heights and the  other dataset is for women's heights, imagine you plot these two  datasets on the same axis, with the Y being frequency and the X being  height. Now I am trying to pick someone at random from the population  and I only know their height, thus I need to define a cut off where I  will say anyone below this height is a woman, and anyone above this  height is a man. I then want to plot a ROC curve as a function of this  cut off. Trivially the best cut off will be where the graphs over lap,  at maybe 5 foot 6, but I want to be able to write a function in python  which takes a cut off point and plots a ROC curve for that cut off. I  know this isn't strictly machine learning but I figured it would be the  best place to ask for help.

As a  side question, is there any better way to classify than a simple cut in  this situation? I also have histograms for (let's say) men's weight and  women's, male hairyness and female hairyness male shoe size and female  shoe size. However these are all different datasets so I can't say  person 351 is male, has a height of 6 foot and a weight of 180lbs  because these are all different datasets and there is no way to link  person 351's weight recording to their height recording.",AskStatistics,2022-07-23 17:20:18,1
"AR(1) model only takes into account the immediate past value (e. g. today vs the day before), and adds a white noise term (uncorrelated error terms with constant variance). A model that incorporates other features alongside past values is therefore not AR(1).

It is a model where future values can be determined solely by past values.",4,w683xe,"What's the difference between adding a variable that captures time (e.g., day of measurement) and using an AR(1) model?  I know what the difference is in technical terms, but I'm struggling to grasp what it means intuitively, for example, when interpreting the results of the model.",AskStatistics,2022-07-23 09:33:07,4
"I’d plan the visits in order to maximize meeting patient need, not maximizing face-to-face time with a priest. 

Therefore, here comes A not-Stats-Answer to a not-Stats Question: 

I think sending the priest on Monday afternoon and Thursday would make sense the most. 

Why?

You can expect hospitalizations to go up on the weekends, as they do for most demographics. So picture an old, lonely lady feeling dizzy in the head on a Friday night: Where would she turn to? The local hospital of course! 

So assuming that old grandma has to stay in the hospital, she will definitely still be there by Monday afternoon. Moreover assuming that her children / grandchildren / nephews are all working, monday would be the worst time to visit, as you have to reschedule, maybe take an interstate flight etc. . 

That concludes Monday and brings us to Thursday. Why should the priest come on Thursday? There are two main reasons:

1) our hospital priests come on Thursday. So, to German priests that seems like a good day. It might as well work for priests in Massachusetts. That’s it. 

2) imagine you’re in a hospital, probably alone because visits are not allowed or heavily restricted because of Covid or because your family can’t make it during the week: Having Someone there on Monday and Thursday leaves you with 2-3 days between each visit. Get in some family or volunteers on the weekend and you might end up receiving a visit every other day - that’s some good continuity for a hospitalized person.

My main sources: 

a) trust me bro 

b) I’m a psychologist in a clinic and I share my office with the priests, who pay a visit every Thursday",5,w6g9a0,"Hi, I'm helping to plan a program where a priest visits local hospitals twice each week to pray with elderly church members hospitalized for life-threatening conditions. 

I tried to research it, but I don't see any study with precisely the info I need. 

The point is to choose the two days of the week when it's most likely that he would be able to visit the greatest number of people from the congregation (or anyone who would like him to pray with them) who have life-threatening conditions. 

I think I may need a recommendation based on combination of statistics related to 1.) day of week with the highest number of elderly admissions for life threatening conditions, and 2.) average number of days between hospital stay and death of the patient.   

So, here's a good way to formulate the question i'm asking: **All things considered,** ***including average time between admission and death of hospitalized patients*****, on which day would a priest's visit be most likely to coincide with elderly patients being hospitalized and still alive?**  

It's a complicated question!  Very grateful to any help you offer.  We are in Massachusetts, by the way.  

Thanks so much.",AskStatistics,2022-07-23 15:40:43,31
"Its not at all clear what you mean by

>where I want to see if a single group of 50 observations is significantly under 0(not mean)

Its not clear how this relates to either of the possible bases for inference about a parameter (random sampling of population or randomization) nor what parameter you might be talking about",1,w6gwhv," Hey guys, I did some regressions/paired t tests on my sample I now want to test for my final hypothesis where I want to see if a single group of 50 observations is significantly under 0(not mean) so dependent <0 and if it is significant by how much as in an average of all the negative values which kind of test would you suggest for this?",AskStatistics,2022-07-23 16:11:00,5
"Your question is a bit vaguely worded, but if you want to see how the effect of one variable on another changes depending on what values a third variable takes, then you will need some form of linear model with an interaction, and possibly a link function, depending on what your dependent variable is.",1,w6bmhj,,AskStatistics,2022-07-23 12:09:36,3
Logistic regression would be a good candidate,2,w5wl0r,"Hello everyone,

quick dumb question, sorry!

I have four conditions, two controls and two treatments. The outcome variable is binary (1= clicked on a link, 0=did not click on a link).

How do I best compare these four conditions? I am interested in all comparisons and I have looked into this but can't find a good/obvious solution. I can't use an ANOVA because the data is binomial, but is there a test that I could use instead? Kruskal-Wallis doesn't work as the binary data is not ordinal. Or do I have to run a series of Chi-square tests and then use some adjustment to the p-value for multiple comparisons? I'm using SPSS.

Thanks!",AskStatistics,2022-07-22 23:04:56,9
"> so I transformed the data using Log10 function on SPSS. 

""not normal"" does not typically suggest a log transformation. You could well make things worse. 

What is your response variable measuring? What values are possible? (not observed, *possible*)

> I also added a constant of 20 because I had some values that were -19. 

<shakes head>

> Any advice would be much appreciated, thanks.

Avoid transformation without a good reason. 

What was the original question/ problem you were trying to answer? What analysis did you do?

How did you decide that ""it's not really showing that""?",4,w5v151,"Back transformation and constants

My data was not normally distributed, so I transformed the data using Log10 function on SPSS. I also added a constant of 20 because I had some values that were -19. 

I then back transformed the data on excel, and now wondering would I have to minus the constant 20 that I added earlier on? 

My original data had negative numbers and now all the back transformed numbers are positive. But I’m looking to show a change over time, so some data would have decreased in value - but now it’s not really showing that. 

Any advice would be much appreciated, thanks.",AskStatistics,2022-07-22 21:40:43,3
"But we can make an estimation.

If none of the other cars are red, then there is a 100% probability that the red car is a Ferrari.

If all of the other cars are red, then 1/6 of all cars are red Ferrari's and 2/3 of all cars are red non-Ferrari's. So the probability that a red car is a Ferrari is:  (1/6) / (1/6 + 2/3) = 1/5 = 20%.

So with the given information, you know that the likelihood is between 20% and 100%",6,w5pod5,"""A recent poll revealed that a third of the cars in Italy are Ferraris, and that half of those are red. If you spot a red car approaching from a distance, what is the likelihood that it is a Ferrari?""

We would need the probability of any car in Italy being red in order to answer this, right?",AskStatistics,2022-07-22 17:13:20,2
"If your questions are each about a different activity, you could try using Mann-Whitney tests for each individual question to compare the two groups.",2,w5z51l,"Hi, I am a vegetable in statistics and I am doing this audit which I have collected data for and want to know which statistical test to use to show significant difference. The audit is about 2 groups of students from different classes (economics vs arts) asking them if they (always, often, sometimes,rarely,never) do some activities differently like studying in library, home, garden... etc. I have gathered information from 20 economics vs 35 arts. Would it be possible to test them, and which test to use. Need an expert help, thanks !",AskStatistics,2022-07-23 01:40:23,1
"P(E| ¬ H) = 3/7 

You know that you do not have three heads, so there is only seven possible outcomes. Out of those seven outcomes, only three have two heads.

Then putting P(¬ H) = 7/8 will get you right back to P(H|E) = 1/4",1,w5yine,"I'm having issues understanding a given probability used in a Bayesian calculation. The problem asked what the probability of three coin flips all being heads given that you were told two of the flips were heads. I know that P(H|E) = 1/4, as out of the eight possible combination, only four of them have two or more heads with one having all three heads. However, when trying to figure out the terms on the RHS I'm getting stuck on P( **¬** H).

    P(H|E) = P(H)*P(E|H)/(P(H)*P(E|H) + P(¬H)*P(E|¬H))

P(H) = 1/8 (All three coins landing on heads is just 1/2\^3)

P(E|H) = 1 (the probability of being told two were heads when all three were heads is 100%)

P(E| **¬** H) = 3/8 (out of the eight possible combinations, only three have two heads occurring that don't have three heads)

P( **¬** H): This is what I'm confused on. I thought this would be 7/8 (i.e. 1-P(H)), but it needs to be 1 given that P(H|E) should be 1/4. This makes me think there's something that I'm not understanding about the probability of the null hypothesis.",AskStatistics,2022-07-23 01:00:50,2
"Logistic regression is what you are looking for. It's a different model than what you probably had in mind but it can be done, sure",10,w5iwvm,"Hi guys, I am wondering whether you can have a dummy variable be the dependent variable in regression?

For example, I want to see whether the amount of discounts a buyer has at checkout correlates to them being a new customer.

Also, I’m curious as to if you can make a regression have dummy variables as both the dependent and independent variables.",AskStatistics,2022-07-22 12:20:22,6
"Very interesting. It should be possible to get somewhere with this but you're probably going to have very little information about the shape in the center of the distribution; a bit more about the tails

If you had a distribution *family* in mind you can do more; you can write the joint likelihood for the max and min in terms of the distribution of the original data and the sample size, so in that situation you can get MLEs for the parameters given the observed statistics. In large samples you could perhaps even approximate the joint by the product of the marginals (the max and min aren't independent but for some distributions and in very large samples, ... not so far off it); it might be good enough for some purposes.

Without a distributional model it's harder, but you can write some nonparametric relationships between the distribution quantiles and the max.

e.g. for a simple example, if we have an i.i.d sample X1,...,Xn  and we let Y1 = min(X1...Xn) and Yn = max(X1,...,Xn) and if q3 is the population upper quartile, then we have P(Yn<q3) = 0.75^(n); similarly for any other quantiles ... so there is information to be had, albeit of a weakish kind. We can go further and write probabilities for quantiles q and Y1 and Yn. (the information about the whole set of quantiles is clearly highly dependent within a given (Y1,Yn) pair). With lots of such samples at a variety of sample sizes we have many independent sets of information of just this kind.

I think it should be possible to turn this into a likelihood. If you make no assumptions about distributional form I expect the MLE of the cdf could well be a step function but I haven't tried to that may be wrong. If you assume some sort of regularity conditions (some kind of smoothness, unimodality etc etc) you can probably do much better, but figuring it out would probably be a decent research project (if it hasn't already been done), it's likely to be a somewhat harder task.",1,w5kmtt,"I work at a store that handles a lot of payments through ACH, and we've been playing a game trying to see who can guess the sum received via ACH per week, using only the max and min e-check amounts each day. I got to thinking and made my way to this idealized model of the problem: 

The procedure is that someone samples a certain distribution N times, bundles those up, gives you the max and min of that bundle, then repeats the process. What can someone say about the distribution from the maxes and mins of these bundles after M bundles have been made, compared to if you had 2M random samplings of the same distribution? My first instinct is that even though each max or min is part of the distribution, just approximating the distribution as if those were just random samples is leaving information on the table, so to speak, so there'll be a difference between the two approximations of the distribution that grows as M gets bigger, but right now I'm floundering on how to quantify the approximation of the distribution that comes from the maxes and the mins. 

My last mathematically-oriented probability course was about a decade ago, so I'm a bit fuzzy on most things. What I remember and what I've been able to dig up so far haven't been much help, so I'm reaching out to y'all. I want to figure this out on my own as much as possible, but right now I just don't  know where to start. It's entirely possible this is basic subject matter, and I just need to take a refresher course or something, but either way, a point in the right direction or a hint or two would be much appreciated.",AskStatistics,2022-07-22 13:33:52,2
"**CLT:** Not the ""sample distribution"", but the ""sampling distribution"" of sample means samples from **non-normally distributed data** approaches a normal distribution as n--> infinity.  The ""sampling distribution"" is about sample means (or other appropriately scaled sums)-- so, if we took 50 sample means, each with n=1000, each of the 50 sample means will behave as if it were drawn from a normal distribution assuming certain ""niceness conditions"" (i.s., ≈68.26% of sample means within +/- 1 standard error, 95.44 within +/- 2, etc.).  Technically, CLT speaks about infinite sample sizes and limits, but other results apply saying that in many cases, smaller sample sizes will be ""close enough"" to behaving like a normal.

Note: If the population of data is already normally distributed, the CLT is not relevant, since a sample of n=1 is already normally distributed (as would be a sample mean of n=2, 3, etc.)

**LLN:**

>As the number of sampling distributions (eg. if we check those weights ten times, resulting in 1000 pooled samples but 10 sampling distributions) increases

No. We only have one sampling distribution for a particular (μ,σ,n), which is a theoretical statement about the probability of a sample mean being in a particular range of values. LLN says that as the number of observations in a sample increases, sample means (and many other sample stats) are expected to be closer to the true population mean-- that is, more accurate. This can be seen just by looking at the formula for the standard error of the mean: σ/√n.  As n increases, this decreases.  Since the standard error is the ""variability of sample estimates"", this decreasing means that estimates should be more likely to be more accurate.",6,w5aubk,"I realized today that I have long considered the CLT and LoLN to be ""the same thing"" since they've always been taught together and both somewhat relate to ""If number high enough, you good."" Today I'm trying to solidify in my brain the differences. (I know there may be other versions of the CLT, in looking into this myself I see ""classical CLT"" and others. Let's presume I'm talking the 'typical/standard used in most basic stats classes' in all cases.)

To my knowledge, and please correct if I'm wrong:

*CLT*

* As the *sample size* (eg. n=100 if you checked the body weight of 100 people in your city) increases, the *sample* distribution should get closer and closer to a normal distribution if that variable checked (weight in this case) is in fact normally distributed in the population. What if it's not so distirbuted?  In other words, is it also the case that the larger the sample size the closer you get to the true distribution *whatever form it takes* if in fact your sample was large enough and was perfectly random/unbiased?

*LoLN*

* As the *number of sampling distributions* (eg. if we check those weights ten times, resulting in 1000 pooled samples but 10 sampling distributions) increases, the mean of the samp*ling* distributions will converge on the true population mean with variance equal to the true population variance.


So, if that's correct (and maybe it's not), would it be a correct statement that CLT = bigger sample sizes lead to normal distributions, and LoLN = more sampling will get you closer to the true mean?

Then is it the case that if I were hypothetically to take two samples of 100 people's weights (and presumed it normally distributed in the population) each sample would be approximately normally distributed (and if I pooled the sample it would be even closer to a normal distribution by virtue of doubling n)? But if I were to compare the two sampling distributions's means, is it the case that those means would be very unlikely to be normally distributed due to the low number of samp*ling* distributions despite plenty of samples within (because in this case the 'n' is the number of sampling distributions and not the number of people checked) and we would need to take several more samples of 100 each to achieve normality of sampling distribution means?

Do I understand these concepts correctly, or could anyone set me straight if I misunderstand anywhere? Or, alternatively, if anyone has online resources (written, videos, etc.) that cover these two topics together to highlight the differences I would also love that!

(Edits because I perioded when I should have question marked and question marked when I should have perioded!)",AskStatistics,2022-07-22 06:37:00,4
You may want to try /r/homeworkhelp,2,w5ln6r,Every phone number in a country has 10 digits and starts with 0 and 5. What is the probability of randomly picking someone's phone number?,AskStatistics,2022-07-22 14:16:06,9
"Assuming no dropouts, I would probably just run a t test between each group and the different time intervals. Think 3 t tests should do. If need be, you can use bonferoni correction but it’s just 3 tests, don’t make it more complicated and just use alpha of 5%. 

Plot out the distributions of each over time and by itself. (You can use boxplot, histogram, density plot)

You can also try running a regression on your variable with a dummy variable for 1 right after treatment and another for some time later and see if statistically significant.",2,w54kg7," Hi! I need to check for significant differences between 3 measurements taken at 3 different time points in a group of 30 people: one before treatment, one right after treatment, and one some time later. The measurements are continuous values. How would you proceed?",AskStatistics,2022-07-22 00:43:34,6
"[Statistical Rethinking](https://github.com/rmcelreath/stat_rethinking_2022) is an awesome Bayesian introductory course for people that already know some statistical modeling (i.e. GLM, HLM, ...) from the frequentist side.

However, I don't think the book/course materials have been translated to Stata. I only did a cursory check though, you might want to look further into it 🤷‍♂️",3,w59mc4,"Hi all,

I am working on a project that has proposed using Bayes stats. I use Stata for analysis and know very little about Bayes. I want to teach myself about Bayes and how to use the techniques with Stata, but I am a bit overwhelmed with the internet. Does anyone have any good references for a frequentist that wants to learn Bayesian stats? Thanks!",AskStatistics,2022-07-22 05:40:08,3
This looks like a pretty standard sort of homework problem. How does the question arise?,2,w56p72,"Hi there, I need some assistance in understanding how a probability calculation works when you need a chain of successful outcomes to reach a certain goal, but failures sets you back just one step. 

**Here's a Senario:** 

I'm standing on the edge of a pool and there are 3 platforms (1,2,3) in front of me. I want to reach platform 3, but in order to do so I need to jump from the edge of the pool to platform 1, then 2 and finally 3. Each subsequent platform is smaller and harder to reach. I can jump to platform 1 with a success rate of 55%. If I succeed in jumping to platform 1 I can attempt to jump to platform 2 which has a success rate of 41%. If I succeed in jumping to platorm 2 I can attempt to jump to platform 3 with a success rate of 27%. If I fail any jump I have to move back one platform from where I jumped from. So a 2 -> 3 failure would mean I have to try at platform 1 again. A failure at 1 would mean I have to start at the edge of the pool. A failure at jumping to platform 1 from the edge would not put me back any further. I want to calculate how many jumps on average would I need to take in order to reach the the third platform.

Any help on this problem would be greatly appreciated :)



**Summary:** 

 What is the average number of attempts to reach 3rd platform, where failures put you back one step.

Probability:

* Platform 1: 55% success rate
* Platform 2: 41% success rate
* Platform 3: 27% success rate",AskStatistics,2022-07-22 02:59:37,5
https://www.reddit.com/r/math/comments/21u1ix/rmath_graduate_school_panel/cgh166w/?context=3,1,w5cyeu,"Hello! I'm over 10 years post-grad with a BS in Psych and I'm coming from a completely unrelated field.

Have any of you career-changed into stats well-after undergrad (or know someone who has) that can share the path you took? I'm going to pursue a MSAS but not sure if I need an entire second Bachelor's first since I've been out for so long, or if any of you ended up just focusing on the pre-reqs vs. an entire second Bachelor's? Thanks!",AskStatistics,2022-07-22 08:08:56,2
"Is this homework?

If they're independent, p(A|B) = p(A) = 1%",14,w4xiye,"Say you have a population of 175 million, and 1% of the population has trait A. If you select a group of 3,000 people based only on them having trait B (i.e. other traits are disregarded in the selection), and traits A and B are not related, what percentage of people in the select group would have trait A? What equation(s)/formula(s) are used to determine that?

EDIT: Thanks for the responses, folks!

EDIT 2: The body text at the end should have said ""what percentage of people in the select group would have trait A?""",AskStatistics,2022-07-21 18:20:49,11
"The y axis is inverted, smaller values are on the top.",1,w55pku,"Why are the data points top heavy, not bottom heavy? What does this indicate? I used (Hedges-Olkin, 1985) random effects model with correlation coefficient as the effect size.

&#x200B;

https://preview.redd.it/e4uy6mpk33d91.png?width=670&format=png&auto=webp&s=77c7597850ee45fad32c37c07fefe1ec8718a36d",AskStatistics,2022-07-22 01:57:35,1
"You can do a two sample t-test, to plot the two distributions. And check the p-value. Even small samples have a significance level of hypotheses testing. Just the result might not be robust, since you are expecting large variance.",1,w4zzuf,"Let’s say I am measuring the population growth/decline of 25 countries.  I have two small samples. One is a set of 7 and the other is a set of 18. The samples differ in one key regard.  The set of 7 has seen a 0% growth rate. The set of 18 has seen a 20% reduction. How can I test to see if this can possibly be due to chance….or if instead some factor is causing the 20% decline. 

I believe there is a factor causing the 18 to see decline. My colleagues believe it is simply chance…and proves nothing. Is there any test I can do?",AskStatistics,2022-07-21 20:21:28,4
"Bonferroni is per hypothesis test of interest.

If you're only interested in the one independent variable, and the rest are mostly there as control variables, and multiple testing isn't much of an issue for you.",2,w50vrh,"Hi everyone, I had to run multiple regressions to see what treatment choices people were selecting out of 8 options and now it’s my understanding that I need to adjust for running multiple tests. When I do the adjustment, do I adjust for each regression (so 8 in this case) or for each independent variable (8 regressions * 8 variable per regression). I’m most interested in a single IV but would like to report/interpret covariants findings in each model if they are significant",AskStatistics,2022-07-21 21:07:24,7
"I'd say it's always helpful if you got good grades in one of their classes. Also network with the grad students.

You might find past theses from graduate students they advised easier to read than the research papers as they include much more background. My school has a website where you can search up completed theses and I can search by advisor, do you have something like that?

Sometimes it's more about your ability to work (being independent, problem solving, etc) then being mature in the knowledge. That said, there is usually an expectation that you're getting pretty good grades in related classes.",2,w4z49b,"Hey! I am an undergrad stats senior student and I would love to do undergraduate research. Unfortunately (or luckily), my college does not have a senior thesis and I am emailing random stats professors for a research opportunity.

I am reading their papers and It's very difficult to understand them without a LOT of effort since 95% of it was not covered in my undergraduate classes. I assume it's ok and no one expects a deep understanding of math and stats from an undergraduate student. 

However, from what I read online, I should at least be familiar with topics of interest and their published papers before writing them an email. 

Do you have any suggestions about how a student could attract a professor to be their research advisor?

What to do If I don't understand their papers?

How statistically/mathematically mature a student should be for undergraduate research?

I would appreciate any suggestions. 

Thank you :3",AskStatistics,2022-07-21 19:37:01,1
"Edit: ignore this...




Check the wiki",1,w4ld7v,What are good and free online ressources for learning statistics (especially in Data Science and AI)?,AskStatistics,2022-07-21 09:43:33,3
"I found this very helpful:

https://www.sharonlohr.com/sampling-design-and-analysis-3e",3,w4sx10,,AskStatistics,2022-07-21 14:53:30,2
"The problem is another: you will be trying to prove the null hypothesis. So the less data you have, the better. If you have a lot of data, you will be virtually guaranteed to detect non normality.

What you try to do is doomed.

You could assess normality from qq plots, but that's not automated.",2,w4na5v,Trying to create a tool at work that determines whether a parametric test would perform better than a non-parametric test. I know normality isn’t the only assumption made. But it’s the primary concern I have for this use case. The tool will be used by business users so it’s necessary to be automated.,AskStatistics,2022-07-21 11:02:01,12
"You might consider comparing the endpoint of a one sided CI (lower bound) on their win rates, perhaps. 

Or you could take a more Bayesian approach; a suitable prior distribution on win rates should result in more plausible estimates/comparisons.",2,w4jsig,"Hi r/AskStatistics 

I'm looking to try and calculate the odds of someone winning X event for a piece of code I'm writing. I'm aware this can end up being a rather deep rabbit hole but essentially I need it be a bit more advanced than person A has won 3/4 events they've attended and person B has won 1/4.  


I'd like to be able to take some form of weighting (apologies if this is not the right terminology) into account, for example, if we had the following set of data:   


|Person|Events Entered|Events Won|
|:-|:-|:-|
|A|100|33|
|B|4|2|
|C|20|10|

What formula could I look at as a starting point to take into account that A has entered far more events so a 33% win rate is more indicative of success than B's 50% after 4 events?",AskStatistics,2022-07-21 08:39:23,1
Only use paired tests when the same individuals are measured under different conditions. Your scenario is independent samples.,2,w4xk38,"I have matched pairs. I matched them based on what affects the price of the bond.

Now the only thing that should be different (obviously not perfect) that affects the price would be the environmental label.

So I have bond A and Bond B that is identically matched but difference is the environmental label.

&#x200B;

I know paired tests are used when a single person is measured at differnt times for example but could you argue that a paired t test would work well too in this scenario?",AskStatistics,2022-07-21 18:22:18,7
"Yes. Typically people will use the first forecast (at time t+1) as the last observation for the next one (time t+2).

For *simple* exponential smoothing this will produce a flat forecast function

See

https://otexts.com/fpp2/ses.html

Particularly

https://otexts.com/fpp2/ses.html#flat-forecasts",2,w4wyck,"Hey again, /r/AskStatistics. Title... I'm an MBA student and a novice with maths generally, but I'm working on a problem where I'm trying out multiple forecasting methods and comparing and the question arose: how do you / can you even forecast more than 1 period with ES? I'm uncertain what the formula would be. Substituting the previous period's actual data with Ft doesn't make sense in the formula.

Thanks for any responses in advance!",AskStatistics,2022-07-21 17:53:46,3
"It's not graphing that's necessarily the problem here, it's the choice of display. *Don't* use boxplots (I'm guessing; it's a bit hard to be sure) for these data; highly skew counts are not at all a good fit for that sort of display.

Without enough information to even guess at suitable example numbers I won't make any strong graphical suggestions right now,  but I expect there are some potentially useful choices.

A table could work but it would be my last choice rather than my first. It's a decent fallback position if you can't find a better one.

(Personally I'd likely have chosen a different analysis as well but I maybe that's not your biggest issue.)

1. How many values are in each group? Is it few enough to give the data?

2. Given that both grouping variables are important (and that these are presumably the same bees), what are you doing about the impact of omitted-variable bias on your analysis?",2,w4rbau,"Hello,

I'm comparing ordinal variables (first is location, second is source) to the number of mites per bee. The problem is that several of the samples don't have mites; others have a TON of mites. My data isn't normal (right-skewed). So I decided to use a Kruskal Wallis test followed by a Dunn's test for pairwise analysis. Now, I do get significance, but my graph looks incredibly wonky and I don't think it is particularly useful to the viewer. I think since the majority of samples had no mites, the median is zero for all my groups. BUT these is a difference between groups.

So my questions are:

1. Do I just scrap this graph and use the table?
2. Would changing the numbers to ranges help? that way the ranking process is easier?

&#x200B;

[Location](https://preview.redd.it/vomalfqbhzc91.png?width=1854&format=png&auto=webp&s=545252d85a729b8eb061e6eb3d4b6b1c1264c1c0)

&#x200B;

[Source](https://preview.redd.it/ucf33hddhzc91.png?width=1856&format=png&auto=webp&s=8fb8efa31ec95f161e2fdd4d619acdc3922821ea)",AskStatistics,2022-07-21 13:47:13,10
"You are correct that they are the same.  One is just the other in reverse.  Just like p values or cutoffs in hypothesis testing; same thing.

My only addition is that calculating the cutoff is less computationally expensive because you only go from z to a cutoff once.  The other way you calculate a z for each entry.

Edit: z was autocorrected to x",3,w4uybp,"I'm working on a data analysis project in the social sciences where we are determining a person's ""Ego Identity Status"" based on a questionnaire. In the literature, there are 4 Statuses that one can fall into based on their responses. The individual is scored on each of the 4, and a ""cutoff"" value is determined by adding the mean plus 1 standard deviation from the sample. If an individual scores above this cutoff value for a single status, and below the cutoff value for the other 3, they are said to be in that Identity status. The survey I'm working with has modified the questionnaire somewhat, and I need to determine new cutoff values. It seems to me that this process can be simplified by just comparing each Status's Z Score to a standard cutoff value of 1, but is this actually equivalent?",AskStatistics,2022-07-21 16:20:54,5
"I don't understand at all how you believe the title relates to the thing circled in red. 

What's the actual question??",6,w4jr6d,,AskStatistics,2022-07-21 08:37:50,1
"For flat priors (which are not always non-informative!), a posterior is proportional to a likelihood, and frequentists use likelihood a lot; thus there are often parallels. However, you have to be careful not to conflate different conceptions; likelihood for a parameter is not a distribution - it needn't even have a finite integral (and is certainly not conceived in anything like that manner in a frequentist analysis), but a posterior is; while the functions sometimes correspond the understanding of what they mean is different in the two frameworks.

Further, while you do often get a correspondence by treating a prior as flat, in many cases this is highly implausible, or even sometimes ludicrous in its implications. A flat prior on say a mean may be more or less plausible in some situations (where it is not limited to the positive half-line), say as a limit of a sequence of more or less plausible priors, but a flat prior on a variance parameter amounts to saying ""no matter how large a variance you think of, my prior effectively places all of the probability to the right of that arbitrarily large variance"". This is (pretty plainly) nonsensical as an actual prior belief, for almost any real-world circumstance.",4,w4izlh,"I came across [this StackExchange thread](https://stats.stackexchange.com/questions/71782/is-it-possible-to-interpret-the-bootstrap-from-a-bayesian-perspective) on the relationship between a non-parametric bootstrap and a posterior distribution.

According to the answers, a sampling distribution generated from a non-parametric bootstrap is equivalent to a “(approximate) nonparametric, noninformative posterior distribution” for the parameter we’re interested in. 

Does this mean we can always treat the sampling distribution of an estimator as a posterior distribution, and conduct any Bayesian analyses we might normally do with a posterior (assuming we’re willing to accept it as an approximation)?",AskStatistics,2022-07-21 08:06:01,3
"You said it yourself, ""sample groups"", do use sample standard deviation etc. statistics. Regardless, sample stats and population stats converge at large n so go ahead and play it safe.

Population doesn't mean ""all the datapoints i have"", it refers to the idea that you have data for ALL the individuals of a group. So if you somehow managed to get saliva data for every single citizen in a country, sure use N instead of N-1.",1,w4sd9d,"Hi all, I have a question surrounding calculating standard deviation, covariance, doing PCA etc etc. 

The dataset used is the Exasens dataset available through UC Irvine machine learning repository. It is a dataset that includes demographic information on 4 sample groups from saliva samples collected in a research project. 

I am having trouble over whether to use N or N-1.

1. I am asked to standardise selected columns within the data. Rescaling the data to have a mean of 0 and standard deviation of 1. Am I correct in saying using N in this instance is right? 

2. Create a correlation matrix from selected columns within the data. N or N-1 in this instance? 

3. Perform PCA. Which uses the previous code to generate the correlation matrix. N or N-1 in this instance? 

4. Later in the assignment we are asked to create a dataset that has a multi variate normal distribution. Am I right in saying any use of N (standard deviation, correlation, LDA) in this instance should be N rather than N-1 because I have the full dataset?

In advance thank you for your help, got fuzzy brain with this one.",AskStatistics,2022-07-21 14:30:36,1
"Enrolling in a MOOC course would be my suggestion.

Also, the books by Trevor Hastie are pretty good.",1,w4dso8,"I am switching to a more “statistics”related field and I was wondering if anyone here has recommendations, or a guideline about how to go about studying stats, the books, the classics, other resources.
 
I am having a bit of difficulty building a syllabus for myself (for self study). I have done stats and math in school and was good at them but I would like to start to revise all basic concepts again. Thank you in advance!",AskStatistics,2022-07-21 04:01:31,2
"one-way anova has a single grouping factor x with x_n levels, two-way anova has two grouping factors x1 and x2 (lets say with x1_n and x2_n levels). For a two-way anova it's important that the factors are crossed such that each level {1 ... x1_n} of x1 is paired with each level {1 ... x2_n} of x2 (and vice versa). So for example if x1 has levels {red, blue} and x2 has levels {low, med, high} then you need to have observed the groups: red low, red med, red high, blue low, blue med and blue high. Then you can run a two-way ANOVA to disentangle effects of x1 and x2 and their interaction.


Of course any two way ANOVA can also be a one-way ANOVA by making a single factor that contains all the levels of x1:x2. In the example above there are 6 such levels. The F-test associated with the combined factor in this design would test the hypothesis that some combination of levels of x1 and/or x2 has an effect on the outcome, but won't tell you what.


Both the two-way ANOVA and the equivalent one-way formulation of a two-way ANOVA have the same overall fit to the data, they just differ in model terms and interpretations.",5,w4fpr2," I have 2 independent variables and one dependent variable.
Independent variables are thickness and orientation 
The dependent variable is Recognition rate.

I would like to know whether the recognition rate is statistically significant when thickness and orientation are both involved",AskStatistics,2022-07-21 05:41:04,2
"I think I understand what you're asking, but conceptually it doesn't make any sense. 

When you have categorical predictors and a categorical outcome, there **isn't** any mean... You're either in one category or (one of) the other(s) on both sides of the equation.

In vanilla linear regression with effects coding (I.e., anova), you can estimate effects relative to the grand mean of the DV, but in your case, there is no such grand mean,  just one category or the other.",3,w4fhxz,So as the title suggests I am attempting to predict a binary outcome Y using a categorical predictor and using a logistic regression to do this. I know that the odds ratio found is relative to a particular category but is there a way to compute an odds ration of being in a particular category relative to the mean instead?,AskStatistics,2022-07-21 05:30:28,1
"Also, as a coder, may I say, this looks like python. Try reducing alpha on your plots and the overlapping points will be easier to see. 

This does not look “funnel” shaped to me but rather there is a decreasing range. 

I would try different combinations of logarithmic transform. 

That can help make curves in scatter plot tends easier to see.",1,w4cgx6,"I believe I am looking up wrong keyterms when searching for solutions, so i'd appreciate some help.

initial situation: I want to analyse if specific forest stands are more prone to deer damage using drone data. I have a) field survey data: Severity of deer damage on a scale from 1 to 8 and b) drone data: lots of different data about forest stand (tree height, canopy variability, etc.)

The drone data seems to be not correlating with deer damage (using e.g. Pearson or linear regression) I also realised that it is not a normal distribution.

Most of my data plotted as scatter plots look funnel shaped.

(In this case, x-Axis = Severity of deer damage, y-Axis is the standard deviation of canopy cover in a radius of 10 m.)

Basically when deer damage is low (0 to 2), the canopy deviation varys completely randomly on a scale from 0 to 20%. When the deer damage is a 6 or higher, the standard deviation is nearly always below 6.

Is there a statistical method similar to correlation or regression to deal with these funnel shaped scatter plots so I can find areas that are prone to damage?

As a beginner, I believe I am using wrong keywords when I'm searching for solutions!

Thank you!

https://preview.redd.it/24dnz61x6wc91.png?width=752&format=png&auto=webp&s=1a8e438fbb93ae64b3ffb4cf0414e85bc293d02f",AskStatistics,2022-07-21 02:42:55,5
"Linearity =/=> normality. 

Let X ~ Bernoulli(p), and Y := aX + B. Rinse and repeat for any non-normal distribution on X. 

What you are really asking about is linearity of the conditional mean E(Y|X=x) in x. It is easy to see that the same thing holds. Just give Y|X=x any distribution you like, so long as E(Y|X=x) = ax + b",8,w3zmwx,,AskStatistics,2022-07-20 15:33:55,4
"With the fair die thing the most common test of it would be a Pearson chi-squared test for a multinomial with fully specified population proportions, per Pearson (1901).

There's a number of other choices, though.",1,w48qab,"Hi brains trust,

I read a [Medium](https://lakshmanok.medium.com/why-estimating-the-number-of-bots-in-twitter-is-not-an-ml-problem-and-100-accounts-per-day-is-da0382847654) post earlier today where Lak Lakshmanan unpacks how he would use the Central Limit Theorem to show that the approach Twitter took to show that they have less than 5% spam on their platform is valid.

I've been out of the stats game for almost a decade so my recollection of the CLT was that the proof was an absolute bitch to study and remember and I vaguely remembered why it is cool.

After reading the article I started wondering if you would be able to use the CLT to see if a die is fair, so essentially using the proportion of 1s, 2s etc to see if any class is over represented. I know you can use the CLT for Bernoulli type trials, but it feels like you shouldn't just do that multiple times to compare the sides against each other.

(To be honest I didn't really think about the die question immediately - I thought about using the same 100 profile checks a day to identify the proportion of Twitter profiles in specific ""verticals"" i.e Spam, news, adult etc.)

Any pointers would be greatly appreciated.",AskStatistics,2022-07-20 22:49:42,1
What have you tried so far?,7,w3tl9q,"How do you derive the formula for pooled variance aka variance of two datasets. Specifically, this formula:

&#x200B;

https://preview.redd.it/i4l97t26nrc91.png?width=588&format=png&auto=webp&s=e5c40416567a3244d3bcbdbcc5f649536476734b",AskStatistics,2022-07-20 11:25:00,3
"ROC and PR are mutually complementary and convey information about different aspects of your analysis. I suspect that the source where read about a PR curve being ""more appropriate"" for certain data sets, really meant to convey that under certain scenarios, the PR curve will reveal problems that the ROC will not...and that seems to be the case here. 

If I was ever forced to evaluate my models based upon only either PR or ROC, I would pick PR (but you really need both for either to have meaning). 

EDIT: found this nice[ explanation from stackexchange](https://stats.stackexchange.com/questions/354704/what-does-it-mean-if-the-roc-auc-is-high-and-the-average-precision-is-low):

>The more intuitive meaning of having a high ROC AUC, but a low Precision-Recall AUC is that your model can order very well your data (almost of of them belong to the same class anyway), but high scores do not correlate well with being positive class.",3,w3q0pf,"Hi,

I have read that for ML models based on imbalanced data sets, precision-recall curves are more appropriate than a ROC curve. My data set is 70-30 %, which is only slightly imbalanced from what I can gather.

My questions are 1) would a ROC curve be okay for my ML model considering the class distribution? Would also give balanced accuracy, sensitivity, and specificity. 2) Can a precision-recall curve be an inappropriate or a bad choice for presenting an ML model in specific scenarios?

My ROC curves, AUROC, confusion matrix, balanced accuracy, sensitivity, specificity, and F1 all look quite good, but the  AUC-RP is not impressive...",AskStatistics,2022-07-20 09:01:45,4
"There is literally zero reason to do this. Assessing whether something is a confound of the variate composed of the linear combination of all your variables is unlikely to be revealing. Simply include all such potential confounds as covariates in your normal model, they will be controlled for.",1,w43y8z," Hi All,

I'm currently undertaking my thesis project and my supervisor has advised me to assess the influence of my proposed confounding variables on my IVs and DVs by running a MANCOVA. She wants the MANCOVA dependent variable to be a composite of all of my IVs and DVs across the regression model that I'm running, and the independent variables for my MANCOVA to be the proposed confounds (e.g., age, gender etc.).

I'm wondering what your thoughts are on this - I haven't seen other papers that do this. I'm concerned that it won't give me an accurate picture of whether my proposed confounding variables are actually confounds, because the omnibus MANCOVA is telling me whether my proposed confounds are related to a \*composite\* variable rather than the raw variables. What are your thoughts?

Thanks!!",AskStatistics,2022-07-20 18:48:48,1
"> In a simple linear regression does the response variable yi have to be normally distributed?

1. No. If you're using the regression for tests or CIs or PIs, then it's the conditional distribution of Y that's assumed normal, not the marginal distribution (the thing you're looking at is not the thing about which there's an assumption). 

   If you're not doing tests or intervals, there's no distributional assumption needed at all.

2. Even if it wasn't conditionally normal, that may not matter all that much - it's not necessarily going to badly impact the properties of whatever you're doing with it. Most tests you might do are reasonably robust to non-normality if the sample sizes aren't small (the other assumptions may matter more).  If you care about power, that might sometimes be worth worrying about


If it is an issue for some reason, with simple regression there's generally things you can do about it (e.g. you can construct nonparametric tests of the slope easily enough).",2,w3yhya," 

In a simple linear regression does the response variable yi have to be normally distributed?

I'm reviewing some notes that says that the OLS can be expressed as a linear combination of the response value yi which shows that the OLS are also normally distributed.

But it is not immediately clear to me how it was established that yi is normal?

Does this imply that when building a SLR, a sample of normally distributed yi must be selected?

I would appreciated some insight from people with a solid grasp of statistics",AskStatistics,2022-07-20 14:46:58,10
"Why did you differentiate between k-means and K-Means?

The colours indicate the different clusters the data has been grouped into.

This might be helpful: https://scikit-learn.org/stable/modules/clustering.html",1,w42ri1,"Hey all, so i took data comparing wages earned vs hours worked. I used 5 different clustering methods and then put them each into their own scatter plot. 

Im a bit confused. Here are screenshots of each scatterplot for each method

1: used AHC (Wards method) https://prnt.sc/ngn7CWB7njUN

2: used AHC (complete linkage) https://prnt.sc/zNY9ROnChBAR

3: K-Means https://prnt.sc/nxrSUNceH5O8

4: k-means https://prnt.sc/8sIAxqhTg223

5: AHC (complete linkage) https://prnt.sc/cd71KbOjaNM0

What exactly is this telling me? what do the different colors mean? i get they're categories but im not really sure how i compare and contrast.",AskStatistics,2022-07-20 17:53:37,1
Hello there I am a statistician and I can help kindly add me on discord,1,w42k01,"I’m interested in a couple online grad programs but didn’t have the best undergrad GPA (below 3.0, had a couple hard times in college due to life things lol, and actually took some time out as well to deal with them). I did my bachelors in stats and work in industry now, but I’m afraid that my under grad GPA will hold me back. Anyone been in my position and have any tips? Also any grad programs you would recommend in particular?",AskStatistics,2022-07-20 17:43:57,1
https://lmgtfy.app/?q=usa+may+holidays,4,w3wmjf,"During my research over the past few days I have encountered google search statistics for certain words (especially ""civil war"" in the US) which had a significant spike in searches every single may. Now I am not a US citizen and therefore I do not posses extensive knowledge about the details of US history.

My question on this topic is whether these spikes may be attributed to a cultural / historic reason or if there is a statistical reason for these annual spikes.

Link to the statistic: [https://trends.google.com/trends/explore?date=all&geo=US&q=civil%20war](https://trends.google.com/trends/explore?date=all&geo=US&q=civil%20war)",AskStatistics,2022-07-20 13:29:43,7
"For intro stats I would expect a reasoning like:

Standard deviation is a measure of spread, so which set has the most spread? C. It could go from 4 all the way to 10.

If you're writing up a solutions guide, maybe calculate the sample standard deviations?",3,w3sisv,"So, I was making a model answers sheet for a friend taking an introductory course in statistics -- the very basic stuff, basically highschool stat. And I wanted to answer this question without doing a lot of calculations. I am pretty sure most people here could just look at that Q and say the answer, but I cannot write that in the answer sheet.

I wanted to write that since these datasets follow a roughly linear progression (the last one is not strictly linear), so if we add the first and last term in each set and keep doing so, we get a constant value. Then, we multiply that constant value by the number of pairs to find the sum.

It's the same idea Euler used to figure out the sum of consecutive number but extended to all linear sets not just sets which increase by a value of 1.

Then, having found the sum we could just divide by the number of elements in each dataset; this will give us the mean.

Now we could calculate a quick estimate for sd by taking the average of the distances of each point in each dataset.

Now, I know this is not really a simplification -- it's an estimate that takes almost as much calculation as just doing the problem normally. I typing this post to ask about whether one could think about this problem in a data distribution framework (e.g. binomial, normal, etc.)

I thought that there should be a distribution for data that follow a linear pattern, so... is there such a thing?

I presume the mean will always equal the median too, right?

I know I am thinking too much about this simple question, but it's kind of bugging me. And google wasn't of much help.

&#x200B;

[datasets](https://preview.redd.it/yq3x6z0lrqc91.png?width=826&format=png&auto=webp&s=7a4ed4a0857426a32b020cd0e714ba3cd4af1409)",AskStatistics,2022-07-20 10:41:39,3
"I’m a statistician, and would go for the second one, it feels more like an extension, while data analytics tends to be easier to learn on your own",16,w34zge,"Would an MS in Data Analytics or an MS in Applied Statistics and Decision Analytics be better?

Some background: I have a BS in industrial engineering and looking at getting into a data science program. I am interested in SQL and related skills. Schools are ranked about the same and tuition waived.",AskStatistics,2022-07-19 15:06:56,10
"I only did a quick skim, but the context of that part appears to be an explanation of the benefits of random sampling. The authors then point out that despite its power it’s not a universal tool. Not every goal can be achieved with probabilistic sampling – some require as much information as possible. It’s framed with this question: “So when are massive amounts of data needed?” Google indexes are used as an example of a problem where sampling isn’t appropriate. Both because of the relative sparsity of rare content versus rare search queries, and because the users expect it to be (as close as possible to) a full catalog.",3,w3693q," Hi !  
I'm reading "" Practical Statistics for Data Scientists "" , but I didn't understand very well this part ""Size Versus Quality: When Does Size Matter? (page 70)"" the author use Google search queries as an example of data that big and spare at same time so we need to give care to the size of data more than the quality , but isn't data science for analysis ""big and spare data"" is these two elements ""big"" and ""spare"" enough to say we should focus on the size more the quality (Random sampling) ?",AskStatistics,2022-07-19 16:01:25,9
"When your alpha value (a) is 1 your giving the largest weight to the most recent period t - 1. When your data is highly seasonal (especially in the case of electricity) it’s going to want to use the most recent period to fit to the current period because they are usually in the same season IE: Winter - Dec, Jan … Summer: July , August

When you take out the seasonal variation you will have noise + trend.",1,w327xn,"Hey r/AskStatistics, I'm an MBA student and am currently in an operations management course. I have virtually no background in statistics and maths in general, so bear with me if I'm just asking a dumb question. Genuinely inexperienced with any of this (liberal arts background and work in media).

I have a problem that I'm working with where we're building a forecast model for electricity usage based on monthly usage statistics for the last 50 years. The instructions specifically state to *not* de-season the data until later and to first use exponential smoothing to develop a model with **α**  = 0.8, then try out different **α** values, as well as using solver, to lower MAD, MSE, and MAPE.

1 is always the best **α** across the board, for all measures. This seems like an errant result to me. But when I create a seasonal index and apply it to adjust the raw demand statistics and make a model from *that*, the **α** values start to seem like they make sense, around 0.1xx, varying depending on the target measure to minimize.

**So, really, my question is:** If data with notable seasonal variances are not de-seasoned, can that mean the seasonal swing creates such a high level of variability that just makes **α** peg to 1.0 so it's just the most responsive possible? Or am I misunderstanding the meaning of the **α** value?

Thanks in advance for any response!",AskStatistics,2022-07-19 13:11:25,5
"*Odds-ratios* are the usual way to compare two sets of odds.

https://en.wikipedia.org/wiki/Odds_ratio

If you want to take differences, the usual thing to do is look at differences of *log-odds*.",2,w386jb,"I have a group of predictors based on logistic regression (mortality as an outcome) and a stratified population based on mediating binary effect (obese vs non-obese). After calculating the odds for one group, I want to compare it to the odds from the other group. Should I simply calculating the difference as OR1-OR2?  


I'd appreciate any help and guidance.

The only thing I found so far in the internet was this 

[https://www.cancer.gov/publications/dictionaries/cancer-terms/def/relative-odds](https://www.cancer.gov/publications/dictionaries/cancer-terms/def/relative-odds)",AskStatistics,2022-07-19 17:29:21,1
"If a respondent has not yet reported the event by Wave 3 then it is *right-censored* starting at the age that they last responded. Otherwise you just use the age they reported in the survey. The more precise the data, the better. If they never responded, then you don't include that data.

There is no reason to weight observations according to how many respondents have reported the event. That should be accounted for in the likelihood calculation assuming you include all the data (one point per respondent).",1,w32zdd,"I am working on a project where I am estimating a survival model of time until an event. Data comes from 3 different waves of survey data, where individuals at each wave could report the age this event happened (if it did happen yet). I have set up the analysis but I cant decide which survey weights (a design weight plus an attrition weight for that wave) should be used. Would I assign a survey weight for Wave 2 if the event was reported at Wave 2? What about if they reported a date of event at Wave 3 that took place before Wave 2 (and they skipped the Wave 2 survey)? 

I can't find any examples of this in existing papers. Is this because weights aren't typically used? Any info or pointing me in the direction of some empirical papers on this would be awesome!",AskStatistics,2022-07-19 13:43:05,5
">the log of my dependent variable as it has nicer properties

Nicer how? Why is this important for estimating quantile regressions?

I recommend that people who take logs of dependent variables should read Manning (2001) ""Estimating log models: to transform or not to transform?""
https://www.sciencedirect.com/science/article/pii/S0167629601000868",1,w2v9gj,"Hello everyone! 

I'm working on a quantile regression problem, in which I see that I could work with the log of my dependent variable as it has nicer properties. But I'm worried that the predicted quantiles could be wrong when I do the inverse transform of them. 

Could it be a problem? Or would the quantiles be greater than the ones I could estimate using the quantile regression over the untransformed variable?",AskStatistics,2022-07-19 08:19:05,2
What type of weights are you using?,1,w2nazf,"I'm running two regressions. One with weights and another one without. When running them, my results indicate that some unadjusted variables are insignificant, but those same variables are significant when weights are inputted. How do I interpret this?",AskStatistics,2022-07-19 01:10:30,2
"I asume you have the internet data usage on different countries? You should analyse if the variation of internet usage within each country is somewhat correlated with the difference in GDP between countries. There's a lot of ways to so this, maybe for starters you could just calculate the mean and std. dev. within countries and plot them together with the GDP to inspect if there is any visible connection. 

You could also do a regression on all data, where the GDP would be constant within countries. No issues with doing that in terms of methodology. 
However, you need to be sure of what your assumptions are. If you have a time dimension in your data and the GDP is the only thing varying between countries, you'll get a coefficient that accounts for all differences between countries. If for instance unemployment varies, then that will affect GDP as well. If unemployment is not in your model, then it will go into the GDP coefficient. 
Personally I would include more explanatory variables for each country to avoid this sort of bias.",1,w2p7ae,"Hey, so I am writing paper for my degree, in which I am interested on analyzing the effect of internet usage on productivity of a particular economic sector (let's just say, farming sector). There is 2 main variable here:
- Internet usage(at work), which is just what it says. The data is taken on household level, but there is also a variable which separates all the individual by economic sector.
- Productivity, to measure this, I use GDP of farming sector, which is taken on national/aggregate level.

This arises my question, is it okay to do a statistical test on a different level of data? If yes, then what statistical method should I use?",AskStatistics,2022-07-19 03:14:51,4
"Both could be appropriate. In a pooled model, you would simply include every racer observation with no (respondent-specific) controls. In a fixed effects model, you control for racer ID and therefore cannot estimate any cross-racer effects (e.g. gender, if it's a constant per racer). Random effects are partial pooling. You estimate a global intercept with a variance and then you draw racer intercepts from that distribution. Because your random effects are shrunk to that global mean, they don't account for all the between racer variance and you can therefore also estimate (probably biased) between-racer coefficients.",4,w2egpk,"Hi all,

It's been a while since my college stats classes, and I'm a little fuzzy on the exact assumptions needed for mixed/fixed effects models. As an example, let's say we're trying to model the effect of age on a person's 5k time, and we have a dataset of race times by person, by year (so multiple observations per person at different ages).

From what I remember, the naive OLS way would be to regress *race time* on *age*. We don't want to do this because there are multiple observations per person, which violates the OLS assumption of independent observations. We can introduce a random effect for *person* to the model, which ""allows"" for each person to have their own intercept and lets us see the within-subject effect of *age* on *race time*. I believe this is a pretty standard way to deal with multiple observations per subject (correct me if I'm wrong).

However, what's the difference between 1. using a mixed-effects model and 2. using a fixed-effects model but using dummy variables for each person? Isn't 2. also effectively allowing each person to have their own intercept?  In essence, why can't we just regress *race time* on *age + Person A* \+ *Person B* \+ ..., where ""Person \[x\]"" is a dummy for a particular person in the data? ",AskStatistics,2022-07-18 17:12:29,3
"I agree (with some potential caveats). 

They'll have the issue of omitted variable bias, and even if the IVs were orthogonal, omitting them would lead to inflated residual variance.

I don't understand why it's so popular, omitted variable bias would seem to make a nonsense of many such analyses.",3,w2m1ic,"In many papers in noticed that when authors have let's say 3 IVs, several control variables and only 1 dependent variable. They run 3 separate regression models for each IV while controls ans dependent remain the same.  (I don't mean separate type of regression model) - it's the same let's say OLS.  

Why do they do that? If each IV is important and there is no multicolinearity they should all be in the same model right? 

Thanks",AskStatistics,2022-07-18 23:48:07,4
"Wow.  Honestly, this is not beginner Stats stuff and shouldn’t be done with out very very clear guidance from someone that knows what they are doing.

1) multivariate analysis is a whole AREA of stats and is a grad level course.

2) you do not have all nominal random variables.  Weight, BMI, cholesterol are continuous numeric. Heart rate the way you have it is ordinal.  The others are binary and categorical.

3) missing values can be handled on multiple ways.  Dropping rows with missing data or imputing.  Decisions on imputing are complex and it takes skilled EDA.

4) you are probably looking and a log reg or another model used to predicting our dependent variable.  This takes training in model selection, feature creation and selection, and training in interpretation.

Any mgr saying go do “multivariate analysis” to someone without formal training in the subject is just handing someone a loaded gun.  This is why I see so much Stats BS in papers in other fields.  Terrifying.",4,w2bgia,"Hi everyone! Hope you guys can help me out, I'm doing a research project and have to use a statistical tool like SPSS as part of the data analysis, but I'm not very good at stats so I would appreciate any and all guidance you all could provide! 

Specifically, I'm doing a clinical study on heart disease and we are looking to see if certain factors can help predict that the patient may have heart disease. So far the data I have collected is: whether the patient has a formal heart disease diagnosis (which I believe is my dependent variable?), as well as potential predictive factors like weight, BMI, cholesterol levels, presence/absence of heart murmurs, whether they have a fast/normal/slow heart rate, and whether they exercise more or less than 15 minutes per day. 

I've done a little reading into SPSS and i understand that there are certain classifications for variables; I think I have collected data on nominal variables so far (please correct me if I am wrong). At this point I have two questions: 

* What statistical test should I use in SPSS, and how should I properly set it up? My manager has not been very helpful, she suggested I do multivariate analysis, and something about a ""rock analysis""? I dont know what this all means....

* I don't have data on all of those variables for all of those patients? What should be done about the patients who I do not have, for example, whether or not they exercise 15 minutes a day? 

Thank you very much everyone! Looking forward to learning from your responses!",AskStatistics,2022-07-18 14:57:59,6
"I think that there's nothing statistically wrong with this statement. If you assume a normal distribution of stupidness, it is true that half of the population will lie under the average (expected) value. The question then becomes: how do you measure stupidness and given the answer to that, is normality a justified assumption? I think in most cases that the answer to the last question will be yes.",47,w1vpt0,,AskStatistics,2022-07-18 03:13:26,39
"> when is it correct to guess at random?

If you *can* eliminate at least one option from consideration (""Well, obviously it's not B"") ... then choosing at random from what's left is advantageous (in that if you're right in your eliminations your expected score goes up). The more answers you can confidently eliminate the better.",6,w288q6,"Hello people of the internet.

Very soon I will take what's probably the most important test of my career and I want to maximize my chances.  
The test is as follows:  
\- 140 questions  
\- 5 possible answers, only 1 is correct  
\- correct answer: +1 point  
\- wrong answer: -0,25 points

My question then is: when is it correct to guess at random?   
The expected value when guessing completly at random is 0, but almost always at least 1 answer is obviusly false and more often than not I can rule out 2 answers.   
 Would it therefore make statistical sense to guess between 4 answers? And 3?   


Thank you for your time :-)",AskStatistics,2022-07-18 12:42:30,12
"When there is only 1 ~~dependent~~  **correction: explanatory variable**, reporting the p value of the correlation is redundant with reporting the regression results. The p value for the slope will be precisely the same as the p value for the correlation.

Where there is more than 1 explanatory variable, I personally do not care about p values for correlations.  I do care about the magnitude of the correlation between explanatory variables, because that helps inform about potential multicollinearity. 

When one ""must"" report anything is field, journal, reviewer, and editor specific.",2,w2829n,"Hi, 

While reporting corellation matrix of the regression. Something authors have reported the significance (p values) values as well while others have not (specially when there is only 1 dependent variable). 

Can someone please explain me the difference? When do one must report the significance values in the correlation matrix and when not?

In my case I just have 1 dependent variable",AskStatistics,2022-07-18 12:34:56,8
I don't follow. Why would you use a count model for carbon volume?,1,w2ieqt,"Using count data (and the average carbon volume in the taxa of cells counted) I estimated carbon volume in picograms. It’s a time series: my x-axis is time and my y-axis is cell carbon volume in picograms. I was using a negative binomial generalized linear model (GLM) for my count data. Is this still an appropriate statistical model, considering I am now working with non-integer values?",AskStatistics,2022-07-18 20:23:34,3
"Because there's a lot of possibilities very close to 7000 that you'd have to add up. Scaling 7/10 to 10,000 people means anything between 6,501 and 7,499 gets rounded to 7/10",9,w2dmtu,"My intuition is that with a sufficiently large number of people deciding between 'a' and 'b', the long-run result of 70% of people picking 'a' over 'b' should appear.

But it seems that the more people (i.e. _n_) I put into the:

_( n! / k! · (n–k)! ) · p^k · (1–p)^(n–k)_

...formula (where _p_ = 0.7 and _k_ = _p·n_), the smaller probability I return that 70% of the next _n_ people will pick the 'a' option that 70% of people prefer.

For example, if 100,000 people make the decision, then [there's only a 0.275% probability that 70% of people pick 'a' over 'b'.](https://www.wolframalpha.com/input?i2d=true&i=calculator+%5C%2840%29Divide%5B100000%21%2C70000%21*30000%21%5D%5C%2841%29*Power%5B0.7%2C70000%5D*Power%5B0.3%2C30000%5D)

Please help: I'm confused. What's the intuition?",AskStatistics,2022-07-18 16:33:13,6
If you know the totals and the percentages for both leavers and stayers you should be able to multiply those back to the counts you need,1,w27lkd,"H0: Their is no statistically significant difference between engagement scores for participants who stay with the company or leave after 3-months. 

Context: 
Anonymous (i cannot see row level data) engagement test. 

I know %favorable, %neutral, %unfavorable
 -% favorable = % of participants who took test and scored a 4 or 5 (scale 1-5)

I know the total # of those who participated in the test and left within a 3-month window. I do not have row level data for leavers.
 
I also have other dimensional %favorable scores and %favorable scores for items within the dimensions. For example diversity dimension has 2items and engagement dimension has 1-item.

Suggestions are appreciated!",AskStatistics,2022-07-18 12:15:25,1
"Nothing is stopping you from doing a 2 vs. 1 differential gene expression analysis, although it is not typically recommended. That is the bare minimum sample size requirement.",2,w27g6f,"We have bulk RNA seq data from diseased enteroids, but only 3 samples (2 diseased and 1 control). 
What analyses could we run to have results which would make sense? Can’t do GSEA, any other suggestions? 

Thanks!",AskStatistics,2022-07-18 12:09:18,7
"None of that is correct.

""A commonly used interpretation is to refer to effect sizes as small (d = 0.2), medium (d = 0.5), and large (d = 0.8) based on benchmarks suggested by Cohen (1988).""

Source: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3840331/#:~:text=Interpreting%20cohen's%20d&text=A%20commonly%20used%20interpretation%20is,suggested%20by%20Cohen%20(1988).",2,w20oz3,"So I need to check a citation/accuracy of an author who claims to be the original source of classification of effect size into small (. 01) medium (. 09) and large (. 25) and stating that this was first published in 2015.

Comments and corrections or verification very welcome indeed!

Also, for extra value and definite triv bragging rights, not to mention my eternal respect and gratitude, can anyone reliably identify who did create that classification and when?

TIA oh wonderful hive mind of reddit :o)",AskStatistics,2022-07-18 07:26:43,4
"Are you saying that you have something like two options, A and B, and valid observations include A, B, and A+B? If so (also true if “neither” is valid) you can treat this as a chi-squared on the set of all allowable answers rather than just the two non-exclusive options.",3,w1yvu6,"I want to analyze categorical data using the chi-square test of independence. However, the data I've collected doesn't satisfy the assumption of mutual exclusion (for neither category).
Googling for this didn't help much as you mostly get beginner tutorials on the regular chi-square tests. Is there anyone who knows any alternative or a means to mitigate this issue?
Thanks in advance! :D",AskStatistics,2022-07-18 06:04:52,4
"In the usual log-link Poisson regression model the fitted lambda is a function of the predictors.

specifically exp(x'b) where b is the vector of fitted coefficients and x is the corresponding set of predictors

I'm not sure how calculating an average lambda for the data set helps you any.",1,w2a56v,"Hi, I am currently trying to test a poisson regression model and was wondering how I can figure out what lambda would be. In the dataset, I am given 3801 ticket requests in a 83 day span, so do I just divide 3801 by 83 to get the ticket per day rate? I am utilizing python packages and methods to assist with the model development. I am trying to formulate a model to predict the number of ticket requests a company receives on a daily basis and seeing the effects of several categorical variables. 

TLDR: how to figure out average rate of Poisson distribution given dataset",AskStatistics,2022-07-18 14:01:55,7
"Note that the expected number of trials for N=10 is not exactly 30, it's Nx[H(N)](https://en.wikipedia.org/wiki/Harmonic_number)=7381/252, or about 29.3.

As in that problem, here as well we can use [linearity of expectation](https://en.wikipedia.org/wiki/Expected_value#Properties): consider N [indicator random variables](https://en.wikipedia.org/wiki/Indicator_function#Mean,_variance_and_covariance) A(k), equal to 1 iff we have observed the k-th coupon at least once in t trials.

Then the desired expected number X of observed unique coupons is the sum E(X)=E(A(1)+...+A(N)), which by linearity is the sum of E(A(k)), which in turn is NxE(A(1)), or NxP(A(1)=1):

    E(X) = N * (1 - (1 - 1/N) ^ t)",3,w1wi9i,"My problem is based on the coupon collector's problem. If you're not familiar with it:

Given N different coupons from which coupons are being drawn independantly, with equal probability and with replacement: How many coupons do you expect to need to draw before having drawn each coupon at least once? 

So if I have 10 coupons, the expected number of trial to collect all 10 is 30. 

My question is, how would I determine the expected number of unique coupons at trial t?",AskStatistics,2022-07-18 04:01:01,2
"None. The whole point of statistical testing is to deal with variability. If you have two values and that's it, all you can do is compare them. Without variation, there is no statistics.",28,w1ovc7,only 1 subject for both groups,AskStatistics,2022-07-17 20:05:03,29
"If you want an omnibus test the usual one would be the chi-squared. If the numbers are similar to these you might run into issues with people complaining about the expected count being below 5, though an expected like you have here (all of them 4 rather than 5) shouldn't be much of an issue. You can deal with that however, if you expect an issue on that front (e.g. via simulation, to get 'exact' p-values).

If you wanted instead to just do all pairwise tests that would also be possible; again  you might have to consider what referees (and later, readers) in your area of work would get into their heads to argue with. If you do multiple testing, of course, people may expect you to adjust for that.",2,w1inby,"I’m not very familiar with statistics but it is necessary for a research paper I am writing, and am unsure what kind of test I should use. 

I’m comparing 3 different drugs and if they cause GI side effects. I used a Yes/No system for collecting data. 

For example:      
 —————-Y          N

Drug #1     7         12

Drug #2    5         14

Drug #3    0         19

Those are not my actual values.

Anyway, I want to figure out if one drug causes side effects more often (more Y’s than N’s) in a way which is statistically significant in order to argue that in my paper, or accept the null. I have 35 data points for each drug. What test could I use and how would it work? 

Thanks!",AskStatistics,2022-07-17 14:59:45,5
"OP here. I got some up votes but no answers which is fine. 

I have reached the conclusion that they have the same likelihood and that they just look similar are different. 

Anyway I decided to see how these two entities behave when I stick a new element in. 

Here is a reputable Mann Whitney U Calculator. 

https://www.socscistatistics.com/tests/mannwhitney/default2.aspx

It requires 5 values in each. So let’s use similar contrived data to before.

The site warns that small n data suffer from a less reliable approximation to the normal. 

TLDR they behave similarly when you stick an element  on one them. 


FIVE TIES 

s 1 2 3 4 5
t  1 2 3 4 5

Result Details

Sample 1
Sum of ranks: 27.5
Mean of ranks: 5.5
Expected sum of ranks: 27.5
Expected mean of ranks: 5.5
U-value: 12.5
Expected U-value: 12.5

Sample 2
Sum of ranks: 27.5
Mean of ranks: 5.5
Expected sum of ranks: 27.5
Expected mean of ranks: 5.5
U-value: 12.5
Expected U-value: 12.5

Sample 1 & 2 Combined
Sum of ranks: 55
Mean of ranks: 5.5
Standard Deviation: 4.7871

The U-value is 12.5. The critical value of U at p < .05 is 2. Therefore, the result is not significant at p < .05.

Result 2 - Z-ratio

The Z-Score is 0.10445. The p-value is .92034. The result is not significant at p < .05.





FIVE TIES THEN A 6

s 1 2 3 4 5 6
t  1 2 3 4 5

Sample 1
Sum of ranks: 38.5
Mean of ranks: 6.42
Expected sum of ranks: 36
Expected mean of ranks: 6
U-value: 12.5
Expected U-value: 15

Sample 2
Sum of ranks: 27.5
Mean of ranks: 5.5
Expected sum of ranks: 30
Expected mean of ranks: 6
U-value: 17.5
Expected U-value: 15

Sample 1 & 2 Combined
Sum of ranks: 66
Mean of ranks: 6
Standard Deviation: 5.4772

The U-value is 12.5. The critical value of U at p < .05 is 3. Therefore, the result is not significant at p < .05.

The z-score is 0.36515. The p-value is .71138. The result is not significant at p < .05.





TEN TIES

s = 1 1 1 1 1
t  = 1 1 1 1 1

Result Details

Sample 1
Sum of ranks: 27.5
Mean of ranks: 5.5
Expected sum of ranks: 27.5
Expected mean of ranks: 5.5
U-value: 12.5
Expected U-value: 12.5

Sample 2
Sum of ranks: 27.5
Mean of ranks: 5.5
Expected sum of ranks: 27.5
Expected mean of ranks: 5.5
U-value: 12.5
Expected U-value: 12.5

Sample 1 & 2 Combined
Sum of ranks: 55
Mean of ranks: 5.5
Standard Deviation: 4.7871

Result 1 - U-value

The U-value is 12.5. The critical value of U at p < .05 is 2. Therefore, the result is not significant at p < .05.

Result 2 - Z-ratio

The Z-Score is 0.10445. The p-value is .92034. The result is not significant at p < .05.




TEN TIES THEN A 6

Sample 1
Sum of ranks: 38.5
Mean of ranks: 6.42
Expected sum of ranks: 36
Expected mean of ranks: 6
U-value: 12.5
Expected U-value: 15

Sample 2
Sum of ranks: 27.5
Mean of ranks: 5.5
Expected sum of ranks: 30
Expected mean of ranks: 6
U-value: 17.5
Expected U-value: 15

Sample 1 & 2 Combined
Sum of ranks: 66
Mean of ranks: 6
Standard Deviation: 5.4772

The U-value is 12.5. The critical value of U at p < .05 is 3. Therefore, the result is not significant at p < .05.

The z-score is 0.36515. The p-value is .71138. The result is not significant at p < .05.",1,w1exh1,"I am mulling something over for some self directed work. This is not home work that I have been set but I am using it for a dissertation, some post grad work. Happy to cite or be pointed to citations.

We are talking about a Mann Whitney U type situation. Comparing two groups. School running teams, lengths of fish from two ponds, etc. 

Or, more mathematically, please imagine two multisets s and t. They are sets which can have more than one instance of each element. They both contain real numbers.

Let’s say there are four in s and four in t. Let’s assume they are two samples from the same big set we can call it B.

Basic question is; are these two contrived orders as likely as each other? Or is one more likely?




Order 1  “four equal pairs”

s = 1,2,3,4

t = 1,2,3,4

s  s  s  s

t   t   t  t 




Order 2 “all the same”

s = 1,1,1,1

t =  1,1,1,1


s

s

s

s

t

t

t

t


These have same U statistic, unless I am mistaken. 

I can’t put a cig paper between them intuitively either. But it feels like maybe they should be different. I can make arguments either way. 

So, if you are interested or have knowledge…Do you think they are as likely as each other under the assumption they are from the same population? Happy for intuitive answers. 

I am coding all this in Julia open source soon, for the curious. This is a public facing safe-for-work account.",AskStatistics,2022-07-17 12:13:13,1
"This is more of a mathematical question, but let's hone in on what you are trying to do a bot more.

1) Define what **you** mean when you say ""deviation"". This has precise meanings in statistics, and don't want your question to get misinterpreted.  Do you simply mean ""distance between two points""?

2) If so, the on to the x-rays. a ""normal distance"" is anything up to 20mm. I assume these x rays are taken from a specific angle, correct? If not, I could move the imaging axis and line the two points right on top of one another.  So, the crucial question becomes, is this standard x-ray angle one that is perpendicular to a line drawn between the two points (thus maximizing the distance between them)?  If so, then this is the correct distance.  If not, then who knows what the actual distance would be.

3) Do you actually have the two Cartesian points? Then just calculate the distances, and compare directly to x-rays, **assuming that the distance in x-rays is correct**.
https://www.cuemath.com/3d-distance-formula/

>the mean deviation in the z axis was 8 mm

4) It seems to me that the angles, coordinate system (and its rotation) and measurement technique of this ""deviation"" are important here. Suppose I have a 20cm pencil on my desk, and I am looking straight down on it. Imagine the coordinates are (0,0,0) and (0,20,0).  If I lift one end of the pencil up off the desk by 8cm, holding the other end at (0,0,0), then the coordinates become (0,18.33,8) using the Pythagorean Theorem, or 3-d distance formula. But this is a simple case since I am starting from a known coordinate system and angles.

5) Once we get the rest sorted out, then: If 8 is the mean, then being above the mean should not be what defines abnormal.",1,w1dw31,"Hi guys,

I would be pleased if you could help me with my research. At the moment I am analysing three dimensional deviation of two points. For the same data as a two-dimensional data set(x-rays) the ""normal"" deviation is defined by other authors as up to 20 mm. In three dimensional imaging there is no threshhold defined. However, the mean deviation in the z axis was 8 mm. Whats a possible solution to formulate the three-dimensional deviation? Should i consider my measured results or add X to the defined 20mm?

Best regards!",AskStatistics,2022-07-17 11:28:07,1
What are you trying to learn from your aggregation?,1,w19fum,"I have a list of studies of different variables for various devices. E.g. age, procedure time and so on. Some of the variables had means and some had medians specified. I wanted to aggregate the values of parameters. I did mean of means but how do I aggregate medians from the studies? Any ideas would be helpful. Thanks!",AskStatistics,2022-07-17 08:05:21,3
[deleted],1,w14sf0,"I have a dataset that contains information on what an individual likes! The question is posed do you get energy from ..... (my X). These X variables are categories. One of these categories is whether someone gets energy from and enjoys playing \`sport\`. It is true if someone answered yes and false if they answered no.

&#x200B;

The target variable is whether someone passed an undergraduate course. This is a binary variable 1 indicating a pass and 0 indicating a fail. 

&#x200B;

504 students indicated they enjoyed and gained energy from playing sports. \`66.4%\` of these students passed the course. 244 students indicated they did enjoy playing sports or gained energy from it. \`65.9%\` of these students passed the course. This seems like a very small difference and given that the dataset is not that large I would not expect this to be significant. Yet, when I perform a logistic regression on this dataset (including the other categories such as social activities, events, entertainment) it shows that sport is highly significant (\`z-value\` of 3) and has a coefficeint of \`0.43\`. If I correct interpret this coefficient value it would mean that someone that enjoys sports has a 53% higher chance of passing the course. This value does not pass my sanity check when comparing it with the mean and the sample size of both groups. What is going on here? What am I missing?

&#x200B;

I am using the \`statsmodels\` api with the \`Logit\` model. 

&#x200B;

Below is the model output generated by the \`summary\` method. 

&#x200B;

[https://gyazo.com/0616c6eeb576d149274efdf14ad54a2a](https://gyazo.com/0616c6eeb576d149274efdf14ad54a2a)

&#x200B;

I added gender as a control variable.",AskStatistics,2022-07-17 03:52:38,2
"If they were exclusively catching wild Pokemon then this is very unlikely; however, the odds of catching a shiny are greatly improved in certain raids (possibly with research tasks, but you'll have to look that up). 

Overall, if they are focusing on raids, or clicking on every possible Pokemon and only catching certain ones, these numbers aren't impossible by any means.",17,w0zsg3,"Hello smart people,

To give a quick backstory to my question, me and a friend of mine are having a wager on who can catch the most shiny Pokémon in Pokémon go in 1 month. 

From everything that I can find online, the odds of encountering a shiny are 1/500, but my friend has caught 13 shiny Pokémon in 1400 encounters. I know that this is possible, but it seems extremely unlikely. 

I was hoping that someone on this sub could help me better understand how probable this happening was.",AskStatistics,2022-07-16 22:21:10,5
"Saying that ""cross country correlations are among the weakest statistical findings out there"" is kinda weird, but there are reasons to be cautious about country level analyses.


Generally speaking, when looking at aggregated data, we need to be careful about [ecological fallacy](https://en.wikipedia.org/wiki/Ecological_fallacy), i.e. incorrectly assuming that individual characteristics can be inferred from from group characteristics. A good example is the relationship between wealth and party leaning in the USA. Wealthier *states* tend to vote democrats, but wealthier *individuals* are more likely to vote republicans. If we tried to answer question ""Are rich people more likely to vote democrats or republicans?"", looking at state level data would give us completely incorrect answer. The second problem is that aggregation lowers variation. It’s very common for correlations on country level to be much stronger than on the regional or individual ones.


Specifically for the plot shown in the video, this is known as the [gender equality paradox](https://en.wikipedia.org/wiki/Gender-equality_paradox). There are several explanations for the findings and the debate is complicated by the fact that no one except for the original authors seem to be able to replicate the results.",6,w11ya8,"I've seen cross country correlations being used in many of my economics classes.

However, in [Unlearning Economics' latest video](https://www.youtube.com/watch?v=LKc_8fT6pGc) on gender equality, he states 'cross country correlations are among the weakest statistical findings out there' at around 37:30.

Is this true?

I find his first argument about there potentially being countries not included that could break the correlation kind of strange (if my grandmother had wheels she would've been a bike). I could be wrong though.",AskStatistics,2022-07-17 00:41:04,3
"I guess it depends on your exact definition of ""commercial property"".  Is a 50 story skyscraper one property, or multiple?

I'd be surprised if the Census Bureau doesn't have a list-- they have a list of all ""dwelling units"" for people, and they do censuses of businesses. They also do a taxable property values survey.  Somewhere in that data, there should be something more accurate than ""5-54 million"".",1,w0ta48,"What is the best way to go about estimating the total number of commercial properties in the US? A quick google search got me answers between 5.9 million and 54 million, but hoping that I can somehow get a more precise estimate. Maybe using some kind of MCMC? Thanks in advance!",AskStatistics,2022-07-16 16:23:27,2
"First, define your acronyms bruh. Second, link the article. Third, the authors probably made a mistake and the reviewers didn’t due their jobs.",3,w0u9v0,"I'm reading a study that uses the LOT-R, a 10 item scale designed to measure optimism. Only 6 of the items are scored (from 0 to 4) resulting in a maximum possible score of 24. Or a range of 0 - 24. So why is it that when this study reports the mean LOT-R score, the range they offer is 12-29? 

How can that be?",AskStatistics,2022-07-16 17:14:28,3
"> Aka a mlb player buys into their hype and ""rises to the occasion"" because of the hot hand fallacy.

I suspect that this would be confirmation bias. People look for examples of this happening and say, ""Ah ha! Maybe there is something to this!""

But how many players *fail* to rise to the occasion? It wouldn't be a memorable ""Cinderella story"" of some player leading their team through the playoffs, it would just be a forgotten story of a team that had a promising season and fizzled out, or took 4th place. It just doesn't stick out in peoples' minds.",3,w0nyz9,"To set the table...the hot hand fallacy is the phenomenon where expectations are biased based on factualizing a recent trend - eg the Yankees have been dominant in the first half of the season so they will continue to be in the second half. 

That makes complete sense - what has happened doesn't dictate what will happen. My question is - how does hot hand fallacy account for ""players"" who buy into the hot streak and overperform because of it?

Aka a mlb player buys into their hype and ""rises to the occasion"" because of the hot hand fallacy.",AskStatistics,2022-07-16 12:09:21,6
"The type I error rate is the probability of rejection when H0 is true.

So you take the null(+whatever assumptions are made) and simulate from the distributions for each group that this specifies (note that the actual means don't matter; as long as the null holds); you'll want a large simulation size. 

You then count the proportion of rejections. You can also compute a standard error of the estimate (it's a binomial proportion after all).",3,w0ixrq," 

Hi,

I want to ask how can I simulate ANOVA type 1 error rate ? I've been using python.",AskStatistics,2022-07-16 08:16:52,9
"What I generally recommend is to look at regression coefficients when using domain knowledge to decide about the strength of a relationship instead of bivariate correlations. They are interpreted in the original metric, and it is much easier to determine whether, say, a 20 dollar increase in revenue per advertisement aired counts as ""strong"" for the given company or not, than deciding whether 0.67 counts as ""strong"". But frankly, just avoid arbitrary labels like ""weak"" or ""strong"" relationships. Report the results numerically and let the readers decide.",3,w0olv8,"Hi everyone,

I'm a bit stuck on deciding which of my correlations to interpret as weak, moderate, strong, etc.

I read in several sources that I should base my interpretation on my field of study. Yet, when I look for business science, I don't find any clear recommendations. More specifically, my correlations regard socio-economic associations, so a mixture of psychology and business.

Any recommendations or clear sources for me to interpret my correlation strengths?

Thank you very much!",AskStatistics,2022-07-16 12:39:04,3
"This is a classic example that you cannot conclude solely based on p-value. In the grand scheme of things, p-value is just a number; you as a reader should check the context and consider whether or not the authors made the right statistical analysis or used an appropriate design for the study.",5,w07qpp,,AskStatistics,2022-07-15 21:12:33,11
"Going fishing for weights that “work” sounds, well, a bit fishy.

What worries you about the non-weighted results? Have you considered robust regression techniques? Or whether the problem is likely to effect the actual regression line or just the standard errors?",2,w0iqxf,"Hi, 

I am dealing with a regression where I am facing issues with the residuals taking a diamond shaped pattern. Which is also a type of heteroskadacity. To deal with it, I tried different weights (WLS). Apparently using square of fitted vales as weights seems to work. Does it sound right? Are there any other ways to deal with this type of heteroskadacity? 

Thanks",AskStatistics,2022-07-16 08:07:52,4
"Is this U(a,b), or U(0,c) or U(0,1)?

I'll do the third one (standard uniform) since the others are relatively easy from that. Let U~U(0,1)

1. Quick way, if you already know the mean and variance:

    E(U^(2)) = Var(U) + E(U)^(2) = 1/12 + 1/4 = 1/3.

2. Integration: 

    E(U^(2)) = ∫ u^(2) f(u) du   (where the  ∫ is from 0 to 1)  
    . . . . . . = 1/3 u^(3) | = 1/3 (1-0) = 1/3     (where the | is also from 0 to 1)

---

The more general expression for U(a,b) is
 1/3 [b^(3)-a^(3)]/(b-a) or equivalently 1/3 [a^2 + ab + b^(2)] .",4,w0eajb,"Long story short I was included in a technical assessment for a ML eng role and one of the problems required (I think) at a certain stage to figure out the expected value of a squared uniform distribution. 
No need to say, I failed but still I want to know, is there an easy expression for those parameters or an easy way to calculate the integral? 
Thanks in advance!",AskStatistics,2022-07-16 04:14:36,5
"> I noticed all data from the 5 locations in every orchards seperate is normally distributed

That is quite a strong statement...  you have 15 sets with 5 observations that you are certain are normally distributed?

>while the combination of them isn't 

If the individual data were normal, the combination would be a mixture of normals? 

>Is non-normal distribution data a problem in this case? ...could I just use non-normal based tests like Kruskal Wallis and Spearman's rho

Impossible to tell-- **exactly what is it that you want to learn from this data?** There are MANY parametric tests that could be fine with non-normal data, especially if the individual groups are normally distributed.  For example, a regression (or other linear model) that controls for/measures the effect of location would be fine in that case.",3,w0gkcj," 

Hi all,

For my thesis, I collected samples on orchards. I took 15 samples at 5 locations in every orchard. Now I would like to use this data in SPSS to compare the different orchards. But I noticed all data from the 5 locations in every orchards seperate is normally distributed while the combination of them isn't (also tested with Shapiro-Wilk test of normality). I think it's due to differences in pathogens, soils etc.. On different places in the field.

Now my question, could I just use non-normal based tests like Kruskal Wallis and Spearman's rho or would they impact my conclusions too much?

Thanks in advance for helping a desperate student!",AskStatistics,2022-07-16 06:21:06,3
"Combinatorics is a foundational principal in probability, which is foundational in inferential stats.  That being said, few statisticians will find themselves doing combinatoric problems on a daily basis. 

Yes, some problems are tricky, but run towards challenges, rather than away from them.  I recommend the course in The Teaching Company series [by Arthur Benjamin \(Discrete Mathematics\)](https://www.thegreatcourses.com/courses/discrete-mathematics).",3,w0f8ax,"some exercices in combinatorics are misleading me and I feel like it could take years of practice to not get trapped by them, how much combinatorics are enough for stats ?",AskStatistics,2022-07-16 05:10:17,4
"Not necessarily. 

\- why are you declaring things to be outliers? What's the purpose of this activity?

\- How are you defining 'outlier' and 'non-outlier'?",8,w09te5,,AskStatistics,2022-07-15 23:15:40,6
Box-Cox,14,vzpyaq,,AskStatistics,2022-07-15 07:23:29,13
"You’re confusing several things.

First off, a line is a line. I can write y = mx + b or z = kp + q or whatever I want to. We generally try to make things clearer by reserving X for the independent variable and Y for the dependent variable with is a function of X.

Second, a regression is a *model*. In models, there are parameters, which are defined by what model you choose, and there are estimates of those parameters, which come from fitting the model to data. In a simple linear regression, your model says that y (or the average of y, really) is a *linear* function of x, with a slope term (often called beta) and an intercept term (often called beta with a subscripted zero). So in that sense, there’s no difference, as they both describe a line.

But models are fit to data. This gets you *estimates* of beta and beta_0 (or b and c if you’d prefer) which we often denote as different by putting ^ over them (which is said out loud as “beta hat”). Your equation for the regression is a particular estimator for the slope given the data. This will be many points of data and the line will not pass through all of them, it is just a description of the relationship overall, but there’s noise. Your definition of m in terms of two y and two x values is the approach for defining the equation of something you know to be a line based on two points.",3,w061rw,"Hello so in the case of a straight line with equation y=mx+c, m is the gradient and is y2-y1/x2-x1

But when we look at regression lines we write Y=bx+c where b is the covariant coeficient r(sdy/sdx)

My question is,why isnt m=b and what,if any, is the relationship between them?",AskStatistics,2022-07-15 19:41:18,3
"> Can anyone speak to how this is actually analyzed, 


I'm not sure what you're asking there. 

If you're asking ""how do I calculate the mean stat for each method"" ... there's more than one approach you could use but in most of those cases Excel's a lot of work compared to using other tools

If you're instead asking ""how did AnyDice in particular calculate these?"" you'd need to check the code (unless the documentation tells you but I expect it doesn't)",2,vztlm4,"This question is specifically related using dice to generate character ability scores in D&D, and how differing methods compare.

In the current edition of D&D the method for rolling for ability scores is roll 4d6 and total the highest 3 values. Repeat 6 times (once for each ability).

Other editions had options such as roll 3d6 12 times and keep the highest 6 totals.  Or roll 3d6 6 times keeping the highest, then repeating 6 times for each ability.

Using AnyDice (https://anydice.com/articles/4d6-drop-lowest/)  I can find the overall mean as well as the expected outcome for each dice using the 4d6 keep highest 3 method.

Can anyone speak to how this is actually analyzed, and maybe how it could be done in something like excel for all 3 cases I gave above.


Thank you!",AskStatistics,2022-07-15 10:00:46,4
">  from which you take 10,000 samples at random.

Do you mean ""one sample of 10,000 observations""?

(see the meaning of [sample](https://en.wikipedia.org/w/index.php?title=Sample_%28statistics%29&oldid=1060080564)\* in statistics)

>  is it still possible to get statistically significant trends

I'll assume you mean ""if you take a smaller subsample""  

a few points:

1. The use of the word 'still' implies that you definitely had significance in the larger sample, which premise is not a given. You can easily not have significance in the large sample

2. You *can* reject H0 in smaller samples

3. A smaller sample size reduces power. A reduction in sample size reduces the probability you will reject H0 compared to not reducing it.

4. It's not clear what you intend by the use of the word 'trends' here. You may be omitting relevant context; one in which there's 'trends' to be testing for

5. >  If the randomization was uniform, my guess is that the bell curve should preserve its rough shape but become more flat 

    This doesn't make sense to me. The way you're writing there seems to suggest you think the sampling is something other than random. I don't follow the ""more flat"" reasoning at all. 

6. Why would there be a bell curve? What are we looking at? Presumably you're picturing something like a histogram but *of what quantity*? The sample values? Sample means? Some other statistic?


---

\* (link to pre March-2022 version is used here because the article has since been merged with *sampling* and the direct definition of 'sample' previously available for many years has been lost, instead you're left to infer it from other things; I decided to link to the version where it begins by saying what a sample is)",1,w00932,"I was wondering what the effect of taking samples of samples would be. E.g. say, you have a population size of 1 million, from which you take 10,000 samples at random. If you were to then take 1000 samples from the 10,000 samples, is it still possible to get statistically significant trends? If the randomization was uniform, my guess is that the bell curve should preserve its rough shape but become more flat => strong signals may still be able to come through.  
It's been a while since I've taken an introductory statistics class, so please let me know if there is a better term for this process for me to read up on :)",AskStatistics,2022-07-15 14:55:25,8
"> if there should be any difference between including an interaction variables as the product of A x B versus including both variables in the regression and using the interaction command in R studio *?

The * operator adds not only the product, but also the individual variables into the regression.  E.g., x1*x2 adds x1, X2, and x1•x2. If you only want the variables multiplied together, then use the : operator instead. 

>Is an interaction term simply one variable x the other? 

Yes; but usually you will also want the original variables in the equation as well.

>When I include the interaction term in the regression I get very different results than when I include a variable that’s the product of the two other variables. 

Different how? Are you now including the variables by themselves when you ""include the variable that's the produce of the two other variables""?",1,vzps2k,"Hi

I am wondering if there should be any difference between including an interaction variables as the product of A x B versus including both variables in the regression and using  the interaction command in R studio *?

Is an interaction term simply one variable x the other? 

When I include the interaction term in the regression I get very different results than when I include a variable that’s the product of the two other variables. 

Hope this was somewhat clear",AskStatistics,2022-07-15 07:15:33,2
"If the surfaces are defined over the same domain, then functional PCA would be one potential option. Can you explain more about what these surfaces are, and where they come from?",1,vzrngq,"I don't have much background in statistics or geometry, apologies for the vague questions. If I had to be succinct, my questions are:

1. What tools are there for telling when two collections of curved surfaces (without boundary) are similar or different when there is huge variations within one collection and no immediately obvious choice of landmarks?
2. Is there a way to automatically identify features characterizing a statistical ensemble of surfaces? Is there a way to do so and get interpretable results?

If it helps, I would like to try a statistical analysis of organoid geometry. These things are homeomorphic to a sphere, but they can have branch-like structures in different numbers and positions and having different aspect ratios.

I have heard of PCA and diffeomorphometry, but these seem like they have potential issues. The obvious way of applying PCA is to do so on the point clouds of points on the surface, appropriately centered. However, this could only applied to one sample at a time, as it's not clear how to orient two organoids the same way, as the shapes are quite different. I don't think applying PCA on the point cloud is the right approach. For diffeomorphometry, the distance can be large, even within the same class, but it still may be helpful or worth checking out.

My main goal of this post is to learn what kinds of tools are out there. I appreciate any assistance.

EDIT: I would like to add that the processed data will most likely be a triangulation of a surface, not a parametric surface, though I could probably make parametrizations with some effort.",AskStatistics,2022-07-15 08:37:09,4
Statmatch library in R should be able to help.,1,vzzir4,"I have two surveys. Survey A asks questions about personality, demographics, and spending habits. Survey B asks about personality, demographics, and political affiliation. Is there a way to link these data so i can look at personality, spending, and political affiliation at the same time? I was thinking maybe i could pair up similar (based on personality and demographics) cases from studies A and B. Then I would impute the missing spending and political affiliation info based on the responses from the paired observation. What are your suggestions?",AskStatistics,2022-07-15 14:21:37,1
"> I understand that it attempts to show the relationship between a given independent variable and the response variable given that other independent variables are also in the model, but how?

By removing the estimated linear effect of all the other independent variables. Basically, the partial residuals are the values that your response variable would be given that all independent variables were equal to 0 except 1.

> Also, I do not understand why the formula is Residual + Coefficienti + Xi — why do we need to add the effect of the predictor to the residual?

You mean (Residual + Coefficient * X_i), right? Well, this is also equal to (y_i - sum(other coefficients * other independent variables)), so you are only removing the estimated linear effect of all the other independent variables.

> And what residual does it pertain? The specific residual for an observation or the overall average residual?

I do not know exactly what you mean here. 

Partial Residuals are seen as a set of values; there is almost no use of computing the partial residual of one observation. It is generally common to plot the partial residual VS the independent variable to see if the linearity condition for this specific independent variable is a reasonable assumption (assuming all the other independent variable are linearly related to your response variable though)",2,vzmf3r,"I’m quite confused. I’m trying to search for articles related to partial residual but there’s not much going on in the web. 

I understand that it attempts to show the relationship between a given independent variable and the response variable given that other independent variables are also in the model, but how?

Also, I do not understand why the formula is Residual + Coefficienti + Xi — why do we need to add the effect of the predictor to the residual?

And what residual does it pertain? The specific residual for an observation or the overall average residual?",AskStatistics,2022-07-15 04:31:16,2
"Are you talking about deviance in generalized linear models or in some other context? 

what related terms?

Please clarify your question.",3,vzv87f,In the context of GLMs,AskStatistics,2022-07-15 11:10:20,5
"

It would have been useful (and probably more helpful for you) if you had linked the article. Otherwise everyone is just left guessing even more than you",2,vzukba,"Hi all, so I recently read a paper where they performed linear regression on performance metrics of two independent algorithms and plotted them. In the plot they mentioned both the R² and Slope. The two metrics showed high positive correlation (R² ≈ 1) and a slope of 2. The authors mentioned that the second metric (on y-axis) was working better than the first metric (on x-axis) just based on the value of the slope of the linear regression. I am confused about what inferences to draw from the slope of a linear regression?",AskStatistics,2022-07-15 10:42:04,3
"Let's not talk about Covid, because you are going to make some incorrect assumptions about how diseases spread.  Instead, let's talk about a game where:

1) There is a 10% chance your friend has some money, and

2) Every hour he lets you flip a coin, and if you get a heads, you win the money (if it exists)

So, what is the chance that you would win money?  

It will not approach 1.0, because there is only a 10% chance that there is any money to being with.

The chances of NOT flipping a heads after 8 hours is .5^8 = .0039, so the chances of flipping a heads is 0.99609.  Multiply that times the probability that there was any money to begin with, and you get 0.099609.",66,vzcbhi,"Question in title. I'm stuck because I wrongly assumed that it'd be: (1-(1-(.5*.1))^8)

It doesn't make sense that the probability will approach 100% the longer I sit with him. Would it be: (1-(1-(.5))^8)*.1?",AskStatistics,2022-07-14 18:40:03,6
"Why did you think you should be ruling out chi-squared?

(I don't disagree with doing that, just curious to know what your reason was; it might be important)

What is the research question you're pursuing?/ What are you trying to find out?

The DV ""categories"" are not mutually exclusive? e.g. a single infant could be both pre-term and low birth weight, right?",2,vzevio,"What statistical test can be used instead of a chi squared test when the dependent variable is categorical(nominal): infant newborn outcomes (classified as preterm birth, small for gestational age, low birthweight) and the independent variable is frequency of cannabis use (classified as high frequency, low frequency, any use, no use)

Thank you",AskStatistics,2022-07-14 20:48:10,6
look up multicollinearity,2,vzaz5h,"Imagine I had a large dataset where each sample had 1001 different variables, and I wanted to find out if the 1st variable could be predicted by any of the other 1000 independently of each other.

If I ran 1000 separate tests for each of the independent variables, I would then have to do multiple corrections. E.g., Bonferroni, multiple each p-value by 1000.

What if instead, I took half my data, and did the same 1000 tests. Then, took the most significantly associated variable and tested that variable a single time with the other half of my data. Would I still need to correct for the 1000 tests before hand with the result of the test on the second half of the data?",AskStatistics,2022-07-14 17:33:05,2
"It’s a Groups (3) x Trials (4)repeated-measures design. You might want to think about planned comparisons such as trend analysis. Also, make sure to use one of the several available methods for dealing with sphericity violations because sphericity is very likely to be violated and lead to an increase in the Type I error rate. The main effect of groups from the RM design will equal the main effect in a one-way using mean scores of each observation as the DV.",3,vz4g87,"Hi there. I'm working on a project in which I have three treatment groups, and I record the speed it takes for each group to complete a trial on a weekly basis. At this point I have the speed from 4 trials for each of the groups. I think I have to use a repeated measures ANOVA, but I'm not sure and keep getting confused by what I am reading regarding the stats. I would greatly appreciate any guidance that you might have!!",AskStatistics,2022-07-14 12:41:32,12
">  I'm not sure what it means to weigh responses by its survey collection

I don't either. If I had to guess I'd think maybe they meant 'by number of responses'

> Maybe if a very small percentage of participants in an event filled out the feedback survey, then maybe those responses should weigh more because they're under-represented in the survey responses? 

That would make sense -- it's a common thing to do -- but it's not clear to me that this is what was being asked for.",1,vz88g0,"Hello! I'm working on a project, but I feel like I received really vague instructions and I'm hoping I could get some help interpreting them. I was given a file with a few thousand responses to feedback surveys for educational events. The surveys gauge a few different things, like how clear respondents are on their roles and responsibilities and whether they learned something. The instructions also say to weigh each survey by its survey collection. I'm not sure what that means though. I understand what it means to weigh survey responses, but I'm not sure what it means to weigh responses by its survey collection. There's a column in the file called ""Survey Collection ID,"" so I'm thinking maybe it has to do with how many people from each event filled out a feedback survey. Maybe if a very small percentage of participants in an event filled out the feedback survey, then maybe those responses should weigh more because they're under-represented in the survey responses? I'd appreciate any thoughts!",AskStatistics,2022-07-14 15:25:35,1
"I don't know too much of the field, but from reading the paper it sounds like they used observational data and tried to match it to peers based on age and social class? My first thought is that they did not mention anything about whether patients with higher risk of prostate cancer get assigned different  procedures. Perhaps only people with a higher family history of prostate cancer get assigned x-rays? 

And it also mentions participants getting sent a survey, and then talking about 5,10, and 20 year time-frames. So I assume that these were on participants currently living as of the time of the study. This means that it could be that the processes that don't have a higher risk is actually because they never detected the prostate cancer early enough and those individuals never survived. 

I'm not an expert on this field, but it does seem like there are a few unaddressed issues in their usage of observational data",3,vz7mti,"[https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2410119/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2410119/)

If I interpreted this study correctly, then a single pelvis xray 2.5x your chance of prostate cancer in your life. That seems sooo high, can anyone explain that? I've read other places that in general xrays increase your risk like 0.1% or less",AskStatistics,2022-07-14 14:59:16,3
"I’ve seen worse resumes, especially for an MS. I did my MS at a top 15 school having not taken real analysis, so I’d definitely say it’s worth a shot. My other advice would be: stats is not Econ, it helps to go to a highly ranked school but there are a significant number of schools outside the top 15 that are worth going to.",4,vz5zhm,"I work in quantitative finance but I only have my undergrad degree. I have a little bit of impostor syndrome as most of my coworkers and peers hold MS or PhDs in a quantitative field from fairly prestigious universities. I want to get my master or PhD, but I do not think i am competitive enough for it. I really enjoy statistics and I think having advanced knowledge will benefit me in the long run.

I want to stay in this industry but I don’t know if it’s possible to climb up the ladder without a graduate degree. Also, pedigree is very important in finance. Here are my credentials:

Pure Math + CS at a very small regional university. 

GPA 3.75
I haven’t taken the GRE. 
I have a thesis but it is not published and it’s insignificant.

1 year of full time experience in risk modeling/portfolio management. Two data analysis (and data science) internships and 3 years of private mathematics and computer science tutoring. I also have a B+ in linear algebra because I was late to an exam (car got stuck in snow). I have an A in the calculus courses.

I’ve taken all courses a math major would take: Real analysis(B), modern algebra(A), All of Calculus, Chaos Theory(A), Probability theory - really theoretical basically measure theory (B), and some others. 

Do I have a decent chance of getting into a good program, say top 15? I’m worried that my school and LA grade is the limiting factor. I also think my gpa is fairly low. If I don’t have a chance, I’ll just switch industries.


Thanks!",AskStatistics,2022-07-14 13:48:15,7
"So I’m no statistics expert so attach a lower priority on my response but I think this is more of a question of preference rather than statistics. If you want to ask “What is the new employees salary compared to other staff?” or “What is the new employees salary in relation to the whole company?” those are different questions. Each would be better under different circumstances, I’m guessing. But again I’m not the most qualified person on this sub so if anyone has other thoughts I’d wanna learn too.",5,vz07d5,"Imagine 10 employees with an mean salary of 50,000. Now I'd like to compare a newly hire employee's salary to that group. Would I now include their salary and recalculate the mean or can I just compare it against it? Is it ineffective to include the new salary, especially if I want to know if the new employee's salary is unusually low or high compared to the mean?

Edit: I did a little side by side comparison where I made up a range of salaries and took their Z scores. In one range, I didn't include the new hire salary (which I purposely made way above the rest of the salaries) and got a Z score of 3.24. In the other situation, I included the new employee salary into the mean and the Z score is 2.2. I think that the second situation makes it appear as if the abnormally high salary (by intuitive standards) now appears totally normal statiscally but the first example would make it appear extremely improbably that the salary is not an outlier.",AskStatistics,2022-07-14 09:36:58,4
"Well, not a statistician here but I'm interested in the answers.

\---

To answer your **2nd question** it'd be cool to know how bedwetting is quantified, as well as the IVs (if they're discrete, continuous etc). The test you'll perform depends on their data types as well as the DVs distributions.

And why would you perform paired t-test followed by logistic regression?

Regarding your **1st question**, I don't see why not. You're studying low SES participants, so this is your subset. Although there are matching methods or tests that control for covariates.

I don't know what your strategy is, but you should take care if it wouldn't bias your sampling in other ways.",1,vyuci0,"OK, so I am designing a cross sectional study involving the comparison of 2 samples of adolescents between the ages of 11 and 14. One sample will be composed of adolescents who still wet the bed. The other sample will be controls. I have two questions. I am bad at this class, so these questions might be stupid.

Question #1 Can I strategically recruit my controls so that this sample is similar to the bedwetting sample in terms of representation of low SES participants in order to control for the confounding effects of SES? Or do the rules of cross sectional studies dictate that I need to recruit a my controls randomly? 

Question #2 I am trying to investigate the prevalence of delinquency, psychosocial problems, and traumatic life events in the bedwetting sample as it compares to the controls in order to see whether there is a significant difference between the two groups. What are some tests that I can perform to analyze my data? For each variable (delinquency, psychosocial problems, and traumatic life events), should I perform a paired t test followed by logistic regression?",AskStatistics,2022-07-14 05:09:02,2
"Your assertion about multiple DVs is wrong. A mixed effects model describes the type of IVs in the model. Generally speaking, mixed effect models refer to models that capture effects at different units of analysis. A typical example is a classroom, where one observation (a student) has individual effects, but multiple students are in the same class, which also has an effect. By building a mixed model you can capture the effect of the class separately to better estimate any individual student traits you might be interested in. Another particularly useful case is with repeated measurements and longitudinal designs, where you can capture time or individual effects.

The exact terminology depends on the field you are working in. Specifically, econometrics and psychology vary in what exact models the terminology describes. In econometrics, mixed models refer to a combination of fixed and random effects, whereas in psychology it mostly refers to different types of random effect models:

https://en.wikipedia.org/wiki/Mixed_model

https://en.wikipedia.org/wiki/Multilevel_model

Edit: I should add the word ""mixed EFFECTS model"" mostly refers to the econometrics version.",5,vykmw1,"Does anyone have a definition or link to a page that describes this statistical test? Is it similar to multiple linear regression? I am reading a research article that I have to analyze and it has multiple DV and IV variables. From what I understand, multiple linear regression has 1 DV that is explained by multiple IV, in multivariate mixed effects linear regression can it have multiple DVs and IVs?

Thank you",AskStatistics,2022-07-13 19:20:10,6
"> using the multiple hypothesis testing correction

There are many different kinds of multiple hypothesis testing corrections, and some are much more conservative than others. If you used Bonferroni, you might want to read up on Sidak's procedure and Family Wise Error Rates (FWER)/ False Discovery Rate (FDR) and see if some other approach might be better, if your work meets the assumptions.  A decent source to get you started with applications similar to what you are doing is Irizarry/Love's book **Data Analysis for the Life Sciences**, (free online) -- see the section called ""Inference for High Dimensional Data"".",1,vyqbq9,"In a recent project, I did multiple pairwise comparisons between treatment groups. In each comparison, I tested the difference of hundreds of genes. Since I tested hundreds of genes, I used multiple hypothesis testing correction to reduce the false discovery rate. Still, I ended up with many more p-values below 0.05 than would be expected by chance (60 had a p-value of less than 0.05 when I would have expected 20). Still, using the multiple hypothesis testing correction, all of these p-values became adjusted to be >0.05. 

Because I did multiple pairwise comparisons, I found that many of the genes had a p-value below 0.05 in two or more comparisons. Is there anything I can do to take this information into account when determining if it is actually significant?

Thanks in advance!!",AskStatistics,2022-07-14 00:51:38,2
">how can the independent variable predict a change that isn't significant?

I'm not 100% clear about what you are doing, so let me try to make some guesses:

A) By ""change score""  and ""not being significant"", do you mean that you took two scores and computed score2-score1?  And your ""significance test"" was to see if, on average, this change was nonzero? If so, then you are concluding that you ""don't have enough evidence to be confident that the mean of the difference in scores is nonzero"". (Note that ""not being significant"" in your case *could* mean that you tested the  Ho: mu2-mu1=74, rather than Ho: mu2-mu1=0)

B) Whether or not a number has a mean of zero has **nothing** to do with whether some other variable can be used to predict values of that number for individuals. Imagine that we gave a test, then divided people into two groups.  Group A got additional tutoring on the information on the test, while Group B was intentionally taught incorrect information.  **On average**, the difference in scores might be zero, but it is not shocking that which group they were in helps predict the change in their scores.

C) This highlights why it is a bad idea to go around saying things like ""change that isn't significant"", or ""variable that isn't significant"": when you use more precise words to describe what that ""lack of significance"" means in context, it helps make things clearer.",2,vyudw8," So I've modelled some change scores using a multiple regression model, with the change scores as dependent variables.

A predictor of interest (IV) is showing significance for being able to predict these change scores, however paired t-tests are showing there is no significant change in the 2 points the change scores are quantifying.

My question is how can the independent variable predict a change that isn't significant?",AskStatistics,2022-07-14 05:10:59,2
"You can't wait to see the results of that test and *then choose to use a different test after you see the result*. If the p value had been 0.15 or 0.015 ... what would you have done?

If you get to choose to sometimes stick with it and sometimes not, you're choosing to bias the distribution of p-values when H0 is true. Which means you no longer have the significance level you claim you do.

You decide - *before you see data* - how you're going to test your hypotheses. 

However, it sounds like the software you're using is multiplying the p-value rather than dividing the significance level for a Bonferroni. This can be problematic from a number of points of view (and could be the cause of this); I hope that appearance is wrong.


---

By the way you double-posted that; I took the liberty of removing one of the duplicated posts.",5,vyoavs,"Hi guys, i'm hoping someone can help me understand how to proceed. I'm conducting a MANOVA exploring whether three personality trait levels differ between current, ex, and never smokers. Post hoc Bonferroni analysis is indicating a p-value of 1 for one of the variables. Do I proceed and report this as p = 1, or would I be better off conducting a Tukey or perhaps Sidak test instead. I would also be very grateful if anyone has any suggestions on papers I could read that discuss the p = 1 following Bonferroni. Thank you!",AskStatistics,2022-07-13 22:40:41,6
"Probably a 2x2 repeated measures ANOVA, with your test versus control as a between subjects variable, and your pre- vs. post test session as a within-subjects variable.",2,vyl6kz,"If I have two cells in my test, each of which has data for a pre-treatment period and a post-treatment period, how do I compare the two cells and judge whether the difference between them is statistically significant? What test(s) do I use?

Each of the four groups of data have trials and binary outcomes.",AskStatistics,2022-07-13 19:48:47,3
"What are your hypotheses? Or what are you trying to investigate? The question reads like a very familiar situation I encounter: A subject-matter researcher (engineer, etc) has data and knows they ""Need statistics on it."" Prior to doing statistics, though, the goal of doing statistics needs to be clarified. Note that this is not a criticism, as I said, this is a common scenario. Mainly I'm saying that because getting an answer might take a bit of back-and-forth to understand what's really the goal.

Are you looking at ""Sex vs Risk Assessment"" and ""Place of Residence vs Risk Assessment"" separately? What would be an example of results that would make you say ""Hey, that's interesting""?

One thing that could be done is to conduct something like a Mann-Whitney test to investigate whether one sex tends to have higher/lower risk assessment. But that might not be something that's really answering a question of interest.",8,vy3prd,"I am currently analyzing the dataset from my study about extreme weather and how people think and react to it. 

Now my professor gave me instructions like ""Chi² of sex and place of residence for risk assessment"". Sex and Place of Residence are dichotomous variables, but risk assessment is metric (Scale from 1-5), so Chi² doesn't work in this case (Right?). Already talked to him about it, he says he sees what I mean, his approach wouldn't work, and I should just do what I think is right. Problem is I don't know what statistic measure I can use instead of Chi². Regular pearson correlations don't work for dichotomous variables, so what non parametric statistic measure do I use?",AskStatistics,2022-07-13 06:50:08,11
"1. There's no reason you should be getting an error for a Fisher exact test with that table. None. 

 There may be some other issue. What program are you using, and what does the error message say?

2. The presence of a 0 is not necessarily a big issue for the chi squared on its own. However the chi squared test should  definitely give you a warning, because the expected for that cell is smaller than 1.

3.  There's no total for Yes there and the No total is flat out wrong. Weird. You might want to check the table is correctly set up there.",3,vygryq,"I'm trying to run stats where one of the values is zero but both of these calculations are throwing errors. What should I do? Below is the table I'm using. Thanks in advance!

[Table](https://imgur.com/a/FFSGn4S)",AskStatistics,2022-07-13 16:10:54,5
"I figure out my error, I needed to be on the workspace with the bray-curtis values. Hope this helps anyone else!",1,vy9xkg," I need to create an MDS Plot for my thesis data, and my committee would like for me to use PRIMER do to so. The help guides all show that there is a simple button for the MDS plots on PRIMER in one of the options on the top ribbon (the analyze option), but I cannot find this on my PRIMER software. Is there a special package I need for MDS plots or is there something else I am missing?

I haven't been able to find anything online since all the manuals show it should be there already. Is there any difference between PRIMER and R? Thanks for the help in advance!",AskStatistics,2022-07-13 11:15:34,1
[Screenshots_Excelfile](https://drive.google.com/drive/folders/1grlrYnyxGwySokDDqqSqjRL6HkzIdO8s?usp=sharing),1,vy0tnu,"I would like to use an anova test but I don't even know if it's suitable for my case.

I have 9 participants in total  with the total of 720 cases.
720 cases = 4 thicknesses x 2 orientations, x 2 amplitudes x 5 repetitions.

NB: in the attached files you will see 8 thicknesses.
0.02 , 0.03 , 0.04 , 0.04 , 0.06 , 0.08 and 0.

Note that because of the resolution of the screen we used in the experiment we found that :
H= 2V(H= horizontal and V= Vertical).

So technically we just have 4 orientations.  0.02, 0.03 , 0.04 and 0

I am aming to look for  the recognition rate of the participants when thicknesses and orientations are involved .

What I need is to  determine if there's a statistical difference between the groups  and also I would like to know whether the horizontal and vertical orientation  statistically differ with the control orientation condition. ... I want it to be compute from the "" correct /not"" column.

The control was a blank data, it represents 0 thickness.

I will attach the link of the excel file and some screenshot of the graphs",AskStatistics,2022-07-13 04:21:04,4
"Could you describe your data a little better? I can't tell what ""70,1"" etc are supposed to be. Is it ""70.1""?

In any case, it seems that the three ""series"" are repeats of the column preparation, so these would correspond to replicates or ""n's"". The 5 measurements of half-life are technical replicates, since each is conducted on the same column.

The easiest thing to do is to average the 5 technical replicates, then, if you have only two column preparation methods, conduct a t test comparing them (n=3). You are correct that there seems to be no reason that the methods are paired. If you have tested more than two column preparations, use an ordinary one-way ANOVA followed by Tukey's test.

There is really no advantage to trying nonparametric tests, because with small n they have no power. Assuming normality will not increase the rate of Type I errors.

Whatever you do, sort all this out before you do any tests. If you try multiple things and choose the one you like, you're p-hacking.",1,vy0dn8,"Hi all, 
I am now processing data for my thesis and wonder what statistical method would be appropriate to use to compare two of the methods I used.

In the first method, I prepare an ion exchange column by following a given procedure. In the second, I change one parameter in the preparation procedure. In each case, I prepare a total of three parallel ion exchange columns (series). 

When a column is prepared, I use it to measure the half-life of a radionuclide (5 times per column) What I want to do in my data analysis is to compare the resulting half-lives from Method 1 and Method 2.

My data set looks something like this: 
Method 1: 
Series 1: 70,1 - 69,2 - 72,0 - 67,5 - 72,1
Series 2: 66,8 - 70,9 - 71,3 - 73,1 - 70,0
Series 3: xxxxx
Same for method 2 

From what I’ve been reading, I believe that paired T-test is not suitable here. Is there any other common statistical analysis I can perform?

i greatly appreciate your help!",AskStatistics,2022-07-13 03:54:36,1
"I think the term you're looking for is [power](https://en.wikipedia.org/wiki/Power_of_a_test). It's possible that your test wasn't powerful enough to find a result. In other words, maybe the recommended products are actually more likely to cause a sale, but the sample was too small and by chance there were more sales in the other products.",2,vxt2a9," 

I am working on evaluating the product recommendation result in a marketing campaign. In that campaign, 100 products were on promotion. Our recommender ranked the 100 products for each member, and the top 5 ranking products were sent to notify the member of this promotion to give a personal touch. Note that the member can still enjoy the rest of the 95 product promotions if they want.

Looking at the result, some of my colleagues argue that because the purchase rate of the 5 products is lower than that of the rest of the 95 products, the campaign (and therefore the recommender) is ineffective. Is there any bias in that conclusion? It seems that the base (5 vs 95) is different in that comparison and one might think that it is neutral to have a higher purchase rate for the 95 products... Please help.",AskStatistics,2022-07-12 20:11:00,8
"> Are the coefficients for x1 and x2 different between y~x1 & y~x2 vs. y~x1+x2

Why not grab (or even invent) some data and try it?

It will make a difference whether x1 and x2 are orthogonal or not.",2,vxugxt,"Think the answer is yes? Line of best fit with a single variable will have a different y-intercept, since it only considers one slope. Multiple variables have multiple slopes to reduce the least squares, so 

if `x1 != x2` then the y-intercept will be different.

if `x1 = x2`  then the y-intercept remains the same, coefficients for x1 and x2 are cut in half?",AskStatistics,2022-07-12 21:29:22,12
">  I received the feedback that no I can't just double my mean I can't assume that the rate will double

I was about to say the exact same thing. That's not a ""poor result"" it's right to the point.

> that doesn't really help me out if I'm running a hypothetical bakery

It seems like there's aspects to your problem that are obscured by your attempt to provide a different context from the actual one. 

This makes giving relevant help difficult.",1,vxq5gt,"So for context let's say I had a muffin shop in a mall. The number of visitors per hour follows a Poisson distribution of  *λ* =3. Recently the mall put forth a campaign that is expected to double the amount of people at the mall. 

If I expected these new people to visit my shop at the same rate as my current clients could I effectively just double my *λ* to 6 as an estimate of the new mean I expect to see from the increase in population? Or would there be a more official way to project what my *λ* will be?

For context I asked this question on stats exchange and to poor result. I received the feedback that no I can't just double my mean I can't assume that the rate will double and I would need to sample the new distribution. So I wasn't provided with a way to forecast until the result already happened.

So I can understand where those answer were coming from, but that doesn't really help me out if I'm running a hypothetical bakery. I don't care how accurate the guidance is, forecasting/projecting is on a best effort basis. If there isn't an official process to project a new distribution would it at least be reasonable to double my *λ based on what I expect?*",AskStatistics,2022-07-12 17:39:38,5
"These are called Mixture Models.

To estimate these in stan:

https://mc-stan.org/docs/stan-users-guide/mixture-modeling.html",10,vxhccs,Basically i want to use my variables to weight how much i should use f(x) and g(x). Additionally f and g will probably be linear models.,AskStatistics,2022-07-12 10:57:34,3
"Is it just the algebra that's confusing you? We have:

    P(H) = 20 - 20P(H)
    P(H) + 20P(H) = 20
    (1 + 20)P(H) = 20
    21P(H) = 20",4,vxklb5," 

Could someone explain to this from a beginner Bayesian textbook: 

“ P(H)=20-20xP(H)

We can remove the P(H) term from the right side of the equation by adding 20xP(H) to both sides to isolate P(H) on the left side of the equation:

21xP(H)=20 “

How is this conclusion made? It seems like the author is saying:

P(H) + 20xP(H) =20-20xP(H) + 20xP(H)

But this does nothing to “isolate P(H).

Thanks",AskStatistics,2022-07-12 13:20:13,2
"With the right data, a logistic regression will probably be sufficient. However, if you don't want to worry about explainability, you can probably throw it into a more complex model like XGBoost and partition the data into testing and training (which you can do with a logistic regression, too).

 Here are a few things you can do to determine what kind of variables to include in a logistic model:

## Cross-Tabulation

Try tabulating the group counts and group open rates of different pairs of cateorical variables (if present and allowable). Groups could be gender, age group, account type, or some binned/bracketed continuous variable associated with an individual or account. 


Gender|Age Group|Open Rate|Count
:---|:---|:---|:---
F | 18-40|20%|1000
F | 41+|10%|1000
M  | 18-40|10%|1000
M | 41+|20%|1000

In the above table, you can see that both females and males have the same open rate (15%)—same with age groups—but when combined into one variable (gender **and** age group) there is a difference.

## Binning

If you have continuous variables, like account age, website visit frequency, or # of purchases, it might make sense to bin those into categories instead of treating them as continuous. As a side-effect, this can eliminate the effect of outliers, but it could also reduce the strength of your model. It also can capture nonlinear effects, and you can use it in cross-tabulations.",4,vxf74j,"Hi all! I'm new to this subreddit, and to statistical analysis in general. I have a bit of experience with linear and logistic regression in R and Weka, and I'm looking to start a project on predicting how likely an individual is to open a marketing email, sent to a mass audience. Trying to balance a high volume of recipients with the reputation of the source. Considering the dependent variable as opened (y=1) or not opened (y=0), what would be the best way to go about this? I appreciate any responses in advance!!",AskStatistics,2022-07-12 09:23:36,2
">They've asked me to calculate how many mice they'd need to inject in order to experience a 5% or 10% response rate

The number of mice they need to inject seems to be irrelevant to ""experiencing a 5% or 10% response rate"". Either the response rate (and I am assuming this means immune response to a certain hapten) is 5% or 10% or it is not. 
 
So, I am not clear on what a sample size has to do with it.  Supposing r=5%, a sample size of 100 might get you 4-6 responses, and a sample size of 1000 would get you 470-530 responses (around 5%).

>one successful trial where all the mice elicited a proper immune response and triggered MAb production

This seems to contradict 5% or 10% response rates, if a ""successful trial"" means 100% response rate.  Is there something that has flown over my head?  (probably ☺)",3,vxdrqo,"A research team reached out to me asking for help in calculating the optimal number of mice in a study. Their job is to stimulate monoclonal antibody (MAb) production in laboratory mice by injecting them with a hapten (i.e. small molecule) attached to a large carrier protein. Their historical dataset is small and non-successful. They've conducted 6 trials, all with varying haptens, all with various types of laboratory mice, and only had one successful trial where all the mice elicited a proper immune response and triggered MAb production.

They've asked me to calculate how many mice they'd need to inject in order to experience a 5% or 10% response rate (2 separate hypotheses). It sounds like they're interested in a general calculation, so non-specific to hapten or mouse type. Normally, I would take historical data, try and identify the underlying distribution of successful immune responses, and then use software to calculate the sample size based on the effect size and desired power, but their historical data reveals very little.

Instead, I guess my approach will be to see if the published  literature reveals anything about ""monoclonal antibody production in mice""? Is this what the rest of you would do? This feels very much like trailblazing work, so maybe a ""best educated guess"" is acceptable. I would just like to be able to support that conclusion with some sound statistical logic and evidence. What other ways might you approach this problem?",AskStatistics,2022-07-12 08:21:56,2
"You may need to include more details about the project, because the probability of choosing a random spot in a matrix is just 1/# of spots in matrix but it sounds like you want there to be some estimate of how close you get to the real optimal spot, which means the places in your matrix are not independent.

So without knowing more, to me this seems like the score you care about finding a p-value for isn't actually the specific location in the matrix, but *distance from the optimal location*. E.g., the euclidean distance from your model's guess to the true optimal spot is like the error from a regression equation that you're trying to minimize. For that question, comparing the prediction metrics of your model to a null/dumb model like the other commenter said becomes more straightforward.",2,vxf1tr," I'm probably framing the question poorly, apologies for that. Here's my attempt at skipping over most details to get to the heart of my problem:

I have a matrix, with dimensions 50x50, no missing values. I performed an experiment that tells me the most ideal values in the matrix, i'll call it the ground truth, is at the 20th row and 30th column. So I guess that'd be coordinates (20,30).

I trained a simple machine learning model on a bunch of data and asked it to predict the ideal/best/optimal value of the matrix given some other, external data. The model predicted the value would be at coordinates (20,31).

Can I associate a p-value with this? Something that would tell me that the probability of choosing this based on random chance alone = ?

I could imagine how this would be done with a vector of numbers, but no idea for a matrix.",AskStatistics,2022-07-12 09:17:15,5
"Not *synonymous*, no, for a bunch of reasons.

e.g. (i) you can have convergence that doesn't involve *curves*

(ii) even if we do just talk about curves, you can have a curve that is continuously becoming flatter as you move right along the axis (e.g. |f'(x+d)| < |f'(x)| for all x>c, for some c in R, and all d>0),  but does not converge to anything

There's more things still; it would be best to identify the specific kind of convergence you mean (your question lacks context on that; it's not clear why you're comparing it to time series) and look to its definition. 

---

As currently phrased (no specifically statistical/probability context) this appears to be a mathematics question rather than a stats question.

If you had a question about say convergence of random variables, like convergence in probability or convergence in distribution say, that would fit here. But it's not clear that there's a stats-specific context to this question. 

More details might help.",1,vxczcb,"hi, is convergence synonymous with the flattening of a curve, for example in a time-series graph?",AskStatistics,2022-07-12 07:47:42,2
"You can simplify it into three groups, and test whether (e.g.) V1<V2 and V1=V2 have the same or different expected V3 values as V1>V2. 

The one thing I will caution you on is that if you're using V1-V2 (or the above three-group variable) and you decide to adjust for V1 (i.e. modeling V3 on (V1-V2) and V1 at the same time), that doing so completely changes the interpretation of the coefficients of the model. The coefficient of (V1-V2), when you adjust for V1 becomes the negative of the coefficient you'd get if you modeled V3 on V1 and V2 simultaneously. It should not be interpreted as the effect of the difference between V1 and V2 any more. 

The SAS code below illustrates this.

`%let seed = 12314;`  
`data test;`  
 `do i = 1 to 100;`  
  `v1 = rannor(&seed.);`  
  `v2 = v1 + rannor(&seed.);`  
  `v1_minus_v2 = v1 - v2;`  
  `v3 = 0.3*v1 - 0.5*v2 + rannor(&seed.);`  
  `output;`  
 `end;`  
`run;`  
  
`proc corr data=test; var v1 v2 v3; run;`  
  
`proc reg data=test;`  
 `model v3 = v1 v2;`  
`run;`  
  
`*The coefficient of v1_minus_v2 equals the negative of the coefficient of v2 in the model above;`  
`proc reg data=test;`  
 `model v3 = v1_minus_v2 v1;`  
`run;`",1,vxdkaf,"I have two strongly related variables, that both are often used as independent predictors of a third variable. I.e. emotional intelligence(V1) and IQ as predictors(V2) of business success(V3).

Im comparing 3 groups here.
V1<V2
V1≈V2
V1>V2
 
Can I simplify this to a new variable with 3 groups, then use that for a linear model on V3?
Or is perhaps a chi2 test more appropriate?
Or is there another approach? It's been ages since I've had to work with statistics so any help would be greatly appreciated",AskStatistics,2022-07-12 08:12:40,2
">  adding squared terms to check for inverted u shaped relationship,

that's not sufficient to demonstrate that the relationship *is* inverted U (indeed you mention 'proof' !!) rather than something else - like say:

        /\/\
       /    \
    /\/      \/\

which if you just fit x and x^2 terms could easily have a significant, negative x^2 term.

Or consider the following function:

  f =   
k-(x-t)^2 ;  x<=t   
k ; x > t

but you only observe x values to the left of t in your data, here again you could get a significant x^2 term with a negative coefficient (and a perfect fit!) but you don't actually have an inverted U-shaped relationship.

You need to eliminate a lot of possibilities to show that it *is* inverted U rather than something else, and some of them are not going to be possible to show can't be the case with noisy data.",5,vxeld8,"Hi everyone, 

I have a hypothesis of inverted u shaped relationship. And for me, the equation of the curve is also of interest. In linear regression, the assumption is that the error terms must be normally distributed. 

In this case, while I am still performing linear regression but adding squared terms to check for inverted u shaped relationship, in this case does the assumption of normality of error terms still hold? 

N> 100

Thanks in advance!",AskStatistics,2022-07-12 08:58:00,5
"What's wrong with the answer to this exact question that was given here:

https://stats.stackexchange.com/questions/581090/does-the-one-tailed-vs-two-tailed-argument-apply-in-glms-i-e-wald-test-in-the

It seems to cover it quite well:

> The arguments about one-tailed vs. two-tailed work exactly the same in this context as the conventional STATS 101 t-test context. 

 

> In your case, I think you should have had a strong a-priori hypothesis, and have built that decision into the design of the study—it's sketchier to decide you want a one-tailed test after you've seen the results.

Both points there seem to be clear and uncontroversial.",5,vx6uuf,"I have constructed a gamma glm modelling hormone concentration with three predictors: treatment (treatment or control), sex (male or female) and site (thyroid, parathyroid). The work is mostly exploratory, and I had no *a priori* hypotheses regarding the effects of sex or site, but I have seen previously that the experimental treatment group produced higher hormone concentrations than the control, so I hypothesised a main effect of increased hormone concentrationin the experimental treatment group, over the control.

In my results I have reported the t statistic and p value for each predictor, but for treatment, my supervisor said this should be one-sided as I have an *a priori* hypothesis (that hormone concentration will be larger in treatment compared to control) and therefore my p value should be halved to turn it into a one-tailed value.

I know GLMs report a t statistic, but I believe it’s a Wald’s test it’s reporting which is slightly different from a t-test? Does the one-tailed vs two-tailed argument apply in GLMs in the same way it would in a Student’s t-test? Is it reasonable for me to simply halve my p-value in this case and report it as a one-tailed value?",AskStatistics,2022-07-12 02:13:53,5
"It is probably better to look into the effects of assumption violations than to test whether the assumptions are exactly met. [This](https://www.tandfonline.com/doi/abs/10.1080/01621459.1974.10480224) is a classic article evaluating robustness for various multivariate test statistics. I haven’t kept up with this literature so you might want to look for more recent studies. 

Since you are doing a multivariate test you may want to report the multivariate analog to eta^2 which I believe is 1-Wilks' lambda.",1,vx6k4r,"Hello,

I'm writing up the data analysis plan for chapter 3 for my one-way MANOVA:

IVs: Placement method 1, placement method 2

DVs: Measure of success 1, measure of success 2, measure of success 3

&#x200B;

Here is what I have so far. Am I missing anything? 

&#x200B;

1. Report effect size report using using 𝜂 2

2. Check assumptions:

   1. Univariate normality using descriptive statistics function of SPSS

   2. Multivariate normality using Mahalanbois distance 

1. check for outliers using chi-square critical value table

   3. Check for linearity using matrix of scatter-plots (SPSS)

   4. Collinearity - two of the DVs are related, which is a singularity, so check using Pearson correlation coefficient.

   5. Homogeneity of variance-covariance matrices using Box's MTest of equality of covariance matrices.

&#x200B;

Finally, run the MANOVA.

&#x200B;

Does this look right?",AskStatistics,2022-07-12 01:53:39,2
"What you want is maybe a ""master"" coefficient. So an their coefficient would be A*x - B*y

X =(number of appearances)
y =(number of times already been master)

You could set A and B at what every value you feel gets you to the most appropriate model

 e.g. if A =2 and B = 1 then someone who had 7 appearances but had been master twice would have a coefficient of 12 but someone who had 2 appearances but had never been master would have a coefficient of  4. So the first person would be 3 times more likely to be selected by a random.number generator.

If you wanted to be a bit more fancy and wanted to devalue people who had been master loooads of times you could make it A*x - B*y^2

So with A=2 and B=1 someone who had attended say 10 times but already been master 3 times would have a coefficient of 11. Which is the same as someone who has attended 6 times but never been master.",3,vx4a97,"Hello everyone,
I'd like to know from your side if there's a better way to create a ranking ""deserve to be"" in the following context.

Every week, my friends and I meet to play with 14 people. We are always 14 and we aren't always the same.

Every week two people (two masters) are chosen who will have to decide how to split two teams (7 players vs 7 players).

We memorize the attendance and who has done the master. However, we do not want to ensure that the masters are always the same and in rotation we would like to give the possibility to others to do so through a meritocratic ranking that takes into consideration these parameters:

1) the total number of meetings that have been made
2) the number of times that people attended the meetings out of the total number (it means how much you know others players)
3) the number of times they were masters

In general, if you have not been present compared to the total number of meetings you deserve a little less, but up to a certain threshold from which you begin to deserve.

For example:
- If there were 10 meetings and you only attended 2, you deserve less than those who have 7 appearances and have already been a master once. And so on. I am not quite clear how this threshold should be set but I hope the concept is clear.
- If you have just 1 appearance you deserve less than those who have 7 appearances
- If you have few appearances you don't know so much the other people to be able to split rightly two teams

Is it possible to generate a ranking to be updated weekly to have an automatic mechanism which allows to choose freely two masters reducing the time lost to choose it in full harmony?",AskStatistics,2022-07-11 23:19:59,5
"A large number of hypothesis tests do not require normally distributed **data**- they require approximately normally distributed **sampling distributions**, or **residuals**, or **conditional distributions** of some sort. For example, in a paired t test the data does not have to be normally distributed, it is the differences between the pairs that needs to be (approximately) normally distributed, but even then it is really the distribution of sample means that needs to be approximately normal.  This is why textbooks (erroneously) talk about the central limit theorem (erroneous because the CLT only helps you with infinite sample sizes). However, we have lots of other results that tell us that in most ""nice"" data, sampling distributions become close enough to normally distributed with smallish, finite sample sizes. 

Even for the tests that explicitly ""require"" normally distributed data, most of these are very robust to moderate amounts of skewness and kurtosis.",45,vwrtb5,"i dont work in science, mostly economics and business data - i have almost never seen NON right skewed distributions. i mean, everything in the business space seems to start at 0 with big outliers. does that mean we are always supposed to use nonparametric methods or something? its just crazy how ubiquitous normal dists were in school and i NEVER see them.",AskStatistics,2022-07-11 13:11:37,28
sounds like more of an economics question,4,vx60yu,"Why do countries  generally use production approach to forecast GDP, especially for long-term forecasts? and not the other 2 approach (i.e expenditure & income)",AskStatistics,2022-07-12 01:17:08,1
"> However, with my data players can enter the league, leave after some time and then re-enter the league again.

Then - unless such events are rare enough to pretty much ignore - a model for a once only event like death isn't a great model. 

Part of the issue here is you need to be quite precise about what variable you want a model for; you mention modelling one thing (probabilities of an event at some time) but then started discussing a model for a different thing (length of time in a state).

If it's an event that happens repeatedly and you want to model the probability of that event you might indeed consider binomial models like logistic regression (if you're looking at each game) or Poisson (if you're looking at modelling frequency). 

But if you were looking at *survival models* then your interest would have to have been on *how long* they spent in the ""playing"" state or the ""not-playing"" state (there's no reason to look at survival models for time if you're not modelling time spent in some state, that's literally what the response is)

So if you want to look at the *times* in those two (or even several) states, perhaps consider some form of model for state transitions, like might be used for sickness or weather, something where (perhaps underlying some Markov model), the transition probabilities themselves might be a function of predictors. 

If I was trying to do that, I'd be using a Bayesian model, personally, it's likely to save a lot of effort if you don't have access to ability to estimate a precanned model just like what you want (though there's probably several such available).",2,vwmjc8,"Hello,

my data consists of different players in a sports league that moved from a non franchise system to a franchise system. I want to look at the effect that it had on the probability to leave or enter in the the league as a player and the joint probability. Meaning the probability of being in the sample as a player.

My data has a form like this:

&#x200B;

|id|time|franchise|entry|leave|played|
|:-|:-|:-|:-|:-|:-|
|1|2020|1|0|0|0|
|1|2021|1|1|0|1|
|2|2019|0|1|0|1|
|2|2020|1|0|0|0|
|2|2021|1|0|0|0|
|3|2018|0|0|0|0|
|3|2019|0|1|0|1|
|3|2020|1|0|1|0|
|3|2021|1|1|0|1|

&#x200B;

Now I am unsure if I can use a discrete time survival analysis. The way I understand it is that such a model looks at time to an event, given that the event **did not happen previously**. Such as the case of when people die. After they die they cannot live again and there are no observations after that. However, with my data players can enter the league, leave after some time and then re-enter the league again.

So, maybe a discrete logit model may be more appropriate because of this. As far as I know, it looks at the probability that an event happens, such as the probability to enter into a game.

&#x200B;

Edit: I cross posted the thread in R/ econometircs. [Click](https://www.reddit.com/r/econometrics/comments/vwmlz4/did_incorporated_in_a_discrete_survival_time_model/) for reference.",AskStatistics,2022-07-11 09:28:57,4
"Hey I was studying exactly this to estimate gender disparity in classrooms. You are probably looking for a binomial test.

The idea is: if there is no gender disparity, then choosing a person at random is equivalent to flipping a honest coin. So, what is the probability that flipping a honest coin results in a disparity as large as the one you observe?

Good luck!!!",2,vwvx77,"Hi there, I am conducting an independent research project. Im 6+ years out of my stats class and im blanking on what test to run for these categorical variable distributions. 

I am trying to see if the distribution of men and women in a leadership role is statistically different from the distribution of men and women in the general population of that workforce (program directors vs employees). I have the percentage breakdown of both, and my sample size, I am just wondering what test I could run or if its possible with this amount of info. Thank you in advance!

If it helps, 71.7% of employees are male, while 28.3% of employees are female (n=32,382 and 12,787). 68.12% of program directors are male, while 31.52% of PDs are female (n=188 and 87).",AskStatistics,2022-07-11 16:05:48,3
"> Now I just take a big sample and calculate a sample mean A and a sample variance B. Why it feels so reasonable to think that a normal distribution with mean A and variance B is the most reasonable one considering our sample 

It (almost) is, depending on how exactly you define ""most reasonable"". In particular, if you use the definition ""best relative chance\* to produce the data"" then you use the maximum likelihood parameter estimates -- the sample mean and the n-denominator variance (rather than the Bessel-corrected variance I expect you intended).

There's other definitions of ""most reasonable"" gives estimators that you might choose instead that will produce your sample mean and some other (1+ c) B  (where c-> 0 as the sample size becomes large, and c changes as you change your measure of reasonableness). 

This is all fine. There's not even a particular problem using your B, it's just not quite maximizing the likelihood but still perfectly reasonable.

Nevertheless with *perfectly continuous* random variables the chance that *ANY* parameter values produce the exact sample would be 0. This is NOT in any way suggesting a problem with your choice of estimate; it's a property of continuous random variables.

Your problem is focusing on the probability of a point- event with a continuous random variable. It makes more sense to consider intervals with continuous random variables.

e.g. consider instead the chance of being within a very small distance  epsilon of each observation (where a suitable epsilon is easy to come up with, I'll explain why in a bit).


---

(further minor edits in here for clarity)

Now let me rock your world. You have probably *never seen continuous data*.  (Indeed, nearly any probability model is an approximation, not a fact about the thing you're trying to observe.)

Consider a set of heights stored on a computer or written on a piece of paper. You might have recorded the values to the nearest quarter inch or the nearest half cm, say. Or maybe you have some super accurate measuring device and you record it to the millimeter, but ... sadly you discover your extra precision isn't meaningful because heights change by way more than that in the course of a day or from day to day so the half-centimeter is probably all you should bother with. Either way, that set of numbers you record is only to *some* finite precision. 

At best you have recorded binned (""interval censored"") data. The chance that your continuous model's parameter estimates produce a sample *indistinguishable from yours* (a value within each observation's bin) is not 0.

Now ... a pretty sensible sort of epsilon to use in the discussion above (that I left for later) should be pretty obvious. Consider, for example, half the precision of your recorded values as one reasonable choice.


\* as long as it's correctly understood what this actually means",6,vwtdmd,"Lets say I want to measure height of men and someone tells me that the height of men is normally distributed but I am not told the parameters of the distribution.

Now I just take a big sample and calculate a sample mean A and a sample variance B. Why it feels so reasonable to think that a normal distribution with mean A and variance B is the most reasonable one considering our sample although the probability to observe our sample is 0 for any normal distribution no matter its mean or variance?

Why this, I think it‘s called abductive inference, doesn‘t work here(at least mathematically it‘s unreasonable) even if it feels so naturally right to me?",AskStatistics,2022-07-11 14:16:33,5
"This is unanswerable without more information. Good for *what*? And the standard error depends on the units of the variable, so it's impossible to give a general answer.",8,vwm67a,,AskStatistics,2022-07-11 09:13:49,11
"You're running into reality, which is that a single number is often a poor summary of any data. You can always compare central tendency (mean/median) between groups, or you can compare dispersion (variance/median absolute deviation/etc), but fundamentally it's important to know _what your question is_.",2,vwel1v,"&#x200B;

[I think it's a pretty common situation. I'd like to say that if you belong to x category you have more chance to have a bigger value considering the numerical variable, but...](https://preview.redd.it/fbrmvnx4vwa91.png?width=516&format=png&auto=webp&s=d008e6097b301e121cfc6ca6903fcbc133aeb4ea)",AskStatistics,2022-07-11 02:51:42,7
"There might be some temptation to model P(A) (etc) as constant within each class, in which case you're comparing multinomial distributions\*, but I think it probably makes somewhat more sense to treat it as a mixture (Dirichlet-multinomial, say).

If you're inclined to consider a Bayesian approach or are you after a more frequentist one?

\* for an ordinary equality null vs general inequality that would be a test of homogeneity of proportions -- say a chi-squared test, or a G-test, a Fisher exact test, etc, but it sounds like you have more specific alternatives in mind.",5,vw3rma,"I have a dataset of movies, where each movie represents an object of one of three classes (1,2,3) that are capable of doing any of events A,B,C.
Events A,B,C are linked in the sense that only one event can happen per timestep, so when I measure frequencies, P(A) + P(B) + P(C) = 1.

What would be the best statistical test to show that between the three classes, class 1 has a higher/lower frequency of doing event A than class 2,3; for event B, class 2 has higher/lower frequency of doing event B than class 1,3, etc.

I'm currently working with a two tailed ANOVA but I'm not sure if that is the correct test to use.

Thanks in advance!",AskStatistics,2022-07-10 16:11:32,3
"Depends on what you mean by ""do a PCA"".",10,vw23ff,,AskStatistics,2022-07-10 14:51:43,5
Are you sure the dummy is not coded in the wrong direction by any chance?,13,vvr9rf,"Hello, 

I ran a multivariate regression using the Regressit add on in excel. I used dummy variables to detect certain home features. Now the results all make sense..for example I find that the presence of a landscape garden increases the property value..so the coefficient is ""79,250"" meaning the presence adds 79,250 euros to the value of the home based on my data and most other variables make sense ... However, a few variables are listed with a negative... So for example ""presence of balcony"" coefficient is listed as -20,139 ... It would make sense if the balcony added 20,139 euros but not took away from the value. Why is it listed as a negative? Is this because I used dummy variables?

Thank you guys",AskStatistics,2022-07-10 06:15:23,29
"V is part of the test statistic for computing the WSRT, and isn't really interpretable on it's own. You can read more discussion of V here:
https://stackoverflow.com/questions/45233658/meaning-v-value-wilcoxen-signed-rank-test",3,vw4qje,"I am told to run a wilcoxon signed rank test (WSRT). I am not sure what the V means.

this is the result I got after doing the test in R

    Wilcoxon signed rank test with continuity correction
    
    data:  rat_beforeTreatment and rat_afterTreatment
    V = 8054, p-value = 0.0149
    alternative hypothesis: true location shift is not equal to 0

The question for the assignment is 

     For the rats dataset perform a	non-parametric-test and	comment	on your	finding 

So i did a WSRT, not sure what to comment and what to do from there",AskStatistics,2022-07-10 17:00:24,12
"This is like asking for a book ""with lots of questions from basic to advanced math"" or ""basic to advanced physics"". I don't really think there is one, so you really need to be more specific:

1) What is your level of mathematical preparation? Algebra, Calculus, Matrix Algebra, Vector Calculus, Analysis, Proofs?

2) What is your previous exposure to stats?

3) Are you interested in theory?  Applications?  Coding applications?

4) Do you just want questions?  Or questions with worked out answers?",6,vvyv09,Can anyone recommend a book which has lot of questions from basic to advanced stats.,AskStatistics,2022-07-10 12:21:18,4
"I got mine in biostats (started my PhD as well but decided 1 year in I didn’t care for it and just preferred to work in pharma industry). If you’re interested in clinical research, consider doing biostats (more theory) or applied biostats (less theory). Once you have your foot in the door, you won’t need a phd and the pay, benefits, and work/life balance are all good. Your background already seems well suited for it.",1,vvxncb,"hello. i graduated in 2021 with my bachelors in biochemistry (gpa 3.97). im currently a research assistant in an academia lab. i don’t really know where to take my career. i’m not interested in getting a phd at this moment. i want to stay within the realm of science  but i’ve been looking to break into clinical research or biotech. however, it’s been hard for me to break into these roles (with decent pay) at this time. i enjoy data collection and i’ve been wanting to learn some programming but it’s a bit hard to learn on my own. i do a little bit of stats (mainly t tests) currently, but i don’t have much of a background in stats or computer science. i took calculus in high school and college and i took stats in high school. i’ve been considering getting a masters in stats since it’s versatile. any advice is greatly appreciated

edit: i’ve also only taken up to calculus II. i haven’t taken calc III or linear algebra would i not be able to apply?",AskStatistics,2022-07-10 11:24:37,2
"A volunteer statistician might be of more value to you if you have little idea about statistics. There are multiple tests that can be performed on likert scale data like paired t test or wilcoxon and there may be things like power of the test that may have to be taken into consideration. There are tools like SPSS, stata, minitab that you may want to look into it doing it yourself",8,vvgl95,"With 0 knowledge of statistics, and in fact have dyscalculia so learning doesn't seem to be much of an option.

I have Likert scale pre- and post-surveys I need to analyze. Research has led me to the point of understanding that I need to do a paired t-test analysis of some sort? Is there any commercial platform that will do this? Or do I need to find a volunteer statistician? Halp please. Not for me, for the children.",AskStatistics,2022-07-09 18:44:13,25
"
> However I suspect that the proportion of last names that occur only once is too high. Is there any other way to demonstrate this than to code up and run a Monte Carlo simulation against the name-frequency file available from the Census Bureau?

1. Why would you need simulation for that? 

2. However, either way, you're testing a hypothesis *suggested by the data*, *on the same data that suggested it*. That's a problem.

 https://en.wikipedia.org/wiki/Testing_hypotheses_suggested_by_the_data",1,vvm5xe,"I have a list of people that's supposedly an outcome of a random
draw from the U.S. population. However I suspect that the proportion
of last names that occur only once is too high. Is there any other
way to demonstrate this than to code up and run a Monte Carlo 
simulation against the name-frequency file available from the 
Census Bureau? Or I could just post a picture of the appropriate
log-log plots for the expert eyeballs here...",AskStatistics,2022-07-10 00:24:13,3
"You describe what sounds like a multinomial distribution on the surface. If you want beginner sources the ""For Dummies"" series has p good quick references just for a brief understanding. ProbabilityCourse.com breaks it down more.",3,vuxoj9,"Hi,

&#x200B;

are there any entry level sources on probability distributions that I can study? I spent a lot of time on google results most of which are too basic where they pick 1-2 distributions and talk about it and some are too technical. 

My purpose is to understand which distributions represent certain risks and then understand how I can use it to develop project risk profiles. Example risks: with client-A 70% of our contracts meet the targets, 20% lose and 10% earn more than the target. when we have product-X in our scope, our delivery is delayed 60% of the time...

When I think about it - it sounds liek a very 'easy' thing to do but when I try to put it on paper / excel etc. i feel like I don't even know what I'm talking about. which I thought might be because of gaps in my knowledge. like fundamental level...

&#x200B;

Thanks in advance.",AskStatistics,2022-07-09 02:05:08,1
"we're not here to do your hw for you. take a stab at answering this on your own and if you can't, show us how far you were able to get and describe where you are getting confused or feel tripped up and we can help you work through that. but we're not going to do your hw for you.",4,vvdhlz,"A nephrologist finds that on the average, 20% of the patients who need a kidney transplant refused the surgery. Suppose 5 patients who need a kidney transplant were selected.

&#x200B;

(a) Let random variable X be the number of patients who will refuse a kidney transplant. What are the mean and variance of X? Explain these numbers in words.

&#x200B;

The variance is the deviation from the mean of X squared.
 

What I understand from this question. 

I understand that 20% is the mean, but variance is what's confusing me. I assume it's calculated as Sigma^2=n*p*q, where p is the probability of success and q is the probability of failure. Can someone let me know, if that's correct?",AskStatistics,2022-07-09 16:00:14,8
"I am quite concerned that there might be any implication that simpler techniques could be considered less adequate than more sophisticated ones. More sophisticated techniques don't necessarily make the results better, or more believable. *Appropriate* techniques, yes, but I think typically the aim should be to use simple methods when they are sufficient. 

As a result I am unclear on the benefit of such labelling, outside some particular circumstances (e.g. it might be a useful classification if you're looking for papers to discuss in classes that have only covered some techniques and not others).",22,vv338y,"When reading academic research papers, what should be classified as ""basic"", ""intermediate"" or ""advanced"" statistics?

Even more important - is it valid/useful to assess research papers in that way?

Is that something statisticians do (or should) commonly? 

If the answers to those last 2 questions are ""yes"", what would you say should fall into each of those 3 categories? Eg is it correct or even just common-practice to classify ANOVA as ""intermediate"" level and MANOVA as ""advanced""? If so, why? 

Any and all advice towards helping me understand what is being attempted in a (unpublished) paper that does just this kind of categorisation will be very helpful. I don't want to criticise unnecessarily but it seems like an odd thing to attempt, and I'm not sure of the validity / usefulness. 

Many thanks in advance hive mind! 

Descriptive stats might be described as basic",AskStatistics,2022-07-09 07:38:33,8
"Hello. This is a genuine answer I am too poorly to do the leg work. You seem cool enough to want to do it if you like D&D. 

Get a bit of paper or a spreadsheet. Do a 20x20 table and leave space for row and column indexes. Write 1 to 20 at the left hand edge for the row indexes. Do the same for columns index along the top.

        1   2   3   4   5   6   7   …   20

1      0   1   2   3   4

2      1   0   1   2   3 

3      2   1   0   1   2

4      3   2   1   0   1

5      4   3   2  1

.
.
.


20   19 18 17 16 …                      0

This fairly represents all the options. 
So the first thing you see is the symmetry. 
Beautiful. 

Anyway the average of the cells is your average.

There are probably clever ways to add them up - if I was Leonard Euler and/or didn’t have a virus I could do it myself but hey. You can take it from here, good luck have fun.",8,vuv1ww,"I play DnD and my group is trying to nerf ‘advantage rolls’. Advantage means instead of using a d20, you use 2d20 and take the higher number. That means if I need a 10 to do the thing and I roll a 3 and a 14, I take the 14 and successfully do the thing.

We are trying to cut this advantage in half by adding a flat +x modifier to the toll when we get advantage instead of 2 separate rolls. However, we don’t really know how much of an advantage 2 rolls actually gives.

My question is: when rolling two 20-sided dice, what is the average difference between the two rolls (ie on average, how much benefit do you get from a second roll?) I have no idea how to approach this kind of math.",AskStatistics,2022-07-08 23:00:35,8
What’s the actual question of interest that you are planning on using correlation to answer?,1,vv2fqn,"Hi, I currently have two variables that share the same geographic distribution, and my goal is to test if there is some correlation between the two, while taking into account the geographic factor of it (so simply taking the correlation of the two doesn't work). I understand that there are measures such as Moran's I to test for spatial autocorrelation, but for my use case where the goal is to test for the correlation of two geographically-related variables (and thus not autocorrelation), are there any good methods? Thanks in advance.",AskStatistics,2022-07-09 07:05:43,2
">I also include B as distance to the next town with the treatment which is obviously 0 for towns with treatment.

This is not obvious, and depends on the treatment. It is certainly possible that a treatment could both affect a town and the neighboring towns. 

Spatial econometrics seems designed for what you are looking at. There are many different flavors of these models, but something like

Y = A + BX +CWX +e

where W are weights telling which towns are neighbors of which other towns, which can involve distances if you like. [Elhorst's book is a pretty good intro](https://spatial-panels.com/), or on my website I have a YouTube series introducing it.",2,vuyham,"Hi all I am a little bit lost, maybe someone of you has literature for this problem or something, I could not find anything helpfull...

I have following regression model for my thesis:

Y = A + B + X +e

with Y for outcomes, a is a dummy variable with A=1 for a certain treatment and 0 if otherwise, X are additional corntrols (and e for errors). To account for spillover of the treatment I also include B as distance to the next town with the treatment which is obviously 0 for towns with treatment.

My questions are:

1. Is this approach viable or do I need to use A and B seperatly?
2. If it is viable how do I account for possible endogenity of A (which would then also affect B) with an instrument variable approach since I can't instrument both variables?

Thanks in advance!",AskStatistics,2022-07-09 03:04:45,1
">see if incidence changes over time?

It is clear that the data **is** changing over time... You don't need statistics or a regression for that. Do you mean that you want to see if there is a time trend?  

If so, the naked eye sees three distinct time trends; up, across, then down.  

The main question is, ""What exact question are you trying to answer here?"" Doing a regression with a straight line assumes that there is a linear trend, either upward or downward, which does not seem to be present in the data. 

Are you trying to predict the future?  There is too much noise in the signal to use a simple line for that here. Are there other variables that should be included here, such as how many sheep are contained in herds?  Or COVID controls on people and sheep movement?",22,vug4bb,,AskStatistics,2022-07-08 10:31:16,12
"Coding as (-2, ..., 3) is the same as coding as 1,..6 - all you change is the regression  intercept, so if you do fit it as a linear term you don't hurt anything doing that ( but neither does it help, nor does it change the interpretation of the coefficient itself in any way). 

If you see the response as a smooth, monotonic-increasing (or  decreasing) function of education, there's ways to model that in a regression context but it's nontrivial. You'll want to use functionality that deals with that for you ... doing it from scratch yourself would be a major project; doable if that's your favourite flex but a lot of effort if that's beside the point for you.

What software are you using?

By default R will use orthogonal polynomials with explicitly ordered factors, but if you keep all the terms it includes by default, that's the same fit as dummy-coding (for all that its parameterized differently and interpreted differently).

What sort of things did you want to find out about the relationship with education?",3,vuoap1,"I know my outcome of interest  is associated with education level. I have three data sets which currently have education as ""did not attend high school, some HS, HS graduate, some college, college graduate, graduate/professional school"" and I'll need to code this for analysis which will mostly take place via regression in R. The outcome variable is continuous, predictors are a mix of continuous and categorical.

This is *surprisingly* my first time coding an ordinal variable for regression, I've just always had continuous or nominal variables in the past. I know I can create *k*-1 dummy variables for this, and I've seen some people support numbering them 1:6 and treating them as continuous variables, but I'm not sure if this is best practice.

Is it feasible to code them as, ordered as above, -2, -1, 0, +1, +2 +3? Generally speaking most of the sample population completed HS or equivalent, so those below that are below ""baseline"" for the sample, thus why I'm thinking of the negative values instead of just 1:6. I feel like this would make for simple and intuitive *coding*, but I haven't worked with ordinal variables in R (and I'm still pretty new to R) so I'm worried I will run into unforeseen problems in analysis.

What would y'all recommend?

**TL:DR Code education for R regression in an ordinal fashion, best practices to not hate myself later?**",AskStatistics,2022-07-08 16:47:18,4
"1. The assumption is not about the marginal distribution of the response, but the conditional distribution at each position in the design-space (X-space). So looking at all the DV values together it pointless, it's not relevant (this is the same in any regression)

2. Note that the assumption is related to the derivation of the test statistic (whatever you're testing, if anything) *under H0*. An equality null is almost certainly false, so the data are not necessarily even particularly *relevant*, even if you consider the correct assumption. 

 It's an assumption about what would happen if H0 were true, so you're typically dealing with a counterfactual, and that involves using a brain, some theory and perhaps some investigation of what things are known about the variable. 

 If you must use data to decide what you're going to assume (i.e. what model you use), an appropriate way to use data to choose models is not to use the same data 

3. Discrete variables **cannot** be normal (conditionally or unconditionally); the relevant question is not ""is the assumption about the population true?"", which is what you're asking about in the post and what a test would try to look at (to which the correct answer, obviously, is **NO**), but ""how much difference might it make to my inference?"" (which might well be ""not a great deal"", depending on the circumstances).

4. It would be useful to say things about how many predictors are in your model, and what values you might anticipate the conditional mean and conditional standard deviation to take as you move across predictor values (move around the X-space)

5. It shouldn't need to be said, but it comes up a lot, so let me be clear: NONE of what I said should be taken in any way to suggest that I don't think assumptions are important or that you don't consider them and their impact. 

  I think they're potentially very important, but you have to worry about things that are relevant and you have to keep the perspective of worrying most about the most impactful assumptions. A lot of people waste a lot of effort on things that really don't have much to do with anything, and many end up screwing up things that were more or less fine as they were (like transforming things that probably shouldn't have been transformed, and so screw with things that tend to matter a lot, like linearity).


---

If you have a good spread of scores everywhere you're fitting the function and the mean doesn't approach the endpoints very closely, normality probably doesn't matter very much. More important will be linearity and homoskedasticity (and both those will be doubtful if you get close to the bounds in some parts of the design-space).

If it does seem likely that the normality would be an issue (e.g. you expect strongish skewness or perhaps very small spread), then depending on what things you want to test (if any), you may be able to do something other than the usual normal-theory inference (but other inferences, too, will also rely on many of the other more fundamental assumptions)

---

If you don't have a good reason to treat the scores as interval-scaled there are suitable methods for ordinal data. 

Can you say more about how the scores arise? For example, are they sums of Likert-like sub-scales? Are they counts of ""yes"" answers? Something else?",7,vuoz56,"Hello all, 

\[Doing my first regression with real data here N=90\] 

I'm performing a difference in differences regression having as the dependent variable a 0-11 score from a test applied to the participants of an intervention (Control and treatment groups). 

I believe that I should not use the normality test (I was doing the Shapiro-Wilk one) to accept/reject the normality assumption of the data due to its discrete nature.  

In your opinion, which can be the best approach to check for this assumption? Does checking for normality assumption, in this case, make any sense?",AskStatistics,2022-07-08 17:21:19,2
"The Wilcoxon-Mann-Whitney doesn't compare medians. 

If your alternative is pure location-shift (though such an assumption is not required with the Wilcoxon-Mann-Whitney), then the measure of difference you're looking at is the median of cross-group differences (in the population, or in the sample, respectively, where it is also known as the two-sample Hodges-Lehmann statistic).

See the second paragraph in this subsection: https://en.wikipedia.org/wiki/Hodges%E2%80%93Lehmann_estimator#Definition which defines the two-sample Hodges-Lehmann statistic.

You might wonder whether the median of all possible differences isn't the same thing as the difference of medians, but they are not the same in general.

[It does work for means; the difference of means and the mean of differences are the same; that's because averages and differences are linear, so the operations are transitive; by contrast, medians are non-linear.]

---

If you really want to test medians there's several ways to go about it. 

The ""usual"" test that's done in that case would be Mood's median test.

https://en.wikipedia.org/wiki/Median_test  (see the comments on the Mann-Whitney there, which agrees with the explanation above)

But you could do a straight permutation test of the difference in medians; this will be an 'exact' test, and will also give you a way to get a confidence interval for the difference in medians fairly simply.",3,vuutqy,"We are preparing statistical analysis of cross-sectional study; and have the following question:

We want to check whether difference btw two age medians grouped by categorical variable (died vs alive) is statistically significant. We used Mann-Whitney U test. Did we choose the right test?",AskStatistics,2022-07-08 22:46:36,2
"It's called a univariate probability transformation. If you search for that term, you should be able to find lots of information online about it. You'll need calculus for the continuous case, such as with a chi-square distribution here.

The distribution still integrates to one. It's easier to see what happens with the cumulative distribution function. Let Z be chi-square with v degrees of freedom, and let Y = Z / (2 N ln(2)). Then P(Y <= y) = P(Z / (2 N ln(2)) <= y) = P(Z <= y 2 N ln(2)). It's scaling the distribution towards zero in this case.

You can get an intuitive grasp on this stuff by simulating data. In R, try drawing z <- rchisq(10000, 10) then setting y <- z / (2 \* 100 \* ln(2)), or whatever, and then plotting histograms.",3,vuei3c,"I am reading about mutual information estimators, and they say that, for discrete variables, the null-distribution of I(X;Y) where X and Y are independent and have finite alphabet size Mx and My is given by   


Chi\^{2}\_{Mx\*My} / 2Nln(2)  


I get that {Mx\*My} is the degrees of freedom, but what does it mean to normalize by the constant 2Nln(N) (where N is the number of samples). Won't that force the distribution to no longer be correctly normalized and not sum to one?   


If I wanted to compute the p-Value of a given mutual information, would it be as simple as computing 1 - \[(1/2Nln(2) \* CDF\] (since, the constant 1/2Nln(2) just factors out, right?)  


Ref: [https://arxiv.org/pdf/1408.3270.pdf](https://arxiv.org/pdf/1408.3270.pdf) Appendix A5",AskStatistics,2022-07-08 09:19:47,6
"I'm unclear on what you mean by ""complexity"", and what you goal is.  Do you mean that you want to create some sort of ""index"" number that agrees with some notion in your head that takes characteristics and outputs a ""Complexity Index Score""?

Or, is it the case that you have some measure of complexity that someone else has created, and want to see how characteristics help explain this preexisting complexity measure?",1,vugs5m,"I've taken 2 statistics classes, but have not come across an approach that I can use for my project. 

I have a dataset with 6 different attributes about a building (numeric and categorical attributes). Many of these attributes have a linear relationship and some are non-linear. I'd like a create and assign a  complexity value to each building from these 6 attributes (and integrate the linear and non-linear information into the complexity value). What concepts should I be researching to do this?

Thank you!",AskStatistics,2022-07-08 11:01:06,5
"I don't have any specific advice about the statistical methods, but review ecological fallacy before you go much further. Without having case data on the variables of interest, you are very limited in the analysis, regardless of methodology.",1,vubvcd,"Hello all, 

I have a dataset that records COVID outcomes (cases, deaths) daily in a tally for the 50 states of the USA. I want to find the influence of various continuous (e.g. percent of seniors in a state, percent of smokers in a state, GDP per capita) and categorical (e.g. whether a state is coastal) variables on these outcomes, along with interactions (e.g. whether GDP per capita moderates the effect of coastal-ness on cases).

I've already done cross sectional univariate multiple OLS regression on the data but the issue is that the relationships seem to change over time -- e.g. some variables will be significant in the first half of 2020 but not in the second half, and vice versa. What I've done at this point, which is admittedly crude, is just take the different tallies for the cases at different dates (at the halfway point of the year, at the end of the year) and do the regressions on that (in R). 

I want a method that integrates time and the fact that I have all this daily data that seems to have important statistical information that I'm not analysing at the moment.",AskStatistics,2022-07-08 07:19:28,1
"You can set this up as a Markov chain, with X\_i = k being the event that you're on level k at the i-th time step. Treat level 5 as an absorbing state of the Markov chain, and let T be the time to absorption. The average number of moves to reach level 5 can be written as E(T | X\_0 = 0). Write p\_{ij} as the transition probability i.e. given that you're on level i, p\_{ij} is the probability that you'll be in level j in the next step.

Denote v\_i = E(T | X\_0 = i), i = 0, .., 4. Using law of total probability as well as the Markov property, you get:

    v_i = E(T | X_0 = i)
        = sum E(T | X_1 = j, X_0 = i) * P(X_1 = j | X_0 = i)
        = sum [1 + E(T | X_1 = j) ] * p_{ij}
        = 1 + sum v_j * p_{ij}.

The summation above is taken over j = 0, .., 4.

Instead of writing out the all the equations, this can be solved pretty easily if we write it in matrix form. Let v be the vector containing the v\_i's, and Q be the submatrix of the transition matrix corresponding to states 0 to 4, let e be a vector of 1's. The above system of equations can be written as:

        v = e + Q * v
    =>  (I - Q) * v = e
    => v = (I - Q)^{-1} * e

i punched in your probabilities in R and got that expected number of steps to reach level 5 from level 0 is \~164.44. it's quite a bit off from OP's numbers have so i'd appreciate if someone can double check what i did",2,vuj3e7,"I'm playing a game that has a refine mechanic. I would like to know how many attempts, on average, to reach the 5th level. The mechanics are:

1. You start at 0.
2. The probabilities of succeeding are:

0->1 = 80%

1->2 = 50%

2->3 = 30%

3->4 = 30%

4->5 = 20%

3. Every time you fail you go down a level e.g. if you're at 3 and you fail going to 4 you go to 2.

I kinda brute forced it on excel and got a discrete answer of 368 attempts and a continuous answer of 296.1388889.

Please let me know if I'm correct.

I kinda forgot my maths now but this is how I calculated it and when you add it all up the numbers above are what I got.

Where:

n is the refine level you're trying to achieve

Pn is the probability of successful refine on n

https://preview.redd.it/xivtvqmqeea91.png?width=520&format=png&auto=webp&s=a25f804c230ea0330b5f7a0fbea08126bed6ffe4

https://preview.redd.it/ewqd96zueea91.png?width=657&format=png&auto=webp&s=a2053e731ae4597e0a60367fafe082eddb9b55c4",AskStatistics,2022-07-08 12:46:23,7
"I’m not sure which topics your elementary probability theory course covered, but my first inclination if you’re looking at master’s programs would be to consider taking the probability theory course. Having familiarity with probability distributions and how to work with them will make your life a lot easier early in the program. If you have already taken a course that covered distribution functions and moment generating functions, then you can probably skip this one if you’d like.

Also, I’m not sure how comfortable you are with coding, but if you don’t have much practice with it, then it’s possible the statistical computing course could be helpful. You will almost surely be using R or Python to some extent in a master’s program, with some programs placing more emphasis on it than others. To me, it isn’t as essential as prob. theory, but could be useful moving forward depending on your background.  However, if you already feel comfortable coding, feel free to skip in favor of more interesting courses.

Otherwise, take whatever interests you most — it seems like you have some nice options there!",4,vu9b7f,"Hi all, I'm going into the last semester of my undergraduate degree in Statistics (with a Biology minor) this fall. As it stands, I need two more courses (+ mathematical statistics and my thesis) to graduate. I'm looking to apply for master's programs in biostatistics for Fall 2023, and am wondering if anyone has advice on what courses I should take. I've already taken classes in stochastic processes, regression modelling, elementary prob. theory and math stats, design of experiments, data mining, numerical analysis, and real analysis. These are the stats classes available for me to take in the fall:

**Probability Theory:**  Introduction to probability, characteristic functions, probability distributions, limit theorems.

**Survey Sampling:**  Basic concepts in sampling from finite populations; simple random sampling; stratified sampling; choice of sampling unit; cluster and systematic sampling; introduction to multistage sampling; ratio estimation; sampling with unequal probabilities and with replacement; replicated sampling; related topics. 

**Applied Multivariate Analysis:**  Selected topics in regression and correlation non-linear models. Multivariate statistical methods, principal components, factor analysis, multivariate analysis of variance, discriminant analysis, canonical correlation, analysis of categorical data. 

**Time Series and Forecasting:**  Time series regression. Nonstationary and stationary time series models. Nonseasonal and seasonal time series models. ARIMA (Box-Jenkins) models. Smoothing methods. Parameter estimation, model identification, diagnostic checking. Forecasting techniques. A statistical software package will be used. 

**Statistical Computing:**  Statistical computing techniques, pseudo-random number generation, tests for randomness, numerical algorithms in statistics; optimization techniques; environments for data analysis, efficient programming techniques; statistics with mainstream software. 

I was leaning towards the 2nd and 3rd ones on the list, but I wanted to ask here in case I would be missing out on something from the other courses (although I may audit some anyway). Please lmk if I need to add any more detail, and thanks in advance!",AskStatistics,2022-07-08 05:10:13,5
"What makes you say that it can't be done by MLE?

(I expect that  its not quite  as simple as *can't* in this instance, but there's probably a good reason they can't just give a formula, not having a closed form expression for an MLE is typical)

If you're trying to do it by hand, beware of blindly trying to hit it with the calculus stick.",2,vuebty,"https://en.m.wikipedia.org/wiki/Generalized_Pareto_distribution

We have MLE estimates for scale and shape but not for the location (threshold) why?",AskStatistics,2022-07-08 09:11:56,7
"If the ""outlier"" is *exactly* consistent with the pattern of the ""non-outlier"" data, then it has no effect. However, if the pattern changes even slightly, the outlier will have a disproportionate effect on the loss function, as any small change in a **linear** manifold/classification boundary will grow proportionately over the univariate space (by definition of linearity). In regression, these are called [leverage points](https://en.wikipedia.org/wiki/Leverage_(statistics\)), although the idea generalizes to SVM as well.

Loosely speaking, I'd say that any algorithm which is linear in some subspace or hyperspace (GLM, SVM, neural nets) can suffer from this problem, while other methods (particularly tree-based) don't really suffer from this.",4,vtzk06,,AskStatistics,2022-07-07 19:15:12,10
"How are you obtaining the z-scores (what mean and variance are you using to standardize them)?

When you say ""significant differences"" what exactly are you comparing? Do you intend to compare their means? Their variances? Their distribution shape? Something else?",1,vu56rs," 

Dear all,

I have three Z scores computed for three populations. I want to know whether there are significant differences between these three Z scores. Is it possible to do this statistically?",AskStatistics,2022-07-08 00:41:53,3
"I don't get how Bayesian Cox Model

> would 'fix' the 'problem' that the standard Cox model isn't finding a significant p value for their pet treatment. 

unless they are planning on blatantly messing with priors and hoping that the evidence doesn't undo the bias they add.

They might as well keep it classical and  p-hack if they want to go hack.",10,vtklo6,"I'm a crusty old statistician, and a much-younger colleague just asked me to help them respond to someone who thought that a Bayesian Cox model would 'fix' the 'problem' that the standard Cox model isn't finding a significant p value for their pet treatment. I have a nice, prepared statement on that issue (citing the ASA statement, of course), but would like a clearer source that spells out how a Bayesian Cox model is actually an improvement. 

* Is it that the standard model's estimates are unbiased, but the Bayesian estimates are slightly more accurate (and what does 'accurate' mean in this context, given the fact that two different Bayesians can get different answers by picking different priors - are both answers more accurate)? 
* Is the Bayesian model more robust to informative censoring? 
* Something else?

Thanks. I've been googling, but I seriously haven't found anything beyond ""this is how you do it"".",AskStatistics,2022-07-07 07:58:23,9
"differences of paired data tend to have smaller variance because the pairs of values would be expected to be positively correlated (due to the similarity of characteristics of the observations making up the pair)

Var[Y(i) - X(i)] = Var(Y(i)) + Var(X(i)) - 2 Cov[Y(i),X(i)]",2,vtzu6w,,AskStatistics,2022-07-07 19:29:40,3
"de Vet, Henrica C. W., et al. Measurement in Medicine : A Practical Guide, Cambridge University Press, 2011. 


The math is that of [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression).

Of course, you could consult the references [here](https://en.wikipedia.org/wiki/Item_response_theory) and ask specific questions afterward.",1,vtq9sq,"Including any required math skills (beyond pre-calc) or any statistical concepts/tests that the theory may build upon (with any more recent theoretical or statistical developments)?

I am looking to get into assessment design and am working to develop my current skillset/knowledge base. 

Thanks you all!!",AskStatistics,2022-07-07 12:04:34,2
"Let's make it a bit clearer. We have two methods:

* method 1: picking 10 IDs at random from a list of IDs without replacement
* method 2: picking 1 ID at random, and selecting the 9 following ID

A priori (i.e. before the game starts), you still have the same chance of getting picked according to both methods. In that sense, yes, it is fair. However, the difference is that in method 2, the game is set after having picked the 1st ID.",9,vtg48q,"Ok, the title isn't really good at describing the question. Me and some friends were debating this situation: we have a contest with 10 winners. Each participant gets an ID. We put all the IDs in a list and we randomly select one entry in that list to be the winner, let's call this the \`WID\`. The next 9 winners are \`WID + 1\`, \`WID + 2\`, etc.

Is this a fair method of selecting the 10 winners? Is there a difference between doing this, and randomly picking 10 winners? 

To me it feels unfair because your ID now plays a role in deciding if you win or not, but I'm not sure.",AskStatistics,2022-07-07 04:09:26,8
"> Some values in my tables are lower than 5 (mostly students who do not answer the item asked)

The criterion you're discussing does not relate to raw counts but to expected counts. The bigger challenge to validity will be the distinct possibility that the missingness was likely *not* at random which will invalidate this attempted inference.",1,vtpi16,"Hello, I am using chi square to compare responses by two groups of students and I have some questions regarding what to include in the table, some formalities, and when it is valid to use chi square.

1) Some values in my tables are lower than 5 (mostly students who do not answer the item asked). As far as I know those shouldn't be included in the table because chi square isn't valid when used with frequencies lower than 5. Am I supposed to specify that those students didn't answer in the text above explaining my table or below the table itself in a smaller font?

2) I was going to use 4 decimals for the P result, however, some Ps are very small such as 0.0000001. I was going to write in those cases p《0.01 but I was told that I had to use the same criteria for all Ps. What criteria is most commonly used in those cases? As many of my results are not significant.

Thank you!",AskStatistics,2022-07-07 11:31:18,3
"I don’t know the answer, but maybe a few starting points.

- Crunchbase: paid, not complete, but very expansive.
- in the UK, there is a public directory like this one: https://www.digitalmarketplace.service.gov.uk/g-cloud/suppliers. It seems to be by sector, and I’m not sure how scrapable/complete it is.",1,xi9r9f,"For example, in the Netherlands, data of all the companies is retrievable, though poor quality. In Switzerland, you can get it for 20 cents per company.

Google Maps Platform API can return max 60 per query given GPS + radius.

What are some ways I can get companies data?",datasets,2022-09-19 04:49:04,4
"Gait data

[https://www.reddit.com/r/datasets/comments/px3kie/gait\_in\_parkinsons\_disease/](https://www.reddit.com/r/datasets/comments/px3kie/gait_in_parkinsons_disease/)

Manning have a live project on pose estimation that must have a dataset attached

[https://www.reddit.com/r/datasets/comments/w2nlwz/any\_parkinsons\_fer\_dataset\_or\_images\_for\_emotions/](https://www.reddit.com/r/datasets/comments/w2nlwz/any_parkinsons_fer_dataset_or_images_for_emotions/)

https://www.manning.com/liveprojectseries/pose-estimation-with-TensorFlowjs-ser

Pose here has some that might be worth looking through [https://www.reddit.com/r/datasets/search/?q=pose&restrict\_sr=1&sr\_nsfw=&include\_over\_18=1](https://www.reddit.com/r/datasets/search/?q=pose&restrict_sr=1&sr_nsfw=&include_over_18=1)

[https://www.reddit.com/r/datasets/comments/kbpc37/rula\_reba\_dataset\_for\_3d\_pose\_estimation/](https://www.reddit.com/r/datasets/comments/kbpc37/rula_reba_dataset_for_3d_pose_estimation/)",3,xh6qf9,Basically looking for a dataset of images that physician would use to analyze someone's posture. Not labeling required.,datasets,2022-09-17 21:14:19,1
"Hey wind_dude,

I believe a `request` flair might be more appropriate for such post. Please re-consider and change the post flair if needed.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/datasets) if you have any questions or concerns.*",1,xfwrdf,"A few years ago I built a classified search engine for autos. The dataset spans about 1.5years from 2019ish into 2020 of crawled data mostly from craigslist. There around 21million listings in it, its about 45gb of json uncompressed.

Use it how you wish.

Should be a good place to start for a number of NLP/classification tasks relating to automotive. Some prediction tasks in terms of pricing.

&#x200B;

[https://www.kaggle.com/datasets/winddude/15-years-automotive-classified-from-20192010](https://www.kaggle.com/datasets/winddude/15-years-automotive-classified-from-20192010)

&#x200B;

**Edit: Important Image update!**

I found the images for now they can be found at either \`[https://automudo-img-cdn-dev.sfo2.digitaloceanspaces.com/](https://automudo-img-cdn-dev.sfo2.digitaloceanspaces.com/)\[PATH\_FROM\_\[\_source.images.path\]\] \` or \`[https://images-cdn.automudo.io/](https://images-cdn.automudo.io/2020/10/c2-00039b708ddc964b40cc74c843a47180084d1b43.jpg)\[PATH\_FROM\_\[\_source.images.path\]\]\`. 95% of the images are on \`[https://images-cdn.automudo.io/\`](https://images-cdn.automudo.io/`) check there first. Some may return a restricted error, if you find lots, let me know. If there is interest in the images, please let me know I will export them as there own dataset, but it will take  awhile, due to latency of exporting from s3 based file stores.",datasets,2022-09-16 10:00:01,5
"Hey WhatsTheAnswerDude,

I believe a `question` or `discussion` flair might be more appropriate for such post. Please re-consider and change the post flair if needed.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/datasets) if you have any questions or concerns.*",1,xgc1i1,"Question as per the post, does anyone know where I could maybe 
1-find a list/csv/excel of all Canadian Postal Codes and 
2-the average population density per square mile possibly?",datasets,2022-09-16 21:01:35,7
"Unless you live in an area where food banks send surveyors out on the ground to tally up every homeless person / impoverished person within a 10 mile radius, you’re going to be hard pressed to find any data that even approaches reliability.",2,xgda50,"Basically I want to know how many food insecure people live over any decently large distance (1mile, 10mile etc) away from a food bank. If anyone could find a statistic I’d appreciate it a LOT. Thanks in advance 🙏",datasets,2022-09-16 22:06:53,3
"One that was in there news recently
https://archive.ukmeteornetwork.co.uk/reports/2022/orbits/202209/20220914/20220914_205933.213_UK/index.html

Probably not massively useful days to lots of people. But if you were looking for a physics image processing project this would be handy",1,xfo11n,,datasets,2022-09-16 03:52:00,1
"Michigan has a good one.  I think it's complete back to 1950
https://michigan-football.com/",3,xffpid,"All,

I am looking for an aggregator of recent US High School Varsity Football Datasets. This will be for a predictive analytics project I am exploring.

I am looking for an aggregator site similar to this one available for College Football.  [https://collegefootballdata.com/exporter](https://collegefootballdata.com/exporter)

I know finding one of these for the entire USA will be impossible but even a regional aggregator for say Varsity High School Football teams in Texas or Southern California or North Carolina etc would be great as a starting point for this project .

Thanks",datasets,2022-09-15 19:59:53,3
"I don’t know what it means either. And I’ve been writing software for 50 years.

Ask your teachers what it means, given that they are seemingly making-up their own terminology.",2,xfgnn9,"Tldr: 
1. Is it necessary to do standardization before we can upload our image dataset? 

2. If yes, then would standardization make the images indistinguishable? 

Hey, this is my first post on this subreddit. My team has made an image dataset of over 25,000 images.

Edit: The purpose of this image dataset is for performing Multivariate image Classification using neural networks. 

We would like to upload our dataset on any website so everyone can use it. However, our teachers are saying we have to ""standardize"" our dataset before doing so. We're not quite sure what it means. 

From what I learnt so far, I think it's by standardization of the mean and std of the images over RGB channels. But to do that, we'll have to work on the 2d arrays of images. 

If standardization is performed over them and we convert these arrays back to images, will the images be recognisable or will they be distorted? I'm sorry I don't know how else I can explain because I'm kinda confused. 

Any help is appreciated. Thanks!",datasets,2022-09-15 20:46:47,9
Remind Me!,1,xfal1c,,datasets,2022-09-15 15:56:03,3
"Not trying to sound like a dick but how much do you know about weight loss? Everyone who works out and trying to lose weight must stick to a diet. Sure workouts do help but if you are not tracking calories you wont lose weight most of the time unless you have very good intuition about eating. 

There are couple of factors in weight loss:

- diet (most important)
- supplementatiom
- non-exercise activity (just moving, walking)
- low intensity cardio
- high intensity cardio (like HIIT)
- weight training
- many others

In short weight loss is not about workouts most of the time. It is much more complicated.

I doubt if there is a dataset for this. If I built an app I would probably use some kind of questionnaire to understand current activity level. Start slowly by increasing the activity, low intensity cardio. If the person is in good shape then something more intense. 

Watch this https://youtube.com/shorts/4jWqkfkAYac?feature=share how to recommend something for someone who is untrained.",12,xetp3o,"Hi, 
I am working on an fitness/health recommender system  that takes in user’s fitness goals and output workout recommendations to users and need data to trains machine a learning model to do so.

I Was wondering if there are any machine learning dataset with

Inputs : user goals ( e.g to lose 2kg weight in 2 months)

Outputs : fitness Recommendations 

Thanks",datasets,2022-09-15 03:53:30,3
"http://openelections.net/ collect and publish this kind of data - they may have what you want.

This looks like Illinois 2008 for example: https://github.com/openelections/openelections-data-il/tree/master/2008",3,xesytz,"Looking for precinct-level 2008 U.S. presidential election results in any format, specifically for IL, FL, IN, NV, WV.

I've grabbed the data from [Harvard's election data archive](https://projects.iq.harvard.edu/eda/pages/about) but can't find these specific states.

Has anyone had luck finding this data or know where to find it?",datasets,2022-09-15 03:12:07,2
"Open street map has a pharmacy shop type I think.

Edit
https://wiki.openstreetmap.org/wiki/Tag:amenity%3Dpharmacy
https://wiki.openstreetmap.org/wiki/Tag:healthcare%3Dpharmacy
https://wiki.openstreetmap.org/wiki/Tag:shop%3Dchemist",1,xexanw,"The original source has a broken link ([https://www.data.gov.uk/dataset/9ea2aeeb-a95c-4dd0-8a4f-0268b93fdf07/location-of-pharmacies](https://www.data.gov.uk/dataset/9ea2aeeb-a95c-4dd0-8a4f-0268b93fdf07/location-of-pharmacies)).

I am wondering if anyone has the dataset in their location machine.",datasets,2022-09-15 06:47:09,1
"Are you familiar with what the blockchain is? Also you’re going to need to be much more specific. That’s kinda like saying ‘what colors exist, real or imaginary?” It’s such a large category of so much stuff",1,xe8yh9,Need help with nft sales dataset..,datasets,2022-09-14 11:10:34,1
NLSY79 or other NLSY datasets. Soooo much info,3,xd9xkc,Part 1 (4 years old): [https://www.reddit.com/r/datasets/comments/akb4mr/what\_is\_a\_dataset\_that\_you\_cant\_believe\_is/](https://www.reddit.com/r/datasets/comments/akb4mr/what_is_a_dataset_that_you_cant_believe_is/),datasets,2022-09-13 08:03:28,6
"Hey OutrageousPriority25,

I believe a `request` flair might be more appropriate for such post. Please re-consider and change the post flair if needed.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/datasets) if you have any questions or concerns.*",1,xdzutg,"I am looking for a dataset containing dimension of a crop namely fruits and vegetables over time. In symmary,  a dataset that contain how are crop is growing over time.",datasets,2022-09-14 04:47:43,1
"You have so many requirements that I'm sure you won't be satisfied until you compile your own list manually. A quick glance at the Wikipedia page for racial slurs shows that many of them are also just normal words (e.g. chink could be ""a chink in the armor"", apparently ali baba is used a slur?). And the other issue is that you can't really create a universal and neutral list of what counts as offensive, many lists will include cunt and bastard while others may fail to include retard or gypsy.",28,xd8pqf,"Hello,

I run an 18+ Discord server and I would like to set up my bots to auto-delete messages with especially offensive terms (not terms like fuck, shit, cunt, etc. but rather slurs like the N-word) 

I've found a few lists, but they almost always include words that require context to be offensive (eg. apple, black, chief, autism, gay) which I don't want to include on a blacklist because they're often used in inoffensive contexts. 

I also don't want to blacklist general swear words like: shit, fuck, bastard, or cunt (fine in UK/AUS).

Is there a database of slurs that are stand-alone highly offensive slurs, but does not include swear words (again like the N-word, but excludes stuff like cunt)?",datasets,2022-09-13 07:14:56,22
"Not as a buyer so far, but still seems to be a long way ahead",2,xd1i8t,"I read this article earlier and it seems promising but so far I'm still a little skeptic.

[https://medium.com/p/13de375dd1b4](https://medium.com/p/13de375dd1b4)",datasets,2022-09-13 00:59:07,6
"I get data like this quite frequently, and I pivot it in to a column (one for Year, one for Results). It should be pretty easy in most systems:
R uses the names Gather/Spread
Excel's Data model is pivot/unpivot",2,xd61ne,"For example. Lets take age 

2017      2018      2019     2020

Bill    18           19           20          21

Jane  25          26           27           28

&#x200B;

Disclaimer: Obviously I am not looking to analyse age over time, but this is the crux of what I would like to figure out lol. 

&#x200B;

Thanks",datasets,2022-09-13 05:20:31,1
"Check out the UCI Machine Learning Repository (just Google that, it'll show up in the results). Lots of good datasets there.",8,xcm2x2,"So i have an assignment where i have to do some machine learning in Python, and i want to do some regression on a dataset. Can anyone suggest a dataset which is public available?",datasets,2022-09-12 12:40:09,9
"Based on the idea that there is a sub for everything I gave it a try.

https://www.reddit.com/r/poop/

Quite an active sub, lol.

If you know how to scrape the pics you might have your answer.

Edit : For those who need the obvious NSFW",1,xcrz7m,I need a dataset for my machine learning to recognize different types of poop this is for my patient.,datasets,2022-09-12 16:46:04,2
"Hi OP. You could check out the API for Adzuna. They operate in 20 countries, including Russia.",1,xcbtav,"Hy all, some wisdom needed:

I trying to find a dataset of Ads / advertisements that promote joining the Russian army, ie. recruitment campaigns. I tried Scrapping via VK API, but no luck with that. If anyone knows a place I could look into, I would highly appreciate it.",datasets,2022-09-12 05:42:06,2
Mintel,2,xc7nxx,I want it for an assignment in which I have to compare the sales of 1947 to current year,datasets,2022-09-12 01:52:43,1
"you might be a bit disappointed with what you find, because a lot of that information is trade secret unfortunately",3,xc0swd,Either nominal or monetary value. Within the United States or worldwide.,datasets,2022-09-11 19:23:59,3
You might want to try the benchmarking site A2Mac1 - but no sure how much is available without paying.,1,xbovzv,"I'm looking for a dataset that will allow comparison of car exterior measurements -- truck beds in particular. Might facilitate conclusions like:

* The **1996 Toyota Corolla's** rear windshield has the same dimensions as the **2001 Toyota Camry**. (Understanding that they're 95% similar would also be useful.)
* The truck bed on a **1999 Toyota Tacoma XtraCab** has the same dimensions as a **2002 Ford Ranger**.

I figure this information might be publicly available in some government car safety database? Haven't had any luck yet. Trying to avoid having to go combing through individual manufacturer manuals.

Any leads/ thoughts appreciated.",datasets,2022-09-11 10:50:53,1
Remind me 1 week,2,xbkijr,Where can I find a dataset of the latest google searches or a random real google search?,datasets,2022-09-11 07:50:38,3
https://www.blu-ray.com/ ?,1,xac2ca,"Blu-ray.com looks to have the largest database of over 300,000 releases, but it doesn’t seem to have an available api or downloadable dataset. Before I go about scraping to form a dataset, figured I’d ask here to see if someone already has formed a dataset or if there’s another dataset of Blu-ray releases.",datasets,2022-09-09 18:12:42,4
Idk if you know scholarly but it is a full package that scrapes google scholar.,3,x9o39f,"Hey guys 🐱‍

I've updated scripts that extracts pretty much everything from Google Scholar 👩‍🎓👨‍🎓 Hope it helps some of you 🙂 

Repository: https://github.com/dimitryzub/scrape-google-scholar

Same examples but on Replit (online IDE): https://replit.com/@DimitryZub1/Scrape-Google-Scholar-pythonserpapi#main.py

Extracts data from:
- Organic results, pagination.
- Profiles results, pagination.
- Cite results.
- Profile results, pagination.
- Author.",datasets,2022-09-08 23:36:29,5
"Can I ask why you need this, what is the use case?

I doubt such a database exists already. You could make one yourself though, with books from project gutenberg. They only have 60,000 books, which is a mere fraction of all published books.",1,x9wv17,"Hi,

I'm looking for a database where you look up a book and it gives you all kinds of data about its writing: mean, median and mode number of words / syllables per sentence, # of sentences per paragraph, usage of word types, usage of punctuation marks, etc.

Any info would be greatly appreciated!",datasets,2022-09-09 07:23:51,3
"You could spend the next few years of your life trying to build this yourself and still not get close to the quality and functionality of a commercial off the shelf tool for address normalization. 

It has been about 15 years since I was deep in this niche of the industry, so i am not up on the current trends, but at the time I was using both the USPS API as well as a 3rd party API database/package.  

with a quick google, this looks like a decent starting point to give you an idea of what is involved and available. 

[https://www.smarty.com/articles/usps-api#usps-address-validation-apis](https://www.smarty.com/articles/usps-api#usps-address-validation-apis)",25,x9bkye,"So long story short, I need to match historical addresses with current addresses.

I am running into issues like Street being St, Drive be Dr, South being S, etc.. But also issues being 406 South Main Street being  406 South Main. 

&#x200B;

People who have dealt with addresses like this, how do you do it? I'm thinking maybe a fuzzy match of some sort? Whether R, Python, or Excel.. any solution.",datasets,2022-09-08 13:58:03,18
Is there labelling involved here? We (www.acmeai.tech) can help out in regards to labelling but unsure if we can help collect said data. Leave a ping if that's something you'd like to discuss.,2,x9on53,For a final project I will develop a intelligent monitoring and maintenance system for space launch site. But now I need to find dataset or database about launch site infrastructure. This data should comprise the datas about equipments and sensors. Contact to me for some help please,datasets,2022-09-09 00:08:45,2
"1/137 is the cosmological constant. It’s what determines the amount of zero point energy in the vacuum 

C is the speed of light. 

These are the constants which determine which of the 10^500 universes we live in. 

https://en.m.wikipedia.org/wiki/Cosmological_constant",3,x9zttv,I would like to hear everyone's thoughts and ideas about the central finite curve in rick and morty. Like what the C in C-137 could stand for or why it would be such a short number considering the infinite dataset. Figured this might be a good place to ask but if not I apologize in advance.,datasets,2022-09-09 09:24:07,4
"you can try newzoo, they publish accurate reports and have amaing datasets. However, I think they have a platform called expert which is behind a paywall.",1,x9pycv,"Trying to look for resources on gaming statistics like:

* company sales data
* company rankings
* per title/game sales data
* online game microtransaction sales data

Anyone know? Because Statista is paid, VGchartz is only for paid games and is mostly outdated, and I can't exactly use Steam's numbers as I need data that is platform-agnostic and all-encompassing (multi-platform + mobile).

Any help will be appreciated.",datasets,2022-09-09 01:31:02,2
G,1,x98jiz,[https://towardsdatascience.com/working-with-openstreetmap-data-37da18d55822](https://towardsdatascience.com/working-with-openstreetmap-data-37da18d55822),datasets,2022-09-08 11:55:13,1
"Hi, I don’t have your dataset but I think OpenStreetMap may have what you’re looking for.",2,x98fy7,"Hey guys! I was wondering if anyone can provide a list of the following stores with store names and full addresses? 

Home Depot
Kohl's
Denny's
Whataburger 
White castle 
Murphy's Express 

I'd appreciate it very much.",datasets,2022-09-08 11:48:23,3
"Reonomy is the goto for a lot of RE data. As far as permits go, maybe [the census data](https://www.census.gov/construction/bps/).",1,x95rgv,"I’m in the Residential Real Estate industry & I want to be able to differentiate myself in my marketplace.  Is there any resource, or inexpensive software that would give me micro economic data relating to housing.  Examples would be;
Permits pulled, Under Construction, # of Foreign transactions, Supply & demand, Mortgage applications & etc.",datasets,2022-09-08 10:00:33,1
This is called geocoding. I would ask in r/gis,3,x8j0j7,I have a list of addresses. What is the most effective way of iterating through them and receiving their suggested lat and lon points?,datasets,2022-09-07 15:47:01,9
"There's a product I think called 2captcha that will open captchas for you, for like a fraction of a penny. If that's what you're looking to do.",1,x88v1n,"I made this a couple months back and I figure I should post it here. 
It has a pretty basic image extractor (use a vpn) and a dataset of 17k images. 
Some of the images have a white border on the side of them, I don’t know why it happens but it doesn’t affect training. (At least if you don’t include it in your labels)

Also, after a couple minutes your IP gets flagged (I’m assuming that’s what it is) but if you are using a vpn (like you should be) you can just change locations. 

Don’t waste your proxy bandwidth on this

If I start working on recaptcha stuff again i will remake this to be more efficient.",datasets,2022-09-07 09:00:32,2
"Incresoble article alec. Maybe one of the most revolutionary bits of data to hit the US healthcare industry for a generation.

Are you aware of any efforts to wrangle this vast resource?

Additionally, are you able to poat the metadata tags?",21,x7eeid,,datasets,2022-09-06 09:06:27,10
"Hey newtoredditahaha,

I believe a `request` flair might be more appropriate for such post. Please re-consider and change the post flair if needed.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/datasets) if you have any questions or concerns.*",1,x7z19o,"Hey guys. I'm looking for a (public) dataset that contains info about crimes of all kinds, including murder. I would like to investigate what has an impact on a possible murder. But for this I need data about murderers (there are enough on the net), but also data about ""normal"" crimes to be able to run a suitable regression. I am glad about any help.",datasets,2022-09-07 00:50:06,1
I think I could guess half of them.,3,x7bhmj,,datasets,2022-09-06 07:05:16,3
https://maadaa.ai/datasets/,1,x7psz3,I am doing a project where I try to draw bounding boxes around all of the characters in a handwritten document.  I don't need to classify the characters I only need to locate them.  Does anyone know of a dataset or a way to get a dataset that is handwritten and has lots of character-level annotations?,datasets,2022-09-06 16:53:39,2
[deleted],2,x7uk7z,"I'm hoping to create a short survey for a group event in a few weeks that operates like a match maker algorithm, where each respondent receives a ""match"" for their counterpart with the most shared responses in common. It doesn't need to be an exact science; for instance, if multiple people had identical responses, it could even output the first identical match. Or alternatively, I'd be happy with a top three set of results for closest matches.

I have an idea of how I could maybe do this with a google survey, but it'd be time-consuming as I'm still skilling up in data analysis. Does anyone know if there's any resources out there that might do this sort of task?",datasets,2022-09-06 20:36:24,1
Try Census.gov,3,x7dfea,"Does anyone know where I can find these or something similar? In general, is there a good open source resource for shape files?",datasets,2022-09-06 08:26:45,7
https://doi.org/10.1093/isq/sqac050,2,x6sxpa,,datasets,2022-09-05 15:12:39,2
[deleted],1,x7c8kk,"Hi everyone,  


I just begun a course at my university where was have to do some machine learning on a dataset.   
Could be Classification vs Regression vs Clustering. Does anyeone have a case that could be exciting to do some algorithm on?",datasets,2022-09-06 07:36:59,1
Textkernel.com has it. If you are nice to them they might help you out.,1,x74sd3,"Where can I find data on teacher job positions at websites like [indeed.com](https://indeed.com)? I was trying to scrape [indeed.com,](https://indeed.com) but it seems to block my scraping.",datasets,2022-09-06 01:09:48,2
FRED might be useful,3,x6pbsl,Looking to find historical macroeconomic data by country with monthly granularity if anyone could help!,datasets,2022-09-05 12:45:17,5
"Don’t know of any that are available. Maybe there is a Zillow API. Check out the ‘housingData’ R package found in the link below. They have data only up to 2016 by county by month. Check to see where they sourced it. This might be a good start. 



https://cran.r-project.org/web/packages/housingData/housingData.pdf


Edit: here is a Zillow link where I think you can download the data you are looking for 

https://www.zillow.com/research/data/",1,x6uleb,"I'm looking for the average house price by county in Missoula. Preferably on a monthly basis. So far I can only find partial sets of some counties, or the average on a monthly basis for the whole state. Would appreciate any info anyone can suggest!",datasets,2022-09-05 16:24:05,2
"My understanding is that since the 70s, BLS publishes the CPI for regional and the largest metros. Previously they published a lot of cool city-level prices. I wasn’t aware they ever produced anything at the state-level.

I’d suggest poking around on this [University of Missouri library site](https://libraryguides.missouri.edu/pricesandwages/1960-1969) for historical price data. It’s a very neat resource, but be warned, it’s mostly links to pdfs. Sometimes I just browse it to see what cool stats used to be published.

A similar resource, but a little trickier to use, is [FRASER](https://fraser.stlouisfed.org/title/cpi-detailed-report-58?browse=1950s). They might also have some relevant pdfs.",1,x6mt5n,I was wondering if anyone might know of any data sets with data on inflation rates (rates for different types of products even better but not required) for specific states (in the US)? - I found some dating back to the late 70s but am looking for data beginning in the late 60s or early 70s.,datasets,2022-09-05 11:03:03,1
Maybe [this](https://www.kaggle.com/code/iamhungundji/book-summary-genre-prediction/data) would work?,1,x6cra5,"Hi,

For a schoolproject I need to make a machine learning app. I decided to make a app where you can swipe books to the left (not interested) or right (interested). I need a dataset where there are genres connected to the books.

I already looked online but couldn't find a right one (a 3,5gb dataset was to much ;).).

Can you guys help me?",datasets,2022-09-05 03:22:35,2
https://www.kaggle.com/datasets/kaggle/recipe-ingredients-dataset,2,x6chyx,Need help finding a dataset with food recipe names and their corresponding ingredients. Any help would be really appreciated!!!,datasets,2022-09-05 03:06:13,3
"I mean honestly this seems like alot. Not to disrespect the effort made to highlight useful resouce but 25 books? 

IWhere should one begin. Which specific book",5,x5l62v,,datasets,2022-09-04 04:54:19,1
"I think you might struggle as this is all relatively proprietary information. Each organisation manages change very differently. It’s also never black or white about the outcomes. 

I’ve worked in this field for 15 years and even the biggest failures of projects are somehow badged as a ‘success’ as they will have delivered something, it’s also unpalatable to badge your investment as a failure.",6,x5e8wk,"Hi,

I am desperately looking for a dataset that has mapped (preferably IT) projects and labeled them as a success or failure (cannot meet deadlines and customer expectation). My goal is to predict whether a project will succeed or fail based on the project metrics such as budget, time, scope, etc.

I have already looked online, but have not been able to find any yet.

Hopefully you guys can help me out. Thanks in advance!",datasets,2022-09-03 21:48:32,3
"Depends on what you mean by ""connect"". But probably you want some programming language to access the data and then format it into a form that you can work with.",3,x4wy2o,"Hello r/datasets, I am currently working on a project with 3 others friends and we are to select 4 different datasets in which each datasets must be able to be connected with the others in a natural way. So far we have looked through kaggle.com, but we seem to be having trouble connecting the different datasets. Any advice on how go about this?",datasets,2022-09-03 08:22:08,15
maybe here if you register https://imaa-institute.org/mergers-and-acquisitions-statistics/united-states-ma-statistics/,2,x4px77,I am looking for a list of the top 100 acquisitions in the US for every year by transaction volume. Do you guys know where I could find this data for free?,datasets,2022-09-03 02:05:12,2
I seem to need to sign up to see it.  Maybe set up a full demo site with 3-5 companies to look at would be a good next step,4,x45lw2,"Get financial data historically for public companies in North America and very granular segments + KPI metrics.

Great data source for investors to do fundamental research.

Let me know what you think! [https://www.stratosphere.io/company-search/](https://www.stratosphere.io/company-search/)

EDIT: quick note, an account signup is needed for the granular data + KPIs because need to protect API call usage from scrapers.  Hope you can understand that!",datasets,2022-09-02 09:17:06,15
!Remindme 3 days,1,x4b1he,"I'm aware that the FDA has such a DB for grocery store item and I would presume they would have such a DB for the menu items from various fast food chains as well but I've been unable to locate any relavent information from them. Would anyone happen to know of a resource that could provide this information, freely, unlimited and that is regularly updated? Thanks.",datasets,2022-09-02 13:04:14,2
https://www.digitaltrends.com/mobile/how-to-save-text-messages/,1,x4i0t1,"For the first couple years I would manually transcribe the ratings into an excel spreadsheet every couple months. It was tedious and as my mental health has gotten worse I was unable to keep up with it. I would like to somehow get the rating numbers into a spreadsheet (or whatever else you suggest) so that I can then find someone to help me graph it. The end goal is to have a graph (or whatever visualization would work best…I’m open to suggestions) and then add notations indicating medication and treatment changes to see what treatments may be worth revisiting because I’ve kind of run out of new treatment options. 

The text thread is pretty much 2 texts a day. One from the doctor’s system asking me to log my mood, and one from me with a number. There have been a few times when I received an additional text from the system or I forgot to log but not many of those times. About a year ago they closed my account because I don’t see that provider anymore but I’ve continued to text my mood rating just so I’d have it in one spot. So for the last year it’s just the texts from me with the number. 

I don’t have much money but could spend about $20 (hopefully that’s enough) if there is some kind of app that could download the text thread to my computer and extract the numbers. If this is the best option I’d be grateful for recommendations of reliable apps that won’t invade my privacy. 

Really any help or suggestions would be so amazing. I’m at a loss, overwhelmed, and don’t know what to do. But I believe this data would be very valuable to the further treatment of my “treatment resistant” depression. 

Thank you for reading",datasets,2022-09-02 18:23:44,14
You can scrape the website if you need to.,8,x406aj,"So there is a project I've been planning for which I need a wordlist of a lot of Anime (preferably MyAnimeList, can also be another website) 

I already have a list with Anime in Japanese but unfortunately the list I found does not contain the English names. For my project I need a .txt file of Anime in **English** and **not Japanese**.

The wordlist of Anime I found by googling is this one: [https://github.com/ryuuganime/animanga-wordlist](https://github.com/ryuuganime/animanga-wordlist)

MyAnimeList website: [https://myanimelist.net/](https://myanimelist.net/)

Maybe someone in this subreddit is able to create one or already has a list :)

Thanks in advance!",datasets,2022-09-02 05:22:53,3
"Perscription opiod dataset [https://www.reddit.com/r/datasets/comments/moc6x9/opioid\_prescription\_dataset/](https://www.reddit.com/r/datasets/comments/moc6x9/opioid_prescription_dataset/)

French data Here is the database for France. https://base-donnees-publique.medicaments.gouv.fr/  https://www.data.gouv.fr/fr/datasets/base-de-donnees-publique-des-medicaments-base-officielle/ or [https://www.data.gouv.fr/fr/datasets/base-de-donnees-publique-des-medicaments-1/](https://www.data.gouv.fr/fr/datasets/base-de-donnees-publique-des-medicaments-1/) 

&#x200B;

I thought Ben Goldacre was behind a similar dataset but I cant find it. [https://wellcomeopenresearch.org/articles/6-360](https://wellcomeopenresearch.org/articles/6-360) https://journals.sagepub.com/doi/full/10.1177/0141076820918238",2,x3r9r3,"Looking to answer questions like: 

How much do US hospitals spend on drugs (of all kinds), from cheap generics & commodity supplies to high price biologics? What is the breakdown?

How much revenue is generated by pharmaceutical manufacturers from US hospitals? 

Appreciate any insights or suggestions!",datasets,2022-09-01 20:42:11,1
Have you checked out the Google places Api?,3,x34f0d,"Is there a way to download review stats (type of site (tower in paris), rating and number of reviews) of every site in a given country? 

Or at least to view them as a list somewhere?",datasets,2022-09-01 03:48:45,8
"What do you need, a large library of images to try this on ? 

The SnapChat ""leak"" would be a fun experiment. Not sure how amazing it'd work though.",0,x2zwxn,"I want to find where in a given image, someone would look the most. It is sort of like a heatmap of where someone would look in an image. Here's an example: [https://ibb.co/RC2FP3n](https://ibb.co/RC2FP3n) 

Is there any dataset that I can use for this?

Edit: A dataset for user attention heatmap.",datasets,2022-08-31 23:07:00,5
"**REIGN DATASET: International Elections and Leaders** Contains data on the tenure and personal characteristics of world leaders, and types of political institutes and regimes. The website suggests that it includes data on their political leanings (e.g., centre, independent, centre-left, right, social democratic, etc.) but this information is not shown on the codebook. Given this:

**ParlGov Dataset** Contains data that classies parties into eight party families: Communist/Socialist, Green/Ecologist, Social democracy, Liberal, Christian democracy, Agrarian, Conservative, Right-wing radical. You can use this as an indicator of leader's political leanings. Also includes data on when elections are.

&#x200B;

EDIT: Also, I found the Reign dataset by Googling 'European leaders dataset'. Why do people never use Google?",1,x37wmv,"I've been looking for some datasets/websites to scrape that have the following information:

European leaders, what party they represent, their election details, their political leaning and when the next election is in their country

I haven't been able to find anything concrete. Any help would be greatly appreciated.

Thanks",datasets,2022-09-01 06:37:38,2
Actual dataset [https://github.com/MIERUNE/GTFS-GO](https://github.com/MIERUNE/GTFS-GO) but I thought the twitter thread explaining what it does with images was worth posting as the lede,2,x2hvzg,,datasets,2022-08-31 09:25:51,1
"There is a data set from the website [hikr.org](https://hikr.org) on Kaggle that might be a good starting point.

Kinda depends what you want to tell with the data, are you trying to find the time / routes to get up a trail or the amount of money that could be brought in if there was more hiking? You could always try to collect GPS data yourself and/or create cheap GPS trackers to hand out to get the data.  It might be easier to  take and find data from similair countries and model it to your area, etc.",1,x2m43m,"As the tittle says i currently searching for databases about  mainly trekking but also other sports like that, what i mean is sports that use GPS or other resources to get data

i want to do some analisis for my finals years in the university and run a proyect of investigation about it, i live in a country with a great ammount of mountains and places for that type of sports but since is a third world country there are almost no data or works arround sports so im searching for data bases about it or similar proyects so i can make analsys and try to present it in my university and country

&#x200B;

any help would be apreciate it, any ammount of data would be great :)",datasets,2022-08-31 12:19:50,2
"I feel like it would vary too much between households to get a true idea unless you had a bunch of people give you the figures themselves. A single person or older couple in a bungalow would be much different to a family of 5 in a larger house.  House age and how well its insulated would also be a factor.  Also how often people are home and weather changes etc.

There are just too many factors to get a clear estimate per week or month as it varies so much between each household with so many variables in my opinion. It could also be much warmer in one part of the UK than it is in another so that would be a big variable to consider.",1,x2ozeu,"As the title says, I'm looking for home energy consumption rates (kWh) for the UK context, on a more granular basis than ""per year"". I believe the average *annual* household energy consumption rate is easy to find, but I'd like to explore what it is on a monthly basis, or even weekly. I'd like to do this for both electricity and gas (separately). It feels like this data should be available somewhere/somehow 🤔

Thoughts?",datasets,2022-08-31 14:18:51,3
try to scrap h1b public data,2,x211m1,"I am looking for salary dataset for tech companies. Is there some dataset with country (and city if possible), experience, company, approx. joined date, any stocks or bonus, etc ?

I need this for my univertsity project so should be at least 8 columns and 1000+ rows (more the better) to perfrom EDA and Regression. I found a few in kaggle and want know if there are more or may be some sources from where I can extrat these data.

Thanks",datasets,2022-08-30 18:52:54,6
https://pointsixtyfive.com/xenforo/wiki/facilities/,3,x1lbc6,"I am looking for U.S.-based air traffic controller staffing levels for each facility, any idea if this is public data?",datasets,2022-08-30 08:00:01,2
"Is there a reason that you’re avoiding the Twitter api? It would have all of this

If this is for your dissertation, you can get academic access which will allow you to pull 10 million tweets per month and give you access to the full archive of tweets.",18,x1m7y3,"I'm looking for a high quality tweet scraping software that pulls tweet content text strings, likes, and retweets. I'm building a dataset for my dissertation research. Please excuse my lack of knowledge in this area. TIA!",datasets,2022-08-30 08:36:17,7
How does one get at this data set?,2,x0xids,,datasets,2022-08-29 12:29:44,5
Us census does have it but they probably don’t use zip code because it’s not statistically relevant.,1,x0qyj3,"I'm trying to find a dataset that lists economic and demographic data by zip code for Virginia , Maryland, and DC.   I would have thought the census site would provide that but I either can't figure that out or it doesn't.  Does anyone know of that data.

The end result is that I want to be able to build a simple profile per school district on those regions.

Thank you for any information.",datasets,2022-08-29 08:01:22,8
Look on statsbomb.  They might have a historical set with that.  Or try Opta.  They definitely have action+location data and they occasionally have a free set of data out.,2,x0xf9t,"Hi there. If anybody knows the source of such data (website, maybe Kaggle?), please share.",datasets,2022-08-29 12:26:14,1
Yoh may use this: https://www.huduser.gov/portal/datasets/usps_crosswalk.html,2,x0tzxc,"This is likely just a newbie question, but has anyone had success relating the US Census Urban and Rural Classification (UACE codes) with ZCTA (zip codes)?

[2010 Census Urban and Rural Classification and Urban Area Criteria](https://www.census.gov/programs-surveys/geography/guidance/geo-areas/urban-rural/2010-urban-rural.html)

[ZIP Code Tabulation Areas (ZCTAs) Relationship Files](https://www.census.gov/geographies/reference-files/time-series/geo/relationship-files.html#zctacomp)

UDS Mapper put together a great crosswalk between zip codes and ZCTA codes, [https://udsmapper.org/zip-code-to-zcta-crosswalk/](https://udsmapper.org/zip-code-to-zcta-crosswalk/). I just haven't figured out how to relate the ZCTA with the UACE codes.

Any thoughts?",datasets,2022-08-29 10:06:31,2
"You might have to build the data set from multiple sources. Small to medium churches, community parks, YMCA, Elks Lodge....",1,wzuonu,"Hello ! Long time lurker. I just wanted to check if it is possible to get dataset of public utility buildings along with latitude and longitude either from google maps or any other sources for any one major city in the world(City actually doesn’t matter since this is more of a personal project).

I do have experience with scrapping but couldn’t find a proper website from which I can scrap data from.

Edit: By public utility building, what I meant was those buildings which can be rented by anyone for a community event. I do understand it’s a very niche use case but any help with direction of thinking can also help.",datasets,2022-08-28 05:47:31,6
"Hey randomaveragecitizen,

I believe a `request` flair might be more appropriate for such post. Please re-consider and change the post flair if needed.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/datasets) if you have any questions or concerns.*",1,wzzyeo,"I'm making a QA system for environment-related topics, and I wanted to share two links I found that could be useful for other environment-related projects:

1. [Index of environmental articles](https://en.wikipedia.org/wiki/Index_of_environmental_articles) \- a page containing all(?) Wikipedia articles on environmental issues, useful for NLP tasks and QA systems
2. [Environmental science databases](https://en.wikipedia.org/wiki/Category:Environmental_science_databases) \- a Wikipedia page with links to some environment-related databases, possibly a good starting point for finding datasets

Feel free to leave links to other related collections below.",datasets,2022-08-28 09:45:39,1
"Honestly a dataset like this will most likely not exist and will require you to build it yourself. This isn't exactly something you can make a FOIA request detailed ""I need everything on Congress"". Fortunately, many U.S. government sites allow scraping and crawling, especially the DTIC.",5,wza8dn,"Hello, I hope looking for data is allowed here. 

I want to download the complete history of US congress as it appears on congress.gov (and [bioguide.congress.gov](https://bioguide.congress.gov) for the older stuff).

Both sites will be fairly easy to scrape but thought I'd ask here before putting in the work. 

Suggestions of better places to download from are welcome! I thought Library of Congress would have something but couldn't find it.",datasets,2022-08-27 11:44:19,10
I would imagine there would be effort put in to make that data not readily available.,2,wzogsb,"Hi! Is there a dataset out there that contains CAPTCHA response data? I'm hoping to find one that contains both human and impostor (aka bot) responses, along with data on things like their timing for clicking, mouse movement, etc. I know it's quite specific and I've had no luck in finding a dataset like this, but I was hoping that I would have better luck here. TIA!",datasets,2022-08-27 23:24:02,1
I believe this is what you are looking for: http://www.omdbapi.com/,3,wz3cr7,"I'm looking for a continually updated database, API or any other resource in general that would allow me to obtain IMDB like data but for free and unmetered. Does anyone have any suggestions for such a resource? Thx",datasets,2022-08-27 06:50:14,2
Omg really?,2,wy64ho,"Hey everyone, just posting here to let you know that I and some colleagues released a public dataset of MI therapy sessions.  


Link to the dataset (Anno-MI) with description: [https://github.com/uccollab/AnnoMI](https://github.com/uccollab/AnnoMI)  
Related publication: [https://ieeexplore.ieee.org/abstract/document/9746035](https://ieeexplore.ieee.org/abstract/document/9746035)  
Plus, little thread covering the main strengths of the dataset: [https://twitter.com/simoneballoccu/status/1552238783042650117](https://twitter.com/simoneballoccu/status/1552238783042650117)  


Hope this can help someone out there :)",datasets,2022-08-26 03:57:53,4
"Pitchbook, CB Insights, crunchbase should have all of this information. The only downside is that it costs money.

You might find summarized reports from the providers for free as well.",1,wy9j2l,Any ideas where to look?,datasets,2022-08-26 06:40:51,2
"You should look into regular expressions (regex). If you want to use R, stringr has a bunch of functions to let you match regular expressions and extract data from them.",2,wxzeki,"I'm working with some old field guides in order to better track and understand the [Shifting Baseline](https://en.wikipedia.org/wiki/Shifting_baseline) effect found in North America. Using excel and Power Query and Power Bi, I have successfully separated out each species of mushrooms and respective information.  I have them even separated out by subsection.  But the English language is causing me problems.  For examples on whether something is edible, I'll have:  


1. ""This mushroom tastes bad but is edible""
2. ""This mushroom is poisonous""
3. ""This mushroom tastes amazing, especially with butter cream""
4. ""This mushroom is not poisonous. But is too small to be of any use""
5. ""This mushroom is edible but it's too rare to become a regular staple""

Using the examples above I would want something along the lines of:

1. Taste = bad, Edible = yes, Poisonous = no, N/A on everything else

2. Taste = N/A, Edible = no, Poisonous = yes, N/A on everything else

3. Taste = amazing (or good), Edible = yes, Poisonous = no, N/A on everything else

4. Taste = N/A, Edible = N/A, Poisonous = no, Notes: too small, N/A on everything else

5. Taste = N/A, Edible = yes, Poisonous = no, Notes: too rare, N/A on everything else

Ultimately I want to be able to run this information through R, or another statistical program, so I can get P-values, bell curves, etc. so I can better compare the differences between mushrooms records then and mushroom records today.   


I have over a dozen sections like this, with several hundred specimens, so doing this by hand would be extremely time consuming and impractical. Does anyone know of a program that can turn human sentences into data points?",datasets,2022-08-25 21:26:07,2
"I needed about anything like that too, its very hard to find",1,wxev7q,"Hi everybody!

I'm looking for a data set that would address the question of how strictly is the internet regulated in a country. The level of online censorship, how many regulatory laws are in place, things like that. Anything come to mind?

Thanks!",datasets,2022-08-25 06:53:14,1
Where have you already searched.,1,wxe2j6,"Hi guys, I'm working on my master degree of statistics and I have this project to compute a model on cancers dynamics x pathologies. 
Do you know were I can find some detailled panel data on the subject ? 

To who read this thank you 😁",datasets,2022-08-25 06:18:47,2
"For Denmark, this is the national statistics: https://www.dst.dk/da/",2,wxm6gw,"I want to run a new project that would be based around analyzing the event industry prior and post COVID in each country. I focus mainly on sector in Nordic, can anyone guide me to some good datasets from Denmark, Sweden, Norway and Finland? I focus on festival, conferences and events over 1000 attendees.",datasets,2022-08-25 11:49:31,2
I endorse this statement.,2,wxly0f,I need help to find a suitable database,datasets,2022-08-25 11:40:12,2
"Best bet is zoominfo, you can try opencorporates but it’s going to be tougher. Hardest ones by far are gonna be franchises.",1,wxk3mj,"Hi all I was lucky to find the restaurant database for Toronto but desperately trying to find the database of Florida restaurants. 
 
Any assistance  is massively  appreciated",datasets,2022-08-25 10:25:58,4
"This can be difficult to find because of HIPPA. Would chest x-rays be good? Try dataisplural.com they have a large list of datasets, with a few on disease",2,wwo61s,"Just as the title, which other websites can I find classification datasets from?",datasets,2022-08-24 09:36:11,5
"Hey KarlaSect13,

I believe a `request` flair might be more appropriate for such post. Please re-consider and change the post flair if needed.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/datasets) if you have any questions or concerns.*",1,wwug38,"Lmk any type of companies / data you are looking for! I have access to a large database where I can get:    
a) vendors they buy from   
b) cost they buy things at, price they sell them at   
c) sales history   
d) all email addresses including employees   
e) total value of inventory  etc. etc.    


Vape Shops, Bars, Retail stores, Cosmetics, pretty much any vertical. Let me know if you're looking for anything like that and we can talk!",datasets,2022-08-24 13:52:42,4
"Closer I can think of is [this dataset of sql injection](https://www.kaggle.com/datasets/syedsaqlainhussain/sql-injection-dataset). The dataset you want seems to be hard to find because you'd need access to the error log of different websites with public datasets, and these error logs are usually private or not easily accessible (and almost never saved in a normalized fashion). And I assume there will be a lot of noise too, for whatever you want this dataset for.

Maybe I'm wrong, would love to read if anyone knows something similar.",1,wworsx,"Hi everyone!

I'm on the look for a dataset that consists of database errors caused through user activity on a web application or web API. It is therefore important that the dataset should include an anonymized form of user or user session identifier that can be used to link each database error in the dataset with a specific user. The database I'm most interested in is MySQL (any version but preferably 8.0).

In summary, the dataset I'm looking for should have the following information for each entry:

**\*** *Anonymized user/user session ID*

**\*** *Timestamp of when the error occurred*

**\*** *SQL query which caused the error (without PII)*

**\*** [SQLState](https://mariadb.com/kb/en/sqlstate/) *value*

**\*** [Error code](https://mariadb.com/kb/en/mariadb-error-codes/)

Kind regards,  
Tolga",datasets,2022-08-24 10:00:24,2
https://www.census.gov/library/stories/2019/08/are-women-really-opting-out-of-work-after-they-have-babies.html,5,wwc4q1,Hi community!! I am looking for datasets that containing data about women quitting their jobs after child birth. If this is not the right place to ask please guide me to the specific sub. Thanks is advance.,datasets,2022-08-23 23:37:47,3
"Hey Mean-Pin-8271,

I believe a `request` flair might be more appropriate for such post. Please re-consider and change the post flair if needed.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/datasets) if you have any questions or concerns.*",1,wwpgbw,I want to start with my personal projects so I need resources to get some ideas.,datasets,2022-08-24 10:28:05,1
Have you seen fbref.com?,1,wwoots,"Looking for data about the different european football teams! Any help is appreciated!
Thank you!",datasets,2022-08-24 09:57:23,4
"ECB SDW has all the main exchange rates and certainly inflation : https://sdw.ecb.europa.eu/


You can always check Eurostat : https://ec.europa.eu/eurostat/databrowser/explore/all/all_themes?lang=en


Both have APIs and plenty of dedicated libraries.",2,wvrvkb,"Hey Guys, I need information about the Monthly and Yearly Inflation rates by European Countries from January/2021 until July/2022. I also need the Exchange rate per month according to Euro for the same period. Where can I find this information? I appreciate any help you can provide.",datasets,2022-08-23 08:33:43,3
"FWIW, Indy has UCR Part 1 data from 2007-present year with date, time, crime, lat/lon

https://data.indy.gov/search?q=ucr&sort=-modified",3,wvquhi,"Many US cities maintain public datasets of their crime data covering a number of years. I'm looking for a WELL maintained data set (includes data dictionary), that includes as much detail as possible regarding any crime committed in that location for 5 years or more, the longer the better. Details would include crime date/time, crime type, crime location (latitude/longitude), perpetrator description (age, race, sex), victim description (age, race, sex), etc. Thanks.",datasets,2022-08-23 07:51:51,2
did you check the fed?,1,wvachp,"I’m looking to make a dashboard and haven’t been able to find anything public, aggregate or otherwise.",datasets,2022-08-22 17:42:38,4
"/u/Onyoursix101, /u/give_me_two, you guys expressed some concern about what I was doing. The thread was locked before I had a chance to respond, so my response is here.

I'm a Bayesian, meaning that I always endeavor to fit my beliefs to the evidence, never the evidence to my beliefs. The evidence should speak for itself, and in this case, the evidence is saying quite clearly that conservatives do not believe the government is run by telekinetic witches.",3,wutuxb,"Responses have slowed down. I'm sure I could get a lot more by reposting, but I'm happy ending things here.

There's this stereotype of the modern conservative as not understanding science, believing in conspiracy theories, and believing in the supernatural. I wanted to explore the reality of this stereotype. Here's what I found.

If you want to look at the raw data, you can find it [here](https://pastebin.com/7Jwi9WfY). Just click download, and save that as ``beliefs and values.csv`` and then you can open it with a spreadsheet program like Excel.

# Results

There is a high correlation between conservative values and Christianity (70%). I don't think this comes as a surprise to anyone.

There is *not* a significant correlation between conservative values and other paranormal beliefs (1%). In other words, conservatives are exactly as unlikely to believe in superstitions, cryptids, magic or psychic powers as liberals.

There is a negative correlation between conservative values and science knowledge, but it's on the smaller side (-28%).

There is also a small correlation between conservative values and conspiracy beliefs, but again, on the smaller side (35%).

There definitely is a sharp divide between fiscal conservatives and moral conservatives. Moral questions, like supporting abortion and traditional values correlate highly with each other, but not highly with fiscal questions like supporting business and fiscal responsibility.

Since I got many of my responses from /r/prolife, let's look at the profile of the pro-lifers. Opposing abortion correlates strongly with conservative values (79%), and strongly with Christianity (63%), and also with moral issues, particularly traditional values (67%), traditional marriage (71%), and the family unit (60%). These same issues also correlate strongly with Christian beliefs.

Long story short, while scientifically ignorant, conspiracy-minded conservatives do exist, they're a minority within the conservative party, and do not represent the party as a whole. The idea that conservatives believe in magic and superstitions seems to be completely inaccurate, as they're actually no more likely to believe in these things than the rest of the population, and significantly *less* likely to believe in these things than more liberal Christians.

Pro-life attitudes correlate with Christianity and other moral conservative values--though there definitely are pro-lifers who are not Christian, and Christians who are not pro-lifers. There are even pro-lifers who are not conservative, and conservatives who are not pro-lifers (well, there was one.)

---

# Methods

I had participants answer four questionnaires:

* The 12 Item Social and Economic Conservatism Scale

* The Conspiracy Mentality Questionnaire

* A slightly edited version of the Revised Paranormal Belief Scale

* Five questions from the Science Knowledge Survey

---

I had everyone answer the entire five-item Conspiracy Mentality Questionnaire, but I ended up throwing out two of the items:

* I think that many very important things happen in the world, which the public is never informed about

* I think that politicians usually do not tell us the true motives for their decisions

The simple reason for this is that pretty much everyone agreed with these two statements. The Conspiracy Mentality Questionnaire is almost a decade old. About a decade ago, I'm sure these questions had value, but anyone who's lived through the past decade and paid attention to the news, no matter what side of the political spectrum they're on, is probably much more likely to agree with these items now.

Standards for what constitutes ""crazy conspiracy thinking"" shift over time. Similar shifts can be seen following the JFK assassination, and Watergate.

In future research, I'll only use the other three questions of the Conspiracy Mentality Questionnaire, and I suggest that others do the same.

---

About two years ago, I questioned people on the entire 25-item Science Knowledge Survey. Based on my findings at that time, I picked out five of the items that best predicted a person's answers to the other 20 items. Obviously some accuracy will be lost, but this is just a simple, informal little survey, and I wanted to keep it short so that more people would respond.

---

The Revised Paranormal Belief Scale is 26 items. I trimmed out three items:

* There is life on other planets.

* The horoscope accurately tells a person's future.

* Some people have an unexplained ability to predict the future.

The first item, I removed because life on other planets is a statistical likelihood, and I feel that a positive answer to this question does not indicate paranormal beliefs. In retrospect, I should have changed it to ""Aliens have visited Earth"". Alas, I did not think of this until after I started getting responses.

The other two items were removed because I saw them as virtually identical to two other items:

* Astrology is a way to accurately predict the future.

* Some psychics can accurately predict the future.

I was trying to keep the survey as short as possible to get more people to respond, and that means removing redundancy when I see it.

There was one additional item that I decided not to score:

* Witches do exist.

After seeing a number of positive responses to this, I realized that many of the people responding were aware of the religion of Wicca, whose adherents call themselves witches. Thus, this question does not accurately predict paranormal beliefs.

I also made slight changes to a few items on this scale. I changed ""I believe in God"" to ""I believe in a higher power"", so we'd get a positive response from non-Christians who believe in a deity.

I also changed ""The number '13' is unlucky"" to ""Some numbers are unlucky"", because 666 and 4 are also considered unlucky numbers to some people.

For further clarity, I separated the four ""Christian"" items from the other paranormal belief items:

* The soul continues to exist though the body may die

* There is a devil

* I believe in a higher power

* There is a heaven and a hell

Obviously these beliefs are not exclusive to Christianity, and can be found in several other religions, but we also expect to find them in the majority of Christians as well, and I felt it important to analyze these separately from the other items.",datasets,2022-08-22 06:36:16,4
Bloomberg? Reuters?,2,wv53i6,Anyone know where I can get expired commodity futures data? Not just front month but the whole chain? Ideally for energy,datasets,2022-08-22 14:02:37,8
I think I saw something on kaggle..,1,wvbohs,Title says it all. Any help appreciated,datasets,2022-08-22 18:43:43,1
"One this is not gun violence data.
It's mortality related to guns. IE death certificate data. 


Two, county level gun deaths are going to be *way* below standard CDC censor size.


As a consequence what you're asking for doesn't exist publicly.


If you have a valid research question and the appropriate credentials and institutional backing you can request county level data from the state health department, and possibly the national level from CDC.

In my experience the state level is going to be less onerous than trying to go through CDC, and state will get you all the details and idiosyncrasies underlying what you get.",4,wunv85,"[https://www.cdc.gov/nchs/pressroom/sosmap/firearm\_mortality/firearm.htm](https://www.cdc.gov/nchs/pressroom/sosmap/firearm_mortality/firearm.htm)

This one is state-level gun violence data. I am looking for county-level gun violence data.",datasets,2022-08-22 01:24:53,9
how do you define a true tweet?,5,wv3vwg,"Hi I hope everyone is doing great over here. 

I’m creating my own dataset as a part of my masters dissertation.
Dataset is related to Russia-Ukraine war. 

There is a “Claims” column where different statements are present from official sources. Like  for example “Putin wish to weaken America’s will. says CIA chief”

In response to that I wish to retrieve tweets of people who have used similar vocabulary in their tweets for True tweets. For false tweets  have to show that the person have twisted the statement in his tweet and verify the original claim using some official source like newspaper CNBC, BBC. 

However I’m struggling to get relevant tweets for both Trie and false tweets. 

Can anyone guide me on how I can do that. This is my first time doing data collection so any help would be great. 

Thanks",datasets,2022-08-22 13:14:00,9
"Hey keinefirma,

I believe a `request` flair might be more appropriate for such post. Please re-consider and change the post flair if needed.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/datasets) if you have any questions or concerns.*",1,wuyjxq,"Hello r/datasets community!

Biogas is big business in Europe - there are estimated to be more than 10,000 commercial biogas plants operating in Germany alone.

Due to the gas shortage affecting many places of the continent, there is an even greater focus on biogas production.

As cultivating biogas can be very finicky and unstable, complex algorithms are used to stabilize the biogas process and increase yield. These complex algorithms (such as ADM 1) depend on a large number of parameters, however. Calibrating these parameters is a challenge and is exacerbated by the public accessibility of datasets. Even if you have the luxury of owning a biogas reactor yourself, the process is also relatively slow, so it is hard to generate data with respect to all the environmental process conditions.

In order to alleviate this problem once and for all, I set out to commission a real biogas reactor and make it available to all for the generation of data.

[The result is VENZ1. It is the world's first publicly accessible biogas generator, where experiments can be run from the comfort of your home.](https://keinefirma.xyz/pages/wastebad/datasite_scheduler.html)

**I now need your help in deciding which experiments to run (see above link)**. I basically want to create this dataset with you.

You can help advance research into biogas from the comfort of your home.

The entire infrastructure is already done. You just have to run an experiment from via the above scheduler. As a thank you for your help (and reading this), I gift each one of you (until my codes run dry) a free copy of the professional biogas simulation software [BioNet](http://go.microsoft.com/fwlink/?LinkId=532540&mstoken=P76JP-4YC74-69M6H-RM667-FRFQZ), as seen on the Microsoft Store.

If this works I'd like to create more datasets with the community.

Thanks for reading this. I am also always open for feedback.",datasets,2022-08-22 09:43:01,1
https://www.cambridgema.gov/departments/opendata,4,wtxst7,Title pretty much says it all.,datasets,2022-08-21 04:53:46,4
"Airdna, alltherooms, or STR have that data",4,wtot38,"These would be the local areas within a few miles of major, year round tourist attractions. Such areas are places where short term rentals, AirBnBs, vacation condos etc are highly profitable year round.",datasets,2022-08-20 19:56:59,3
You can access prior year’s Forbes lists by using Wayback Machine and the url the current list is at.,2,wtrdu4,I need data from the new billionaires by country from 2020 to 2022. Forbes publishes a base by person but I can’t access the 2020 because now it is the 2022.,datasets,2022-08-20 22:17:03,1
Probably is better on tripadvisor,2,wtj2ti,"I want to get an estimate of the number of vegan restaurants for each of the 107 provinces of Italy. Data doesn't have to be 100% accurate so I'm wondering if there's a way i can lookup at restaurants with ""vegan"" in their name / description on Google maps. Any way to do this?",datasets,2022-08-20 15:17:44,5
"EconChrisClark on TikTok of all places did a series about recent changes to Russia's economy, including oil exports for as much as there is recent data. Should be included under his TikTok source notes here:  https://econchrisclarke.wordpress.com/tiktok-sources-and-notes/",1,wt5k8g,I want a country by country basis of how much Oil they are importing from Russia after the Russia-Ukraine war happened. I can't really find any other than historical datasets which isn't something I want. Any suggestions?,datasets,2022-08-20 05:09:11,3
First google response: https://cmp.felk.cvut.cz/~spacelib/faces/grimace.html,2,wtcfj4,"Hi, I want a datasets of images for people's expressions for analysis using deepface library in python",datasets,2022-08-20 10:20:25,1
"If you know how to use R, you can get all of this in a matter of minutes using the tidycensus package.",6,wseyv7,"I'm sure this is available via census or some other government website, but I'm having trouble finding it. I'm simply looking for a data set that has income per household per zip code. Adding in population would be a bonus.

An even lower level of detail(tract, block, etc...) would be great, but I figured zip code would be the easiest to find/work with.

Thanks for any help!
-C",datasets,2022-08-19 07:08:57,10
"I'm the CEO of DoltHub (https://www.dolthub.com). We buy data in the form of data bounties and then share the purchased data for free under creative commons license.

[https://www.dolthub.com/bounties](https://www.dolthub.com/bounties)

We use this subreddit to keep people informed about new bounties and how completed bounties went so they can play with the data if they want.",21,wrvzth,I came across this subreddit a few months ago when I was searching for a specific type of dataset (thanks for the help btw!). I’ve been somewhat frequently looking at the posts made here and this got me wondered whether people in this subreddit are willing to buy datasets and if people who conducted their own data acquisition process and have valuable information are willing to sell them?,datasets,2022-08-18 14:53:04,23
"Very neat effect, I'd love to follow your work! Are you publishing at other places than Twitter (GitHub, LinkedIn)? I am afraid I'm not using Twitter myself.

When it comes to data, Gapminder comes to mind, but from what I've seen there all the data is badly outdated.

If you know some NLP, you can use an API of some newspaper (The Guardian has a good one, and it's free) and extract news relevant to your topics. It's only a bit of  a hassle, and could benefit you down the  line---it's something that is easy to automate.

When it comes to unchanging data, did you try querying OSM with tools like osmium? You can then extract data there and save it in the format that you require.

Sorry if I wasn't of much help---keep up the effort, I really like the result!",1,ws7ka3,"I am creating an interactive map which covers issues in this world for my curiousity. You can see the previous discussions. 

[https://www.reddit.com/r/GlobalTalk/comments/wiwzf9/global\_do\_you\_know\_an\_interactive\_map\_which/](https://www.reddit.com/r/GlobalTalk/comments/wiwzf9/global_do_you_know_an_interactive_map_which/)

&#x200B;

I searched on Google a lot. But I could not find datasets as below.

Necessary data:

\*locations of Base station for internet

\*locations of ODA projects

\*locations of research centers and universities for locusts

The data can be shp file, geojson, csv, etc. 

&#x200B;

I wish someone know where they are!

&#x200B;

You can see what I am doing from here.

https://twitter.com/TsuchidaWataru/status/1560520777883328512?s=20&t=h40MolfC2\_xJJ2Hgw9L83A",datasets,2022-08-19 00:23:31,4
"US Energy Information Administration is your best bet. It's all about stats and analysis.

Also Bureau of Ocean Energy Management has awesome stuff too.

https://www.data.boem.gov/

[https://www.eia.gov/](https://www.eia.gov/)",4,wrx9za,"Hi All,

&#x200B;

I am looking for a dataset that has all of the offshore oil rigs of the world and also their geographical location and maybe even barrels per day?  My goal is to understand how exposure is concentrated if, for example, a hurricane goes through the Gulf of Mexico and knocks out a few oil rigs like bowling pins.  Any help would be much appreciated, thanks!",datasets,2022-08-18 15:46:47,3
"Wikipedia has a ratio for many schools. 
You’d have to identify what you consider a university and not just a college. Names can be deceiving. 

Keep track of which ones you get values for while understanding that some might be for current enrollment and others are a couple years old. 

Otherwise you could send each an email and ask.",1,wrrfam,How do I get the male-to-female ratio of all universities in USA?,datasets,2022-08-18 11:48:33,3
drugbank.ca maybe?,1,wrr01t,"Hello, I am looking for a dataset that has a large list of medications and specific allergies associated with each medication. Specifically looking for medications used in palliative care but it is not completely necessary.

&#x200B;

Thank you.",datasets,2022-08-18 11:30:48,1
"Here's some: 

https://www.kaggle.com/competitions/home-credit-default-risk/data

But only non-commercial use is allowed.",2,wrijs1,"I am looking for example, 1000+ people or businesses who have applied for loans and some of their attributes:

Default, Credit amount, unpaid balance, overdue balance, debt, type of loan and other information related to the use of the loan and payment history. (doesn't have to be all of this)

Thanks",datasets,2022-08-18 05:40:04,2
"1. Go here: https://overpass-turbo.eu/
2. Navigate to your area of interest (so that it is visible in the window).
3. Click ""Wizard""
4. Type in `shop=pet` as per the [wiki](https://wiki.openstreetmap.org/wiki/Tag:shop%3Dpet)
5. Click ""build and run query"".

But this is an incomplete record. Another possibility is your area's [Business Registry](https://en.wikipedia.org/wiki/Company_register). They might offer similar services.",1,wrfp2u,Doing a project on pet growth and market segmentation. Any help would be fantastico!!,datasets,2022-08-18 03:12:11,3
"Do you have any python familiarity? Pandas read_html would do this easily 

https://oxylabs.io/blog/pandas-read-html-tables",3,wr3vyz,"trying to extract this [https://www.inc.com/inc5000/2022](https://www.inc.com/inc5000/2022) into an excel file. The whole list is fine, but even better would be just the software companies (they have an Industry category and you can tick the Software box on it to get just software companies)

I tried this in R with some code I found in another Reddit thread for this same list but the 2021 version of it but I must have fucked up somewhere so thought I'd ask again. Any help would be appreciated.

EDIT: thanks for all the suggestions/help. much appreciated. ended up using code I found in the other Reddit thread.

[https://gist.github.com/MattSandy/14242b5af9dce69102647e2000848bcc](https://gist.github.com/MattSandy/14242b5af9dce69102647e2000848bcc)

Courtesy of [https://www.reddit.com/user/mattindustries/](https://www.reddit.com/user/mattindustries/)",datasets,2022-08-17 16:43:35,9
CDC 500 Cities,1,wr3mb7,i need to find datasets for some heart diseases for my graduation project any help,datasets,2022-08-17 16:31:30,1
https://avibase.bsc-eoc.org/checklist.jsp?region=LK,8,wqobrx,Thanks in advance for any help!,datasets,2022-08-17 05:57:35,3
Interesting thread [https://twitter.com/\_HannahRitchie/status/1559856733794844672](https://twitter.com/_HannahRitchie/status/1559856733794844672) where they go through this data and ask for help making corrections,2,wqm3gt,,datasets,2022-08-17 04:03:57,1
"Well, start with the tools you have. The most likely tool you have Excel. You can make the dots what ever you want.",8,wqhb6w,"Not sure if this is in the correct subreddit, but I need to plot a scatter graph for my school project but i’m not sure was program is best to use. One thing i need is that some dots need to be green and others red. Any have any recommendations? Thanks!!",datasets,2022-08-16 23:12:13,11
"i saw this in relation to this map of  european drought [https://twitter.com/dr\_xeo/status/1558789225327345664](https://twitter.com/dr_xeo/status/1558789225327345664)  
I have no idea how to make the map or the exact dataset used. It could be [https://cds.climate.copernicus.eu/cdsapp#!/dataset/cems-glofas-forecast?tab=overview](https://cds.climate.copernicus.eu/cdsapp#!/dataset/cems-glofas-forecast?tab=overview)  


But I thought the data seemed interesting enough that someone interested in the area could find useful data at this source.",1,wqkthl,,datasets,2022-08-17 02:50:17,1
"I've created large-ish datasets in python using pandas and custom lambda functions that generate randomized variables of my specification before. The export to csv. 

Alternatively you could pay for a service like [mockaroo](https://www.mockaroo.com/) that does exactly this (1000 free rows).

1mil observations is a lot though. Good luck!",3,wprch3,"Not exactly looking for an already available dataset since it doesn’t exist, but I’m trying to create a fake dataset for personal use.

• How do I produce over 1 million observations efficiently? 
*Not trying to use regular expressions in Python since I would like it in CSV.

• Any relational characteristics to mimic real datasets? Something that all datasets have?

• Any other comments or suggestions is fine.",datasets,2022-08-16 04:02:43,3
Look at UCI machine learning repository,2,wpvztk,"I need it for a hw, and needs to include categorical, continuous, and discrete variables.
Can be pretty much about anything, as long as it is relatively relevant for any topic.
Thx in advance.",datasets,2022-08-16 07:38:00,3
"At the zip code level, you can use the most recent 5-year ACS. You can find all the different combos (household income by race, etc) at nhgis.org.",10,wpae92,"As the title states.  I'm looking for the best place to get wage and employment data (i.e. individual income, unemployment, median household income) broken down by demographic groups (i.e. age group, marital status, race, ethnicity, number of children) and sortable by ZIP CODE.  I understand that the [census.gov](https://census.gov) site has some materials, but I was wondering if anyone knew which survey or data set specifically has the most updated information for these breakdowns.

&#x200B;

Appreciated.  Thank you.",datasets,2022-08-15 13:44:43,6
"[https://data.gov/](https://data.gov/)

[https://www.kaggle.com/datasets](https://www.kaggle.com/datasets)

[https://www.tableau.com/learn/articles/free-public-data-sets](https://www.tableau.com/learn/articles/free-public-data-sets)

Just to start …",4,wprigu,"Hello, 

I am looking for data any type of data. Which site would you guys recommend? 

  
Thanks,",datasets,2022-08-16 04:11:26,2
"You might have to scrape a website like Mr Skin or something similar. I think you may even have better luck and fun doing your own research. You know, for science.",2,wpixcj,"This is a serious post. I’m looking for this dataset because my wife thinks I’m basic and only rate movies with nudity in them highly and that all dudes are like this. I would like to create a scatter plot showing the average IMDB rating on one axis and total screen time with nudity on another axis to hopefully show no, or a negative correlation between the two. Any help appreciated!",datasets,2022-08-15 19:58:58,1
https://ourworldindata.org/grapher/hadcrut-surface-temperature-anomaly?tab=chart&country=IND,2,wp18po,"I am doing a project on analyzing and predicting severe heatwaves in the Indian subcontinent. I a looking for an extensive historical dataset (preferably going back decades) for it.  
Any sources, references, or help would be greatly appreciated.",datasets,2022-08-15 07:34:30,3
No Antarctica (good luck) but https://sedac.ciesin.columbia.edu/data/collection/gpw-v4,2,wp91jm,"Needs to:

* Capture and represent populations of small islands (estimates are fine, just can't be blank or show no population)

* Include all permanent settlements, including Antarctic bases like McMurdo Station

* Span the entire earth from pole to pole (all latitudes and longitudes captured)

* Show close to continuous population (rather than by country) either with very small area segments, regularly spaced points, or anything else that looks like a heat map in the end",datasets,2022-08-15 12:50:58,2
"Hey, nice to read this, This is Very interesting since I have worked on a Bet365 golf project before, it makes me scratch my head. Are u willing to buy this kind of data if I can acquire it from Bet365 golf PlaybyPlay ?",1,wp346d,"I'd be interested in datasets that contain information about both the swing action (swing speed, ball speed, launch angle, spin rate, etc) and the resulting shot (carry distance, height, distance off line, etc).

I can't seem to find any such data in the obvious places and anything would be greatly appreciated!",datasets,2022-08-15 08:54:26,2
"I hope this can help: [More than 30,000 books in spanish of different narratives with image and ISBN and other features. Given in three different formats: CSV, XLSX and JSON.](https://www.kaggle.com/datasets/danielalbarracinm/books-with-isbn-of-different-genres?resource=download)

It has more info than you requested but there are Spanish titles in this dataset.",8,woygr4,"Yeah, weird question maybe.

I'm trying to find a list of book titles in Spanish. Not books written in Spanish or by Spanish writers or anything, I need all kinds of books. I just need their Spanish titles.

Does anyone have a clue where I could find anything like this or where I could scrap the info from?

Thanks!",datasets,2022-08-15 05:30:39,2
"pdb is most likely where you want to go. I think it's the largest. Google for protein database pdb. Good luck, I hope you are a biologist :p",2,woko1i,"It seems like Uniprot uses a notation like Short= to include short names, but I can't download  a tsv with the short names. I am able to get a tsv of just under 80,000 protein names, which I'm assuming is all the known human proteins. But for example, it will say 'nerve growth factor' but I need to retrieve NGF. For many proteins, the short name matches the gene name or the entry name field, but not all of them. Even if all proteins don't have short names, even just a file with the proteins that do would be great, thanks!",datasets,2022-08-14 17:03:04,1
"The [NOAA](https://www.ncei.noaa.gov/access) website has all the  weather stations data that is publicly available, plus a whole lot of other geographical data sets. (Note: My adblocker broke the website for me, so I had to disable it). You can either access it with an API or download parts of the data set. The whole data set is HUGE however, probably over a few terabytes.

Not sure where you find predictions data though.",1,wolrr9,"Hello, I'm making an application that will require world climate change data,  I'm looking for basic climate data (mean max temp, mean precip, etc) preferably at the lat/lon level. Does this type of dataset even exist?  


Additionally, is there any future prediction type data? I'd love some future prediction stuff. I've found some rasterized data here and there, is there anyway to convert that into something like csv format?  


Thanks! Sorry for all the questions.",datasets,2022-08-14 17:54:48,7
"Population data? Economic data? Etc

Be more specific",9,wn9unt,"I need data for my own analysis work.
Local berlin data or Germany data will be useful.
Do you know where and how can I get it?
You suggestions will be useful.

Thanks!",datasets,2022-08-13 01:57:01,4
There's a project scraping all of reddit and they provide all data to the public https://files.pushshift.io/reddit/,12,wmptm4,"Hi everybody.

I just coded a Scrapy python project to crawl the top 1000 posts of a subreddit's most upvoted posts of all time. It is just the top 1000 because it seems Reddit just returns 1000 for a query. I couldn't find a way to crawl all posts of a subreddit. if anyone knows how to do that let me know.

This is my Github repo for this [https://github.com/kiasar/Reddit\_scraper](https://github.com/kiasar/Reddit_scraper)",datasets,2022-08-12 09:39:45,5
"So I did some digging and I found [this article](https://www.disabled-world.com/calculators-charts/life-expectancy-statistics.php) from [disabled-world.com](https://disabled-world.com) that gives information on Human Life Expectancy by country as of 2022 (I believe).

PRB 2014 and 2015 World Population Data Sheets. [This](https://www.prb.org/collections/data-sheets/) is PRB's website that on the face of it gives data sheets from 2015 - 2021. I tested changing the URL from 2015 to 2014 & 2013 and both came up with data sheets for those years, so while I don't know how far back they go, this may give you Web 2.0 era information.

[The Wiki for Life Expectancy](https://en.wikipedia.org/wiki/Life_expectancy) does list all the way back to the Paleolithic era and most of the periods are backed up with sources. One of those sources may lead you to something more tangible and already collected.

With that being said, it looks like this is data that's ripe to be collected and organized.

I hope something in what I've presented helps you!",5,wn07m0,Does anyone know of a source that would contain data on life span distributions throughout history?,datasets,2022-08-12 17:13:40,3
"WideWorldImporters superseded ADVWorks as MS's exemplary, large-scale database. Have you looked into it? It is significantly more complex in many ways.",3,wmpql5,"I'm looking for any documented dataset similar to Adventure Works with multiple different ""operations"" in it. Anyone has a suggestion?",datasets,2022-08-12 09:36:06,2
I have been working on this for the past few months while also developing the labeling tool it is made with. I hit 100 images which was my goal for an initial release :) The dataset will continue to grow and contributions are absolutely welcome! Thanks for checking it out!,2,wm08v5,,datasets,2022-08-11 12:40:57,2
Does this bounty already have the dataset from the first bounty? Or how do you prevent loading data pulled directly from the first bounty's dataset?,1,wm1ega,,datasets,2022-08-11 13:29:40,3
"It's called big data, and we may need it in the future. /S",3,wlycov,Let me know in the comments or dm if anyone's got any data stored like that and is not using it.,datasets,2022-08-11 11:21:46,1
Is it possible to get contribution to a shared dataset?,1,wlmig7,"I have created a dataset containing some obfuscated and not obfuscated programs. I would be glad if you contribute using any programming language or obfuscation tool as it is described in readme.

[https://github.com/Terminou/Obfuscated-Program-Dataset](https://github.com/Terminou/Obfuscated-Program-Dataset)",datasets,2022-08-11 01:48:24,1
"The github with links to R code to use on the data [https://github.com/Zoological-Society-of-London/rlpi](https://github.com/Zoological-Society-of-London/rlpi)

Article about making visualisations (and more on what the data means) at https://findingnature.org.uk/2022/08/10/biodiversity-stripes/",1,wkzqo9,,datasets,2022-08-10 08:03:56,1
"I haven't seen a dataset for that, but if you want to build one I run [https://mekabytes.com](https://mekabytes.com) for collaborative data labeling. It supports object detection and classification.",1,wl98km,Title,datasets,2022-08-10 14:23:51,1
"No, because that stuff is stupidly lucrative.",3,wks7cy,As title says. I would like to find an ESG raw data database regarding publicly traded companies.,datasets,2022-08-10 01:39:41,6
Smart irrigation devices? There are some domestic ones at iotdb.com,2,wl81h1,"Hi all, 

I need a smart irrigation dataset.

Thanks in advance",datasets,2022-08-10 13:34:40,2
"Hey mrjsmathematics,

I believe a `request` flair might be more appropriate for such post. Please re-consider and change the post flair if needed.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/datasets) if you have any questions or concerns.*",1,wl189x,"Looking for free dataset for any animal hospital or vet clinic (real or sample) with possible fields related to animals, revenue, clients, wait time, doctors, products used, etc. for the creation of a dashboard for a personal project.

Appreciate any and all help!",datasets,2022-08-10 09:04:10,2
This looks really promising. Do you know how to get access to the data?,3,wkacj6,,datasets,2022-08-09 11:11:31,4
"I was looking for a similar dataset yesterday.

Here is what I found:
https://commonslibrary.parliament.uk/constituency-data-election-results/

Has the candidates for 2019, and who won, and their gender.
I don't think it has BAME status, perhaps you can augment the data with your own racial data.",1,wkiowh,"I'm looking for a dataset that has a list of candidates that ran in each UK constituency in the 2019 general election with BAME candidates indicated and details on whether or not they were selected via all-women shortlists. I know parties collect this data, but do they publish it? Or is there anywhere that compiles it?",datasets,2022-08-09 17:01:02,2
"Check out census.gov or opendatanetwork.com
    
Opendatanetwork is my go to. Just search ""business owners"" ""local business""and you'll get a bunch of stuff.",2,wka7fm,"I'm looking for the names & zip codes of registered small business owners in the U.S.

Anyone know if something like that exists?",datasets,2022-08-09 11:06:03,7
Why hasn't this gotten any attention?!?! This is an amazing dataset and I can't wait to get the time to look into it,1,wke8fp,"CAH posted an amazing dataset with almost 3k respondents in a poll related to the topic of abortion only in states where abortion is banned or partially banned.  It contains answers to over 40 questions.  It's a really useful dataset.    
Source: [https://abortionpoll.cardsagainsthumanity.com/](https://abortionpoll.cardsagainsthumanity.com/)   
Dataset: [https://abortionpoll.cardsagainsthumanity.com/CAHProvesYourStateSucks-Data.zip](https://abortionpoll.cardsagainsthumanity.com/CAHProvesYourStateSucks-Data.zip)",datasets,2022-08-09 13:47:30,2
"Hey diraceusse,

I believe a `request` flair might be more appropriate for such post. Please re-consider and change the post flair if needed.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/datasets) if you have any questions or concerns.*",1,wjm5v2,"Hey guys

I'm looking for a database with information about companies such as industry and employee size and of course, ideally free (only for us companies) .
Any ideas where I could get this info?",datasets,2022-08-08 15:27:10,10
Hi. Thanks for your efforts.,1,wk3smc,"Hi all,   


Been working on a side project to provide free annotations and tooling for unlabeled datasets in a community driven open source way. If you have a dataset that needs to be labeled, check it out. [https://sharedata.ai/](https://sharedata.ai/). Also would love any feedback. Thank you!",datasets,2022-08-09 06:48:48,1
"As a former school board member, I doubt there’s any meaningful data collection out there, but it would be helpful to know what you are asking for. Votes by whom? And about what specificity? Schools are funded by Federal, State, and local taxes, plus various grants, donations, and occasional fees. The budget for my district was printed in a binder of several thousand pages. And there are 130 school districts in my state alone.",1,wjitl5,"Hello, 

As the title says I am looking for a list of upcoming school budget voting data in the US. I looked around for a few days and haven't found anything outside of information posted on individual school districts websites. Was wondering if anyone had some good resources for data sets on schools in the US. I've pulled a list of schools in the US from Dolt HUB [https://www.dolthub.com/repositories/dolthub/us-schools](https://www.dolthub.com/repositories/dolthub/us-schools) and would like to see what schools in this in the list are having budget votes and when. Apologies in advance if I broke any of the posting rules here mods please pull down the post if I've violated any of the rules. 

&#x200B;

Any help is greatly appreciated! 

Cheers,",datasets,2022-08-08 13:11:22,5
"I'm not sure what you mean by amygdala diagnosing, but this (https://openneuro.org/) is the biggest open database of fmri I know of, most others require institutional affiliation. You could try searching bells palsy there.",4,wj970i,"I need a data set for amygdala diagnosing of normal people, and another one of Bell's palsy patients. Does anyone know where can I get them",datasets,2022-08-08 06:42:53,2
"I have such dataset， but it's paid. If you are interested, pls message me.",1,wjkmmj,I’m looking for datasets of either of these things. Does anyone know any good ones or personally have copies?,datasets,2022-08-08 14:23:46,1
"Hi, 

Take a look at these. They may help:

[https://research.ibm.com/haifa/dept/vst/debating\_data.shtml#Overview](https://research.ibm.com/haifa/dept/vst/debating_data.shtml#Overview)

[https://paperswithcode.com/task/argument-mining](https://paperswithcode.com/task/argument-mining)

&#x200B;

Best",2,wjjgbj,"Hi, I am looking for my master's thesis for datasets on argument mining. My research on the internet has lead me in the end to two sites hosting or linking to datasets:

- https://webis.de/data.html
- http://corpora.aifdb.org/index.php

Some of them are restricted. For some I contacted the authors and I am waiting for an answer. 

Does anybody know more?

Thanks in advance!",datasets,2022-08-08 13:36:46,1
"This is giving me life. Not for practical purposes, just humour",3,wix9to,"Dataset of ""Would you Rather"" style questions along with the number of votes for each option

[https://www.kaggle.com/datasets/charlieray668/would-you-rather](https://www.kaggle.com/datasets/charlieray668/would-you-rather)

Data scraped from [either.io](https://either.io)",datasets,2022-08-07 19:36:52,2
https://www.bls.gov/bls/wages.htm,2,whsyeo,"Edit: Thank you for the support here! I found \`CES0000000001\` from a few sources linked!

I am trying to recreate this graph I found on Twitter from BSL data. I have all the Python figured out but I just can't seem to find any series going back to the 1970s that has anything to do with Payroll. My interest is simply to gain some data engineering skill by replicating what I see others demonstrating. Any guidance on what series this might be or how to better search for it myself in a more obvious way would be appreciated. Thanks!

[https://twitter.com/biancoresearch/status/1552396151999135747](https://twitter.com/biancoresearch/status/1552396151999135747)",datasets,2022-08-06 09:59:59,7
"Hmmm. My first thought is it doesnt exist per postal code. It does exist per Dissemination Area as per Census Canada data. 

But maybe it does exist, who knows.",3,wh5owe,"I have a list of 500 or so unique postal codes from across Canada, and was hoping to get average household income data for each. Does anyone know of an existing (and hopefully recent) data set with HHI listed by postal code, or know of a resource or method for gathering this data? Any help is greatly appreciated!",datasets,2022-08-05 13:37:04,5
"Hey SaluteOrbis,

I believe a `request` flair might be more appropriate for such post. Please re-consider and change the post flair if needed.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/datasets) if you have any questions or concerns.*",1,wgqnho,"Data redditors, I need your wisdom (not sure if this is the right place to post, Im sorry if it aint). 

For a journalism project, we are looking for data / site / dataset that has statistics / data on pollution in rivers in the EU. e.g. if I want to compare (plastic) pollution between Hungarian and Ukrainian rivers. Does anyone know any resources I could consult? 

Thanks in advance!",datasets,2022-08-05 02:12:34,3
"https://www.epa.gov/ground-water-and-drinking-water/drinking-water-data-and-reports

Each water district publishes their own reports and distributes them their own ways. Typically, a yearly bulk mailing and a website.

But they are required to file reports with EPA - a Federal agency.

Try the link above.

I doubt you are going to get data broken down by zip-code. Water distribution systems and reporting areas are larger than a zip code and can cross zip codes.

For example, San Diego County has 3 primary distribution systems, and they report separately on each one. With the caveat thst there are areas that receive a mix of water from two distribution systems.

TL;DR: it’s ‘Merica, and so it’s complicated!",4,wgcnal,"I'm trying to do an analysis of water quality and availability in the United States but I can't find a comprehensive data set. I know its possible because there are website where you can look at your zipcode and learn about your water quality. What are they pulling from? 

&#x200B;

Just for some context I have very little data background and need a bit of handholding.",datasets,2022-08-04 14:21:57,6
Ty,2,wgqb85,,datasets,2022-08-05 01:51:44,1
I do not know the area. But are you looking for something like income versus chance of doing of covid? It TB or some other similar disease. I've seen datasets that can be used for that if that is reasonable,1,wg9h5b,I’m currently studying logistic regression and I find it really interesting. I would like to apply what I’ve learned so far on public health data.,datasets,2022-08-04 12:08:42,3
I will send you some literature,2,wg4nno,"I am looking for a library or a dataset that provides a list of words that have been used to detect at-risk cases of suicide in text. Does anyone know of such a list that is legitimate? This is for research, so academic articles are welcome!",datasets,2022-08-04 08:50:59,5
if you have the address you can use google's map api to get long/lat.,2,wg3gj9,"Good day all - would anyone know of an existing dataset of US music venues that has longitude and latitude coordinates? Working on a sales radius viz project that requires long/lat.  I have city and state, and venue name, but my internal source unfortunately does not have coordinates.  I have tried searching [data.world](https://data.world), but couldn't find what I needed.

Thanks very much!",datasets,2022-08-04 08:01:24,5
"So real estate is tricky because most of the sites you mention block crawlers or are reactive. Regardless you can mostly still scrape them but it can be a bit tricky (you would have to use vpn, splashy + scrapy for example in some cases). A lot of the times the site will be a bit older and there will be a hidden api that gives the browser access to data to display it (for example to display the site's map, etc). To get to that you just go right click > inspect element > network  and then refresh the page. Watch what it downloads...you can click on each request and then see a preview. A lot of the stuff the browser downloads will be javascript, images or fonts but sometimes the site will provide a JSON with info on listings. From there you can right click on the request and copy as CURL and then with that you can download it to any format with the terminal (just paste the curl in the terminal and redirect the output to a file with > ). If you're really lucky the site will have a query string in the URL that you can modify to access the database directly. Usually when you copy the CURL command it will have a URL in it and a lot of times the query string is in there (starts with a ? and then it's like a dictionary with a key=value pairs for each element you want to search). No matter what the country there is usually at least one old real estate site that doesn't properly hide/protect their data and provides it in this way but you have to go digging for it.",5,wfbrap,"Hey everyone, starting a personal project in real estate and was wondering what would be the best method for getting accurate datasets regarding location, rent prices, vacancy, etc. Trying to analyze accurate data to compare between states in America.

Fairly new to datasets & webscraping so I was looking for some ideas on how I should go about getting/creating the data. I know there's paid datasets but I think It'd be more challenging to play around with the data collection myself to have exactly what I want and customize it if I need to.

I was thinking of scraping sites like Airbnb, Zillow, and other home listing sites to start, if there's a better way to approach this, let me know!",datasets,2022-08-03 09:55:57,24
"The only good resource I could find until now is this one, maybe it might help someone in future.  
It consists of U.S. Supreme court decisions from the 18th century to the present day:  
[https://www.english-corpora.org/scotus/help/texts.asp](https://www.english-corpora.org/scotus/help/texts.asp)",1,wfs0v4,"I am searching for a source which provides different editions of the **same** publication.

For each edition, I would need to fetch the whole text. The goal is to study the development of the writing style over **large periods of time.** This means that the time that should pass between one edition and the other should be of min. 10 years. 

Example: University X publishes study Y in year 1870 (edition A). University X then publishes an actualized version of study Y in year 1880 (edition B). I need the whole text of edition A and B. 

I hope I could make myself clear and would be glad for any help from the community.",datasets,2022-08-03 21:45:05,1
"Pretty cool. Just a side note pretty sure ""Scraping"" google maps API is against the TOS. I just recently explored using Google Maps API and they basically say you can't keep the data for more than 30 days. They want you to call the API every time so they can track and charge you for it. So my company won't let me use it to scrape geo codes. We had to switch to Bing because they allow storage of geo codes.",1,wf9sq6,"We're former data scientists/ researchers/ analysts and wanted to build something that makes access to external data easier. So we built [Databar.ai](https://Databar.ai) \- a platform that lets you query any API without code - all through a spreadsheet UI. 

We first shared [databar.ai](https://databar.ai) v1 with r/datasets back in December '21, and since then we've been working on improving our product by adding new data sources and features.

Some of the APIs currently online: CoinGecko, Financial Modeling Prep, Coincap, CoinLib, Google Maps/ App Store/ Google Play Store/ SERP scrapers, Weatherbit, Telegram stats. Wikipedia coming soon. :)

Most parameters are pre-populated, you can use our API key to make requests (no additional sign-ups required), and we've set up all the headers, pagination, etc. so you don't need to spend time reading docs.

Here are some of the things you can do with the site:

*Query builder:*

* [Scrape App Store and Google Play Store reviews](https://databar.ai/explore/app-store-spy/scrape-app-store-apps-by-id)
* [Scrape Google Maps locations and reviews](https://databar.ai/explore/outscraper/google-maps-places-scraper)
* [Get the popularity and engagement for any Telegram channel](https://databar.ai/explore/tgstat/historic-telegram-channel-subscribers)
* [Screen public companies using detailed filters (i.e. Beta, market caps, sectors, industries, and locations)](https://databar.ai/explore/financial-modeling-prep/stock-screener)
* [Get financial ratios for any public company](https://databar.ai/explore/financial-modeling-prep/public-company-financial-ratios), track their prices, and RSS feeds automatically
* [Get crypto market data](https://databar.ai/explore/coin-gecko/coins-market-related-data)
* [Set up real-time flight and ticket trackers](https://databar.ai/explore/aviation-edge/real-time-flight-tracker-with-filters)

*Enrichments:*

* Enrich emails with names and company data
* Enrich a list of stock tickers with their price, volume, market cap, and multiples data
* Enrich ip addresses with locations

There are a few hundred more use cases so I'm not going to list them all here. :)

\--

Databar makes the life of an analyst a bit easier, but I think there's still a long way to go - would really really love to get your feedback on the site! We have a free plan but if you send us a message on Discord we can upgrade you to Pro for free. :)

Hope this is the right place to post/please let me know if I'm in the wrong place!",datasets,2022-08-03 08:38:58,5
"The extension could be one of these file formats. https://file.org/extension/bd

Or it could be a custom file format by the software. Which means if it’s not one of the formats above it could be anything.",3,wfhxuk,"So, I want to mess around with the files from Crash Twinsanity, but I don't know how. There's a file on the .ISO called [CRASH.BD](https://CRASH.BD), but I can't find any software that opens that type of file. Please help ;w;",datasets,2022-08-03 14:01:16,2
scrape any large instagram accounts post for comments,3,wez0zn,"Trying to create a model that identifies spam instagram comments, would appreciate if someone could help me out by providing some directions.",datasets,2022-08-02 23:28:15,17
"“This statement is a lie.”

I can’t begin to imagine who would be sitting on a corpus of statements sorted by whether or not the author believed them to be factual. Seems pretty absurd.",3,wevj2t,Just as the title says.,datasets,2022-08-02 20:21:03,9
"Hey echo_awesomeness,

I believe a `request` flair might be more appropriate for such post. Please re-consider and change the post flair if needed.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/datasets) if you have any questions or concerns.*",1,weyx1r,[Energy consumption in schools dundee](https://data.dundeecity.gov.uk/en_GB/dataset/energy-consumption-of-schools/resource/96e14a3c-4828-49fd-a69a-057d8f99bc85) dataset has been removed from their website. Is there some place else where i can get this data?,datasets,2022-08-02 23:21:32,1
Why use regular expression and not use beautiful soup (https://pypi.org/project/beautifulsoup4/ ) for scraping?,9,we7cvc,"Hey guys 👋 The following script extracts data from [Google Finance Markets](https://www.google.com/finance/markets/). 

You can run the script via available CLI arguments. To find them, type in your terminal `python main.py -h` and it will print you available arguments options.

JSON output is in the GitHub Gist link.

You can grab the code from GitHub Gist (there's also a tutorial link): https://gist.github.com/dimitryzub/33dff4ee7afd4c3caeb62afc6f199972

Full code:

```python
import requests
import json
import re
import argparse
from parsel import Selector

parser = argparse.ArgumentParser(prog=""Google Finance Markets Options"")
parser.add_argument('-i','--indexes', action=""store_true"")
parser.add_argument('-ma','--most-active', action=""store_true"")
parser.add_argument('-g','--gainers', action=""store_true"")
parser.add_argument('-l','--losers', action=""store_true"")
parser.add_argument('-cl','--climate-leaders', action=""store_true"")
parser.add_argument('-cc','--crypto', action=""store_true"")
parser.add_argument('-c','--currency', action=""store_true"")

args = parser.parse_args()

def main():

    # https://docs.python-requests.org/en/master/user/quickstart/#custom-headers
    # https://www.whatismybrowser.com/detect/what-is-my-user-agent
    headers = {
        ""User-Agent"": ""Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.5060.134 Safari/537.36""
    }

    if args.indexes:
        html = requests.get(""https://www.google.com/finance/markets/indexes"", headers=headers, timeout=30)
        return parser(html=html)

    if args.most_active:
        html = requests.get(""https://www.google.com/finance/markets/most-active"", headers=headers, timeout=30)
        return parser(html=html)

    if args.gainers:
        html = requests.get(""https://www.google.com/finance/markets/gainers"", headers=headers, timeout=30)
        return parser(html=html)

    if args.losers:
        html = requests.get(""https://www.google.com/finance/markets/losers"", headers=headers, timeout=30)
        return parser(html=html)

    if args.climate_leaders:
        html = requests.get(""https://www.google.com/finance/markets/climate-leaders"", headers=headers, timeout=30)
        return parser(html=html)

    if args.crypto:
        html = requests.get(""https://www.google.com/finance/markets/cryptocurrencies"", headers=headers, timeout=30)
        return parser(html=html)

    if args.currency:
        html = requests.get(""https://www.google.com/finance/markets/currencies"", headers=headers, timeout=30)
        return parser(html=html)


def parser(html):
    selector = Selector(text=html.text)
    stock_topic = selector.css("".Mrksgc::text"").get().split(""on "")[1].replace("" "", ""_"")

    data = {
        f""{stock_topic}_trends"": [],
        f""{stock_topic}_discover_more"": [],
        f""{stock_topic}_news"": []
    }

    # news results
    for index, news_results in enumerate(selector.css("".yY3Lee""), start=1):
        data[f""{stock_topic}_news""].append({
            ""position"": index,
            ""title"": news_results.css("".mRjSYb::text"").get(),
            ""source"": news_results.css("".sfyJob::text"").get(),
            ""date"": news_results.css("".Adak::text"").get(),
            ""image"": news_results.css(""img::attr(src)"").get(),
        })

    # stocks table
    for index, stock_results in enumerate(selector.css(""li a""), start=1):
        current_percent_change_raw_value = stock_results.css(""[jsname=Fe7oBc]::attr(aria-label)"").get()
        current_percent_change = re.search(r""\d+\.\d+%"", stock_results.css(""[jsname=Fe7oBc]::attr(aria-label)"").get()).group()

        # ./quote/SNAP:NASDAQ -> SNAP:NASDAQ
        quote = stock_results.attrib[""href""].replace(""./quote/"", """")

        data[f""{stock_topic}_trends""].append({
            ""position"": index,
            ""title"": stock_results.css("".ZvmM7::text"").get(),
            ""quote"": stock_results.css("".COaKTb::text"").get(),
            # ""https://www.google.com/finance/MSFT:NASDAQ""
            ""quote_link"": f""https://www.google.com/finance/{quote}"",
            ""price_change"": stock_results.css("".SEGxAb .P2Luy::text"").get(),
            ""percent_price_change"": f""+{current_percent_change}"" if ""Up"" in current_percent_change_raw_value else f""-{current_percent_change}""
        })

    # ""you may be interested in"" at the bottom of the page
    for index, interested_bottom in enumerate(selector.css("".HDXgAf .tOzDHb""), start=1):
        current_percent_change_raw_value = interested_bottom.css(""[jsname=Fe7oBc]::attr(aria-label)"").get()
        current_percent_change = re.search(r""\d+\.\d+%"", interested_bottom.css(""[jsname=Fe7oBc]::attr(aria-label)"").get()).group()

        quote = stock_results.attrib[""href""].replace(""./quote/"", """")

        data[f""{stock_topic}_discover_more""].append({
            ""position"": index,
            ""quote"": interested_bottom.css("".COaKTb::text"").get(),
            ""quote_link"": f""https://www.google.com/finance{quote}"",
            ""title"": interested_bottom.css("".RwFyvf::text"").get(),
            ""price"": interested_bottom.css("".YMlKec::text"").get(),
            ""percent_price_change"": f""+{current_percent_change}"" if ""Up"" in current_percent_change_raw_value else f""-{current_percent_change}""
        })

    return data


if __name__ == ""__main__"":
    print(json.dumps(main(), indent=2, ensure_ascii=False))
```",datasets,2022-08-02 02:42:07,4
"Yes, i know what you’re up to and i can help.",1,we25ej,"Hello all, 

Is there any way where i can find a list of the Shopify accounts that takes Lebanese Pounds LBP as a currency?",datasets,2022-08-01 21:26:30,2
"How the experiment was run:

https://youtu.be/mpBAdzIFdGg",6,wd7mca,"First Unsuccessful Run: 

[https://drive.google.com/file/d/1oyKEOcQ1zg6jPz3yhQAvAE1IrLLWIswC/view?usp=sharing](https://drive.google.com/file/d/1oyKEOcQ1zg6jPz3yhQAvAE1IrLLWIswC/view?usp=sharing) 

Second Successful Run:

[https://drive.google.com/file/d/10Gug7J3Tq6t\_Pf6v-V-TTmLcZPo6X1Pi/view?usp=sharing](https://drive.google.com/file/d/10Gug7J3Tq6t_Pf6v-V-TTmLcZPo6X1Pi/view?usp=sharing)

The first run still added likes to the accounts, so I'm including the data set.",datasets,2022-07-31 20:58:17,6
"You can download almost everything. If you're writing an essay shouldn't your sources have their methodology?   

One example here
http://files.pushshift.io/reddit/",16,wcnu1p,"I am trying to write an essay about training AI chatbot with reddit posts and comment. I have seen a post about someone compressing 250GB of comments which was from 7 years ago
how much data is available these days? and have anyone done this?",datasets,2022-07-31 05:42:10,6
"Strava Metro, but pretty sure you have to pay for that.",1,wcwsb5,I am interested in doing pose estimation and would like to focus on people riding bikes but I'm having trouble finding a source. I have only been able to find photography companies for sporting events like triathlons or bike rallies that sell to participants. Has anyone come across an image dataset like this before?,datasets,2022-07-31 12:30:17,1
"I imagine you probably already did your diligence with searching online. 
I imagine a lot of PhD students would be open to filling out something like a google form with additional questions for additional covariates if that’s what you’re interested in.
Seems you’ll validate that we are sooooooo underpaid. But I imagine you could get a rich database for at least this year",8,wc0obu,"I am trying to compile a database of  stipend data for US PhD programs, but I cannot find how much they were paying people except for self-reported data that only goes back about 5 years. Is this something I need to submit a FOIA request for? Is this publicly available elsewhere?",datasets,2022-07-30 08:58:15,4
Maybe the LinkedIn leak had that ?,1,wc4240,"I am trying to find a dataset full of the type of posts that clutter everyone's Linkedin feed, the more the better. I tried the Kaggle Linkedin influencer dataset, but it doesn't contain full posts, only the snippets until the ""read more"" link.",datasets,2022-07-30 11:30:17,2
"If you want to skip straight to the dataset, here it is!  


https://app.surgehq.ai/datasets/financial-transactions-intent-and-expense-category",4,wbg32z,,datasets,2022-07-29 14:21:42,1
Oh my....,1,wbe2df,,datasets,2022-07-29 12:53:07,1
"Hey RamblerUsa,

I believe a `request` flair might be more appropriate for such post. Please re-consider and change the post flair if needed.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/datasets) if you have any questions or concerns.*",1,waia70,"Actual ports of entry data are hidden by undocumented APIs (DHS) and poor UI design.  Updates of what  fence existed on the southern border, which stretches of border fence may have been fixed, replaced or updated are unavailable or simply not maintained or published.   GIS accessible fence data should not depend on who hates / hated Trump more, or at what point in time.

Although some of the secrecy about facility location is understandable it's hard to believe that there are no cohesive immigration data showing the totality of onshore or near-US offshore immigrant communities trying to land a boat on US shores.   Understandable that the Border Patrol folks don't want or need a continuously updated map to all their shifting requirements up and down the border.  

My intent was to create a simple project to see the southern border issues on a map.  This soon became a much more difficult task due to inconsistency and lack of GIS data maintenance.  Concerted effort with Google / Bing showed searching for such data to be a failure.  Scoping the problem is not possible due to the lack of organization of GIS data as well as the apparent lack of Federal GIS professionals in tending to such a complex dataset.  

No wonder the vast majority of US citizens have no idea of what is happening at the border of the US and Mexico.    Without a coherent map showing the depth of the problem there will never be a means to educate US citizens on its scope.

Pointers to data defining location and types of port facilities would be much appreciated.  Similarly, maps of fencing would be helpful; old, new, replaced and / or history would be useful.",datasets,2022-07-28 12:05:01,7
"No I can’t, but LinkedIn can. They categorize people into entry, mid, senior, exec, etc. don’t know how to get it but there’s your watering hole 🕳",1,wazl78,"I'm currently doing a uni research project about companies' organizational structures to get a deeper understanding of how human capital is employed across each department/division. Can you help please?

Thanks in advance!",datasets,2022-07-29 01:47:09,1
"[Library of Congress](https://guides.library.cornell.edu/filmbib/screenplays#:~:text=The%20Library%20of%20Congress%20subject,is%20held%20by%20another%20library) has some thoughts on the matter",3,wavc8p,I don't want to scrape the web. Does anyone know of a zip file with thousands of scripts in it?,datasets,2022-07-28 21:40:37,4
"Allô for me a great source of open data is the uk is the EPC https://epc.opendatacommunities.org/

It’s very oriented around building and energy consumption estimation but I think it’s a really cool dataset 

I worked with it on my previous job and experiment in my blog https://www.the-odd-dataguy.com/2019/03/23/energy-in-the-uk-analysis-of-the-energy-performance-certificates/",1,wb0e4e,"I'm looking to do a project where I integrate UK open data sets together by location and am wondering if anyone can point me in the direction of resources that were useful to them.  Climate, financial or demographic data would be interesting but I'm open as just at the start.",datasets,2022-07-29 02:38:39,2
You should share this to r/algotrading,3,wa79hj,"Hey guys! 🌞 Does anyone here like finance datasets? Or anything related to finance? Curious if I can do something interesting for you. 

Just in case (as usual👀), here's a script to scrape Google Finance main page in Python:

```python
import requests, json, re
from parsel import Selector


def scrape_google_finance_main_page():
    # https://docs.python-requests.org/en/master/user/quickstart/#custom-headers
    # https://www.whatismybrowser.com/detect/what-is-my-user-agent
    headers = {
        ""User-Agent"": ""Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.5060.134 Safari/537.36""
        }

    html = requests.get(f""https://www.google.com/finance/"", headers=headers, timeout=30)
    selector = Selector(text=html.text)
    
    # where all extracted data will be temporary located
    ticker_data = {
        ""market_trends"": [],
        ""interested_in"": {
            ""top_position"": [],
            ""bottom_position"": []
        },
        ""earning_calendar"": [],
        ""most_followed_on_google"": [],
        ""news"": [],
    }

    # Market trends top results
    ticker_data[""market_trends""] = selector.css("".gR2U6::text"").getall()
    
    # Earnings calendar results
    for calendar_quote in selector.css("".d3fRjc""):
        ticker_data[""earning_calendar""].append({
            ""quote"": calendar_quote.css("".yaubCc::text"").get(),
            ""quote_link"": f'https://www.google.com/finance/quote{calendar_quote.css("".yaubCc::attr(href)"").get().replace(""./quote/"", ""/"")}',
            ""short_date"": calendar_quote.css("".JiAI5b"").xpath(""normalize-space()"").get(),
            ""full_date"": calendar_quote.css("".fVovwd::text"").get()
        })

    # Most followed on Google results
    for google_most_followed in selector.css("".NaLFgc""):
        current_percent_change_raw_value = google_most_followed.css(""[jsname=Fe7oBc]::attr(aria-label)"").get()
        current_percent_change = re.search(r""by\s?(\d+\.\d+)%"", google_most_followed.css(""[jsname=Fe7oBc]::attr(aria-label)"").get()).group(1)

        ticker_data[""most_followed_on_google""].append({
            ""title"": google_most_followed.css("".TwnKPb::text"").get(),
            ""quote"": re.search(r""\.\/quote\/(\w+):"",google_most_followed.attrib[""href""]).group(1),            # https://regex101.com/r/J3DDIX/1
            ""following"": re.search(r""(\d+\.\d+)M"", google_most_followed.css("".Iap8Fc::text"").get()).group(1), # https://regex101.com/r/7ptVha/1
            ""percent_price_change"": f""+{current_percent_change}"" if ""Up"" in current_percent_change_raw_value else f""-{current_percent_change}""
        })

    # news results. If empty -> run once again. For some reason it could return [].
    for index, news in enumerate(selector.css("".yY3Lee""), start=1):
        ticker_data[""news""].append({
            ""position"": index,
            ""title"": news.css("".Yfwt5::text"").get(),
            ""link"": news.css("".z4rs2b a::attr(href)"").get(),
            ""source"": news.css("".sfyJob::text"").get(),
            ""published"": news.css("".Adak::text"").get(),
            ""thumbnail"": news.css(""img.Z4idke::attr(src)"").get()
        })

    # ""you may be interested in"" at the top of the page results
    for index, interested_top in enumerate(selector.css("".sbnBtf:not(.xJvDsc) .SxcTic""), start=1):
        current_percent_change_raw_value = interested_top.css(""[jsname=Fe7oBc]::attr(aria-label)"").get()
        current_percent_change = re.search(r""\d{1}%|\d{1,10}\.\d{1,2}%"", interested_top.css(""[jsname=Fe7oBc]::attr(aria-label)"").get()).group()

        ticker_data[""interested_in""][""top_position""].append({
            ""index"": index,
            ""title"": interested_top.css("".ZvmM7::text"").get(),
            ""quote"": interested_top.css("".COaKTb::text"").get(),
            ""price_change"": interested_top.css("".SEGxAb .P2Luy::text"").get(),
            ""percent_price_change"": f""+{current_percent_change}"" if ""Up"" in current_percent_change_raw_value else f""-{current_percent_change}""
        })

    # ""you may be interested in"" at the bottom of the page results
    for index, interested_bottom in enumerate(selector.css("".HDXgAf .tOzDHb""), start=1):
        # single function to handle both top and bottom 
        # ""you may be interested results"" as selectors is identical

        current_percent_change_raw_value = interested_bottom.css(""[jsname=Fe7oBc]::attr(aria-label)"").get()
        current_percent_change = re.search(r""\d{1}%|\d{1,10}\.\d{1,2}%"", interested_bottom.css(""[jsname=Fe7oBc]::attr(aria-label)"").get()).group()

        ticker_data[""interested_in""][""bottom_position""].append({
            ""position"": index,
            ""ticker"": interested_bottom.css("".COaKTb::text"").get(),
            ""ticker_link"": f'https://www.google.com/finance{interested_bottom.attrib[""href""].replace(""./"", ""/"")}',
            ""title"": interested_bottom.css("".RwFyvf::text"").get(),
            ""price"": interested_bottom.css("".YMlKec::text"").get(),
            ""percent_price_change"": f""+{current_percent_change}"" if ""Up"" in current_percent_change_raw_value else f""-{current_percent_change}""
        })

    return ticker_data


print(json.dumps(scrape_google_finance_main_page(), indent=2, ensure_ascii=False))
```

Outputs:

```json
{
  ""market_trends"": {
    ""top_position"": [
      ""Market indexes"",
      ""Most active"",
      ""Gainers"",
      ""Losers"",
      ""Climate leaders"",
      ""Crypto"",
      ""Currencies""
    ],
    ""bottom_position"": [
      {
        ""index"": 1,
        ""title"": ""Tesla Inc"",
        ""quote"": ""TSLA"",
        ""price"": ""$824.46"",
        ""price_percent_change"": ""+0.59%""
      }, ... other results
      {
        ""index"": 6,
        ""title"": ""BEL 20"",
        ""quote"": ""Index"",
        ""price"": ""3,774.05"",
        ""price_percent_change"": ""+1.15%""
      }
    ]
  },
  ""interested_in"": {
    ""top_position"": [
      {
        ""index"": 1,
        ""title"": ""Tesla Inc"",
        ""quote"": ""TSLA"",
        ""price_change"": ""+$47.88"",
        ""percent_price_change"": ""+6.17%""
      }, ... other results
      {
        ""index"": 6,
        ""title"": ""BEL 20"",
        ""quote"": ""Index"",
        ""price_change"": ""+22.01"",
        ""percent_price_change"": ""+0.59%""
      }
    ],
    ""bottom_position"": [
      {
        ""position"": 1,
        ""ticker"": ""Index"",
        ""ticker_link"": ""https://www.google.com/finance/quote/BEL20:INDEXEURO"",
        ""title"": ""BEL 20"",
        ""price"": ""3,774.05"",
        ""percent_price_change"": ""+0.59%""
      }, ... other results
      {
        ""position"": 18,
        ""ticker"": ""PFE"",
        ""ticker_link"": ""https://www.google.com/finance/quote/PFE:NYSE"",
        ""title"": ""Pfizer Inc."",
        ""price"": ""$51.95"",
        ""percent_price_change"": ""-0.67%""
      }
    ]
  },
  ""earning_calendar"": [
    {
      ""quote"": ""Apple"",
      ""quote_link"": ""https://www.google.com/finance/quote/AAPL:NASDAQ"",
      ""short_date"": ""Jul28"",
      ""full_date"": ""Jul 28, 2022, 11:00 PM""
    }, ... other results
    {
      ""quote"": ""Occidental Petroleum"",
      ""quote_link"": ""https://www.google.com/finance/quote/OXY:NYSE"",
      ""short_date"": ""Aug2"",
      ""full_date"": ""Aug 2, 2022, 10:00 PM""
    }
  ],
  ""most_followed_on_google"": [
    {
      ""title"": ""Apple Inc"",
      ""quote"": ""AAPL"",
      ""following"": ""3.71"",
      ""percent_price_change"": ""+3.42""
    }, ... other results
    {
      ""title"": ""Tesla Inc"",
      ""quote"": ""TSLA"",
      ""following"": ""1.49"",
      ""percent_price_change"": ""+6.17""
    }
  ],
  ""news"": [
    {
      ""position"": 1,
      ""title"": ""This kind of shock to the economy will have consequences"",
      ""link"": ""https://www.cnn.com/2022/07/27/politics/fed-interest-rate-volcker-what-matters/index.html"",
      ""source"": ""CNN"",
      ""published"": ""10 hours ago"",
      ""thumbnail"": ""https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRcLNm7uU5YfuvveVMWNvlQGUMcCPi4-7QJAqfKcDJgq7A3n1E_wiy53--_FFA""
    }, ... other news
    {
      ""position"": 9,
      ""title"": ""The 20 Best Netflix Shows of All Time -- Ranked"",
      ""link"": ""https://www.rollingstone.com/tv-movies/tv-movie-lists/best-netflix-shows-1386323/"",
      ""source"": ""Rolling Stone"",
      ""published"": ""20 hours ago"",
      ""thumbnail"": ""https://encrypted-tbn3.gstatic.com/images?q=tbn:ANd9GcSsaABpxAxYW29MTnyeSHb1Z9ex1bMvXQQnFB5RJqz9LogWOR9zyOKw9YrjClI""
    }
  ]
}
```

A step-by-step tutorial: https://serpapi.com/blog/web-scraping-google-finance/#code-explanation",datasets,2022-07-28 04:21:17,5
"Going to be market specific. For real estate for instance I use county data, census data, and MLS data. Another interesting indicator in real estate is license trends.

Pick a market/commodity of interest and research the regulating body for the industry or union for workers. Usually that’s going to be the place to get data.",6,w9uypl,"We're looking for data in the financial industry that researchers and analysts typically use to analyze long term financial trend (stocks, bonds, ETF, etc) movements.

I'm aware of economic indicators such as those provided in FRED. Do people know what else analysts typically use?",datasets,2022-07-27 17:33:56,15
"Hey shannad9118,

I believe a `request` flair might be more appropriate for such post. Please re-consider and change the post flair if needed.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/datasets) if you have any questions or concerns.*",1,w9va8v,"I’m an aspiring Data Analyst that wants to build a portfolio around data that I would actually use in a Strategic Pricing Analyst role so I can get that job in my current company. 

Edit: The industry is animal feed manufacturing and I’m looking specifically for the micro ingredients that are used in animal feed (typically vitamins, minerals, enzymes and amino acids). They are not reported on in agricultural data sites.

The closest sites for data I’ve found are Glowlit.com and Feedinfo.com but they are not free/easily accessible.

Does anyone know where I could find this data? It seems to be under lock and key and not freely available. I checked multiple data repositories and nothing specific to animal feed (micro) ingredient prices. Commodity pricing (corn, wheat, soy, etc) is readily available which is not what I need.

Any help is much appreciated!",datasets,2022-07-27 17:48:29,8
"If B timepoints 1+3+5 are aligning with A, just select those fields and rbind() them to one another I guess. If you have no idea how to connect two datasets I would argue its best to contact the corresponding author of each study and discuss with them",1,w9zewh,"Hi, I'm my dataset, I have some data that are from a study (hereafter referred to as study A) where there are 3 different timepoints as well as data from another study (hereafter referred to as study B) who have 5 timepoints. The problem is that I don't know how to match those data (i.e., the age of the participants) together. For example, time point #1 of participant #1 of study B might correspond to timepoint #1 of study A; but for participant #2, that timepoint #1 from study B might corresponds to timepoint #3 in study A. I'm new to R (the software R) so I don't really know if someone has encountered a similar problem before. In any case, I would be grateful to receive any advice. Thanks",datasets,2022-07-27 21:01:02,1
"Sounds a bit too specific. Have you tried fetching a more generic dataset, like short stories (there are some out there, but you can also scrap it from a subreddit, twitter, tumblr etc), and then filtering by those that include some keywords related to morality like “moral”, “ethical”, or the values you’re interested in.",2,w95rnc,"I'm looking for a dataset (or just a collection of short stories) that has lots of characters talk about their values and take actions that try to achieve their values. I'm specifically not looking for datasets like [ETHICS](https://arxiv.org/abs/2008.02275), which only have short text fragments and not full stories. Basically, I'm looking for values embedded in a somewhat natural linguistic context.

Thanks for your help!",datasets,2022-07-26 22:07:09,6
"It varies by school district.  This site looks to have a list.
https://publicholidays.com/us/school-holidays/",1,w9erk2,Where can I find a list of back to school dates 2022 across the USA? Much appreciated!,datasets,2022-07-27 06:26:39,2
"Here is my first thought:

https://www.energystar.gov/productfinder/advanced",2,w8ovbo,"Hi, I'm looking for a dataset containing power consumption of consumer electronics like TVs, routers, fans, coffee machines etc.  My search has come up short so far, maybe one of you has a pointer for me?  I'd appreciate it.

Thanks!",datasets,2022-07-26 09:56:23,5
I think you could do this linking separate data sets.  It would be easy to get soccer results (what level are you looking for?) it should list home and away.  Then do a city search and then merge with a weather set for that day.,3,w8gvvp,"Can anyone share soccer data: need game results, stadium name, and weather conditions on game day?",datasets,2022-07-26 04:03:41,2
Are you sure that data may be used in accordance with privacy laws?,1,w8nryx,Title. I dont know if i have searched really bad or if there arent any good datasets with faces with open eyes and closed eyes. Preferably in color.,datasets,2022-07-26 09:12:06,1
"https://m.imdb.com/interfaces/

Any good?",11,w7rxjz,"I have to create a graph with 100,000 nodes for my data structures class. Does anyone have any ideas? Thank you! :)",datasets,2022-07-25 08:32:20,10
"I'm pretty sure you can get historical records for the USA from data.gov. Maybe this one?  
https://catalog.data.faa.gov/dataset/traffic-flow-management-data---integrated-terminal-weather    
It's been a while since I investigated this. If you require real time, then I wouldn't be surprised if that's a paid service.",2,w7xz4p,"I'm aware of the tons of payware APIs that let you query a status of a single flight to see whether it's delayed, cancelled or scheduled.

What I'd like to get instead is a bulk overview of flights, say like a list of flights planned on a certain day with the latest status (departed, delayed, cancelled...).
This is something that's available for instance on flightradar24, but not available to download even with a business subscription (which I have).

I'm wondering whether such open dataset exists somewhere on the internet or if only paid solution are the way to go.",datasets,2022-07-25 12:33:04,2
"I recently found a load of free/open datasets on the Snowflake Data Marketplace.

I’ll check out what’s there for world wide Ag",3,w7rvyf,"Hey folks,  
I'm working on a project about farmers in Nigeria and require data related to it.  
The data points include but are not limited to

* Average financial income
* Area of farmland
* Crop produce
* Access to healthcare facilities
* Access to schools
* Literacy level
* Location coordinates

What could be the possible data sources (preferably open-source) for this?

Thank you so much for your attention and participation.",datasets,2022-07-25 08:30:32,4
There's been datasets posted here of plant leaf diseases. But you want stuff that effects the trunk?,1,w7m20j," I need a dataset on diseases that cause symptoms on the plant body.  I have received most of the common datasets on the Internet, but I need more data about the plant body.",datasets,2022-07-25 04:06:00,4
"Nice! 

I wonder what kind of scale of scraping this takes you up to? I usually just use one of the [Google Search APIs](https://medium.com/nerd-for-tech/5-serp-apis-that-can-beat-google-at-their-own-game-665b86aa822c) out there so I won't get blocked.",1,w7hpii,"Hey guys🍔 Here's a script for scraping Google images in Python. This one is a DIY solution without pagination support. Also, if some of you remember my post about [scraping all ResearchGate publications](https://www.reddit.com/r/datasets/comments/v2bx0w/script_scraping_researchgate_all_publications/), it's now finally in early progress.

At the bottom, there's a Github Gist link with an API solution that supports and shows a pagination example.

```python
import os, requests, lxml, re, json, urllib.request
from bs4 import BeautifulSoup
from serpapi import GoogleSearch

headers = {
    ""User-Agent"": ""Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.5060.114 Safari/537.36""
}

params = {
    ""q"": ""mincraft wallpaper 4k"", # search query
    ""tbm"": ""isch"",                # image results
    ""hl"": ""en"",                   # language of the search
    ""gl"": ""us"",                   # country where search comes from
    ""ijn"": ""0""                    # page number
}

html = requests.get(""https://www.google.com/search"", params=params, headers=headers, timeout=30)
soup = BeautifulSoup(html.text, ""lxml"")

def get_original_images():

    """"""
    https://kodlogs.com/34776/json-decoder-jsondecodeerror-expecting-property-name-enclosed-in-double-quotes
    if you try to json.loads() without json.dumps() it will throw an error:
    ""Expecting property name enclosed in double quotes""
    """"""

    google_images = []

    all_script_tags = soup.select(""script"")

    # # https://regex101.com/r/48UZhY/4
    matched_images_data = """".join(re.findall(r""AF_initDataCallback\(([^<]+)\);"", str(all_script_tags)))
    
    matched_images_data_fix = json.dumps(matched_images_data)
    matched_images_data_json = json.loads(matched_images_data_fix)

    # https://regex101.com/r/pdZOnW/3
    matched_google_image_data = re.findall(r'\[\""GRID_STATE0\"",null,\[\[1,\[0,\"".*?\"",(.*),\""All\"",', matched_images_data_json)

    # https://regex101.com/r/NnRg27/1
    matched_google_images_thumbnails = "", "".join(
        re.findall(r'\[\""(https\:\/\/encrypted-tbn0\.gstatic\.com\/images\?.*?)\"",\d+,\d+\]',
                   str(matched_google_image_data))).split("", "")

    thumbnails = [
        bytes(bytes(thumbnail, ""ascii"").decode(""unicode-escape""), ""ascii"").decode(""unicode-escape"") for thumbnail in matched_google_images_thumbnails
    ]

    # removing previously matched thumbnails for easier full resolution image matches.
    removed_matched_google_images_thumbnails = re.sub(
        r'\[\""(https\:\/\/encrypted-tbn0\.gstatic\.com\/images\?.*?)\"",\d+,\d+\]', """", str(matched_google_image_data))

    # https://regex101.com/r/fXjfb1/4
    # https://stackoverflow.com/a/19821774/15164646
    matched_google_full_resolution_images = re.findall(r""(?:'|,),\[\""(https:|http.*?)\"",\d+,\d+\]"", removed_matched_google_images_thumbnails)

    full_res_images = [
        bytes(bytes(img, ""ascii"").decode(""unicode-escape""), ""ascii"").decode(""unicode-escape"") for img in matched_google_full_resolution_images
    ]
    
    for metadata, thumbnail, original in zip(soup.select('.isv-r.PNCib.MSM1fd.BUooTd'), thumbnails, full_res_images):
        google_images.append({
            ""title"": metadata.select_one("".VFACy.kGQAp.sMi44c.lNHeqe.WGvvNb"")[""title""],
            ""link"": metadata.select_one("".VFACy.kGQAp.sMi44c.lNHeqe.WGvvNb"")[""href""],
            ""source"": metadata.select_one("".fxgdke"").text,
            ""thumbnail"": thumbnail,
            ""original"": original
        })

        # Download original images
        # print(f'Downloading {index} image...')

        # opener=urllib.request.build_opener()
        # opener.addheaders=[('User-Agent','Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.54 Safari/537.36')]
        # urllib.request.install_opener(opener)

        # urllib.request.urlretrieve(original, f'Bs4_Images/original_size_img_{index}.jpg')

    return google_images
```

Full GitHub Gist containing related search results extraction and a step-by-step tutorial link: https://gist.github.com/dimitryzub/9d1c5de0613610a02e3fdc96e05e86a1",datasets,2022-07-24 23:43:18,3
https://www.surgehq.ai/datasets/financial-transactions-dataset,2,w7tud0,I'm looking for US/Canada-based consumer credit card transaction datasets for an AI/ML project.,datasets,2022-07-25 09:48:28,2
https://www.ons.gov.uk/peoplepopulationandcommunity/birthsdeathsandmarriages/divorce/datasets/divorcesinenglandandwales,2,w7p951,"Looking for data on what common reasons people give for divorce in the UK, anyone know where I can get that.",datasets,2022-07-25 06:40:06,2
"These (plus Clearbit an other important mention) usually collect info officially by providing a free tool/app/add-on that can be used in gmail or other business mail tools and giving some level of free data enrichment in exchange for consenting that they will extract data out of your business email data. But besides that, I bet that they use lots of linkedin or other not-so-much official data as well from third party vendors.",1,w7odm7,What is their data source? Do they pay for 3rd party data access and do you know what that source is?,datasets,2022-07-25 06:00:23,1
sure you searched long enough? https://openflights.org/data.html,9,w71c9v,"Hey, for a project, I'm looking a for a list of international airports, sadly my search so far has come up dry.",datasets,2022-07-24 10:46:36,4
"Hey Double-Lavishness-77,

I believe a `question` or `discussion` flair might be more appropriate for such post. Please re-consider and change the post flair if needed.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/datasets) if you have any questions or concerns.*",1,w69pb6,"Is there a dataset with Short simple sentences of facts and rules.

For example :

apples are red  
apples are sweet  
apples are red or green  
blue is a color  
cars have four wheels  
doors have knobs  
dolphins are mammals  
apples are not oranges  
dogs can be pets  
python is a programming language  
nlp is abbreviation

&#x200B;

if Luke is son of Vader then Vader is a father  
if light is green then cross the street  
if the ball is on the floor then pick it up",datasets,2022-07-23 10:44:03,8
"I know this isn't your game but dota2 has some stats if apex doesn't work out for you.  
    
Opendota.com
    
Dotabuff",1,w5xhpr,I'm needing the pickrates for each legend but finding them online is looking to be a hassle and im hoping for some help. Im gonna cross post with the apex subreddit too but if anyone can help it would be greatly appreciated,datasets,2022-07-22 23:58:39,5
"[ONS gender pay gap datasets include staff numbers](https://www.ons.gov.uk/employmentandlabourmarket/peopleinwork/earningsandworkinghours/datasets/annualsurveyofhoursandearningsashegenderpaygaptables), but reporting is only required for companies with >250 staff",2,w5zw44,Companies House is a good start but staff numbers is challenging.,datasets,2022-07-23 02:28:03,1
"There's a problem with your question.  A zip code  isn't an area.  It's a linear route that mail carriers use.  

So, you probably want to use the zip code tabular area that the US census uses.  They describe the issue here https://www.census.gov/programs-surveys/geography/guidance/geo-areas/zctas.html  Or even in more detail here https://carto.com/blog/zip-codes-spatial-analysis/

Anyway, try this https://www.nber.org/research/data/zip-code-distance-database  I think they compile a dataset of all zcta within 5 miles of a particular zcta.  That's probably close to what you are looking for.",22,w5k16i,I am looking for a data set that has all of the zip codes then also has which zip codes border that zip code. Any ideas?,datasets,2022-07-22 13:07:58,7
"[https://dawadocs.dataforsyningen.dk/dok/adresser](https://dawadocs.dataforsyningen.dk/dok/adresser)

[https://www.openstreetmap.org/](https://www.openstreetmap.org/)",2,w5bknr, title,datasets,2022-07-22 07:09:45,7
"Really interesting. Use this to generate some scores in sets of datasets, then you can sort that and get the Best Fake Data",1,w4rej1,,datasets,2022-07-21 13:51:02,2
Depends on what you want to do with the data.,12,w4m1yq,"I am currently having an issue to store 100TB of timeseries data, I am thinking of:   
\- AWS: Amazon Redshift

\- AWS: Amazon Timestream

\- TimescaleDB

\- An alternative to TimescaleDB  


Any suggestions ?",datasets,2022-07-21 10:11:29,60
[deleted],1,w4z5ay,"All I want is a simple table with one column for year, and a second column for average global temperature above pre-industrial levels projected out to 2150.  Even better if there were additional columns which took different assumptions into consideration for multiple scenarios.  I can't find this anywhere and I'm shocked that we don't make it readily available data for the public to see for themselves.",datasets,2022-07-21 19:38:30,1
I don't get it... What is this exactly?,9,w4f7wx,,datasets,2022-07-21 05:16:06,7
https://www.zillow.com/research/data/,3,w4pvgh,"Hello,

&#x200B;

I am looking for demographic info on home purchases in the United States. I know [Census.gov](https://Census.gov) has some data but doesn't seem to have this info.   


Anyone know another data source ideally divided by county or zip code.",datasets,2022-07-21 12:47:27,5
"I thought the dataset is interesting. The actual code might not be needed by many here but I thought why not post both for those that do want to see it. Actual kaggle link to the data [https://www.kaggle.com/datasets/dmitrynikolaev/youtube-dislikes-dataset](https://www.kaggle.com/datasets/dmitrynikolaev/youtube-dislikes-dataset)

&#x200B;

I have no connection to the site or article.",1,w4aihk,,datasets,2022-07-21 00:38:05,1
Check out this [lab at Yale](https://cpsc.yale.edu/research/natural-language-processing) doing natural language processing on tweets; they might have some resources which can set you on the right path.,1,w4cqyd,"Hello everyone, i want to do a project on detection of abusive language on social media. I already downloaded two datasets from Kaggle for the same. But they are not that diverse. If anybody can provide some links from where I can download related datasets kindly drop in the comments. It would be of great help. 


Thank You in advance.",datasets,2022-07-21 03:00:09,2
"The US Census Bureau [TIGER Data](https://www.census.gov/programs-surveys/geography/guidance/tiger-data-products-guide.html) would be the best source for this. They maintain the official data and it's all free. FTP and API available. 

Time zones and zip codes are maintained as separate datasets. To get them together you would need to do either a spatial join if you just want the attributes, or a spatial intersection if you want the geometries to be chopped up together.

I'm on my phone or I would link to more resources, but a Google search of ""spatial join tiger files {R, python, postgres, whatever tool you are using}"" should return a good amount of blogs/tutorials. 

Good luck",10,w3wx7i,"I've been looking for this for a while and haven't found anything. 

Thank you",datasets,2022-07-20 13:41:55,14
Super interesting! Will check and get back. Cheers,1,w3o78m,"Hi, data experts,

We created a large [public datasets storage](https://github.com/upgini/) for ML models. And I would like to ask experts, what is missing from my list of datasets for solving typical ML tasks?

1) Historical weather & Weather forecast by postal/ZIP code

2) International holidays & events, workweek calendar

3) Consumer Confidence index

4) World economic indicators

5) Markets data

6) World demographic data by postal/ZIP code

7) Public social media profile data for email & phone

8) World mobile network coverage by postal/ZIP code

9) Geolocation profile for phone & IPv4 & email

10) World house prices by postal/ZIP code",datasets,2022-07-20 07:48:00,2
"All broker-dealers that route equity and option orders on behalf of customers are required to file quarterly 606(a) reports. 

Unfortunately, there is no central repository for 606 data, nor is there a central place that lists all the links. You have to search through each broker's website, where it will usually be listed under something like, ""Regulatory Disclosures.""",1,w3u4zw,"Hi all,

I am interested in looking into some Rule 606 data. Does anyone know whether there is a centralized platform that collects all 606 reports or keep track of the list of brokers that need to comply with the Rule 606? This website ([https://daytradingz.com/payment-for-order-flow/](https://daytradingz.com/payment-for-order-flow/)) has something of interest and I reached out to them, but haven't heard back for a long time.",datasets,2022-07-20 11:47:16,1
"Thank you, this is a great dataset to play with. What was your motivation for scraping and sorting this data?",2,w340kj,"I recently created a dataset with over 100k job descriptions scraped from LinkedIn, Indeed, Glassdoor and Monster Jobs. Here's the link to the Google Drive!

[Dataset](https://drive.google.com/drive/folders/1XxNuhiei5taFR6gziofYAx0oWfGeV7y9?usp=sharing)",datasets,2022-07-19 14:25:49,6
"[data.census.gov](https://data.census.gov) won't combine estimates for you.  If you have count estimates, you can add them together.  For percents, you need to get the numerator and denominator for each estimate, and then calculate the percent from those.

If you are using survey data (I'm assuming American Community Survey, but this applies to all surveys) there will uncertainty.  For ACS, they publish the margin of error.  There are ways to approximate MOEs.  Check out the Instructions for Stat. Testing and the Worked Examples documents (located here: [https://www.census.gov/programs-surveys/acs/technical-documentation/code-lists.html](https://www.census.gov/programs-surveys/acs/technical-documentation/code-lists.html)).",2,w3sb2u,"Hi everyone,

I'm working on getting various pieces of social, economic, and demographic data for a particular neighborhood in Los Angeles using data.census.gov. I've been able to retrieve all of the census tracts that are within the boundaries of the neighborhood, however, when I try to get the data I need and turn it into a table, I am provided with data for each individual census tract, rather than the combined census tracts that make up the neighborhood as a whole. 

How can I combine the individual census tract estimates into a unified estimate that reflects the entire neighborhood? I'm new to using data.census.gov in this capacity, so it could be that it's a simple user error on my part, so please forgive me if this is an elementary question for this sub. I've tried following along the various data gems/ census academy videos, but even when I follow their methods, I still have to sort through several individual census tracts instead of the whole neighborhood.

Thanks a bunch!",datasets,2022-07-20 10:32:54,1
So this would be something like the price of heroin on the silk road for the last 5 years? It's a really interesting data idea,8,w2x4by,"Hi! trying my luck here - anyone knows of any comprehensive datasets on the dark web's products and its prices? Ideally 2016 onwards - so far i've only found the [Darknet Market Archived (2013-15)](https://www.gwern.net/DNM-archives#works-using-this-dataset) which was mentioned in a previous similar post.

I'm trying to see the impacts of cybersecurity policies on dark web pricings (the regions i'm looking at rolled out relatively recent policies, around 2015-2016, so I want to assess their impact), but so far have only been able to find [https://www.privacyaffairs.com/dark-web-price-index-2022/](https://www.privacyaffairs.com/dark-web-price-index-2022/) \- if someone can tell me about this datasets' credibility, that would be much appreciated too!

thank you :)",datasets,2022-07-19 09:38:44,5
I would be shocked if this data wasn’t proprietary,1,w2y9ct,"Hi, all.

Does anyone know a source for song history, from an arbitrary date, for a given SiriusXM channel?

Specifically, I want to analyze the songs that have been played on the seasonal (Jul1-Aug29) Beach Boys channel (ch 105).

Does Sirius, or someone else, maintain this kind of data somewhere? Thanks.",datasets,2022-07-19 10:26:17,4
"I would carefully look through here for old datasets  
Parkinsons  
https://www.reddit.com/r/datasets/search/?q=parkinsons&restrict\_sr=1&sr\_nsfw=  
Faces  
[https://www.reddit.com/r/datasets/search/?q=face&restrict\_sr=1&sr\_nsfw=](https://www.reddit.com/r/datasets/search/?q=face&restrict_sr=1&sr_nsfw=)  


In particular this is not what you want but it could well be the closest= you will get [https://www.reddit.com/r/datasets/comments/px3kie/gait\_in\_parkinsons\_disease/](https://www.reddit.com/r/datasets/comments/px3kie/gait_in_parkinsons_disease/)

  
There was some tutorial on gait analysis that i cant find now. If this is just a college project I would combine fairly standard gait analysis with that dataset quickly and then try make it good. Possibly these courses will help  
[https://www.manning.com/liveprojectseries/pose-estimation-with-TensorFlowjs-ser](https://www.manning.com/liveprojectseries/pose-estimation-with-TensorFlowjs-ser)  
https://www.manning.com/liveproject/human-pose-estimation-with-deep-neural-networks",3,w2nlwz,"Hi, I am asking if there anybody who knows how to get a dataset of PD patients facial expression recognition

Or even can help me creating a new dataset based on DeepFace analysis and values by just giving me a set of images",datasets,2022-07-19 01:30:31,2
Thanks!,1,w1zqg5,"1. Visit [WorldData.AI](https://worlddata.ai/) and signup with a free account.
2. Verify your Email.
3. Go to User Profile (click on a circle with your name in the top right corner, then select User Profile) and at the bottom of User Profile enter the Secret Key: KDWD76345

 Link for more info: [https://www.kdnuggets.com/news/subscribe.html](https://www.kdnuggets.com/news/subscribe.html)",datasets,2022-07-18 06:44:57,2
"There was some research about living close to the hwy and quality of air wrt these problems.

In any case, if it isn't too much trouble, I'd love to read your paper eventually.",3,w24zjy,"I plan to do a research study this upcoming fall semester on asthma, mainly if there is a relationship between these factors (age, sex, environmental issues, smoking/non-smoking) and hospitalization rates. My supervisor said I should look for datasets, and I discovered this subreddit, so here I am.",datasets,2022-07-18 10:25:56,5
"wiki leaks possibly? I checked out some of the leaked emails ""vaults"" and most of them are just general conversation. You should be able to export them as well.

hope that helps.",3,w22ys6,"I'm looking for a dataset of email conversations. To be clear: single Emails will not be enough. I'm interested in the answering behaviour and context between mails. It would be nice if it's not only available in English. French, Spanish and/or German would be great too. Although I could potentially generate such languages using a translation API.

Thank you",datasets,2022-07-18 09:01:09,1
How does this compare to PurpleAir?,1,w1w1nb,,datasets,2022-07-18 03:33:36,1
"Hey absyes0,

I believe a `request` flair might be more appropriate for such post. Please re-consider and change the post flair if needed.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/datasets) if you have any questions or concerns.*",1,w1j570,Looking to understand trends in job roles / job types changing over time by company and sector.  Any inputs on what I can use would be appreciated.  Thanks...,datasets,2022-07-17 15:21:34,3
"Can you use nba play by play data? You can filter a table of plays towards the last minute of the game and filter for free throws?

You can scrape play by play data / could be available open source / might be able to pay for it",1,w19u1a,"Hi all, I’m doing a research paper to study choking under pressure in NBA. Essentially I want to analyse, what effect does free throws taken in the last 15, 30, 60 seconds of the match contributes the pressure of players causing them to miss the free throw. I am trying to replicate the results of this paper with a different data set
[https://www.researchgate.net/publication/227574446_Performance_Under_Pressure_in_the_NBA](https://www.researchgate.net/publication/227574446_Performance_Under_Pressure_in_the_NBA)

I am struggling in finding a data set that  has details whether free throws were shot in last few seconds of the match and what the score difference was between the teams. I’m in a bit of a time crunch as I need to give in this paper as soon as possible.

Any help is appreciated!",datasets,2022-07-17 08:23:14,2
SEC also has EDGAR filings that will provide most (all?) Of that info in spreadsheet or via api.,11,w0tp71,"Basically I'm looking for a giant database or spreadsheet of all the data companies are supposed to disclose to the public. 

The SEC has a big database of public filings, but you have to download the PDF, check table of contents, scroll to the appropriate page to find out how much the CEO made last year. So is there a database that has all that information?",datasets,2022-07-16 16:44:57,17
"Hey 1sjcdude,

I believe a `request` flair might be more appropriate for such post. Please re-consider and change the post flair if needed.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/datasets) if you have any questions or concerns.*",1,w0zuz4,"Hi, I am doing some research and wanted to know where I can find a typical grocery store inventory  list - all the things that are sold in a typical grocery store,  that includes grocery, alcohol, etc.

Thanks",datasets,2022-07-16 22:25:40,1
"If the data is being made available, sure. If not, no.",1,w11duf,Can i use social media API to get data on how they affect business branding/marketing,datasets,2022-07-17 00:03:17,2
"You could check EMS data. 
https://nemsis.org/view-reports/public-reports/ems-data-cube/",2,w0kry2,I'm trying to find data on non-vehicular incidents involving humans and wildlife- specifically deer being aggressive toward/attacking people. I've scoured opendata/dec/cdc and starting to wonder if I'm not looking for the right terms or looking at the wrong agencies; I would have thought it'd be the DEC or CDC. Please point me in the right direction!,datasets,2022-07-16 09:42:26,5
[deleted],1,w0m5zt,Does anyone have a good dataset on landfills? Would love to understand where my trash goes and how trash moves around the world,datasets,2022-07-16 10:46:09,1
"As a former astrophysicist, I appreciate the enthusiasm but the answer is you can't do much directly with the data.

Complex analysis requires way too much domain knowledge and anyone who needs that will be looking for a PhD student, not an amateur enthusiast.

Simpler stuff will already have teams on it. I'd be amazed if things like automatically cataloguing galaxies in JWST data isn't being done already.

However, all is not lost! I'm saying you can't just download some JWST and do something useful in a self directed way. I'm not saying you can't use data science to help astronomy at all!

You can look for hackathons and competitions such as the [Ariel data challenge](https://www.ariel-datachallenge.space/) where research groups are actively looking for short term input from data scientists. These kinds of things are a perfect way for someone with a lot of data science knowledge and enthusiasm but no domain expertise to help out.",20,vzr6wa,"Hey! I'm new to this sub and I was looking for some data scientists who can tell me what's possible to do with NASA public datasets coming from their space telescopes, such as Kepler, Hubble, and now the new and incredible James Webb.

My question is: what can be achieved with it? Is it actually possible to discover new things using these public datasets? Like what?

I'm a data scientist myself but I'm not even at a junior level, so I'd love if someone linked me some materials about this topic so I can learn and hopefully play a role in discovering new and exciting things from my home.",datasets,2022-07-15 08:17:25,9
Where have you looked so far?,1,w0azpv,"I'm doing a project to understand the current situation of different sectors in the US economy (eg: BFSI, Health Care, Travel etc.). For this I need to analyse some key metrics/indicators, for example, Revenue Growth, GDP(of the sector) etc. Where can I get such data that refreshes on a monthly or quarterly basis?",datasets,2022-07-16 00:31:23,3
"What do you mean ""their"" datasets? It is community driven collection. Anyone can upload a dataset there. Depending on what you are looking for you may find a good quality datasets there just because there are so many of them.",4,w04vfm,"I’m newer to the realization that I love data and the possibilities, and just saw that Kaggle has a dataset area. Does anyone have any experience with their datasets and how robust they are?",datasets,2022-07-15 18:40:25,4
Like kaggle?,3,w00vbc,"I’m considering making a platform to help people crowdsource/gather and access datasets. It would enable people to open repos and pay others to help them build their needed dataset; they could also just use the platform to build their dataset there.

The platform would have app and web interfaces where helpers or owners can upload data (e.g pictures, videos, etc.).

Wanted to gauge y’all’s thoughts on something like this 🤔 

Thanks!",datasets,2022-07-15 15:22:48,3
"I don't know of a dataset, but [plantix](https://plantix.net/en/) is a company in that space.",2,vzhpsq," PlantVillage and Kaggle do not contain the data I need for a machine learning project. I need a dataset that contains images of different plant nutrient deficiencies 

 This is information about the lack of nutrients. Deficiency of nitrogen, phosphorus, potassium and other deficiencies",datasets,2022-07-14 23:29:03,2
In Florida they are required to submit publicly available documents which sometimes can include price tables,5,vzrqy1,I'm trying to price analysis for some insurance companies and was wondering- do they have information regarding their pricing via an API? Or just listed in general anywhere?,datasets,2022-07-15 08:41:21,3
is the raw data available anywhere?,1,vytsks,"Given that Americans are struggling to pay their medical bills, inflation is rising quickly—at a pace of 9 percent—and that gas and food prices are at all-time highs, this subject is one of the most popular and alarming ones for all Americans. To give the most recent data on this subject, DebtHammer has conducted research regarding bankruptcy in the US.

**397.000** **Americans** filed for bankruptcy in **2021** alone.

DebtHammer conducted a research on this matter and gathered some of the latest data and statistics: [`https://debthammer.org/bankruptcy-statistics/`](https://debthammer.org/bankruptcy-statistics/)",datasets,2022-07-14 04:39:41,3
"It might take some digging and search/FOIA requests, but look for data sources for these.

https://dshs.texas.gov/thcic/hospitals/Potentially-Preventable-Readmission-Reports/",3,vyoi2h,Looking for a heart failure hospital readmission dataset. Can anyone help me?,datasets,2022-07-13 22:52:49,3
"Hey Unlucky_Research2824,

I believe a `question` or `discussion` flair might be more appropriate for such post. Please re-consider and change the post flair if needed.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/datasets) if you have any questions or concerns.*",1,vxxxyz,I'm doing some research on finding connections between LARGE sets of data and looking for same or similar dataset.,datasets,2022-07-13 01:14:30,7
I doubt you’d do better than scraping leafly,5,vxdj2l,"I have been looking for an Weed API to integrate into my application that has information on companies, strains, and products. I have found a few with tons of broken links that don’t seem to be actively supported (Leafly, Cannabis Reports, Otreeba). Are there any that are being supported currently as well as into the future?",datasets,2022-07-12 08:11:05,3
[You could download the entire StackOverflow database](https://www.brentozar.com/archive/2015/10/how-to-download-the-stack-overflow-database-via-bittorrent/) \- comes in an easy to restore SQL Server backup - it's around 400GB.,9,vwwg38,"I'm trying to find some good SQL databases for practising SQL and data analytics. I've downloaded the contoso retail dataset and was hoping to find more .Bak files similar to this.
Does anyone have any suggestions?",datasets,2022-07-11 16:30:20,9
\-Friendly reminder. Is anyone able to help a student out?,1,vwkkpw,"Hi there,

I am writing my dissertation about alternative lending. I have seen that statista has the datasets I need. However, I do not have a premium account and was wondering if anyone on here could help a student out?

It regards the following data: transaction value, loans, global comparison and key market indicators.

Link: [https://www.statista.com/outlook/dmo/fintech/alternative-lending/worldwide](https://www.statista.com/outlook/dmo/fintech/alternative-lending/worldwide)

I would need the worldwide data in order to make a comparison between countries and regions.

&#x200B;

Thank you so much in advance!!",datasets,2022-07-11 08:04:54,1
"Hey hemi2hell,

I believe a `request` flair might be more appropriate for such post. Please re-consider and change the post flair if needed.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/datasets) if you have any questions or concerns.*",1,vwjqzz,I am looking for standard job descriptions provided in BLS ( for eg Network Admin) and number of companies in different employment ranges (eg. Network admin 10k in orgs with 0 to 10 employees). Any idea how I can find this data ?,datasets,2022-07-11 07:28:31,2
"Lots of reasons to consider ready-made datasets.

- Budget

- Labeling constraints

- Gold standard (e.g. NOAA, weather)

For example, a hedge fund like Blackrock might look at satellite-driven air quality information before buying up a neighborhood.",2,vwbnqd,,datasets,2022-07-10 23:23:36,1
Where is the link?,1,vvy2vy,"> Disney+ Hotstar (also known as Hotstar) is an Indian brand of subscription video on-demand over-the-top streaming service owned by Novi Digital Entertainment of Disney Star and operated by Disney Media and Entertainment Distribution, both divisions of The Walt Disney Company. 

This data is scraped by me and contains all different tv and movie details offered by Disney + Hotstar. (\~6.3k content)

Edit: https://www.kaggle.com/datasets/goelyash/disney-hotstar-tv-and-movie-catalog",datasets,2022-07-10 11:45:02,2
"A couple of tables of a couple of GB does not make a big-data project or create a need for a data lake.

Data lake is usually comprised of lots of data from lots of tables and usually gathered from multiple sources. That’s why it’s a “lake”.",2,vvz5dp,I am looking to create a datalake for practice in aws however I can only find one or two tables data. Looking for some complex data consisting of couple of tables and size in a couple of GBs to get hands on with nuances of big data project.,datasets,2022-07-10 12:34:46,5
"Cool work.. 

I need to get into some games, any online web based ones , strategy, war etc , that tou would recommend?",2,vvct1n,"[https://www.kaggle.com/datasets/yanivaknin/fafdata](https://www.kaggle.com/datasets/yanivaknin/fafdata)

I scraped this dataset and made the Kaggle page. The ""About"" text from Kaggle (without hyperlinks) is quoted here for your reference:

>Perhaps you heard about Supreme Commander, an acclaimed Real Time Strategy game by Chris Taylor and now-defunct studio Gas Powered Games. Here's the original game trailer.  
>  
>Over the years, an incredible community of enthusiasts came together  and built a multiplayer game lobby and many other improvements, under  the name FAF (Supreme Commander: Forged Alliance Forever). Here's a video of a 2020 FAF match between two top rated players.  
>  
>But now the cool bit: FAF has a scrapable API tracking every game  ever played, and a parseable binary replay of every command issued by  every player in every game. For example, here's the game metadata for that 2020 match, here's the map it was played on and here's its binary replay. I just *had* to scrape and publish that.  
>  
>The dataset offers general metadata for \~10M FAF games (who played vs  who, on which map, what's the scoreboard, etc) and deeper replay  analysis of some 700K 1v1 games on the official FAF ladder (which type  of units did the player use, how fast did they click, etc).  
>  
>In addition to the data files you can download here, I publish all interim data products in BigQuery. The datasets I use are fafalytics.faf\_ladder\_1v1 and fafalytics.faf. Try SELECT \* FROM \`fafalytics.faf\_ladder\_1v1.playerstats\_extended\` in BigQuery to start exploring.

If you're interested in this dataset, you may also be interested in the [data engineering toolkit](https://github.com/yaniv-aknin/fafdata) I made to scrape it.",datasets,2022-07-09 15:26:26,5
Very cool! Would love this in R,2,vv3q6p,,datasets,2022-07-09 08:09:23,3
You're going to have a hard time obtaining this data unless you partner with a bank. Banking data is highly confidential and very difficult to anonymize.,6,vubijx,"I am looking for a dataset that looks like the one at [https://www.kaggle.com/datasets/demodatauk/full-banking-transaction-log-sample](https://www.kaggle.com/datasets/demodatauk/full-banking-transaction-log-sample) , which is only a sample. I like the data format, but it does not include enough data. Their data come from [demodata.ai](https://demodata.ai) , but their datasets are expensive.

The dataset I am looking should include information about different branches/locations of a bank, customers, and a transaction log. Ideally this could be covered within 3 tables.

I know this is a long shot, but if you know of any similar dataset, or any source where I could find data that looks like that, please let me know!",datasets,2022-07-08 07:02:47,2
"Diffbot may have what you're looking for. SafeGraph may as well. 

&#x200B;

Disclosure: I'm an ex Diffbot employee.",1,vugizg,"Hi Everyone,

I'm wondering if anyone here has ever worked with data sets, either commercially provided or not, that contain information on all locations of a given company. I'm researching for a project where we would like to know how many operating locations a given company has in the United States. This is not just limited to chain retailers or restaurants, but also corporations, law firms, etc.

A quick browse on this subreddit has led me to resources such as EDGAR, which didn't quite fit my needs, Linkedin's API, Crunchbase, OpenCorporates, etc. I'm wondering if anybody has any other suggestions for data sets that may be able to address this question, or if anyone has personally worked with any data sets of this sort and can vouch for their validity?

Thanks so much in advance!",datasets,2022-07-08 10:49:53,1
"It's against their ToS, technically, but it isn't against the law or anything.",14,vu4cde,"Sorry if it is a noob question. As the title says, can i use scrapped image from google? I searched here and there for the answer but got a mixed answer. And also, can i publish that dataset in communities like kaggle?",datasets,2022-07-07 23:45:07,9
Datasets like these cost a lot of money.,2,vtu28y,"I’m looking for some measure of education like GPA, or SAT score or graduation rate than is based on zip code, school district, or location. This is based on the fact I already found a dataset for household income based on wealth. 

If you have other datasets that are better than what I have eg. One that has some measure of education and wealth that would be nice.

(Education data for high school)",datasets,2022-07-07 14:51:18,3
"Can't help but this sounds like a fun, interesting project.",3,vtswxq,I'm trying to create an ML model to recognize engine types based on sound recognition. For this I need a database with vehicle sounds and their engine types.,datasets,2022-07-07 13:59:14,6
"Library of congress has it.  But maybe not in the format you are looking for.  It would take a lot of work to make a nice dataset.

Here's Chicago Tribune https://www.loc.gov/item/sn84031492/?st=calendar&year=1880",2,vttt9n,"I'm looking for a data set of historical newspaper headlines from around 1870-1930. I'd prefer a major newspaper, or even several. I've tried looking, but just can't seem to find much.

Thanks!

EDIT: To clarify, I'm looking for data I can analyze, so I need text, not images. It doesn't need to be parsed, just in text format. ",datasets,2022-07-07 14:38:04,2
What ML package are you using? Genuinely curious cause I am doing something similar for work,2,vtgfob,"I need training data to conduct sentiment analysis on Reddit posts of some politics-related subreddit.

I checked some popular datasets like IMDB Movie Reviews Dataset, Sentiment140 etc. but none of them seems to address my need so far.

All I need is a dataset with just two columns in which the first column contains sentences/paragraphs/posts/texts/tweets/reviews etc. and the second column contains the labels “positive”, “negative” and “neutral”.

I know that I can use VADER or TextBlob to automatically label the posts that I scrape from Reddit -I already did in fact- but the point is I want to compare the success of the lexicon approach with the ML methods for identifying the actual sentiment of a post. Therefore, I need a more reliable, scrutinized dataset where I can have more confidence in the correctness of the labels associated with the texts.

Sentiment140 dataset would just suffice if only it had ""neutral"" in its labels.",datasets,2022-07-07 04:28:31,7
"I have worked in data analytics/data viz for around 8 months and have been using Tableau just as long. I decided to work on something at home and I kept getting self-induced scope creep. I’ve been working on this project for around 4 months all-together and lord knows how many hours. I bit off WAY more than I could chew, but I finally got it to a point I feel good about. 

Any and all feedback/constructive criticism is welcome!!!",2,vt2s6g,,datasets,2022-07-06 15:41:12,5
[datasetsearch.research.google.com is your friend.](https://datasetsearch.research.google.com/search),2,vtapgg, I'm doing my final year project. I'm trying to create recommendation system for best player to best position. I want to fin NHL Player stats data set for my research. any can help me to find best data set for my research project? Thank you,datasets,2022-07-06 22:16:28,3
"Shanghaied!

TL;Dr: allegedly, “this exploit happened because the gov developer wrote a tech blog on CSDN and accidentally included the credentials.""

Unintentionally. Perhaps the biggest open government dataset ever!",28,vs9ol9,,datasets,2022-07-05 15:09:40,4
How long is the history? What’s the frequency?,2,vshzms," Hi, I have created this new Dataset on NIFTY-500 Stock, this contains Stocks information of the Top 500 listed companies on the NSE. If you have some time, then you can check this out: [https://www.kaggle.com/datasets/iamsouravbanerjee/nifty500-stocks-dataset](https://www.kaggle.com/datasets/iamsouravbanerjee/nifty500-stocks-dataset)",datasets,2022-07-05 22:05:28,3
Robinhoodtracker,1,vsmq2s,I am looking for a comprehensive dataset of Robinhood retail investor trading information. Any suggestions?,datasets,2022-07-06 03:24:33,4
You should be able to get this from their Edgar Public reports.,2,vs2bc8,"Does anyone have a compiled list of OTT datasets that includes service launch date, revenue, subscriber count (if publicly available), number of hours streamed, geographical location of subscribers, etc…. (Anything else)",datasets,2022-07-05 09:46:45,4
Has anyone made Jupyter notebooks to start processing this stuff?,2,vrmbt3,"[Kaggle Link](https://www.kaggle.com/datasets/jasonmobley/united-states-gun-violence-data-20142022)

All pulled from [GunViolenceArchive.org](https://GunViolenceArchive.org) in 2000 row sets (as their database csv export won't do more than that at a time) and consolidated by hand. Cleaned and organized by date using R.

I hope this provides better availability of information on this topic for any analysts out there exploring Gun Violence in the United States.",datasets,2022-07-04 18:23:11,3
"I would say that if white was the largest group in the data (e.g. we're looking at a European country) and I was simply splitting into white/non-white, then mixed ethnicities would go into non-white. That's assuming you're literally reporting only that single stat. If you're doing a wider analysis, then this metric needs to fit the context of the piece as a whole, which may require a different split.

Most important thing is to make it super clearly defined when it's presented so that your audience can't misinterpret what they're being shown.",2,vs4nu5,"When you calculate % white or a similar value, do you usually just use the single race variables (e..g ""white alone"") or do you take the time to add up all the mixed race values that include white as a race?

What is your approach?",datasets,2022-07-05 11:30:08,2
"We have a giant FBI-NIBRS one:

[https://www.dolthub.com/repositories/dolthub/fbi-nibrs](https://www.dolthub.com/repositories/dolthub/fbi-nibrs)

DISCLAIMER: I'm the CEO of DoltHub.",2,vrtrb9,"Hello, Can you help me with large, structured datasets that I could run SQL queries against? My use case is to convert the dataset to a columnar file format (e.g. parquet) and run SQL queries against them.

So far I'm only aware of TPC-DS, TPC-H and the NYC Billion Taxi rides datasets. Any others?",datasets,2022-07-05 02:18:15,1
"That info is available in redfin data exports. Build a query and use the ‘export to csv’ option listed below the results listing. Unfortunately, you’ll probably have to string together multiple queries.",3,vrg5yb,"Hello,

I searched everywhere for a dataset that has the average HOA fees by either state, county, or even zip, but fell short. Does anyone know where I can find a dataset like this?

&#x200B;

Thank you",datasets,2022-07-04 13:10:21,9
"Hey fuzwz,

I believe a `request` flair might be more appropriate for such post. Please re-consider and change the post flair if needed.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/datasets) if you have any questions or concerns.*",1,vqufj7,"Hello, I'm looking for a way to extract vector data of U.S. land parcels. I know certain company providers like Regrid that offer this data; see e.g. the polygon at this url:   


[https://app.regrid.com/us/hi/hawaii/papaikou-wailea#b=none&p=/us/hi/hawaii/papaikou-wailea/94616&t=property](https://app.regrid.com/us/hi/hawaii/papaikou-wailea#b=none&p=/us/hi/hawaii/papaikou-wailea/94616&t=property)  


Some states have the same data:   


[https://qpublic.schneidercorp.com/Application.aspx?AppID=1048&LayerID=23618&PageTypeID=1&PageID=9875&Q=109532823&KeyValue=270080170000](https://qpublic.schneidercorp.com/Application.aspx?AppID=1048&LayerID=23618&PageTypeID=1&PageID=9875&Q=109532823&KeyValue=270080170000)  


Is there an open source / public way to obtain the dataset of U.S. land parcels in vector form? Any guidance would be greatly appreciated!",datasets,2022-07-03 16:58:16,7
"Hey, we need the `self-promotion` tag only in case you work for a website and are promoting it. Posting links from Kaggle etc is fine.",2,vqagp1,"https://www.kaggle.com/datasets/aaddrick/ny-times-fiction-best-sellers-19312022wikipedia

Started digging into a dataset from OpenData Ottawa about library book holds and wanted to cross reference against the NY Times Best Sellers list. Couldn't find a consolidated source online that was easy to play with so I manually scraped the various Wikipedia pages and cleaned the data up a bit.

Here's hoping the next person who needs this can find it easier :)",datasets,2022-07-02 22:38:24,7
"Hey Stuckatpennstation,

I believe a `request` flair might be more appropriate for such post. Please re-consider and change the post flair if needed.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/datasets) if you have any questions or concerns.*",1,vq0npc,Would this have to be done through each state one by one or is there a federal dataset that has all of them,datasets,2022-07-02 13:39:05,12
"Hey Responsible-Goat-158,

I believe a `request` flair might be more appropriate for such post. Please re-consider and change the post flair if needed.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/datasets) if you have any questions or concerns.*",1,vptdz4,"hi fellas, how is everyone doing on this vibrant Saturday afternoon? I am in a predicament, trying to build some applications around credit decisioning and I am looking for an SME dataset I could use. I have been struggling to find anything useful for some time now. plz help 🥺",datasets,2022-07-02 07:48:22,1
"Hey, we need the `self-promotion` tag only in case you work for a website and are promoting it. Posting links from Kaggle etc is fine.",1,vpr6cq,"Hi everyone, recently i've made my first dataset for a Natural Language Processing project.  
With a web scraper i've gathered 526 italian reviews from four Italian Car Finance.  
Let me know what you think, advice are welcomed!

Kaggle: [https://www.kaggle.com/datasets/vincenzors8/car-finance-italian-reviews](https://www.kaggle.com/datasets/vincenzors8/car-finance-italian-reviews)",datasets,2022-07-02 05:53:38,2
Are you offering it ?,1,voyb8n,,datasets,2022-07-01 04:12:10,2
"[https://data.worldbank.org/indicator/IC.BUS.NREG?most\_recent\_value\_desc=true](https://data.worldbank.org/indicator/IC.BUS.NREG?most_recent_value_desc=true)

That should help",3,voybhw,"Hi there, I'm looking for a dataset on how many startups are located in each country. Would be grateful for every tip!",datasets,2022-07-01 04:12:36,2
"I've seen a couple of talks by the people at ntropy, I think they have an Api",3,vo7zue,"Does anyone know of a mapping that exists for line item credit card name —> cleaned company name?

For example, it would map XXTNZ KROGER PAYMENTS CREDIT —> Kroger

Thanks!",datasets,2022-06-30 06:08:27,2
"are you using gpu or cpu? it depends upon that and yes, DL libs are slow",3,voiwjb,hey i'm using a library of \[deep learning\] named \[easy ocr\]  for detecting text but it takes a long time because a have many papers for extracting \[data\] from them so  i don't know if there is a way to minimize the time of execution of this library,datasets,2022-06-30 14:06:00,2
"Hey captainschnarf,

I believe a `question` or `discussion` flair might be more appropriate for such post. Please re-consider and change the post flair if needed.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/datasets) if you have any questions or concerns.*",1,vnw08s,"Hi!  Does anyone know where I can find a large dataset capturing demographic information (e.g., race, gender, age) about friendship groups (among adults)?  Thanks!",datasets,2022-06-29 18:30:06,2
the BLS has wages data by occupation and a few other groupings. Their site is crap on mobile so I can’t find it but I’ve used it before and I’m sure it’s updated at least yearly.,7,vnnyer,I would be really grateful if anyone knew of any places to get wage data going back past 1960. Especially if you can get median (instead of average) wages from it.,datasets,2022-06-29 12:26:35,6
"To download the dateset, scroll down on the left hand side bar to 
""Reports/Export Data"" and click on the item that says ""Australian facilities (CSV)""",2,vnkmn7,,datasets,2022-06-29 10:01:40,1
"“More likely”. More likely than what, specifically?",1,vmx36m,,datasets,2022-06-28 13:32:51,1
What films?,2,vmphyu,"I've been able to find a couple of websites that share production costs for a number of films I'm using for a research project, but haven't been able to locate costs for certain films. Does anyone have any idea where I can find film production cost data for free?",datasets,2022-06-28 08:09:44,3
I'm so confused but so interested. Where can I start learning from basic than beyond. Please,1,vmoisw,"Hi all,

Hoping this proves to be a valuable resource for the community. Roboflow (disclaimer I work there) has made 80k labeled and annotated datasets for computer vision / machine learning searchable and open for use [https://universe.roboflow.com/](https://universe.roboflow.com/)

The goal is to help advance computer vision research and applications but maybe this community has use cases beyond that as I've often seen requests for image datasets here. Feedback and suggestions appreciated since it is brand new. Curious to know what could make it more helpful for this community.",datasets,2022-06-28 07:26:09,1
"I gotchu fam, just DM me your email",3,vmoc1b,"Hi there, 

I am writing my dissertation about peer-to-peer lending. Now I have seen that statista has the dataset I need. However, I do not have a premium account and was wondering if anyone on here could help a student out?

it regards the following data:

[https://www.statista.com/outlook/dmo/fintech/alternative-lending/marketplace-lending-consumer/worldwide](https://www.statista.com/outlook/dmo/fintech/alternative-lending/marketplace-lending-consumer/worldwide)

I would need it for certain political regions in order to make a comparison between them i.e. EU, US, BRIC.",datasets,2022-06-28 07:17:35,3
Academic (Research) is the highest clearance AFAIK however you need an institution's approval. I'd say go for it.,1,vmrp6d,"It's not explicitly clear, but it seems like I can? I'm weighing up if there is any benefits of using one of the respective Full-Archive Search endpoints vs the other.",datasets,2022-06-28 09:46:11,1
"Just scrape Twitter using their API.  Extract the text of the tweet, strip out any URLs, and use that.",7,vm5fs0,"I'm looking to spam a company that keeps messaging me. If anyone knows of a dataset of text conversations, random or not, that I can use to pipe through a program to message these folks over the course of 24 hours, please let me know.",datasets,2022-06-27 13:43:56,7
"I think this is the raw data, so you can computer monthly stats from it.

https://www.cdc.gov/nchs/data_access/vitalstatsonline.htm#Mortality_Multiple",1,vm9y0x,"wonder.cdc seems to only have data from 2020 - 2021. It can be deaths per 1,000, total deaths, or a rate. Any ideas?",datasets,2022-06-27 17:35:32,1
Check out IEX Cloud - - https://iexcloud.io/,1,vm6z2r,"I have questions about the stock market and I'd like to try and answer them by building this database and learning data analysis & visualization with it.  I'd appreciate any advice, thanks",datasets,2022-06-27 15:15:34,1
"Spend the absolute minimum amount of your time to create a simple regression model or similar for the most basic stats. When you present it, just make sure to use the terms ""AI"", ""machine learning"", ""predictive analytics"" plenty. Sounds like they're excited about those words rather than anything you could possibly need actual ML techniques for.",17,vlto04,"I have a problem statement where a factory has recently started capturing a lot of its manufacturing data (industrial time series) and wants Machine Learning/Data Science applications to be deployed for its captured datasets. As is usual for customers, they have (almost) no clue what they want. Some use cases I already have in mind as a proposal include:

1. Anomaly/Outlier detection
2. Time series forecasting - (demand forecasting, efficient logistics, warehouse optimization, etc.)
3. Synthetic data generation using TimeGAN, GAN, VAE, etc. I already implemented quite a lot of it with Conditional VAE, beta-VAE, etc. But for long sequence generation, GANs will be preferred.

Can you suggest some other use cases? The data being captured is in the domain of Printed Circuit Board (PCB) manufacturing.",datasets,2022-06-27 04:49:08,8
"Simon wood, author of mgcv, created  a package on cran called gamair which contains lots of datasets which gams can make the most of.",1,vm0afm,"Good day! I am taking a class where I have to show that I can display and perform a GAM process in R. From what I've seen, I have to have a dataset with many characteristics that can be modeled and shown with this function. I am given creative freedom on the data I use and am not sure what I should pick.  Any ideas or links to datasets that would be perfect for this kind of model?",datasets,2022-06-27 10:02:06,2
"I downloaded and unzipped the files.  In the excel version there is a 2nd tab.  There is a table that explains the column names.  At the bottom, it says ""Note: Fields suppressed for confidentiality are assigned the value (S).""  

I took a look at the data, and there are always 2 or more percent columns with an ""(S)"".  If you sum the percents for all the race groups almost all of them sum to 99.9, 100.0 or 100.1% (probably due to rounding).  But the ones with an ""(S)"" in the columns are off by 0.2% or more.  So, it's to protect confidentiality.",15,vlbq4z,"I am working on a project using some Netflix catalogue and some 15000 names of actors/directors, trying to estimate their ethnicities using datasets of names and what ethnicity they are most common in. I found the US Census Data of surnames and % of people who identify as certain ethnicities, but it's riddled with ""(S)"" values amongst the legit floats. Unfortunately the data dictionary [from 531](https://www.kaggle.com/datasets/fivethirtyeight/fivethirtyeight-most-common-name-dataset?select=README.md) or the [census methodology](https://www2.census.gov/topics/genealogy/2010surnames/surnames.pdf) [PDF] detailing did not clarify things.

Does anyone know the significance of that value? 

https://www.census.gov/topics/population/genealogy/data/2010_surnames.html",datasets,2022-06-26 11:57:41,4
"Hey geenmilk,

I believe a `request` flair might be more appropriate for such post. Please re-consider and change the post flair if needed.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/datasets) if you have any questions or concerns.*",1,vlb7xi,"Hello there, I’m looking for datasets like e-commerce datasets or sales datasets for even any economic problem data sets such as cost of education etc. for my project to make five business questions from my imagination and how to answer this questions and find solutions with a visualization help.
Thanks for any help :).",datasets,2022-06-26 11:34:21,7
"Here is few large Ecommerce datasets

\- [Target store dataset (800K+ products)](https://crawlfeeds.com/datasets/large-target-store-products-dataset)

\- [Home Depot products dataset (around 2 millions records)](https://crawlfeeds.com/datasets/home-depot-products-dataset)

\- [Otto German products database(1 million records)](https://crawlfeeds.com/datasets/otto-german-products-database)

\- [Mercari US retail products dataset (more than one million)](https://crawlfeeds.com/datasets/mercari-us-retail-products-dataset)

\- [eBay products dataset (1.2 million records)](https://crawlfeeds.com/datasets/ebay-products-dataset)

\- [Download Flipkart E-commerce dataset (5.7 million records)](https://crawlfeeds.com/datasets/download-flipkart-e-commerce-dataset) ... many more",2,vl0h9c,"I’m looking for a dataset of a large catalog containing up to millions of products with descriptions, prices and categories.",datasets,2022-06-26 01:58:34,5
Not completely certain this has what you are looking for but [I would try IPEDS](https://nces.ed.gov/ipeds/use-the-data).,7,vksrme,"I've been tasked by a client to reach out to every college and university in the United States with an undergrad enrollment of over 10,000 students.  I found [this academia.stackexchange question](https://academia.stackexchange.com/questions/18971/downloadable-list-of-all-accredited-u-s-colleges) that got me a list from the US government with every accredited institution, but it doesn't have enrollment numbers.

This seems like the kind of information US News would have, but from quickly looking through their website I didn't see a way to get it in an easy-to-consume (CSV) format.

I also came across [this website](https://www.stateuniversity.com/rank/tot_enroll_rank.html), but it stops at 500, when the enrollments are still listed at \~11,000.  The website says they offer datasets at a price, so I've sent them an email to ask the price, but I'm wondering if the data is available anywhere else.  \[Update: I actually didn't notice it but this link was a list of *total* enrollment, not *undergraduate* enrollment.\]

I could also of course just Google every name in the US Government spreadsheet with the word ""enrollment"" to see what Google says, but there are \~3,000 of them, so it would take a while.

&#x200B;

\[Update: [IPEDS](https://nces.ed.gov/ipeds/use-the-data) gave me the data I needed. Here's the data as a pastebin in case someone comes across this in the future: https://pastebin.com/bY0DYQD9\]",datasets,2022-06-25 17:51:04,7
"If you are a college student, check with your school library. My university library has a subscription….",2,vkxhcu,"Hey guys.

Is there any of you guys that have a corporate account there and could help me out with some data? i really need it for an important thing Im working on but of course, im broke as fuck lol.

and btw if u have any trick to at least be able to see the info of the graphics ill appreciate it too haha

[https://www.statista.com/outlook/dmo/eservices/online-food-delivery/chile](https://www.statista.com/outlook/dmo/eservices/online-food-delivery/chile)

thanks in advance guys",datasets,2022-06-25 22:32:05,3
Here are over 600+ FREE data sets from the Univ of California Center for Machine Learning and Intelligent Systems:  https://archive.ics.uci.edu/ml/datasets.php,7,vkcqow,,datasets,2022-06-25 04:08:31,3
Is it legal to collect LinkedIn user profile data?,1,vkmnuz,,datasets,2022-06-25 12:39:39,2
"Are you asking for data outside the data used for the Microsoft tutorials, such as the World Wide Importers and Adventure Cycles?",1,vkix3z,"I’m not even sure if anything like this exists. The ideal would be data split in dimensions, so that I can model it as needed. The format isn’t important csv or json is fine.
Daily or weekly updates would be great, so I can work on incremental updates.
Anyone know of a source I can use for this?",datasets,2022-06-25 09:39:16,3
Naked or clothed?,1,vkljhh,I have been looking for something like this for a while but I can mostly only find faces/very small datasets,datasets,2022-06-25 11:45:24,3
"Hey SweetPotayto23,

I believe a `request` flair might be more appropriate for such post. Please re-consider and change the post flair if needed.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/datasets) if you have any questions or concerns.*",1,vkdgfh,"I have been trying, unsuccessfully to obtain some heightmap data for a particular area of the US as well as the UK in order to;

\- Create an UE5 environment (for fun)

\- Visualise the data based on a bunch of criteria

\- Create shaded relief maps for fun/3D printing

&#x200B;

I am struggling to find good sources of heightmap information that I can use and i'm wondering what are some of the best sources/workflows for creating/obtaining heightmaps. I have just started experimenting with QGIS and also have world builder for UE5 (but more for creating fictional locations).

Thanks in advance!",datasets,2022-06-25 04:54:54,2
"You can check these websites:

[https://abortion-policies.srhr.org/](https://abortion-policies.srhr.org/)

[https://reproductiverights.org/maps/worlds-abortion-laws/](https://reproductiverights.org/maps/worlds-abortion-laws/)

However, I'm not sure if they have a data export option, I just found it through this: [https://dataset-finder.netlify.app/](https://dataset-finder.netlify.app/) but didn't investigate further",1,vk1vqu,"Abortion legality/status by country, by year - when did each country allow abortions to happen?

I need this data for analysis.",datasets,2022-06-24 16:53:14,2
[https://www.gdeltproject.org/](https://www.gdeltproject.org/) might have Reuters as well.,1,vjzc42,"Hello everyone,

I was looking for a good news source dataset, for example, a dataset coming from [reuters.com](https://www.reuters.com/), covering at least the last \~20 years.

The only relevant things I found was the remains of an [old financial dataset from 2015](https://github.com/philipperemy/financial-news-dataset) and an un-maintained [news crawler](https://github.com/LuChang-CS/news-crawler) since Reuters changed their [archive website](https://www.reuters.com/news/archive) and hard-limited the historical data you can access, starting 2020/03/08 - Thus, limiting my goal of having a widespread dataset in time.

This feels like a dead end. If you have some suggestions regarding crawling data from Reuters please tell me. I'm also open to new sources of reliable and relatively unbiased new sources that I'm not aware of, any ideas internet?",datasets,2022-06-24 14:47:51,2
"""All US federal spending data"" is perhaps a bit overselling it. I've personally done a lot of analysis of this data for a company that mostly sells to the US federal government.

In general, this is a good source of information about government appropriations (i.e. spending) with some caveats (not all government appropriations are in USA Spending) and a number of quality issues that can present some challenges (e.g. subcontractor data may as well be non-existent in it due to how sparse it is). It's more of an aggregator of the data as well. It originally consumed a lot of the award/spend data from FPDS. But, FPDS is now replaced with sam.gov.

Also... if you want all the data, you have to download a PostgreSQL database backup and restore it to your own PostgreSQL instance. It's a bit odd to say the least. Though, the website does give you access to a lot of useful information through it's interface that you can drill down into, etc.",1,vj7jvo,,datasets,2022-06-23 14:23:43,1
"Any text?  How about [Loghub](https://github.com/logpai/loghub)? Maybe you could visualize links in error messages for failure analysis, for example does a seemingly harmless warning seem to be linked to a much worse problem that occurs later?

I don’t have a dataset handy but access logs might also be interesting for intrusion detection or visitor analytics.",2,vjq27v,"I'm working on a tool that can visualize any text as a network. I was wondering if any of you knows of some datasets I could throw at my algorithm to test how well it works. It is really good in detecting the main topics and the relations between them but also the structural gaps in a discourse. So it could be customer reviews, movie descriptions, books, research papers... Maybe you have an idea? Will appreciate any leads! Thanks!",datasets,2022-06-24 07:43:14,1
Yep seems broken on that link!,1,vjb63t,"I am having a hard time downloading this [data from the NOAA](
https://www.ncei.noaa.gov/metadata/geoportal/rest/metadata/item/gov.noaa.ngdc.mgg.dem:410/html#).

When I click to download the NetCDF file nothing happens. Is the server having a bad time? Do you need an account for NOAA data? It is my first time trying to access this sort of data so I am not sure if it is me or them.",datasets,2022-06-23 17:18:24,1
"Hey WhatsTheAnswerDude,

I believe a `question` or `discussion` flair might be more appropriate for such post. Please re-consider and change the post flair if needed.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/datasets) if you have any questions or concerns.*",1,vj7kj4,"Hi data peeps,

Working on an ad hoc data project, would anyone know if there is data I could find/attain as far as hotel bookings in the month of May and June possibly for the US (or where I should look)? Similarly, does anyone know if there's any data I could possibly find concerning interest for booking hotels at all in the next couple months?

I will be looking at Google Trends to say the least but was just wondering if booking info for hotels might possibly be available somewhere?",datasets,2022-06-23 14:24:31,4
"Hey Argentox69,

I believe a `request` flair might be more appropriate for such post. Please re-consider and change the post flair if needed.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/datasets) if you have any questions or concerns.*",1,vj2xp1,Where can I find a data set of EV registrations over the last 10 years,datasets,2022-06-23 10:58:58,1
Very Interesting dataset indeed! Issue is that most cyberattacks are lost into IT incidents (at least in my region of the world..) in public reports. Maybe the US has a different and more fine-grained perspective on security incidents.,2,viorlz,"Hey all, I am doing a research project (for myself) looking into cyber attacks (of any kind) towards municipalities (such as police departments, public hospitals, schools, etc). If anyone can help with this (like in my title) I will award a platinum and also power up this server.

If you have any questions about the specifics, please let me know in the comments or via pm.

Thanks in advance.

edit: the data set does not have to be visualized at all, for just need the raw data.",datasets,2022-06-22 21:52:55,5
"One reason might be that males tend to have more interesting, distinctive features. Male peacocks have the bright, showy feathers while females are smaller and have plain brown feathers. This might also explain why the difference is higher in birds (40% female) Vs mammals (48%).",37,viay1a,,datasets,2022-06-22 10:45:44,10
"https://stackoverflow.com/questions/4600656/access-googles-traffic-data-through-a-web-service

The next best source would be the relevant state government's department of transportation website. As far as I'm aware, most states don't log time data, so it'd probably be incomplete for your use.",2,vibtbt,I'm looking for hour by hour traffic count data and thought some of you might be able to point me in the right direction. My use case: I'm opening a number of breakfast restaurants and need to find out where the heavy morning traffic is and what side of the road its on so I can pick the right locations. I have basic GIS skills and could dump it into QGIS if its raw. I was wondering if there's any way to pull it from Google Maps?,datasets,2022-06-22 11:24:17,11
Check FRED,1,vi4kil,"I can't seem to find a complete and downloadable dataset for S&P 500 closing prices? Does anyone have one, or a source I might be able to use?",datasets,2022-06-22 05:52:37,1
Can you be more specific about where on Baseball Savant you're looking?  It's a big site.  Maybe drop a link to wherever you're selecting fields?,3,vhs7m7,"Hello everyone! I'm a complete novice trying to learn R. I love baseball so I am digging through the baseball savant website and generating some cool stats.

The problem is that when I select the fields I want shown (i.e homeruns, 1B, etc.) and try to download the data via the ""download results CSV file"" the file that gets downloaded always has the same default fields in it.

How can I get Baseball Savant to include the selected fields in the data frame?

&#x200B;

Thanks in advance :)",datasets,2022-06-21 17:43:52,2
"Hey cebasss25,

I believe a `question` or `discussion` flair might be more appropriate for such post. Please re-consider and change the post flair if needed.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/datasets) if you have any questions or concerns.*",1,vhkfek,Hey everyone thanks in advance! I have been looking for the last couple days and I've had no luck finding a comprehensive dataset. If anyone has any ideas where I could find this it would be greatly appreciated!,datasets,2022-06-21 11:46:45,4
"Hey WhatsTheAnswerDude,

I believe a `question` or `discussion` flair might be more appropriate for such post. Please re-consider and change the post flair if needed.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/datasets) if you have any questions or concerns.*",1,vgzb3t,"Hi my fellow data peeps,

I could use your possible help? I'm tasked with trying to find data on US consumer trends concerning booking flights for the month of may (and possibly June), and what the trends through the month were.

Does anyone know where I could maybe find data concerning how many flights are getting booked in the US (and whether its up or down week by week, etc)?

Would really help me!!

Thanks everybody!",datasets,2022-06-20 16:45:10,3
how to deploy,1,vgfx7q,,datasets,2022-06-20 00:40:33,1
I can only tell you that most of us are in the rat race,5,vg8cmg,"How would I go about finding race datasets of employees for both public and private companies?

I took a quick look at BLS, US Census, and the Equal Employment Opportunity Commission but couldn't find detailed information to a specific company. If I am not searching properly, and it is possible please let me know

My goal is to make a comparison between the racial makeup of a city vs. the employee makeup of the companies in the city. I have the first part, the data for the bigger geography (American Community Survey) however I'm struggling to find the latter.   


I really appreciate your help.",datasets,2022-06-19 17:10:08,6
"I don't think anyone makes such data publicly available. Generally it's considered confidential data, and even government reporting doesn't require more granularity.

But you can download the Tableau superstore dataset for free from Tableau website.",1,vgjk7n,"Hello. I am learning about data analysis. As part of my practice, I am looking into creating store specific visualizations for the amount and types of losses that retailers incur.

&#x200B;

I am unable to figure out the perfect search term for this and hence I couldn't find anything on kaggle, bigquery datasets or opendata websites. Any suggestions?",datasets,2022-06-20 04:44:46,2
I think here https://covid.cdc.gov/covid-data-tracker/#wastewater-surveillance,1,vf5zwq,I've been looking around for inaugurate datasets but it seems like ‏‏‎ not very many states either have a program up and running or don't have a public repo. I was wondering if you resourceful lot might be able to sleuth better than me. Here's a couple of examples of what I'm talking about: Colorado: [https://opendata.arcgis.com/datasets/daeb2297e10c4b9f92437d06dfb72b81\_0.csv](https://opendata.arcgis.com/datasets/daeb2297e10c4b9f92437d06dfb72b81_0.csv) Ohio: [https://coronavirus.ohio.gov/static/wastewater/ODH\_Dashboard.csv](https://coronavirus.ohio.gov/static/wastewater/ODH_Dashboard.csv),datasets,2022-06-18 06:13:55,1
[FRED](https://fred.stlouisfed.org/) has some datasets that might be of interest. One I always find interesting is the [smoothed probability of US recession](https://fred.stlouisfed.org/graph/?g=QJLd).,1,vf13n0,"What are some good datasets that have effect on the financial things but not necessarily the stockmarket?

examples: 

gas prices in correlation to COVID

Inflation in correlation to COVID outbreaks

Commodity prices in correlation to maritime data",datasets,2022-06-18 00:40:53,2
“Find me a dataset so i can make a product and sell it”,4,vf4o4d,Looking for a data set that contains labelled transphobic comments to be able to be identified them. Any help would be brilliant and I want to make a system that can help identify such comments and find ways to reduce the revenant online.,datasets,2022-06-18 04:54:55,6
"Kearney are a consultancy and so will likely have used some of their own research to compile this index (eg running consumer surveys), and will likely not publish their methodology or data.

Developing countries are normally much harder to find for this kind of data as the primary research is more expensive, and the resale value of the data is lower.

However, places that have this kind of data:

**Cheap**

* Statista: [https://www.statista.com/statistics/1074217/national-retail-sales-of-selected-countries/](https://www.statista.com/statistics/1074217/national-retail-sales-of-selected-countries/)
* \[this will not cover your use case, but Eurostat publish retail index data for all EU countries: [https://ec.europa.eu/eurostat/databrowser/view/teiis210/default/table?lang=en](https://ec.europa.eu/eurostat/databrowser/view/teiis210/default/table?lang=en) \]

**Expensive**

* Euromonitor International: [https://www.euromonitor.com/retailing](https://www.euromonitor.com/retailing)
* Nielsen
* Kantar",1,veojgs,"Hi all - I am looking for a dataset that contains total retail sales (in US dollar terms) for the top 50 most populated countries. I would like that dataset to include developed as well as developing countries (like Bangladesh, for instance).

I am trying to determine how Kearney got the data visible in the fifth column (titled National retail sales US$ billion) in this link: [https://www.kearney.com/global-retail-development-index](https://www.kearney.com/global-retail-development-index)

I searched through datasets on the World Bank's website but could not find one that had total retail sales by country.",datasets,2022-06-17 12:53:12,1
I know a guy who does some metrics in an analytics function for a power company. Although i’ve tried sharing analytics tools with him in the past and it went no where :/ i may try this but i know it’s a little over his head,3,vdwlh2,"I found an experiment to find out how ML can be useful in the energy sector. In my area, voltage surges are a common thing (and annoying), so I found interesting a model to predict if the electrical grid is stable or not. Although author wasn’t able to check the model performance in real conditions for lack of special equipment, it worked well on the test dataset. 

I think if this project is scaled up, it can help to troubleshoot the electrical network in a timely manner and avoid serious breakdowns.   
Full experiment:  
[https://www.hackster.io/alexmiller11/detecting-unstable-electrical-grid-with-tinyml-927963](https://www.hackster.io/alexmiller11/detecting-unstable-electrical-grid-with-tinyml-927963)",datasets,2022-06-16 14:34:00,3
"PPP data set. Easily available for free, contains every company that took a PPP loan which is a whole fuckload and NAICS is a data point. Thank me later.",9,vdzln4,"Hi...

  
I am seeking a data set of companies and their associated NAICS codes.  I have found that everyone is selling contact info sets which I do not want.  Does such an set exist?",datasets,2022-06-16 17:00:28,11
"Hi u/uhyped09, IEEE DataPort has a category devoted to [Covid-19 datasets](https://ieee-dataport.org/topic-tags/covid-19)! There are 33 Open Access datasets available in that collection.",1,vdhq21,"Carried on from [Third Discussion Thread](https://www.reddit.com/r/datasets/comments/n3ph2d/coronavirus_datsets/)(Archived)
> Carried on from [Second Discussion Thread](https://www.reddit.com/r/datasets/comments/i5krmb/coronavirus_datasets/)(Archived)
> > Carried on from [Original Thread](https://www.reddit.com/r/datasets/comments/exnzrd/coronavirus_datasets/)(Archived)
> > 
> > > You have probably seen most of these, but I thought I'd share anyway:
> > 
> > > **Spreadsheets and Datasets:**
> > > 
> > > * [https://www.worldometers.info/coronavirus/](https://www.worldometers.info/coronavirus/)
> > > * [John Hopkins University Github](https://github.com/CSSEGISandData/2019-nCoV) confirmed case numbers.
> > > * [Google Sheets From DXY.cn](https://docs.google.com/spreadsheets/d/1jS24DjSPVWa4iuxuD4OAXrE3QeI8c9BC1hSlqr-NMiU/edit#gid=1187587451) (Contains some patient information \[age,gender,etc\] )
> > > * [Kaggle Dataset](https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset)
> > > * [Strain Data](https://github.com/nextstrain/ncov) repo
> > > * [https://covid2019.app/](https://covid2019.app/)  (Google Sheets, thanks /u/supertyler)
> > > * [ECDC](https://www.ecdc.europa.eu/en/publications-data/download-todays-data-geographic-distribution-covid-19-cases-worldwide) (Daily Spreadsheets, Thanks /u/n3ongrau)
> > > 
> > > **Other Good sources:**
> > > 
> > > * [BNO](https://bnonews.com/index.php/2020/02/the-latest-coronavirus-cases/) Seems to have latest number w/ sources. (scrape)
> > > * [What we can find out on a Bioinformatics Level](https://innophore.com/2019-ncov/)
> > > * [DXY.cn Chinese online community for Medical Professionals](https://ncov.dxy.cn/ncovh5/view/pneumonia) \*translate page.
> > > * [John Hopkins University Live Map](https://gisanddata.maps.arcgis.com/apps/opsdashboard/index.html#/bda7594740fd40299423467b48e9ecf6)
> > > * [Mutations](https://nextstrain.org/ncov) (thanks /u/Mynewestaccount34578)
> > > * [Protein Data Bank File](https://3dprint.nih.gov/discover/3DPX-012867)
> > > * [Early Transmission Dynamics](https://www.nejm.org/doi/full/10.1056/NEJMoa2001316) Provides statistics on the early cases, median age, gender etc.
> > > 
> > > **\[IMPORTANT UPDATE:** *From February 12th the definition of confirmed cases has changed in Hubei, and now includes those who have been clinically diagnosed. Previously China's confirmed cases only included those tested for* [*SARS-CoV-2*](https://en.wikipedia.org/wiki/2019_novel_coronavirus)*. Many datasets will show a spike on that date*.\]
> > > 
> > > **There have been a bunch of great comments with links to further resources below!**  
> > > \[Last Edit: 15/03/2020\]
> 
> > - [COVID-19 Mobility Data Aggregator](https://github.com/ActiveConclusion/COVID19_mobility)  [^[source ^comment]](https://www.reddit.com/r/datasets/comments/i5krmb/coronavirus_datasets/g0rwpih/)
> > - [County level mask mandate data set(US)](https://docs.google.com/spreadsheets/d/1CYmIiNDeBrUcVyazo21jCXyE4eemE1hmc71jCOnoGW8/edit#gid=796552815) [^[source ^comment]](https://www.reddit.com/r/datasets/comments/i5krmb/coronavirus_datasets/g0pz1qw/)  
> > - [NYT county level cases and mask usage](https://github.com/nytimes/covid-19-data) [^[source ^comment]](https://www.reddit.com/r/datasets/comments/i5krmb/coronavirus_datasets/gdk385v/)

> - [Data on COVID-19 (coronavirus) by Our World in Data](https://github.com/owid/covid-19-data/blob/master/public/data/README.md)
> - [Vaccine allocations by state, provided by the CDC](https://www.reddit.com/r/datasets/comments/n3ph2d/coronavirus_datsets/h7w0qrr/)
> - [FOIA request with the CDC to get access to vaccine wastage reports (doses that went unadministered)](https://www.reddit.com/r/datasets/comments/n3ph2d/coronavirus_datsets/hbuhfv3/)
> - [COVID-19 Variants and Prevalence, Excess Mortality during the COVID-19 pandemic, Government Response Tracker, Pulmonary Abnormalities, Top cities and trending searches](https://www.kaggle.com/ruchi798/datasets?tags=16575-Coronavirus)

- [Covid-19 category of IEEE DataPort](https://ieee-dataport.org/topic-tags/covid-19)

- Please check the comments of the previous threads for more datasets.



Original thead by /u/Mars-Is-A-Tank",datasets,2022-06-16 02:10:54,3
"Hey podviaznikov,

I believe a `request` flair might be more appropriate for such post. Please re-consider and change the post flair if needed.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/datasets) if you have any questions or concerns.*",1,vdswld,Does anyone know joe can I find data from all public companies and the percentage of their revenue coming from other countries. Eg if saw Duolingo recently said that they had 1% of total revenue coming from Russia. I wonder if there is dataset with such data across all the companies or industries.,datasets,2022-06-16 11:41:36,3
"I'm the CEO of DoltHub. We run data bounties constantly to build open datasets. We just started building a museum collection database.

[https://www.dolthub.com/bounties](https://www.dolthub.com/bounties)

We'd love to have you in our community. We all hang out on Discord and it's very collaborative even though there is money at stake.",6,vd42vy,I want to help contribute data to real-world used datasets like OpenStreetMaps (OSM). Are there any of these sorts of datasets/projects I can contribute passively to in my free time?,datasets,2022-06-15 13:38:46,5
Sounds like you could fairly easily scrape one? Or maybe I am underthinking it,1,vdu1vu,"I want a dataset with at least 1M Instagram posts that should be either the entire population of IG posts or a sufficiently random sample.

Where can I find such a dataset?",datasets,2022-06-16 12:34:20,7
Hi! Try catalog.data.gov,1,vcye8z,"Hi everyone!

I have been looking for data on rural school enrollment by state and race/ethnicity for over a month now (e.g. What percentage of Asian students in California attend rural schools?). It seems like the National Center for Education Statistics would have that data, but I can't seem to find it anywhere. Any help on this would be greatly appreciated!

Thanks!",datasets,2022-06-15 09:24:37,1
"You could try scraping wikipedia from something like [https://en.wikipedia.org/wiki/Lists\_of\_universities\_and\_colleges](https://en.wikipedia.org/wiki/Lists_of_universities_and_colleges) or [https://en.wikipedia.org/wiki/List\_of\_aerospace\_engineering\_schools](https://en.wikipedia.org/wiki/List_of_aerospace_engineering_schools) as a starting point (or using their API)

&#x200B;

But creating a language model to find a university might be over-engineering this decision!",1,vczh2w,"Even if this dataset simply contains a long description of the university, that would suffice since I could use langauge models to identify these features.

Basically, I'm searching for a university that fits all the features I want but I'm finding a simple google search very difficult because I want an enormous amount of features:

1. Open curriculum
2. Transdisciplinary, even more preferably antidisciplinary
3. Employs just-in-time learning
4. Employs goal-driven learning based on the students needs, organizes curriculums, teaching and so on based on what the students stated goals are and only teaches concepts relevant to students goals and how those concepts can help them achieve those goals.
5. Preferably has some fully-funded scholarship/stipend, etc.
6. Has some understanding or implementation of gradients of change (a la differential reinforcement, programmed instruction, etc.)
7. Is taught in English
8. Is open to international students

And so on, there are about a hundred other features.",datasets,2022-06-15 10:12:53,4
https://en.m.wikipedia.org/wiki/Metropolitan_statistical_area,1,vd1x0j,"I've been hunting for a simple excel or .csv file I can download with a list of all US metropolitan areas and the cities that fall under them. For example, the ""nashville-davidson-murfreesboro-franklin-tn-metro-area"" includes the obvious as well as Antioch and satellite cities such as Spring Hill. 

It would be a huge help if someone can point me in the right direction. Seeing as there are less than 50,000 cities in the US, I'm hoping I don't have to connect to a database (I'm not a developer, but willing to try if needed).",datasets,2022-06-15 11:59:38,5
Nice! I've seen a number of startups at Northeastern University using AI to predict mechanical failures in the field and they've done pretty well.,5,vcees6,"Hi everyone!  
I come from the Jharkhand state of India, and issues with access to processed potable water is a common thing in my region. People have to rely on underground water, and compression water pumps are the only option in such cases. Like any other machines, water pumps should undergo maintenance and repairs due to wear and tear, but ordinary men don't have the skills, time, and know-how to do that. As such, if heavy wear and tear occurs, people have to wait for almost a week for the pump to be repaired and use as little water as possible.  
I thought about how to address this issue using machine learning and built a fast scalable solution for compressor water pump predictive maintenance. It will help to avoid any severe issues and extend the life of compressor pumps by taking preventive measures. Hope you’ll find the case useful, provide full version via the link : [https://www.hackster.io/vilaksh01/predictive-maintenance-of-compressor-water-pumps-a47cd5](https://www.hackster.io/vilaksh01/predictive-maintenance-of-compressor-water-pumps-a47cd5)",datasets,2022-06-14 14:43:48,2
"The Census Bureau is one of the best for a lot of things IMO. It’s lacking on some opinion and political data, but when describing the population and businesses it’s great. It does a lot more than just counting everyone.",6,vc4z1m,"I am interested in the market and what makes leading companies succeed.

* Which commercial (or free) providers do you know/use/recommend that provide public datasets like product data from e-commerce websites or pricing from travel websites or social media data, etc? 
* What is good about them?
* What is lacking/missing bad?",datasets,2022-06-14 07:40:04,17
Hey u/InfiniteLeverage! Did you manage to get this dataset?,1,vbtdbu," 

Hi, I'm trying to install Scale AI's Ukraine dataset they just release to the public. You can find details here: [https://scale.com/blog/ukraine-detection](https://scale.com/blog/ukraine-detection)

The download link in the page is: [https://drive.google.com/uc?id=17lpMFngiqeegIm1U0RzUYfB-9GjmI7fk&export=download](https://drive.google.com/uc?id=17lpMFngiqeegIm1U0RzUYfB-9GjmI7fk&export=download)

But it's giving me a 403 error.

Does anyone have the dataset?",datasets,2022-06-13 19:48:23,1
"We don't have Edmonton, but we do have data for major cities in Southern Ontario, BC, and Montreal.  You can download high-res DSMs/DEMs in this app: maps.equatorstudios.com",1,vbtkvk,"Hi there,

I was wondering is there any source where I can get high resolution, accurate digital surface models on Canadian cities (especially Edmonton). And also, (I'm very new on this), how do I get the height info from them, ideally, I'd like to open them in MATLAB as an 2D array, any help is appreciated.

Thank you!",datasets,2022-06-13 20:00:01,1
"Do you mean crude oil? I'm asking because these prices are global. There are prices like the Canadian Western, WTI, etc. There are only a handful. Gas prices can be at the granularity that you are looking for though.",3,vbf06u,I am looking for historical data on oil prices at the county or city level. Yearly would work but something like monthly or even daily would be perfect.,datasets,2022-06-13 08:38:35,4
Just use the API in my opinion...,2,vbgtut,"Hello,

I’d appreciate it if anyone has any leads on recent (past two years) Twitter network datasets (includes following/followers data to be able to make a graph) related to any topic. 

I did a search but the published datasets I found are old (ex: 2010 from the Max Plank institute) so to hydrate those accounts/retrieve them is going to have significant loss or reduction in dataset size if any accounts were closed or deleted.

Would appreciate any help. 
TIA!",datasets,2022-06-13 09:59:54,8
"I've found Unacast quite compelling, for what I do is pretty interesting.",1,vbso4q,"There are lots of datasets out on the Snowflake Data Marketplace now, What cool things have people done with them? What are the best datasets to use?

  
[https://www.snowflake.com/data-marketplace/#datasets](https://www.snowflake.com/data-marketplace/#datasets)",datasets,2022-06-13 19:11:34,1
"Great thread here in how these records were rescued 
https://twitter.com/ed_hawkins/status/1536263962915921920",1,vb9tk6,,datasets,2022-06-13 04:18:53,1
"Might ask Corelogic if they could generate that dataset for you.  Not free, but they have a nationwide  parcel dataset.

Look for a national association of golf courses,  greens, golf pros, etc.  Not sure if the PGA would include such courses as unlikelyto host a tournament, but also worth trying.

Similarly,  a national organization of HOAs might collect such information although if there is one or more the information you seek may be in text or comments .

If it is an academic study you might look at Zillow APIs.  Alternatively,  they might be able to do this for a fee, but their desire to take on such projects changes from time to time.",2,vbn34u,"Looking to scrape all addresses for communities like The Yellowstone Club in Big Sky Montana, Tributary Club Idaho, Promontory Club Utah, etc...

So far this has seemed nearly impossible, any ideas?",datasets,2022-06-13 14:34:02,1
Thank you!,2,vafirb,,datasets,2022-06-11 22:57:21,3
"If I understood this correctly, you just want a small dataset that basically has countries in one column and another column populated with 1s (an election in that period) and 0s (no election in period). Correct?

If so, use the ParlGov database and code based on the elections data contained within.",2,vasmyi,Btw I looked at OECD and World Bank and I found nothing,datasets,2022-06-12 11:53:59,6
"GitHub would be a great source for this.

You can get tons from this search 

-	https://github.com/search?l=&p=1&q=language%3AMarkdown&type=Code

Or you can leverage the API, 

-	https://docs.github.com/en/rest/search

Based on the other search.",3,vanut5,"Hello, I am looking for a markdown dataset that has lot's of test cases but also very big files with complex structures, such as links (can be dummy links), lists, headers, etc. Does anyone have such a dataset for me to test? The final goal will be to create a Markdown Parser.

Thanks all!",datasets,2022-06-12 08:04:38,3
"There is a link to the code and data at the bottom but I thought the article was interesting enough to post by itself

The data is at https://zenodo.org/record/5005868#.YqR-vBPMK1s",7,v9vreq,,datasets,2022-06-11 04:38:42,1
"Scraping.  Why?  You control the collection and processing.

Free data sets are often - not always, but often enough - light on details about collection methodology.  How were reviews selected?  Was the data cleaned or processed in any way before packaging it and offering it for free?  Maybe reviews with profanity were removed, or something similar that might bias the reviews.  Without collection methodology, I have to rate the credibility of the data as ""sketchy as fuck"" in any write-up.

If you're looking for a recommendation for a scraper, I've had a lot of luck with Selenium + R's rvest package.  The RSelenium package vignette steps you through how to set up everything you need; the setup is a little bit of a pain if you don't know docker, but the details are all there.  You can drive Selenium through Python as well.",2,va29yv,"Hi!
What are the best tools to collect publically available reviews as datasets from multiple websites, like G2, Trustpilot, etc? Are there some free datasets, or they should be collected only via scraping tools?",datasets,2022-06-11 10:26:20,3
"Not clear at all what you're looking for, you'd need to specify",7,va249o,"but I'm currently in the middle of scrapping an API for data that I will eventually build a dashboard with.

The finished product continues to evolve but that's the over arching goal.

I'm grabbing state related stats from random sites (while avoiding paying for CSV files) and I couldnt help but wonder if there's general statistics already somewhere ready to pull and use in a dataset - maybe from a git repo? I wasn't able to find anything after a few searches and thought it just didn't exist . . . . yet.

Seems easy enough to do and maintain but thought I'd ask nonetheless.

Thanks, all.",datasets,2022-06-11 10:18:14,5
"
Here is an example


https://www.dropbox.com/s/gulu0z71zf0jz7e/3168.json.gz?dl=1",1,va481w,"I've downloaded this, but the json files loaded into R seem very messy,  just samples a couple of them.  Has anyone worked with these, preferably in R, but python will do, too, in order to get some easy to use dataframes?


https://www.crossref.org/blog/2022-public-data-file-of-more-than-134-million-metadata-records-now-available/",datasets,2022-06-11 12:04:09,3
"Databrokers are your best option I guess. Check out these for example: https://adressen.schober.de/de

They have pharmacy addresses (filter by Branche -> Ärzte, Gesundheitswesen und Sozialwesen -> Gesundheitswesen -> Gesundheitswesen, sonstiges -> Apotheken)

I remember seeing some data from them in my former job, but have never interacted directly. They seem to give you a free snapshot of the data once you get a login though.

Otherwise you could always go the route through Google Places API.",2,v9abzd,I am looking for a current dataset of pharmacies in germany. Can be paid/subcription based. Everything but html crawl appreciated. Format is not important.,datasets,2022-06-10 08:30:17,11
Looks like something I could be helpful with. Which parameters do you need exactly ?,1,v9cy93,"Hi guys, hope everyone is well, been looking for a dataset which describes US businesses owner, to know how many of them are owned by latinos, americans and other nationalities, it can be detailed or not, is to figure out our market cap.",datasets,2022-06-10 10:29:39,2
"The police stops dataset from Stanford Open Policing Project.  You can use EDA to examine different jurisdictions and the raw numbers, then you can dig into the dataset and start to use advanced techniques to examine racial profiling and biased police behavior.  There are over 100M stops recorded.

https://openpolicing.stanford.edu/data/",16,v8em02,"Hello! I'm looking for ideas about interesting datasets/topics to perform EDA on. I would like to avoid classic datasets like housing, stock market, sports related etc and find something a bit more unique. I would also like to avoid medical datasets as I have zero knowledge on the topic. 

I would like to find a dataset on which EDA can provide valuable information using graphs.

More specifically, ideally I'm looking for a dataset with these characteristics:

* Interesting, intriguing, unique topic
* More than 10-15 features
* Mix of feature types but mainly numeric or ordinal
* Minimum a couple of hundred instances
* Datasets that can be used in Machine Learning/Deep Learning

I'm eager to hear your suggestions. I would also love to hear what's the most interesting/unique dataset you've worked with even if it's not publically availliable or doesn't fit into my list of characteristics.",datasets,2022-06-09 04:22:53,11
Ahh! I think you could programmatically pull them down.,1,v8odq9,I want to create an AI that can generate a story based upon a writing prompt. To achieve this I want to take writing prompts from r/WritingPrompts and the top stories on those writing prompts and make a dataset out of it. But I have never made a dataset and have no idea how to achieve this. Can someone tell me how to do it?,datasets,2022-06-09 12:13:07,2
My first thought was distributions with a positive or negative skew. Let me know what else you come up with!,1,v8o7vv,"Hi Guys ,

What distributions can be applied to create a dataset based on repetition of data .
Example 

> for uniform frequency of distribution , i have 100 rows of data with same value.
> For Normal distribution , i can have a value 'a' for .1 % of data , 'b' for 2.1% of data , 'c' for 13.6 % of data , 'd' for 34.1% of data , 'e' for 34.1% of data ,etc

Are there any other distributions that i can use and please feel free to share any resources as well.

Regards",datasets,2022-06-09 12:05:46,4
"This what you are looking for?

[https://www.dolthub.com/repositories/dolthub/us-president-precinct-results](https://www.dolthub.com/repositories/dolthub/us-president-precinct-results)

I don't think it has all 50 states but at least 40.",1,v8ra7r,I am requesting L2 voting data from all 50 states and the District of Columbia with no fees required.,datasets,2022-06-09 14:22:22,3
"Have you tried WikiData? I think some of what you are after is there. Also ESRI had all this data on their AGOL platform. They call it livingAtlas. I’m not sure about costs.

Edit: grammar",1,v8gzui,"I'm putting together a database collating information about as many cities around the globe as possible including mean temperature, population density, precipitation levels etc - where is the best place to find such data, or even better, some datasets?  I can find the data on a country-level but struggling to find urban environment data. Tried UN, FAO, WHO, kaggle, github etc",datasets,2022-06-09 06:35:04,1
"Why do you need actual portfolios?

Why not build random portfolios or use some vanguard sample portfolios?",1,v889kz,"Hello, I am trying to do a personal project related to stocks and I am looking for a data set of full (ideally real, but paper trading also fine) portfolios. They can be institutional portfolios or personal, both are fine to me. To be clear, by portfolio I simply mean a list of all the stocks that each (anonymized) person/entity owns.

Does something like this exist anywhere? My first instinct was yahoo finance, but google doesn't turn up any results. Is there a good place I could scrape this from myself?",datasets,2022-06-08 21:10:40,7
Why not provide the data without requiring personal information and subscribing to your newsletter?,6,v7lr9t,,datasets,2022-06-08 02:49:29,3
"Hi, why not use Python? If you strip all the white spaces and create a para break every time you encounter an all caps word, you will get a sectioned text. Cleaning the text of symbols may also be straightforward looking at your examples",2,v7geem,"I'm not sure if this is the right reddit for this but I've been working on a project for a few years on shifting baselines (how things change over time and how that affects our generational perception of said thing).  This requires me taking old field guides and other such documents and turning them into a dataset that I can then run through in the R programming language.  


Problem is, in many of these documents, they are old enough to not be written in any sort of data friendly way or in any way that allows for easy comparison against other documents, old or otherwise. In some cases the entries are not even easily comparable against other entries in the same document.  This means the excel ""Text to Columns"" and other such programs have been basically useless to me, as the words either belong in multiple columns, specific columns with only sentence context, or require specific manipulation of words to work.   


Now I don't have any problem converting these documents to text files. But splitting them up into the useful data is painstaking.  On a good day I can get one full entry done in a half an hour or so. But with hundreds to thousands of entries, and with me doing mostly by myself (I occasionally have help), it will take me too long to get much done.  


My question is how would you guys overcome these problems?  Is there a program or something I can use?  I'll post some examples of the text I'm working with in the comments below.  Thank you in advance.",datasets,2022-06-07 21:03:39,9
"I know pornbub publishes some statistics, I don’t know if it’s by state. You could try contacting them.",13,v716gy,This is for an economics paper and I cannot for the life of me find data to use. Any help would be appreciated!,datasets,2022-06-07 09:52:51,5
[I'd load the boundaries into R or QGIS and find the centroids myself](https://datahub.io/core/geo-countries),1,v7j5fz,"I'm doing an analysis, comparing a bunch of different factors to a country's average lifespan. I sunk a couple of hours into trying to find a dataset that gave the average latitude of a country's people but the closest thing I could find was a visual map ([https://www.reddit.com/r/dataisbeautiful/comments/dfg6xa/centre\_of\_population\_for\_each\_country\_in\_the/?sort=old](https://www.reddit.com/r/dataisbeautiful/comments/dfg6xa/centre_of_population_for_each_country_in_the/?sort=old)) that had the exact information I wanted but did not provide the data in a usable form.",datasets,2022-06-07 23:52:08,1
"I will get down-voted for this but I cannot resist...

So, you are looking for German Nuts?",1,v715cl,"Hello everyone,

&#x200B;

I want to find some German Nuts 3 data to do some regressions. So far I have found Nuts 3 data from  [eurostat](https://ec.europa.eu/eurostat/web/rural-development/data). That covers things like transport and GDP. However, I would want some data on for example human capital, human resources or science and technology or the some data on industrial structure. Like the  ratio of the value added services to the value added of the industrial sector in a region. 

In essence I want to find data that may affect regional innovation to do some regressions.

&#x200B;

I would be very happy if someone has any idea where I could get some more specific Nuts 3 data for Germany.",datasets,2022-06-07 09:51:47,7
StackExchange,1,v6ox9e,"I am working on a research project, and I need text/image data on a particular topic. I need to do text analysis and maybe build a text model on top of that. Where can I get data on specific topics? Some of the websites I can recall are Reddit, quora. Does anyone know other sources? Please share.",datasets,2022-06-07 00:00:53,3
"Did you miss my dataset post few days ago haha ;) 

[Kaggle Post](https://www.kaggle.com/discussions/general/327550) 

[Kaggle Dataset ](https://www.kaggle.com/datasets/deepcontractor/monkeypox-dataset-daily-updated)


[Reddit Post](https://www.reddit.com/r/datasets/comments/uytenv/monkeypox_dataset_daily_updated_kaggle/?utm_medium=android_app&utm_source=share)",0,v6qqja,"Hello there,  


With more than 1.200 confirmed cases as of today, Monkeypox is an ongoing outbreak and possible trouble in the making.  
Because of this I published a dataset called Global Monkeypox Cases [here](https://www.kaggle.com/datasets/andrewmvd/global-monkeypox-cases).  
It is daily updated and includes location, symptoms, means of confirmation and more.

&#x200B;

All credits are due to [Global.health](https://Global.health) , which they requested to be cited as:

> Global.health Monkeypox (accessed on YYYY-MM-DD) 

  
For reliable information about this disease, please refer to [this article by WHO](https://www.who.int/emergencies/disease-outbreak-news/item/2022-DON385).  


Stay safe",datasets,2022-06-07 02:05:16,1
Just download turbotax and fill it out for each state of interest. It doesn't charge until you go to actually submit your tax forms.,6,v6czef,"Hello all!

I'm looking for a dataset that contains all taxation rules for all US states.  I'm looking for income, property, vehicle, sales, payroll, etc.

I'm moving residence and have the option of several states to pick from.  So, I want to create a model that should take my income and calculate what my tax expenditure should look like in every state so that I have a good approximation of what my paycheck will like in each state.  So, I'd really appreciate it if someone could point me where I can find all of these pieces of information.",datasets,2022-06-06 13:36:01,5
"You can pull data directly from the NHTSA:

\- [https://www-odi.nhtsa.dot.gov/downloads/](https://www-odi.nhtsa.dot.gov/downloads/)

This includes complaints on a component level basis (engine, power train, etc.). You can also access their API 

\- [https://www.nhtsa.gov/nhtsa-datasets-and-apis](https://www.nhtsa.gov/nhtsa-datasets-and-apis)

which produces a JSON file. Here's a sample:

\- [https://api.nhtsa.gov/complaints/complaintsByVehicle?make=acura&model=rdx&modelYear=2012](https://api.nhtsa.gov/complaints/complaintsByVehicle?make=acura&model=rdx&modelYear=2012)",3,v57krr,"I don't know if this exists but I wonder if there is any dataset which has aggregated statistics for various car models. Things like number of transmission, engine, etc. issues. Or something like average mileage of all cars currently on the road for a given model.",datasets,2022-06-04 23:33:17,1
"Ask in r/GIS.  This is a common problem and there are numerous datasets usually available as orthophotos.

Try Houston-Galveston Council of Governments as an example of data over time.  Obviously this an example for the US.  May require knowledge of some AI tools to classify landuse types in an urban setting",2,v5hjz9,"Hello, 

I'm looking for a dataset which lists all cities with their built-up area, meaning the land within city limits excluding large uninhabited areas (like forests). The closes thing I found is the following dataset from the European Commission: [https://ghsl.jrc.ec.europa.eu/ghs\_stat\_ucdb2015mt\_r2019a.php](https://ghsl.jrc.ec.europa.eu/ghs_stat_ucdb2015mt_r2019a.php) 

The problem is that it lists urban centres independent of administrative boundaries. 

I would appreciate your help! 

Thanks,",datasets,2022-06-05 09:59:11,4
"From the paper https://arxiv.org/abs/2204.05836v1

I've no connection to this I just think it looks cool.


There's a big reading of Ulysses at the moment and it's not in the booklist. I'm tempted to try add it but it might make the data very weird https://mobile.twitter.com/ulyssesin80",2,v5a3zn,,datasets,2022-06-05 02:52:47,1
"Cool dataset. One thing I see is that the job descriptions are pretty messy. 


These are most of the ""Android"" type jobs and there's a lot of overlap and misspelled records.
https://i.imgur.com/6w44rAr.png",2,v4r9qy,"Hi, I have created this new Dataset on Kaggle of Software Professional Salaries - 2022 (This Dataset contains information on Salaries for 22700+ various Job Profiles). You can check it here: [https://www.kaggle.com/datasets/iamsouravbanerjee/software-professional-salaries-2022](https://www.kaggle.com/datasets/iamsouravbanerjee/software-professional-salaries-2022)",datasets,2022-06-04 08:31:26,4
Great initiative. I agree that Turks need more transparency. Thanks!,0,v43tzk,,datasets,2022-06-03 09:59:08,1
2nd,2,v4a0o1,Dataset for the number of health care clinics or health practitioners adjusted for population worldwide? Or for as many countries as possible,datasets,2022-06-03 14:52:55,4
"Not sure what data you want, but I’d start here,

https://www.google.com/search?hl=en&q=site%3Alinktr.ee

Then scrape whatever you want. 

Maybe exclude what sitemap.xml has too. I",2,v3k1zk,Sry if this is not the right sub for this question.,datasets,2022-06-02 14:59:08,4
Try something like https://www.adsbexchange.com/,2,v3b9mu,"Is there such a dataset/source where I can see flights ending in a city, on a city by city basis? Even better if the data is available historically e.g.  July - London - 50k, July - Istanbul - 40k etc. Even better if it's day by day.

I'm a newbie when it comes to flight/travel data, so would appreciate being pointed in the right direction. Even if there is a similar but incomplete dataset that may require more sifting.",datasets,2022-06-02 08:20:30,3
"I'm not sure if that exists, but if it does it will be listed here 
https://www.transtats.bts.gov/DataIndex.asp",1,v3b97q,"Is there such a dataset/source where I can see flights ending in a city, on a city by city basis? Even better if the data is available historically e.g.  July - London - 50k, July - Istanbul - 40k etc. Even better if it's day by day.

I'm a newbie when it comes to flight/travel data, so would appreciate being pointed in the right direction. Even if there is a similar but incomplete dataset that may require more sifting.",datasets,2022-06-02 08:19:59,1
"Uchikoshi said his DS VNs ( Zero Escape Series ) were way more popular in American than in Japan, which probably means you can get accurate data for titles released on consoles ( Most VNs have ports for every platform due to the fact even Pong requires more CPU power than them ). 

Sure, Steam probably sells more VNs than any console, but you'll already have a nice amount of data there. 

Also, and this is probably a dumb idea, but maybe Ren'Py's creators have data on where their engine ( The most popular VN engine by a ridiculous margin ) is the most popular ? You'd have to ask them, though. 

EDIT: For knowledge of how used Steam is for every country perhaps Google trend or public DNS info/stats ?",1,v33pbu,"I've tried Steam, Steamdb, and even paid Steamspy, but I can't find per-country data on how popular western visual novel games are, or even just an updated demographic of Steam users per country. Please help.",datasets,2022-06-02 00:58:17,1
"So would a dataset aggregated by 30 arcsecond Square (approx. 1 km Square) work for you? Some datasets exist with this data stored as a tiff file. I condt remember where but this ought to give you the search terms.

DO NOT GET THE DATA FROM NASA THOUGH AS YOU NEED TO GIVE THEM THE RIGHT TO SEIZE YOUR HARDWARE",2,v2l5r9,"Does anyone know where I can find population density data, organised by latitude and longitude, for each year (going back as far as possible)?

The sedan.ciesin.columbia.edu site has data for every 5 years going back to the year 2000, however I need to go further back to at least the 1960s.

The ghsl.jrc.ec.europa.eu site has data for 1975, 1990, 2000 and 2014 so that goes back further but only every 10-15 years.

I can't find a dataset that goes back far enough in time and has data every few years or so. Does such a dataset exist?",datasets,2022-06-01 09:37:19,4
Any1 wants to upload that as a torrent ? No reason to hammer their servers,8,v2bx0w,"```python
from parsel import Selector
from playwright.sync_api import sync_playwright
import json


def scrape_researchgate_publications(query: str):
    with sync_playwright() as p:

        browser = p.chromium.launch(headless=True, slow_mo=50)
        page = browser.new_page(user_agent=""Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.64 Safari/537.36"")
        
        publications = []
        page_num = 1

        while True:
            page.goto(f""https://www.researchgate.net/search/publication?q={query}&page={page_num}"")
            selector = Selector(text=page.content())
            
            for publication in selector.css("".nova-legacy-c-card__body--spacing-inherit""):
                title = publication.css("".nova-legacy-v-publication-item__title .nova-legacy-e-link--theme-bare::text"").get().title()
                title_link = f'https://www.researchgate.net{publication.css("".nova-legacy-v-publication-item__title .nova-legacy-e-link--theme-bare::attr(href)"").get()}'
                publication_type = publication.css("".nova-legacy-v-publication-item__badge::text"").get()
                publication_date = publication.css("".nova-legacy-v-publication-item__meta-data-item:nth-child(1) span::text"").get()
                publication_doi = publication.css("".nova-legacy-v-publication-item__meta-data-item:nth-child(2) span"").xpath(""normalize-space()"").get()
                publication_isbn = publication.css("".nova-legacy-v-publication-item__meta-data-item:nth-child(3) span"").xpath(""normalize-space()"").get()
                authors = publication.css("".nova-legacy-v-person-inline-item__fullname::text"").getall()
                source_link = f'https://www.researchgate.net{publication.css("".nova-legacy-v-publication-item__preview-source .nova-legacy-e-link--theme-bare::attr(href)"").get()}'

                publications.append({
                    ""title"": title,
                    ""link"": title_link,
                    ""source_link"": source_link,
                    ""publication_type"": publication_type,
                    ""publication_date"": publication_date,
                    ""publication_doi"": publication_doi,
                    ""publication_isbn"": publication_isbn,
                    ""authors"": authors
                })

            print(f""page number: {page_num}"")

            # checks if next page arrow key is greyed out `attr(rel)` (inactive) and breaks out of the loop
            if selector.css("".nova-legacy-c-button-group__item:nth-child(9) a::attr(rel)"").get():
                break
            else:
                page_num += 1


        print(json.dumps(publications, indent=2, ensure_ascii=False))

        browser.close()
        
    
scrape_researchgate_publications(query=""coffee"")
```


Outputs:

```json
[
   {
      ""title"":""The Social Life Of Coffee Turkey’S Local Coffees"",
      ""link"":""https://www.researchgate.netpublication/360540595_The_Social_Life_of_Coffee_Turkey%27s_Local_Coffees?_sg=kzuAi6HlFbSbnLEwtGr3BA_eiFtDIe1VEA4uvJlkBHOcbSjh5XlSQe6GpYvrbi12M0Z2MQ6grwnq9fI"",
      ""source_link"":""https://www.researchgate.netpublication/360540595_The_Social_Life_of_Coffee_Turkey%27s_Local_Coffees?_sg=kzuAi6HlFbSbnLEwtGr3BA_eiFtDIe1VEA4uvJlkBHOcbSjh5XlSQe6GpYvrbi12M0Z2MQ6grwnq9fI"",
      ""publication_type"":""Conference Paper"",
      ""publication_date"":""Apr 2022"",
      ""publication_doi"":null,
      ""publication_isbn"":null,
      ""authors"":[
         ""Gülşen Berat Torusdağ"",
         ""Merve Uçkan Çakır"",
         ""Cinucen Okat""
      ]
   },
   {
      ""title"":""Coffee With The Algorithm"",
      ""link"":""https://www.researchgate.netpublication/359599064_Coffee_with_the_Algorithm?_sg=3KHP4SXHm_BSCowhgsa4a2B0xmiOUMyuHX2nfqVwRilnvd1grx55EWuJqO0VzbtuG-16TpsDTUywp0o"",
      ""source_link"":""https://www.researchgate.netNone"",
      ""publication_type"":""Chapter"",
      ""publication_date"":""Mar 2022"",
      ""publication_doi"":""DOI: 10.4324/9781003170884-10"",
      ""publication_isbn"":""ISBN: 9781003170884"",
      ""authors"":[
         ""Jakob Svensson""
      ]
   }, ... other publications
   {
      ""title"":""Coffee In Chhattisgarh"", # last publication
      ""link"":""https://www.researchgate.netpublication/353118247_COFFEE_IN_CHHATTISGARH?_sg=CsJ66DoWjFfkMNdujuE-R9aVTZA4kVb_9lGiy1IrYXls1Nur4XFMdh2s5E9zkF5Skb5ZZzh663USfBA"",
      ""source_link"":""https://www.researchgate.netNone"",
      ""publication_type"":""Technical Report"",
      ""publication_date"":""Jul 2021"",
      ""publication_doi"":null,
      ""publication_isbn"":null,
      ""authors"":[
         ""Krishan Pal Singh"",
         ""Beena Nair Singh"",
         ""Dushyant Singh Thakur"",
         ""Anurag Kerketta"",
         ""Shailendra Kumar Sahu""
      ]
   }
]
```

A step-by-step explanation at SerpApi: https://serpapi.com/blog/web-scraping-all-researchgate-publications-in-python/#code-explanation",datasets,2022-06-01 01:12:38,25
"Excited to check it out. 

Would love to get pricing for labor and delivery + the 24 hour after stay. Not sure how to even obtain this info.",2,v1xxiw,,datasets,2022-05-31 12:28:17,3
"Hey kuwala-io,

I believe a `question` or `discussion` flair might be more appropriate for such post. Please re-consider and change the post flair if needed.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/datasets) if you have any questions or concerns.*",1,v1192a,"location data providers are often in the press with negative headlines. Those services aggregate movement data from apps and aggregate the data to derive movement patterns which might be helpful for marketers. In fact, I had two moments in my life where I evaluated a PoC with those location data brokers. 

1. They were all **shady about where the data comes from** which is important to understand the Bias of the data. I never got a good answer. 
2. The data often **just represented < 0.4% of the population** (at least in Europe - different game in the USA). For a big city they might have 20K unique users while in the city were more than 3M users living. 
3. They dismiss **any professional data analytics principle**. The data comes in CSV (if a lot of data they give you like 10 separate files). Data was not always plausible in itself

Those experiences brought me to build certain parts of those data brokers but only with open-source data:

1. If it is about **location data you should know OpenStreetMap**. It's the biggest Database with meta info on location. It's not perfect but big companies like Mapbox, Apple, and Microsoft rely on it. Since the API is kind of messy, you can load with this repository whole cities information smoothly into a PostGres --> https://github.com/kuwala-io/kuwala/blob/master/kuwala/pipelines/osm-poi/README.md

2. Googe Popular Times: **Movement data can be also found on Google**. When you search a location it is often shown how frequently a place was visited (on an index of 0-100). With this libary you can access all the Popular Times data for location and entire cities --> https://github.com/kuwala-io/kuwala/blob/master/kuwala/pipelines/google-poi/README.md


3. Global Admin Boundaries: A huge problem that often people feel when working with location data is aggregating the data into different geo-based slices (country level, admin level, or even smaller into sub-districts). Here is a repo that cleaned the data out of Open Street Map for geo boundaries worldwide from very broad to a very small granularity --> https://github.com/kuwala-io/kuwala/blob/master/kuwala/pipelines/admin-boundaries/README.md

I think with those Open Source Tools and some data science magic you can generate similar outcomes as those location data providers but totally anonymized and free. **Would be awesome if anybody is interested in building a case around it :-)**",datasets,2022-05-30 07:19:32,1
"this dataset? https://fdc.nal.usda.gov/download-datasets.html

Which chemicals are you referring to? I.e peservatives are well known.

IMHO:  to go through the 1.7million  entries you need to write a program automatically group foods using different strategies... and show you the statistics until you get a feel for how the data behaves.  From there you can tweak the grouping strategy.",3,v08iop,"I'm doing a project on the nutrient difference between minimally processed plant and animal foods, i downloaded the whole usda dataset but there's not a single file with processing amount tag, what's the best way to go about this?

i can use chemicals that are commonly added.. but some of them are naturally found in foods, i can go through manually and remove rows with certain keywords (with added, flavored, mcdonald, etc..) but that is manually and it will never be completely accurate unless i somehow go through all the 1.7m entries.

any advice on how i can tackle this issue?",datasets,2022-05-29 03:38:53,3
Why use this instead of idk….. the census? That is already free?,9,uzo90t,"A couple weeks ago, I shared my site, EverythingByZipCode.com, which is a zip code database that spans nearly 900 columns wide across multiple public government sites. I posted it to get feedback on the database and the general concept.

Link of the original post:

https://www.reddit.com/r/datasets/comments/uh6g2b/free_zip_code_database_800_columns/

Based on some feedback, I’ve added a free option of the same database, but a slimmed down, standard version that isn’t as extensive. It’s actually the same database that cost $40-$50 on other sites, like zip-codes.com. 

Free option:

https://www.everythingbyzipcode.com/product/free-zip-code-database-lookup-file

It’s free and up to date! Enjoy!

If you have any recommendations, feel free to DM me on Twitter @bresslertweets.

David",datasets,2022-05-28 07:16:45,16
Whut,3,v04chk,"specifically from beginning of 2015 to end of 2021, at least. thanks!",datasets,2022-05-28 22:19:26,3
"Good question! I think there's a list of shootings in Wikipedia. As for more detailed ones...  For research...Unfortunately the N for school shootings (not to be sensible) is not high enough to get any statistically significant results. This Is simply due to the fact while school shootings are impactful, it often happens less than one or twice in a year (yes you may think this is a lot and it's also very sad... But statistics unfortunately doesn't care due to LLN) so it's hard to get statistically significant results. In fact it was even less common 10 years ago so data is very sparse. 

What I'm trying to say is there are severe limitations due to any statistical inference attempted with school shooting incidents being treated as a unit observation

I think investigating firearms deaths is more feasible but even then due to poor tracking of gun sales it's rather difficult to do anything good. It's a real policy question",14,uzbwg5,"Sorry if this comes across as insensitive, my heart goes out to the victims and families.
Is there a dataset or database for this? 
Kinda curious if there are places/things to avoid, in case I have kids, and have to send them to school…",datasets,2022-05-27 17:35:37,38
https://www.sciencedirect.com/science/article/pii/S1361841522001323,1,uzjdhs,DigestPath: a Benchmark Dataset with Challenge Review for the Pathological Detection and Segmentation of Digestive-System,datasets,2022-05-28 01:56:52,2
"I’d go to scientific literature for this. You should be able to find some datasets with motion capture.

You may also look at the practice datasets from markerless motion capture systems like freemocap or deeplabcut (though that will have a lot of animals as well). 

Feel free to dm with more details of what you are looking for. I’m currently using motion capture for my PhD so may be able to better direct based on more specifics",2,uyydp7,"Hi, I'm looking for a dataset where humans were tracked and the 3D locations of their bodyparts (joints, head, ...) is recorded. Preferably they are moving around or doing sports. Thanks!",datasets,2022-05-27 06:35:27,2
I can't help but if you can find a big weather dataset with lat long you can build this for yourself. But unusual to use a post office geography for weather data though. More likely to find lat long of the weather stations,1,uyho41,"Trying to identify zip codes that had temperatures above X in the last 12 months. I’m seeing datasets where I can download historical data for individual zip codes, but having trouble finding a data set that allows me to identify zip codes that have been known to rise above certain thresholds. Thanks for your help.",datasets,2022-05-26 14:12:28,1
"If you can't find anything, I'll be willing to develop it and put in on play store",4,uy8epe,"I want to record some activity labels from my phone during the day. Is there any app that allows me to do that easily? Ideally, something that involves widgets where I simply tap on the desktop icon, and an event is recorded with a timestamp.

Are you aware of anything like this?",datasets,2022-05-26 07:06:09,2
"A bit unusual to have to go backwards from model to dataset. Anyway, you can add an additional layer of ""reverse engineering"":

Look up frequently used packages that implement beta and gamma regression. For example, `GammaReg` and `betareg` in CRAN. Then simply search `<package> data` or `<package> model` and you will find many examples. Some of them are synthetic datasets, but others are examples of research papers in which the authors had some reason to believe such models fit their data.",1,uygmb5,"Hey everyone, pretty much what title asks for. I have a project to make over the weekend using two more advanced regression models and I'm at my wit's end trying to find something that works. Most of the time if I visually think that gamma will work, tests prove otherwise or there are so many outliers by pearson's residuals, that it is unusable. So yeah, a dataset that could work for Beta or Gamma regression would be nice.",datasets,2022-05-26 13:23:18,1
"You could just process the datasets to grayscale first either with a standalone application or in your import code.

imagemagick can do this as well as several other CLI utilities that can be scripted.

A quick google search for `imagemagick convert to grayscale` got me plenty of results.",6,uxyoy9,"Hello, I am looking for grayscale datasets for my research. I am using some tensorflow datasets, but most of them are RGB and was looking for some grayscale datasets such as MNIST and Fashion MNIST. However, I would like that the datasets be kinda different. For example, I saw in tensorflow datasets that there was a dataset similar to MNIST and another one for japanese hiragana. Any recommendations or should I take a dataset such as cifar10 and just convert it to grayscale?


EDIT: If you are wondering what I wanna do. I am researching reinforcement learning for deep learning compression. I wanna train my model to choose the best methods to compress a model using a set of datasets and then test with another dataset to see if it really picked the best combination of techniques or not.",datasets,2022-05-25 20:39:22,3
What words predict if the speaker is Dem or Rep? What words predict which year the debate is held? What words predict the election winner?,6,ux019h,"Hi everyone! First post here. I have made a dataset containing all US presidential and vice-presidential debate transcripts from 1960 to 2020. More information, accredition and the dataset itself can be found here on Kaggle: [https://www.kaggle.com/datasets/arenagrenade/us-presidential-debate-transcripts-19602020](https://www.kaggle.com/datasets/arenagrenade/us-presidential-debate-transcripts-19602020).

How would you guys use it?",datasets,2022-05-24 13:20:08,7
"[https://www.reddit.com/r/datasets/search/?q=compounds&restrict\_sr=1&sr\_nsfw=](https://www.reddit.com/r/datasets/search/?q=compounds&restrict_sr=1&sr_nsfw=)

[https://www.reddit.com/r/datasets/search/?q=chemicals&restrict\_sr=1&sr\_nsfw=](https://www.reddit.com/r/datasets/search/?q=chemicals&restrict_sr=1&sr_nsfw=)",1,ux92op,title,datasets,2022-05-24 21:10:10,2
"Hey OP. are you looking for free or paid data? Someone who can help you might give you one or the other and might want to know too
Sorry I can't be of more help",1,uwuvcj,"Hi all, is there a way to find the percentage (or number) of jointly owned properties in the UK - when there is more than one person on the legal title? 
Thank you!",datasets,2022-05-24 09:31:03,2
"You can try ""Complaint files"" here for New York: [https://www1.nyc.gov/site/hpd/about/open-data.page](https://www1.nyc.gov/site/hpd/about/open-data.page)  


I've seen something similar to this from hpd/hdb in New York for a client project I did a few years ago.",2,uwzjhz,"As the title says, if anyone can point me to where i can find these records I would really really appreciate it.",datasets,2022-05-24 12:58:51,4
You can create a key-value NoSQL database with the words as keys and the values being the month and year,1,uw5dc0,"Hi,   
i want to create My First Dataset/Databaase.   
I want to Scan the Contents Site of a Magazin and make it a Dokument via OCR.  

Afterwards i want to Build a Database wich Contains the Scanned Words and is tagged with the Month and Year of the Magazin.  

So when i search the Database for a Word i want to see in which Month and Year the Magazin has an Article about it.   

I have no coding Experience and would use only existing Programs.  
Has Anyone some tips for me what i could use and make this as easy as it could be?  


Thanks",datasets,2022-05-23 10:22:11,2
"Usually each port has their own data services. Check out

Opendatanetwork.com for a ton of US data.

I think USGS might be what you need.

https://www.usgs.gov/centers/national-minerals-information-center/historical-statistics-mineral-and-material-commodities",3,uviq06,I think volume off item at a given port and price per quantity would be ideal. The idea is to visualize trends in volume and pricing across locations and time. Thanks for any tips!,datasets,2022-05-22 12:53:27,1
"If you find that list of ""ALL"" tech products, I'll give you a free dataset of ""ALL"" the stars in the universe",2,uvlcl7,"I am looking for data on all tech products released. Trying to build a small project for school.  
Is there a place where I can this data? any recommendation on how I can get this done please?",datasets,2022-05-22 15:01:12,4
"Hey Inmortal2k,

I believe a `request` flair might be more appropriate for such post. Please re-consider and change the post flair if needed.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/datasets) if you have any questions or concerns.*",1,uv6fee,"Hi Reddit, 

I have been working on an application that displays medical prices for all hospitals in the San Francisco area. I have it running but I need more data. ([here's the data](https://data.chhs.ca.gov/dataset/chargemasters))

How would you approach parsing different files formatted differently, one from each hospital? I have thought to write a script where I input manually what sheet and what columns to parse, and hopefully I would get through the 300 hospitals in ~3h of manual input. Any suggestions?",datasets,2022-05-22 01:06:17,9
"I think if you were able to scrape a dictionary, you should be able to use the phonetic pronunciation to give you the info you want. You’ll have to do some text wrangling, but if you key off the spaces or dashes between syllables and the hyphens marking the accent syllable, it should be possible.",3,uuuu2b,"I tried the  Carnegie Mellon Pronouncing Dictionary, but the number of words it has is quite limited and it also included a bunch of names (easy enough to filter out, but the resulting dictionary is quite small).

Basically I want a dataset where the meter of words have been noted. For instance, where 1 is a stressed syllable and 0 is unstressed it would have something like:

Hello - 01  
Spandex - 10  
Literary - 1000  
Aardvark - 10  
Library - 100

And so on. Or at least it would have that data in a way that I could extract something akin to those numbers.",datasets,2022-05-21 13:03:38,1
"There might be an issue with kaggle setup.Here is a mirror: `https://archive.org/details/comments_202205210329.csv`

Will be updating this dataset monthly.",3,uuia8f,,datasets,2022-05-21 00:40:22,4
"if you search ""Dun And Bradstreet sic codes list .xlsx""   one of the first result in bing will give you an excel file with about 18k lines of SIC codes.",3,uu5ebi,"I need a file listing all SIC and/or NAICS classification codes and descriptions. Anyone know where to find it? I’ve looked at a lot of sites, but no one offers a DL. Thanks.",datasets,2022-05-20 12:13:22,5
"If you know exactly how to devise posts of search terms and for each of those you know where on the site you can find the connecting dots, and you know a little bit of programming, you can try webscraping to harvest all of this",1,uu3ycd,"**Edit 8/14**

[https://drive.google.com/file/d/1Nw7zTEGPcfD\_OC2zrFh09kOS9GSLp2mj/view?usp=sharing](https://drive.google.com/file/d/1Nw7zTEGPcfD_OC2zrFh09kOS9GSLp2mj/view?usp=sharing)

should delete dummy search tab now.

tried to make an exception to handle a pubmed internal error (error on pubmed's side)

tried to make a change to slow down selecting the download abstract option to prevent from downloading csv by mistake.

&#x200B;

**Edit 8/10:**

[https://drive.google.com/file/d/1pQgQSp-ESTnnvJUDB0DR7nCqhFs2dFC5/view?usp=sharing](https://drive.google.com/file/d/1pQgQSp-ESTnnvJUDB0DR7nCqhFs2dFC5/view?usp=sharing)

here is an experimental script. The code is sloppy, references are just generally for myself, but it tries to download a txt file of the abstracts from each of the searches. It does a dummy search first to change the sorting order to most recent first.

Once you have the txt files of the abstracts by themselves in one folder, in windows you can merge them with a the original filename as a seperator  using this cmd in windows cmd line:

`FOR %f IN (*.txt) DO type %f >> newfile.log & (echo. & echo. & echo %~nxf & echo. & echo.) >> newfile.log`

This allows you use 'find' to skim through the merged txt file.

Not sure if everything works, but working for me so far. I may try to continue improving if/when I find glitches. I'm not sure if it works well when Pubmed has an internal error. Also, when a query produces only 1 result or 0 results, those are both special instances, so there might be some glitches, but I haven't had any errors yet in those situations. There might be some new python modules installed than the last file, but I don't think so. A search typically takes several minutes to complete, so you have to be patient, because in the special instances, it might be waiting for something to timeout before moving forward.

**Original Post:**

Wondering if it's hypothetically possible to search Pubmed using a domain of all the species names in the World Flora Online Datasets. For example, in PubMed, searching:

""Nigella Sativa"" OR ""Angelica Keiskei"" OR ""Curcuma Longa"" CReB neurite

Will yield an article that has the keywords CReB and neurite within the search domain of those three species. Since there are only 400,000 known species of flora, as included in the World Flora Online dataset, Is it possible to search only that domain of all 400,000 species within the PubMed dataset, so someone would be able to just input:

CreB neurite

as the search term. It seems like a positive that both datasets are public domain and are downloadable, but I don't know much about this stuff. Thanks for the help and time!

&#x200B;

**Maybe more stable script?:**

**Edit 08/09:**

ok this python script should work ok with Google Chrome only. It gives me a depreciated warning when I ran in visual studio. Still lot of duplicate articles. You probably need chromedriver, need python and then through pip install selenium, webdriver-manager (the hyphen is important) and pyperclip. It is important you run the script after logging in to Pubmed so you can preset your search results to sort by most recent in Pubmed.  The script will ask you for search terms (like heart disease or neuropathy) and then use the terms to search through all the genus' of plants for hits:

[https://drive.google.com/file/d/1kNrcavfZqBaTd5MmD5UX0rWGKKRP8OlQ/view?usp=sharing](https://drive.google.com/file/d/1kNrcavfZqBaTd5MmD5UX0rWGKKRP8OlQ/view?usp=sharing)

Thanks for the help!

The last thing I would like to do is get all the abstracts from the search results from the 19 searches into one csv without manually downloading each of the 19 csv's from Pubmed

&#x200B;

&#x200B;

&#x200B;

&#x200B;",datasets,2022-05-20 11:04:16,8
Sounds like you want the Reddit api…[documentation ](https://www.reddit.com/dev/api/),3,utl376,"I am learning the R programing language for statistical analysis, and I would like some reddit data to practice it on, and maybe generate some pretty visualizations.
I have seen some visualizations of the conectiveness of some of the subreddit, I believe they used overlapping user comments to create it, I want to do something similar, maybe to some more specific groups like ""Portuguese language subreddits"". 
I want to find some data, a list of all existing subreddits would be a good start, maybe I can build a script to scrape them, but a dataset of the subreddit, and it's users usernames would be perfect.
I have no idea whete to start looking for it tho, can anyone help me?",datasets,2022-05-19 21:13:24,2
"I'm curious!  Also my company is VERY familiar with Bubble, always glad to see others build on that platform.",6,ut4ukb,"At my old job, it seemed near impossible to find data, and data documentation was horrendous. Every team used various Excel files for data dictionaries.

The last year I spent building a platform focused on making data documentation transparent, collaborative, and straightforward for anyone or company. It integrates well with most cloud database, CSVs , and BI tools.

Would anyone be interested in giving it a spin? This is the second iteration of the product. The first I built with Bubble (no-code), now we have an actual enterprise-ready solution!",datasets,2022-05-19 07:37:28,33
"Best way I can think of is looking at quarterly earnings releases of publicly traded companies. You can do this through various stock trading websites. For example on yahoo finance you can find the cash a single company is holding under finance -> balance sheet - > total assets - > current assets - > cash & cash equivalents.
Knowing this it's a matter of using a stock API and collecting the number and changes for a couple of companies but make sure to read up on the meaning of current assets so you understand what you are dealing with. Cool thing is that earning releases are every 3 months so changes in cash are trackable.",2,utbgj3,"Simply put, I'm wanting some data on companies and how much they had saved up for emergency purposes prior to the pandemic, and how much of the reserves were depleted due to the outbreak?",datasets,2022-05-19 12:43:54,1
"Zillow is a good place to start, however, without proxies it may be difficult to scrape yourself (throttling/IP Blocks).

You could look into paid web scrapers or paid data sets.

Whatever you're currently doing, you could try Airbnb and see if you're able to obtain data through them.",11,uspq4m,"Hey everyone, does anyone know where I can get datasets for residential housing in the United States? I’m mainly looking for large datasets displaying rent prices, mainly 1bdr and 2bdr condos. I plan on building a model to determine accurate average rent per state as well as various other trends, and feel like looking at this type of data would help significantly. Initially, I tried to scrape Zillow with Selenium but I’m not too proficient in web scraping/coding so I was looking for alternatives. Anything helps!",datasets,2022-05-18 16:26:01,10
"Hey TehDonkey117,

I believe a `request` flair might be more appropriate for such post. Please re-consider and change the post flair if needed.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/datasets) if you have any questions or concerns.*",1,usvozj,"If tobacco product category is included I would also like to see if those products usage increased more in states where cigarette usage decreased.

I went to the CDC and found survey sat but my professor said it wasn't good enough.

I am open to other ideas, need to show potential correlation between two or more items. I am trying to avoid race and crime since that seems to be covered by multiple students already.

Again, you know of one or more data sets with a different topic that works too. I just need to show I can comprehend the fundamentals of the class.",datasets,2022-05-18 21:53:45,3
"SEC filings often have subsidiary lists, specifically as an appendix of form 10-K:  

Example - [Coca-Cola](https://www.sec.gov/Archives/edgar/data/0000021344/000002134422000009/a20211231exhibit211.htm)  
Link - https://www.sec.gov/edgar/searchedgar/companysearch.html",1,usbla4,"Hi, I just need to find the company's subsidiaries up to date for 2020 (the later the better). It does not have to be a dataset necessarily, maybe an API, but everywhere I have found this information, it doesn't work for me.

I have found something similar:

1. On [sec.gov](https://sec.gov), but there, a link to the subsidiaries list is not always present in the same form, which is not too big of a deal, but the page with subsidiaries themselves is also always different which means that I cannot reliably parse the page. Moreover, It seems that their data is old.
2. On Wikipedia's query service with [this](https://query.wikidata.org/#SELECT%20DISTINCT%20%3Fitem%20%3FitemLabel%20WHERE%20%7B%0A%20%20%7B%0A%20%20%20%20SELECT%20%3Fitem%20WHERE%20%7B%20%3Fitem%20%28wdt%3AP31%2Fwdt%3AP279%2a%29%20wd%3AQ43229.%20%7D%0A%20%20%7D%0A%20%20%3Fitem%20%28wdt%3AP127%7C%5Ewdt%3AP199%7Cwdt%3AP749%7C%5Ewdt%3AP1830%7C%5Ewdt%3AP355%29%2B%20wd%3AQ312.%0A%20%20SERVICE%20wikibase%3Alabel%20%7B%20bd%3AserviceParam%20wikibase%3Alanguage%20%22%5BAUTO_LANGUAGE%5D%2Cen%22.%20%7D%0A%7D) query, I got something really close, but it takes ages and includes some strange subsidiaries which are not even present on the main site.
3. Directory of Corporate Affiliations sounded like a good call, but it is either very old or is a literal book, not to mention that it costs $2700 (for some reason). Call me cheap, but this kind of information should be free.

I am planning to use this with python, so if there is a library for reliable parsing something or just a website to reliably parse from, it is good for me. 

Any suggestions will be greatly appreciated.",datasets,2022-05-18 05:20:08,5
[here](https://www.census.gov/geographies/mapping-files/time-series/geo/tiger-line-file.html),4,ust7hy,"[https://www.census.gov/programs-surveys/popest/technical-documentation/research/evaluation-estimates/2020-evaluation-estimates/2010s-cities-and-towns-total.html](https://www.census.gov/programs-surveys/popest/technical-documentation/research/evaluation-estimates/2020-evaluation-estimates/2010s-cities-and-towns-total.html)

I downloaded all of these CSV's that cover all 50 Free and Independent States in Constitutional Republic, of The United States of America. + DC & Federal All.

It's a beautiful dataset; however it's lacking 1 set of data (longitude and latitude).

Does anyone know how to get longitude and latitude for a dataset like this or similiar? Perhaps a different source link. This is all I have been able to find that lists all the States w/ a CSV.

However the U.S. Census Data 2010-2020 looks absolutely huge in other formats and source pages i've found.

I could use some assistance if anyone knows how I can get a complete set. I need to use this as a base database once converted using Python3 and pandas (CSV 2 MySQL); as a Joomla 4.1.2 Custom Component/Extension for American Constitutionally Guaranteed an Protected Places, Locations and Jurisdictions.

Any assistance greatly appreciated!

Thank you in advance!

&#x200B;

Best Regards,

&#x200B;

Brandon",datasets,2022-05-18 19:30:58,2
I'm not sure that the government collects inflation rates at that level of granularity. Would it be acceptable to just look at the cost of living index in each urban area instead?,2,uski9e,"My client is updating their attribution lift model and DMA/store allocation for paid media efforts. Currently the control and exposed store locations are not the best allocation because some areas have a higher average HHI.

I am getting more demographic details from census files - but would ideally have current/forecast inflation by zip code - or DMA if zip code isnt possible.

Any thoughts?",datasets,2022-05-18 12:18:43,1
"What kind of transactional data are you looking for? If it is about general consumer and retail transactions, there is usually three big branch of obtainable data that might be available:  
\* Paper based and e-eceipts: some mobile apps, which help consumers track spending, by taking a picture of the receipts sell the data to other parties. There are also software add-ons which track user spending based on e-receipts in an email inbox. In this case, you have info on the content of the purchase, and some anonym identifier of the buyer.  
\* Credit card transactional data: Generally all the big names (Visa, Mastercard) sell this partially anonymised data, and there are a bunch of other data vendors who some way or an other get their hands on this. In this form you do not get any info on the content of the purchase, only the sum value and some vague name of the merchant. It is even hard to get some anonym id for the buyer.  
\* POS data: some bigger companies like Nielsen, IRI, SPINS etc. collect syndicated POS data from the retail stores itself. This contains the product, price, number of items sold in an aggregated manner. It is not tied to the buyer, you have no id to track who buys what, and mostly you can not have the exact detail of the point of sale itself, as those are aggregated.",2,us7hmb,"My organization is looking to acquire dataset for R&D, with plans of improving our product. 
We re looking at public datasets and paid services for customer-merchant transaction data. 
Some of the leads we had got from datarade portal but most of them did not have the kind of data needed, is there any other alternative or any possible leads to this.",datasets,2022-05-18 00:37:10,4
"Yes! I just found one a few days ago that should have what you want:  https://www.kaggle.com/datasets/rtatman/lego-database

There's also the Rebrickable API:  https://rebrickable.com/api/",1,usdsj2,Basically looking to see if anyone has already scraped or maybe knows of a good api to use to build a dataset of lego sets with their attached parts lists ?,datasets,2022-05-18 07:08:50,2
"I managed to find [this](https://taxfoundation.org/state-and-local-tax-burdens-historic-data/) from the tax foundation website. You will have to select the each individual year yourself, however.",6,us0j7c,"Hello all,

I'm looking for historical data (period of interest roughly 1970-2020) on state income tax rates. An optimal dataset would be unique by State-Year-Bracket(i.e. Income Bin)-(Marginal or Avg.)Tax Rate, but I'm happy to look at other options as well.  Would appreciate it if you happen to know any dataset, thanks!!",datasets,2022-05-17 17:30:55,1
"You can write a simple python script using the ""requests"" library to loop through the numbers and save the JSON for each pokemon

[edit] yeah the JSON structure is pretty straightforward. check the ""sprites"" element.

[edit 2] this should help https://realpython.com/python-requests/",10,urklaw,I need about 100 images per class (pokemon). There should be 898 pokemon's images starting from [https://pokeapi.co/api/v2/pokemon/1](https://pokeapi.co/api/v2/pokemon/1) to [https://pokeapi.co/api/v2/pokemon/898](https://pokeapi.co/api/v2/pokemon/898). It will be really helpful if someone could provide. Thank you so much!,datasets,2022-05-17 04:46:00,7
"Hey avocadotoasting,

I believe a `request` flair might be more appropriate for such post. Please re-consider and change the post flair if needed.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/datasets) if you have any questions or concerns.*",1,urv3hc,"Hi! I'm doing a class project on eating disorders, but it's really hard to find data on this topic. I've only found this one: [https://ourworldindata.org/grapher/prevalence-of-eating-disorders-by-age](https://ourworldindata.org/grapher/prevalence-of-eating-disorders-by-age) (ED prevalence and age and region). Sorry for not having specific criteria for the dataset, but I'm basically going to frame my project based on whatever datasets there are available since I can only find one anyways. Maybe data on the reasons EDs develop, or the physical health of individuals with EDs? 

&#x200B;

I'll use a combination of any of the following two models to analyze the data, if that helps: linear, spatial, network, contagion/diffusion, game theory",datasets,2022-05-17 13:07:34,1
"try the IPUMS web site and you will want to look at the 5 year ACS survey data. This is only for US data, your request didn't say for what area.",3,urnw3p,"I’m hoping you guys might be able to help. I’m looking for an xls data set of poverty for a specific city, year over year. I’ve been trying to find it on census but I’m coming up empty with finding what I’m looking for. Any ideas? Thanks.",datasets,2022-05-17 07:37:44,2
Clipper probably have some data,2,urp203,I'm looking for anything regarding clipper lighter does anyone know about the existing of a dataset?,datasets,2022-05-17 08:32:20,1
upvoted,1,urnlo7," product data like manufacturer, weight, price, ingredients etc",datasets,2022-05-17 07:23:43,1
I learnt about parquet 2 months ago in my big data class and did an assignment on it comparing its efficiency vs csv. Can confirm it’s faster lol but not as intuitive,6,uqu0vz,"Parquet has become the new default so from now you'll need to use the dd.read\_parquet() method instead of your usual dd.read\_csv().

If you really, really, REALLY want to use an inferior file format with slower parallel I/O, less compression options and no column pruning or predicate pushdown ;) you can still access the CSV data in the csv\_backup directory: df = dd.read\_csv(""s3://nyc-tlc/csv\_backup/yellow\_tripdata\_2012-\*.csv"")

**The only drawback of the NYC TLC Parquet files is that downloading these parquet files takes rather long, as there are 12 very large partitions per year. Run a ddf.repartition(partition\_size=""100MiB"") to repartition to a more optimal size for parallel IO and faster computations.**

Some reasons why Parquet is a better file format anyway: [https://coiled.io/blog/parquet-file-column-pruning-predicate-pushdown/](https://coiled.io/blog/parquet-file-column-pruning-predicate-pushdown/) \[disclaimer: I work at Coiled\]",datasets,2022-05-16 04:45:54,5
"I'm no entomologist, but this may be relevant to what you're looking for?

[Trigona Hypogea](https://www.discoverlife.org/mp/20q?search=Trigona+hypogea&guide=Apoidea_species&cl=BL&flags=HAS:)",1,ur79z4,"Hey everyone I’m trying to find geospatial data on Trigona hypogea and Meliponini melipoa. I’m looking mostly for coordinate data. All I can find so far is random articles stating neotropics and Costa Rica, both of which have large variances in ecology.",datasets,2022-05-16 15:00:40,1
"You might want to check these out thoroughly and properly but found these datasets on Kaggle! It may help you out:


A) https://www.kaggle.com/datasets/datafiniti/hotel-reviews

B) https://www.kaggle.com/datasets/jiashenliu/515k-hotel-reviews-data-in-europe

C)  https://www.kaggle.com/datasets/PromptCloudHQ/hotels-on-makemytrip

D) https://www.kaggle.com/datasets/huypui/data-515k-rating-hotel",1,uqwb0s," Hello , i'm currently building a hotel recommender system but unfortunately i'm lacking data. Does anyone know a good hotel reviews dataset ? i need datasets with the user id (or the username) the name of the hotel, and its location, and the rating given by the user to the hotel. Thanks",datasets,2022-05-16 06:43:49,2
"[https://www.motherjones.com/politics/2012/12/mass-shootings-mother-jones-full-data/](https://www.motherjones.com/politics/2012/12/mass-shootings-mother-jones-full-data/)

It's the main source of data, and actually answers important questions.

>This wasn’t just a social media phenomenon. The Huffington Post published Sprouse’s tweets as a “Powerful Take on Whiteness and Mass Shootings.” An article in Elle called the link between white men and mass shootings “a general rule” and proposed that “our refusal to confront toxic white male violence is why this problem will metastasize.” The progressive news site ThinkProgress said that “when we talk about mass shootings, we are talking about white men.” Newsweek wondered if “white men commit mass shootings out of a sense of entitlement.” A CNN opinion piece bemoaned the fact that “America has silently accepted the rage of white men.”  
>  
>\[…\]  
>  
>What those initial Mother Jones numbers showed, though, was that white people weren’t overrepresented among mass shooters. The media outlet had found that roughly **70 percent of the shooters in mass killings were white**—certainly a majority. But according to Census Bureau estimates for 2012, **whites accounted for 73.9 percent of all Americans.**

[https://slate.com/news-and-politics/2017/10/what-the-white-mass-shooter-myth-gets-right-and-wrong-about-killers-demographics.html](https://slate.com/news-and-politics/2017/10/what-the-white-mass-shooter-myth-gets-right-and-wrong-about-killers-demographics.html)",3,uqvd9d,"This is predictable, in light of recent events in NY, but I have been interested in this for a while. The FBI hate crimes dataset (which I have) doesn't include any variables that might shed light on motivation or demographics (except in very broad strokes).

*Edit*: I've since found [RAND's page](https://www.rand.org/research/gun-policy/analysis/essays/mass-shootings.html) with information about different databases listing mass shootings. Motivation is apparently hard to come by. The datasets vary widely in where they get their information, what variables they include, and how they define mass shootings.",datasets,2022-05-16 05:57:48,5
"Here you go:
https://doi.org/10.1038/s41597-021-00909-8

Their Excel data file is in reference 29. Here's the direct link. https://doi.org/10.6084/m9.figshare.13271111

Note that, while I'm an LCA practitioner, I haven't fully reviewed this article yet. I'd suggest reading the article to verify its content. Also note that any CO2 footprint is necessarily an averaged value (a guesstimate at best) and that any particular product that you buy may have different impacts than something in these datasets. Most of these datasets also only have a limited number of products. Looks like some consulting firms have a larger number of products, but those aren't open sourced.",3,uqtu5f,Do you know any datasource of food CO2 footprint? Thank you!,datasets,2022-05-16 04:34:56,3
Not exactly what you’re looking for but there’s TSA throughout data for US airports https://github.com/mikelor/TsaThroughput,4,uq89lb,"I am looking for a dataset with daily passenger data for airlines. It can be any destination, and for one or many, but the data granularity has to be daily - number of passengers on a single flight from airport A to airport B, if possible. Or, at least, number of passengers on all flights from airport A to airport B on the same day (in case there are more than 1 flight per day).  


The datasets I found either aggregate data for a month, or are daily, but for all destinations for an airline on the day.   


Any help would be great!",datasets,2022-05-15 08:19:31,3
"Measuring sugar intake historically is probably no simple task, but there are a few ways to go about it:

-	Find annualized domestic sugar production in the US from ‘77 onward (not easy). the US grows most of its own sugar. 
-	find some database of nutritional facts that tracks values since the start of the nutritional facts label regulation by the USDA. 
-	find some reputable source that tracks this directly (seems like you’re already struggling with this). 

Potentially go ask a librarian if they can help you get access to archives of market research in the US food industry.",2,uqh9jr,"Hello 

For a project I am looking for a dataset that will help me show how Americans doubled their consumption of sugar from 1977 to 2000, because around that time the dietary guidelines that they released underestimated the impact sugar in food had on obesity, specially children obesity.

The main objective of this would be to prove with data what the documental ‘Fed Up’ is about, but try as I may, I can’t find data about obesity rates with date stamps AND sugar intake. 

This is for a project and a requirement is to attach the CSV or any files of the datasets I am using. 

Maybe I can get lucky and get help from here.",datasets,2022-05-15 15:29:39,1
Sounds like iris dataset on steroids,20,upms3p,I’m looking for a dataset on a large set of flowers species and how many petals each flower has. Need to put this code into matlab. Running out of time :/,datasets,2022-05-14 11:03:38,7
"Hey, did you find someone for tis?",1,upvpa4,"This has been asked successfully before but do 1-2 people want to share a Crunchbase Pro account? 

I just need to export one list",datasets,2022-05-14 18:58:29,2
Boosting signals of extremely rare classes and/or examples of rare signals within a class space.,20,uozyib," I am interested in learning more about what use cases people have for fake data.   (e.g. don't have access to production data, early stage company with no production data, compliance, privacy  or security reasons etc.).",datasets,2022-05-13 12:56:37,13
"Link not working for me, DNS address could not be found.",1,uovawn,,datasets,2022-05-13 09:15:45,4
"Hey illusionalsucker,

I believe a `request` flair might be more appropriate for such post. Please re-consider and change the post flair if needed.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/datasets) if you have any questions or concerns.*",1,uolpod,Trying to make an argumentative stats project based on average salary compared to education. If you know any data sets available I would appreciate it very much.,datasets,2022-05-12 23:45:57,6
"If you're looking at the US, the Behavioral Risk Factor Surveillance Survey, BRFSS is run by the CDC and offers a lot of those data points.",1,uovvbi,"Hi everyone, for an econometrics project I'm looking for a dataset containing results of some survey regarding personal social status, education, family life, work and experience with mental health issues. Has anyone ever encountered something similar and could share?",datasets,2022-05-13 09:42:41,2
"Google TMY3 weather data, California actually has some of best publicly available data in my experience.",3,uoejvv,"Hi everyone,
I am searching for specific California weather data from 2015 to 2022. It should have the cities name, temperature, pressure, humidity and wind speed.
If it is not possible to get this data without paying for it, please let me know.",datasets,2022-05-12 16:45:05,8
"Here's a research paper citing a variety of estimates for mortality under Mao https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4331212/

See the appendix tables.",3,uogtbu,"I'm interested in comparing the civilian deaths of these murderous dictatorships over time. I recently started reading Bloodlands, which makes the point that Stalin was the biggest killer of civilians between himself and Hitler from about 1930-1939, and want to make a chart comparing the two (plus Mao).

The datasets would need to include political prisoners and victims of famine like Ukraine's Holodomor and starvation victims under Mao.

I expect to need to patch this together somehow, but couldn't easily find any month-level data.

Thanks in advance!",datasets,2022-05-12 18:47:47,2
Hey did you ever find an answer / can you share what sources you’ve found? Thanks,1,unyxnl,"Hi there,

Does anyone know where I can find shareholder proposed resolutions? 

For those not familiar, investors in companies can put forward ideas for the other investors to vote on for the company to do. It's a very rich set of information on what some investors want a company to do.   

I found a few sources, but I cannot find a source of all the proxy voting info. Does anyone know where this is?  I thought the SEC would have it but I cannot find it.",datasets,2022-05-12 04:31:23,2
4.8TB http://blackbird-dataset.mit.edu/,27,un9665,I need it to test the boundaries of our analytics solution,datasets,2022-05-11 05:53:32,20
[MIT Cost of Living Data](https://livingwage.mit.edu/states/41),3,un8zy9,"Hi, 

Im looking for a dataset about living expneses in US dollars(Not index) per state for 2021. Any  one could help me here?

thanks in advance",datasets,2022-05-11 05:44:12,2
"The bigger question is why? 

Also you should probably expand the search to include letter, otherwise you are excluding an entire continent.",1,unatqn,"Hello! 
 I'm looking for some A4 paper documents with shadows but can't find anything on kaggle.com and paperswithcode.com. Maybe anyone can suggest some relevant keywords for my search?  
Here are some examples of what I'm looking for:  
https://imgur.com/a/2QFEDyn",datasets,2022-05-11 07:14:36,4
"Hey WhatsTheAnswerDude,

I believe a `question` or `discussion` flair might be more appropriate for such post. Please re-consider and change the post flair if needed.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/datasets) if you have any questions or concerns.*",1,umla2v,"Hi Reddit Data hivemind, I need your help. 

I'm looking to validate some data in the US concerning sunny days/weather. I'm trying to verify if there was some type of data that would confirm whether certain cities had over 3 hours of sunlight/sunny weather/amount of hours the sun is out for said cities?

Was looking possibly for a mic if Boston/LA/Orlando/Chicago/Dallas possibly?

Would anyone here know where I could find a dataset that would confirm this?",datasets,2022-05-10 08:26:13,9
"Hey IamNotGorbachev,

I believe a `request` flair might be more appropriate for such post. Please re-consider and change the post flair if needed.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/datasets) if you have any questions or concerns.*",1,umeoy1,"Recent data from 8th and 9th May 2022 containing Amazon bestseller products (top 100 of all categories). Given as samples of 1k, 1m or full data (latter on request) per country. No payment or registration needed (I tried and can download it).

[https://bolddata.org/blog/free-amazon-bestsellers-datasets-2022-05-08](https://bolddata.org/blog/free-amazon-bestsellers-datasets-2022-05-08)",datasets,2022-05-10 02:28:08,3
It's probably here https://paleobiodb.org/,3,umk6ni,"[EER diagram](https://imgur.com/Zb3H6mA)

I'm in an introductory database class where we've been working with MySQL Workbench and I've got an open-ended assignment to create a schema and populate it with some data. I decided to build around paleontology and have already manually entered some data, but I'd like to have some more because I'd like to keep playing with it afterward. I'm looking for an open dataset (preferably CSV) that I can use to populate this thing. I'd obviously have to tweak the formatting a bit and I'm fine with this. Specifically, I'm looking for (in genus) `_,genus name, genus description, oldest years ago that the genus existed, youngest years ago this genus existed,_,who discovered it`, (for species) `_,species name,_,oldest years ago it lived, youngest, _,_, discoverer, country where it was discovered, description, average weight in kilograms, average length in meters, average height in meters` , and some similar stuff for related data.",datasets,2022-05-10 07:37:09,2
Maybe this one: https://universe.roboflow.com/marius-lee/dirty,2,umk035,"I'm working on a project to visually identify the level of dirtiness of a customer’s car to grade a vehicle. Basically, it involves developing an algorithm that would be able to identify if a car is dirty once it enters the forecourt of a service station. Does anyone know of any publicly available data that would help me to identify if a given vehicle is dirty or not?",datasets,2022-05-10 07:28:38,1
"Try here: https://docs.google.com/spreadsheets/d/14ZtQy9kd0pMRKWg_zKsTg3qKHoGtflj-Ekal9gIPZ4A/pub#

From this resource https://ourworldindata.org/maternal-mortality",1,umjetv,"Hi all,

&#x200B;

I'm working on an investigative project, and having trouble finding a dataset or sets that has what I am trying to find. Any help tracking this information down would be greatly appreciated.

THE ASK: I need to find a dataset that has the maternal death rate in the United States by year going back to 1950. Ideally I'd love to find a set that has that information for other countries as well.

&#x200B;

The issue I am running into is most sets I've found online in the usual places like world bank only go back to 2000, or early 1990s when the trend I want to investigate began earlier.

Even 1980 to 2014 or whatever would work for me. Thank you for any help you can give.",datasets,2022-05-10 07:01:24,1
Link to r/dataisbeautiful post: https://www.reddit.com/r/dataisbeautiful/comments/ului9z/oc_hospital_list_prices_vs_what_medicaid_pays/,2,ululat,,datasets,2022-05-09 09:04:30,6
Great !,1,uloglb,"Hi folks,

Over the weekend, I scraped employee reviews of over 25 companies over \[Blind App\]([https://www.teamblind.com/](https://www.teamblind.com/)) to build a dataset that comprises a company's rating, pros, cons, resignation reasons and an overall description of an employee's review about the company.

Blind is an anonymous workplace network where verified professionals connect to discuss about compensation, work-life balance, interviewing, pros/cons and much more. The data was scraped using a Python script and is now available as a CSV and a JSON for each of the 25 companies.

Check it out on Kaggle here: [https://www.kaggle.com/datasets/harshcasper/blind-app-company-reviews](https://www.kaggle.com/datasets/harshcasper/blind-app-company-reviews)

You can also submit feedback and bug/feature reports on GitHub: [https://github.com/HarshCasper/Blind-App-Reviews](https://github.com/HarshCasper/Blind-App-Reviews)",datasets,2022-05-09 03:48:35,1
Check out the American Community Survey.,8,ulao9x,"I am looking for annual county-level socioeconomic data that contain information about population, age, poverty rate, unemployment rate, etc.",datasets,2022-05-08 13:40:33,11
I uploaded the iris dataset and this is pretty impressive. It feels just like a Shiny app.,2,uklt9j,"Hi Guys, 

I created this site because often times I wanted to just take a quick peak at some dataset I found online but did not want to go through the effort of writing code. No email - login - or anything like that required, it's meant to be simple, quick, and fairly basic. 

site: [https://regress.me/](https://regress.me/)

source: [https://github.com/SuljicAmar/Regress.me](https://github.com/SuljicAmar/Regress.me)

There is a vid of the site in the readme on github, let me know if you have any questions or comments",datasets,2022-05-07 13:36:17,6
UK has these statistics too. Link escapes me sorry,1,ukv91f,"A 2020 research paper used the following data from France’s official statistics department, in which every populated 1km by 1km parcel of land in France is labeled by income and number of residents: https://www.insee.fr/fr/statistiques/6215140?sommaire=6215217

I would not have found or understood this dataset without knowing French or having this specific paper recommended to me. Therefore, I am curious if there are similar datasets for other foreign countries where such income data is available in 1km by 1km blocks, and if so which countries? I know the US keeps data on census blocks, but this is the extent of my knowledge.",datasets,2022-05-07 22:50:57,1
Remindme! 6 hours,1,ukve6i,"Hey all, we are running a data challenge on athlete marketability online (e.g social media). If you might be interested, check out our expression of interest post [HERE](https://www.linkedin.com/feed/update/urn:li:activity:6927760517264547840).

Prizes and freelance opportunities for best submissions.",datasets,2022-05-07 23:01:17,3
"When in doubt, start with Kaggle:

https://www.kaggle.com/datasets?search=Health

Or UCI:

https://archive.ics.uci.edu/ml/datasets.php",13,ukfwxb,"University level biostatistics course  


Just want to know where I can get some free good quality health dataset (like a CSV file). Honestly lost wasted half a day looking and only see graphs but no data set. Novice so am I doing something wrong?

Any good websites?",datasets,2022-05-07 08:40:18,5
"Hey Jeffrosslostson,

I believe a `request` flair might be more appropriate for such post. Please re-consider and change the post flair if needed.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/datasets) if you have any questions or concerns.*",1,ukhdrt,"Hello everyone, I'm currently doing a memoir on accelerated proximal point algorithm and I need to prove the efficiency of my accelerated algorithm with a lasso regression. The thing is I need to have a dataset which has a lot of features ( to show the efficiency of lasso) like between 1000 and 2000 , do you know where I can download such dataset for Lasso Regression, that would be a huge help Thank You!!!",datasets,2022-05-07 09:51:33,1
"Although this method is not exactly legal, you could use the command line tool *youtube-dl* together with *ffmpeg* to download entire YouTube playlists as .mp3 files (or whatever format) all at once. 

When using the YouTube search tool, you can filter results to only show playlists, so you could probably create a list of playlists and write a script that automatically downloads all playlists at once.",5,ujwap5,"Hello everyone,

I am looking for a large high quality music database of around 200k 30s music snippets (1600 hours). The genres should be as balanced as possible. I DO NOT need any sort of metadata. Just the raw audio.

I am aware of the ""large"" subset of the FMA dataset. However it is a) not large enough and b) I am afraid about the song qualities and that they might not necessarily reflect the ""popular"" songs in our society (most of the songs are experimental and there are many rather strange songs in it). My intuition was to sort the FMA songs by popularity and get the top x songs of the FULL dataset (i.e. complete songs not just 30s snippets). But then I realized that music is usually highly repetitive and an entire song split into multiple 30s snippets might contain redundant information. I am also aware of the million song dataset, but it doesn't seem to be accessible anymore?

I am therefore asking for suggestions about collecting high quality music snippets. I am also open for ideas about scraping preview tracks from music websites such as 7digital. Unfortunately I did not (yet) get an API key.

&#x200B;

**Update:**

my approach now is to scrape [rateyourmusic.com](https://rateyourmusic.com). I can get the best albums of all time for many different genres + their spotify ids and apple music ids (and sometimes youtube and soundcloud aswell). Afterwards I use the spotify API to get the tracks of that album. Among many other things the track data contains a popularity score + the URL for a 30s preview mp3 file. Sometimes however the track doesn't have a spotify preview and I am not sure what I will do then. Maybe just skipping it or search for the track on 7digital and download the preview there or from apple music (though it is not straightforward for me rn how to downlaod the preview file from apple music).",datasets,2022-05-06 13:18:34,5
"http://opencouncildata.co.uk/downloads.php

I don't think it's updated for 2022 yet.",1,ujzrxq,"I know I could find them on each individual council's website, but wondering if there was a central, collated source for this. More or less in this format:

&#x200B;

|Council|Party|Councillor|
|:-|:-|:-|
|...|...|...|",datasets,2022-05-06 16:04:04,2
Thanks for this looks super interesting.,3,ujdhsk,,datasets,2022-05-05 19:34:16,1
"Bls.gov, data by MSA, district, household 

Also coded by NASIC code

Some of the latter sets you're asking for are sold by Alacra",1,uje7go,"Does anyone know where I could get the percentage breakdown of the 4 types of construction? Looking for global data but happy with US or anything honestly. Four types:

1. **Residential buildings**, like single and multi-family homes
2. **Commercial buildings**, such as offices or warehouses
3. **Industrial facilities**, like factories or large-scale production facilities
4. **Infrastructure projects**, such as roads, bridges, airports, or sewer systems",datasets,2022-05-05 20:15:00,1
Finishing an econometrics thesis right now. Make sure you choose something with readily available data. I went from having like 3 dope ideas to having to settle for something easy that I could find data on. I’m no code wiz either and we’re using R just thought I’d throw out my 2 cents,3,ujfebz,"I'm currently a student and my final project is to use Python, SQL, and Tableau to create data visualization and inquire about interesting ideas and or topics to do this research project on. 

I was wondering if you guys were instructors, and what sort of data visualization would interest you. My only idea was to look at the statistics of college graduates vs people who didn't finish for whatever reason. I think interesting metrics would be comparing everything from scholarships/out-of-state/gender/race/age etc. Or if that has been done a million times and find something more unique.",datasets,2022-05-05 21:26:00,9
"We have a free database of home sales including New York:

https://www.dolthub.com/repositories/dolthub/us-housing-prices

I don’t think it has # of rooms",7,uilgy9,"Are there any open source datasets which contain either up to date and/or historical housing information and prices for the state of New York? 
Ideally it should include the number of rooms and bedrooms, and the price.",datasets,2022-05-04 18:03:11,7
Nice. Thanks for doing this. Some of the columns could use some description of what they represent.,1,uiy07r,,datasets,2022-05-05 06:55:46,7
Looks like a static data set download? Well it be continuously updated ?,2,ui2g3x,"Hello, Is any one interested in airline flight datasets?

Here is a one which might be usful  (it covers some top airports from europe/asia/america/africa), with airline info , air ticket price, Co2 (carbon) emission etc. Good for those who care about the airline sector or those who want to study then environmentals of the aircraft . Feel free to leave any thoughts or ideas. 

Dataset Link : [Link to Dataset](https://barkingdata.com/?flight-dataset/)",datasets,2022-05-04 02:11:52,2
"It should be part of this dataset, NOAA global summary of the day
 https://www.ncei.noaa.gov/access/metadata/landing-page/bin/iso?id=gov.noaa.ncdc:C00516",2,ui642u,"I'm building a personal portfolio project and I need India's metrological data(covering a time period of approx 30 yrs). I'm looking to study the ongoing heatwave and I'm aiming to find out some pattern(if it exists) and correlation with other environmental factors.

Thanks for the help.",datasets,2022-05-04 06:04:05,8
[deleted],9,uho67i,"Social Media would have me believe there has been a recent uptick in 'sabotage' activity: fires, explosions, bridge collapse etc. Are there any datasets that show these events?
The closest I can find is the Global Terrorism Database, but this isn't quite what I'm looking for.

Thanks",datasets,2022-05-03 12:44:19,5
"Ooo very cool, just learning Python/Pandas right now so even the 1-click download is some good data for messing around with visualization. Thanks!",4,uh6g2b,"Hey everyone! I created a website, www.EverythingByZipCode.com, a dataset that acts as a unified view of information for every US zip code - spanning across multiple public domains/APIs.

It’s a pet project I launched in February and have made 50 sales through just PPC ads, but would much rather receive qualitative feedback as opposed to paying customers at this point.

I have several updates rolling out in the next few weeks, but would love some input from folks like yourself that are obviously passionate about data analytics.

There’s other sites out that that sell the same premise, but cost more and only contain basic information from the Census. This dataset truly is more for less. Also, the other sites look like they haven't been updated in 20 years.

It has a lot of business use cases, but also acts as a great dataset to play around with, whether it be predictive modeling or data visualization.

Here’s a free promo code: REDDIT. I’ll be updating the promo code to 50% off after the first 50 customers, then 25% for the next 25. Like I mentioned earlier, it’s ideally a for-profit business :)

Note - If you sign up through the promo code for free, you’ll only have access to the current file and won’t receive updates.

If you use it, again, I’d greatly appreciate your feedback or a follow. Feel free to message me at @bresslertweets on Twitter!

Thanks so much!

David",datasets,2022-05-02 19:49:37,32
https://www.huduser.gov/portal/datasets/usps_crosswalk.html,3,ugqiy6,"Anyone know where I can find a file that has all of the Zip Codes in an MSA? I feel like the Census website has it, just cant find it.

Thanks!",datasets,2022-05-02 07:10:59,5
"In the US code is speech and if you wrote the code your free to release it. Doesn’t mean someone won’t harass you but they would not likely have any legal ground. As for scraping data, as long as it’s not copyrighted data your fine. Property history and listing prices should be public facts so there’s no copyright on that. Things like value estimates are copyrightable and using that data would be limited. Personal / commercial only use comes into play in regards to copyrighted material and the holder’s terms of use.

Not a lawyer but have read up on the matter.",38,ug1by8,"I have a private repo I'm working on for personal reasons (I'm looking to buy a house), but I'm happy to make it open source for others to use. I don't want to make money from this, nor do I want my code used to make money etc.",datasets,2022-05-01 07:42:34,50
"Hey po-handz,

I believe a `question` or `discussion` flair might be more appropriate for such post. Please re-consider and change the post flair if needed.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/datasets) if you have any questions or concerns.*",1,ug6vxw,Driving me insane. Wtf does CMS spend my tax money on if they can't post a fucking reference document. I had one at my last job but for the life of me can't find my way back to that shit hole website,datasets,2022-05-01 12:05:33,4
American Community Survey (ACS) tables [B24122](https://data.census.gov/cedsci/table?q=b24122&tid=ACSDT1Y2019.B24122) and [B24123](https://data.census.gov/cedsci/table?q=b24123&tid=ACSDT1Y2019.B24123) (median earnings for men) and women by detailed occupation.  Near the upper left you can change the year.  I would recommend you use the 1-year data as this table is only available at the national level and you run into issues if you use overlapping 5-year data.  You will have to calculate the wage gap after downloading the data.,10,ufmro5,"I have looked on multiple websites like Kaggle, [data.gov](https://data.gov) us, US census bureau and many others but haven't found what I need or a lot of websites I looked at just have the data for purchases and this is just a school project and some of that data is worth like $20 for just 48 hours so I can't get those, or you need to be in some specific universities to get to it. Does anyone know any good datasets or have access to them that you could let me use it for my data science project?",datasets,2022-04-30 16:26:55,9
Try here https://www.bp.com/en/global/corporate/energy-economics/statistical-review-of-world-energy.html,1,ufw2az,I can't seem to find any datasets regarding total U.S energy consumption and types. Please help.,datasets,2022-05-01 02:12:53,1
"Hey killMontag,

I believe a `question` or `discussion` flair might be more appropriate for such post. Please re-consider and change the post flair if needed.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/datasets) if you have any questions or concerns.*",1,ufhqza,"I’m looking for datasets on Rugby players, mainly their physical, mental and technical attributes with their playing positions. This is to see how their physical, mental and technical attributes determine the position they play. 

Appreciate if anyone could help me out!",datasets,2022-04-30 12:06:10,3
[deleted],1,ufmdg6,"I had an idea to look at the data for the amount of diagnosis’ pre Covid and post Covid.  Maybe create some sort of visual notating the differences, if there is one.

I am turning to you guys as I have seen some remarkable datasets mentioned.",datasets,2022-04-30 16:05:49,1
"Datasets aren’t typically specific to any particular database flavor. If they are, that’s usually a fail on the part of whoever created the dataset.",3,uf8b06,"Hi!
I am currently working on my bachelor's in which I compare SQL syntax in MySQL, MSSQL and Oracle - optionally in Postgres as well.

I worked with MySQL in college and am currently working with MSSQL at my job and they are very similar and easy to use. I am having more trouble with Oracle because I just don't know how to set it up - but I'll leave that for some other subreddit.

For now I am searching for a dataset that would work for all of these with the setup easy enough for me to understand and actually make it work. I did use Docker in the past but it was setup by someone else, so I know very little about it. If any of you could point me to the right direction or suggest anything I will be forever grateful - about datasets or the whole idea how to start and what would be best practice.

Thanks in advance!",datasets,2022-04-30 03:21:49,7
"Yeah this is out there for sure!

[https://data.humdata.org/dataset/wfp-food-prices](https://data.humdata.org/dataset/wfp-food-prices)

Here's a place to start.",12,uekyqc,"is it possible to get the global average price for certain foods? for example i have recently decided to go on a keto vegan diet and i don't have unlimited income, i want to see the average price for items like nuts, fruits and vegetables. i did a few quick googles and i found articles that are representing some data about food, is it possible to get access to the data itself?",datasets,2022-04-29 05:36:01,5
"I dont know if there is a dataset for this but there are tools for OCR training data generation. It wouldn't be hard to generate.

See here
https://github.com/Belval/TextRecognitionDataGenerator",2,ueffyy,"I found the MNIST dataset but it doesn’t have basic mathematical operators. Along with the digits, I need the four arithmetic operators, the decimal point and parentheses. Is there a good dataset that contains all the operators and digits?",datasets,2022-04-28 23:09:13,2
"[https://www.kaggle.com/datasets/theriley106/valid-addresses-by-us-zip-code](https://www.kaggle.com/datasets/theriley106/valid-addresses-by-us-zip-code)

Does that work?

EDIT: Noticing now that it is not every US zip code, seems like most of them though.

Here is a list of AT&T stores nationally:

https://www.att.com/Common/merger/files/pdf/Store\_List.pdf",3,uem80b,"Trying to determine serviceability by zip code for a project, but cannot run it off zip code alone--i need a physical address to determine if that zip code is serviceable.


I was thinking that perhaps a list of the physical addresses of post offices would be a good proxy, since there's probably one in just about every zip code, but anything else that may be available would be fine. It could also just be random addresses, provided that it's a real address in that zip code.


Let me know if anyone has anything like this!",datasets,2022-04-29 06:43:15,1
Scrape SEC data for oil and gas company financial reports.  Wildly inconsistent.,20,ue4t8x,"I frequently read that a lot of time in data related jobs is spent cleaning data. I am looking for datasets to get some good practice cleaning data with but it seems like a lot of the ones at common places (Kaggle, [data.gov](https://data.gov), etc.) are already pretty cleaned up or just not that large.

Does anyone have any recommendations for where to look? Will scraping my own be my best bet? Would also be a bonus if they were financially related since that is my background.

Thanks!

EDIT: Thanks for all the help everyone! Great community",datasets,2022-04-28 13:46:47,13
"Hey Ancgate,

I believe a `request` flair might be more appropriate for such post. Please re-consider and change the post flair if needed.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/datasets) if you have any questions or concerns.*",1,uecavj,I know I can look at the SEC website and some other web site to find the financial statement of some public companies. But how about the Small Business Companies. Where Can I find their financials statements? or earnings they have reported?,datasets,2022-04-28 20:00:22,2
"Hey bobbyelliottuk,

I believe a `request` flair might be more appropriate for such post. Please re-consider and change the post flair if needed.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/datasets) if you have any questions or concerns.*",1,udqlxs,"Sorry for asking since I know it's probably been asked before, but I'm teaching an introductory data course and I'd like to know useful sources of data that the learners can practice with. Ideally, datasets that they can download as CSV files.

I'm simply looking for interesting datasets not Javascript or anything like that.

I know about Kaggle but are there others?",datasets,2022-04-28 01:53:37,13
"Hey guys,   
My friend created a project how to quickly develop TinyML models to recognize custom, user-defined drawn gestures on touch interfaces, embedded in low-power MCUs!  


What do you think about it?",1,uds9zw,,datasets,2022-04-28 03:55:23,2
"That's a nice initiative! I wonder, since there is so many conferences, does that really leaves us time to code?
Also, was there a ""theme"" for the projects last year?",1,udkhbf," 

Hey guys! I’m excited to share with you an exciting upcoming hackathon, High Tech Hacks 2.0! High Tech Hacks is a free, international 24-hour hackathon on May 21-22nd, 2022 open to all high schoolers hoping to learn a new coding skill, compete for awesome prizes, or work with other like-minded hackers. Let’s invent, create, and push the boundaries of technology (as much as we can at one hackathon)!

What to expect:

* Last year, participants learned the basics of web development, Python, virtual reality, and how to make a Discord bot from current software engineers at Microsoft, Amazon, Twilio, other tech companies, and Columbia University SHPE.
* Thanks to our company sponsors, each participant last year received nearly $400 worth of free software and swag.
* Register to earn FREE swag (t-shirts, water bottles, stickers!)
* Network with other passionate STEM high school students from around the world! (Last year we had participants from 26 countries signed up already!)

This year we have even bigger prizes, competitions, and speakers so stay tuned!

Reach out to me with more questions or email [hightechhackathon@gmail.com](mailto:hightechhackathon@gmail.com). Happy hacking! :D

Sign up here to confirm your interest and get on our mailing list: [Click Here to Register](https://docs.google.com/forms/d/e/1FAIpQLSeuEUvQkLy-SOAE1wsi-wnPp4MQalNeirkwFXGGnmOCd3ZM8A/viewform?usp=sf_link)!

Also, meet other hackers by [Joining our Discord!](https://discord.gg/3SVxpAcpc4)

For more, [Check out our Website](https://www.hightechhacks.com/)",datasets,2022-04-27 19:27:11,1
Usually the data from my client's process is behind a wall of impenetrable red tape.,6,udcl3y,Either by their unavailability or by them requiring a significant number of labelers to get them out the door.,datasets,2022-04-27 13:08:35,10
"- 1032 scanned models in the dataset
- Mostly common household items
- Lots of pots/bowls, shoes, and product boxes

 https://app.ignitionrobotics.org/GoogleResearch/fuel/collections/Google%20Scanned%20Objects",5,ud0dnz,,datasets,2022-04-27 03:27:46,2
"Futbolin is the spanish word, in english is foosball",2,ud0znn,"Im trying to do a good dataset for my futbolin matches with my friends. To put you in context, futbolin is a table football game played by 4 players (2vs2), and each player controls 2 bars of players (you can google it to understand it better), in each team one player plays in the front (attacking) and the other on the back (defending).

So, my intention is to gather data of all the matches we play, and i thought of filling up a form that will contain the following information: date of the game, name of the player and position (attack or defense), goals scored for each player and end score of the match.

If possible, id like to be able later to resume info such as: how many goals scored each player and in wich position was the player playing when scoring (and the total), how many games were played each day (this variable can be in date format i think), how many games won or lose each player... 

My questions are: how do i code this into variables? Each player should be a case or a variable? Im a bit lost thinking how do i create the dataset with the info i gather.

All help will be welcomed, and ofc, if you think about new data to gather that could be interesting to foresee let me know!

Thanks in advance!",datasets,2022-04-27 04:08:01,3
Did you see https://wonder.cdc.gov/?,2,ucn51f,I am working on a forecasting project and I desperately need to find time series data about health in the United States... I tried the CDC website but most of it is not time series or not complete. Any advice would be appreciated.,datasets,2022-04-26 14:23:45,1
What do you mean by 2 response variables? Multi class Classification?,1,ucu1rl,"For my class project, I need to work on a dataset that has the requirements mentioned above. Do you guys have any suggestions? TIA!",datasets,2022-04-26 20:18:24,2
For which country/area?,1,ucx5l6,Where can i find raw data or even the final plot will work.,datasets,2022-04-26 23:31:51,4
"Hey Ordinary_Magician_22,

I believe a `question` or `discussion` flair might be more appropriate for such post. Please re-consider and change the post flair if needed.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/datasets) if you have any questions or concerns.*",1,uc1had,Anyone know where to find all the texts…not just the handful released in news stories?,datasets,2022-04-25 18:53:21,5
National survey of principals and teachers should have that data,1,uch1gt,"Teacher Shortages--correlate staffing shortages

How do we measure teacher shortages? Turnover Rate?

I went ahead and pasted some notes I took. My team and I are interested in teacher turnover rate/shortages. We have other measurables that are readily available that we could use to find possible correlation with teacher turnover. But we are not sure where we can find this information.

Our state education agency may have something we could use but they usually put out the info a year at a time. We want to possibly capture as most recent as possible. Hs anyone used this kind of information before?

&#x200B;

Edit: Let me expand. Is there a way to get recent data? ",datasets,2022-04-26 09:50:37,2
"We built a free one about 6 months ago:

[https://www.dolthub.com/repositories/dolthub/us-businesses](https://www.dolthub.com/repositories/dolthub/us-businesses)",5,ubpjbl,"TL;DR

Does someone knows where this information can be found for free?

[www.businesslistdatabase.com](https://www.businesslistdatabase.com)

&#x200B;

&#x200B;

Hi everyone, hope you're doing good.

I have spent a couple of hours trying to find a database with my specific needs, mainly to correlate an already existing database with Leads for my company, I want to save time searching for contact information of each business so I thought of making a relationship with a business list database, first I thought of Yelp database but it has a License of academic use and also has only part of the information available.

  
I've found the website [www.businesslistdatabase.com](https://www.businesslistdatabase.com) which has all the information I need for $300 but I am sure this could be found for free, just don't know where.",datasets,2022-04-25 09:45:53,2
"Hey Beautiful_Location97,

I believe a `request` flair might be more appropriate for such post. Please re-consider and change the post flair if needed.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/datasets) if you have any questions or concerns.*",1,ubob4l,"I am working on project and i need access to a data set with heart rate and spo2 with warning like in the chartevents table in the dataset, but i was working on the demo data set until i've decided to work on the full dataset but the access is so hard my college is not an institution verified in CITI  and the course is so expensive, so is there any way to access the data without the extra steps or a similar dataset that i can use",datasets,2022-04-25 08:53:03,5
"Spyfu.com and neilpatel.com, these website both have apis you can scrape from but neilpatel requires captcha solver",2,ubmb51,"I would like to find a dataset, or even better an API which would take in website URLs as input (eg [bmw.com](https://bmw.com)) and output a set of search phrases which drive most traffic to this site (bmw x5, german luxury autos, ...)

Has anyone come across such a dataset/API?",datasets,2022-04-25 07:22:07,2
Try character-level simplebooks-92 and text8.,1,ubmzcw,I'm looking for any NLP corpora with small vocabulary sizes. I've tried looking to see if there's some index of children's books or something but anything I've found also tends to be much too short (i.e. kids' books with small vocabularies also tend to be very short!). Anyone able to point me to such a thing  would be great!,datasets,2022-04-25 07:54:43,3
"I don’t know of any free, pre-aggregated dataset for you. And what you’re asking for sounds valuable so I don’t expect you to find it. 

Some helpful data is out there for free - Publicly traded companies will have financial statements to show growth and revenue. Among some other tidbits of info about their risks, maybe emerging markets. Things shareholders care about. All companies will have a list of locations on their website. That’s about all I think you will find for free without a good bit more manual work. Companies like Nielsen may be able to sell you data that’s helpful about those respective retailers. 

This project doesn’t sound like you were setup for success on 0 budget, or is hopefully a very directional level of guidance/support to decision makers at your company.",2,ubkq7r,"Hi, I've been tasked with doing market research on Home Improvement Stores(Ace,Home Depot, Lowe's, etc), Department Stores, and eCommerce channels regarding hardware sales to draw a conclusion on where to focus our efforts in the US.  
Datas such as revenue, market share, growth, customer satisfaction, etc for each category in general and within a specific industry (safes and secure cabinets in this case).  
I'm kinda overwhelmed cause I have no experience doing this kind of research and under pressure, because they gave me 2 full weeks of doing only this.  
A lot of business reports and datasets seem to be not free and I wasn't given any budget to work with.  
If anyone can point me to the right direction, I'd be extremely grateful.  
Thank you!",datasets,2022-04-25 06:06:12,2
https://archive.org/download/reddit-place-2022-datasets,7,uaqfb4,So the dataset files were deleted 2 days ago. And I had no idea it would happen because 2017 files were still available. I haven't downloaded all the files on my s3 yet. I would appreciate any info on an s3 bucket from a project or a mirrored torrents archive.,datasets,2022-04-24 01:45:17,4
"Hey Proud_Sheepherder243,

I believe a `request` flair might be more appropriate for such post. Please re-consider and change the post flair if needed.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/datasets) if you have any questions or concerns.*",1,uaikb9,"Hello everyone,

I am looking for an open data that contains the average salary per job. It would be great if the data is available via an open API but not necessary. I saw that glassdoor has an API but i don't think that this data is available. I am trying to avoid scrapping if possible. I am also interested in data for the current hiring/ tension on each Job (exemple : if there is a lot of demand currently for a certain job).

Thanks in advance :)",datasets,2022-04-23 17:22:30,9
https://www.imageannotation.ai/fruits-dataset,5,ua7yvk,"I've searched for a good dataset for a good while now but all I find is the fruits-360 dataset and fat dataset is really really bad. Really pisses me off that the first 20 or so results are projects that work with that dataset.

Edit: After hours of gruelling research I stumbled upon a website that provided me respite: https://images.cv/category",datasets,2022-04-23 08:32:59,7
"Guessing that you've already collected photos from Kaggle?

https://www.kaggle.com/code/pradeepjina/ukraine-war-images-search/data?select=ukraine_war",1,u9us4a,I'm looking for datasets related to Ukraine War. Specially image datasets. The max. I have found till now is 2000 images. Is there any bigger and reliable repository than that?,datasets,2022-04-22 19:06:56,2
"You are looking for NOAA.

[https://www.ncei.noaa.gov/access/search/data-search/daily-summaries](https://www.ncei.noaa.gov/access/search/data-search/daily-summaries)

On DoltHub, we have an import and cleaning that might be easier to use:

[https://www.dolthub.com/repositories/dolthub/noaa](https://www.dolthub.com/repositories/dolthub/noaa)

Here's a blog about it:

https://www.dolthub.com/blog/2020-03-02-noaa-global-hourly-surface-data/

DISCLAIMER: I'm the CEO of DoltHub so this is sort of self promotion.",8,u9gmyf,"Hi all - I am doing a school project and I need help finding daily weather (temperature, precipitation, humidity - preferably) over *x* number of years for a specific latitude longitude point. I understand this would be quit a large data set but I am having trouble finding any with ease. 

What I have found is that I need to go day-by-day and manually record data from websites or the bigger files come in weird formats that I have not been able to figure out how to use. 

Ideally (and potentially unrealistically) someone has a .csv or .xlsx or something similar for daily weather stats over 10ish years. Anybody know where I can find something like this? Any adivce is appreciated. 

I know there are also weather API's but I am having trouble finding any free ones

Thank you in advanced!",datasets,2022-04-22 07:58:55,9
"[https://en.wikipedia.org/wiki/ISO\_3166-2:GB](https://en.wikipedia.org/wiki/ISO_3166-2:GB)

They do, not sure this question belongs here, looks like it belongs in a Google query box.",10,u9wayx,Thought this belonged here.,datasets,2022-04-22 20:32:01,6
"Sure!

Edit; I’d suggest a “no preference” for the seasons and a “preferably not” for adult/elderly to give everyone some options",1,u9lt8t,,datasets,2022-04-22 11:52:26,1
"DISCLAIMER: I'm the CEO of DoltHub

We have a SQL database of US home sales with \~90M sales:

[https://www.dolthub.com/repositories/dolthub/us-housing-prices](https://www.dolthub.com/repositories/dolthub/us-housing-prices)

You can match the owner of a property using a query like:

`select * from sales where buyer=<buyer>`

You can find the owner of your property with something like:

`select * from sales where physical_address=<address> and city=<city> and zip5=<zip>`",2,u8uvf1,,datasets,2022-04-21 12:00:31,2
[deleted],2,u7zhqa,,datasets,2022-04-20 08:46:46,2
Find research papers and email the authors for their data.,1,u8ccmy,"Sun exposure and cancer have a strong relationship, research confirmed. However, I cannot find any public dataset about it",datasets,2022-04-20 18:51:38,1
[deleted],2,u81vtj,,datasets,2022-04-20 10:34:32,1
Is this even useful without grooming it?,0,u7pezm,"
[GitHub](https://github.com/chanzuckerberg/ChemDisGene)",datasets,2022-04-19 22:42:43,2
It’s not that straight forward. Check page 17. https://www.waterfootprint.org/media/downloads/Report47-WaterFootprintCrops-Vol1.pdf,2,u7zqxn,"Hello everyone,  


I'm working on a project based on environmental food sustainability. I'm searching for a dataset containing a list of food with:

1. CO₂ Emissions
2. Land consumption (production process)
3. Water consumption (production process)

I've already searched through both ""FAO"" ( Food and Agriculture Organization)  and ""Our World in Data"" datasets, but I only managed to get data from 2010, while other year's data seems to be missing.",datasets,2022-04-20 08:58:44,1
"While I don't have a labeled set I can recommend, weather or air quality ones could work.

I don't think they're labeled anywhere, but if you look at the distribution, you should be able to create that with ranges. 

Here's an example for air quality, https://www.epa.gov/outdoor-air-quality-data/download-daily-data",1,u7s1g9,"Hello everyone,

I am searching for a dataset which consists of some data like sensor or network data (e.g. temperature or pressure measured by a sensor) as well as plain text data for any anomalies/events that occurred. In a nutshell, my goal is to combine the sensor data with the text data to create a predictive model which correlates the sensor data with the respective text data.

Thank you in advance!",datasets,2022-04-20 01:52:03,1
"I believe you could generate data with these patterns

However, could you contextualize why you need it? Maybe that would help me or other people to find something similar to what you need",1,u7wj5t,"I'm looking for something like:

20% 20/04/2020 oil",datasets,2022-04-20 06:28:37,2
"I think you might be outgrowing Access and your solution.

I'm not sure your background in terms of programming, but if that's your background, you can build a web app that just handles the input. Then a couple dashboards for reporting.

If that's not your background, there actually are a couple of ""no code"" platforms out there that can do this. I'm actually looking into Budibase, https://budibase.com.

It has a free version with some limitations on the website, but they also have a self hosted one. I'm currently evaluating to see if it would work for some uses for us, but I think it might be something interesting. 

There are other options like https://anvil.works/. A bit more powerful, but still requires programming to tie it all together. And if you want to self host, it's a bit of annoying. I haven't tried self hosting budibase though to compare them.

Your other option is using something a CMS. I always lean towards Drupal (PHP can be hosted anywhere, they have good security). All the data gathering can be done. You can export the gathered data too. Just not sure on what reporting you need and if it has to be done within your new platform.",1,u7vu7p,"Currently, I use an Access DB with tables that pull certain information (for example - when I enter an item number, it automatically populates the description for that item, the rate is calculated when a report is ran, etc.).  I simply open a blank table, choose the date and shift, and begin entering the data.

We want to make this process easier.  Faster.  Less work, basically.  We would like to automate as much of it as possible, if possible. 

I'm not even necessarily against the idea of continuing to use Access.  I just think there has to be a better solution here.  Is there any software that can dump user-input information into the Access DB?  As in, some kind of program that would allow line leads to enter their own information into a form that is user friendly, which would then populate the Access DB?  

Is there some other solution?  We are also open to completely revamping the way we do it.  We just aren't sure where to start or what to look for.",datasets,2022-04-20 05:54:09,1
"There's no mention of csv in the indexed filetypes on google:

https://support.google.com/webmasters/answer/35287?hl=en",2,u7wcg7,I am looking for a csv dataset on google and used filetype:csv but then get zero returns regardless of what I type next. Can someone help with this?,datasets,2022-04-20 06:19:14,1
"Mapbox is pretty easy out of the box and has some powerful features if you're willing to get your hands dirty. 

You can start by just uploading a csv and it will plot the data into a map. Note that results need to be geocoded which you can do for free using a Google sheets geocoding service or just Google one.

Good luck",5,u6th4j,"Hi!

I’ve been trying to visualize a data set with a list of towns in a state associated with a subset of numbers. 

I’ve tried with using the map graph that Excel provides, but it’s just not loading. 

Any ideas on what sites or programs I can use to visualize the data set are welcome.",datasets,2022-04-18 18:44:51,7
"Hey Stuckatpennstation,

I believe a `request` flair might be more appropriate for such post. Please re-consider and change the post flair if needed.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/datasets) if you have any questions or concerns.*",1,u6ijnk,"Hey y'all, starting to get ready to apply for jobs soon and I wanted to practice mining and cleaning data. Can anyone point in me a direction where I can access data that has not yet been cleaned and organized? The data can be of any subject. Thank you.",datasets,2022-04-18 10:28:30,10
Will Reddit comments do? [This might be up your alley.](https://socialgrep.com/datasets/reddit-r-nonewnormal-dataset),2,u6f8k5,"Hi guys, do you know some datasets containing tweets about vaccine, possibly labeled according to their date and their content (misinformation/fake news/real  news, or even pro-vaccine/anti-vaccine etc.), and possibly data about retweets?",datasets,2022-04-18 08:02:37,4
"A number of commercial websites (as well as .mil) have lookup tools for BAH. So, presume the dataset is readily available.

Here's everything about how they COLLECT the data every year, might be helpful to understand the process, and maybe a source burried in there somewhere...

(Sorry for the damn Google link, it's a recent Google ""improvement""? Would love to know how to tell Firefox to just go directly to the site instead of through Google when the search result is a PDF...)


[BAH Data Collection Guide - army.mil](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&ved=2ahUKEwi7__ec0qD3AhWJKEQIHR9AB-0QFnoECCIQAQ&url=https%3A%2F%2Fdcsg9.army.mil%2Finstallationservices%2Fassets%2Fdocs%2F2019%2520BAH%2520Data%2520Collection%2520Process%2520Guide.pdf&usg=AOvVaw3XdUqijxt-cutPwifoQvpb)

You might also poke around on homes.mil, might be some pointers there, perhaps a place to download a dataset. You will need to establish a login.gov account for full access.

I've only used the online lookups. (To help somebody make rental listings.)",2,u6hf20,"Or just BAH rates would be fine. Thanks!

BAH = Military Basic Allowance for Housing",datasets,2022-04-18 09:37:46,1
"Go try ESRI free software. Trial license. 

GIS is just a backend DB with a nifty graphical interface.",2,u6ux04,"Help me in shortlisting
1. Free mapping tool.
2. Saving all the locations in the form of CSV.
3. Connecting api of that mapping tool with website.",datasets,2022-04-18 19:57:59,1
[Cost of Living Database (CLD) — Atlanta Fed](https://www.atlantafed.org/economic-mobility-and-resilience/advancing-careers-for-low-income-families/cost-of-living-database),5,u5vib0,Looking for data on cost of living index by CBSA in the USA. Would you know where I could find this?,datasets,2022-04-17 13:30:13,3
"Bruno, Thank you for your dedication to this project, it’s very touching. I have an occupational therapist friend I will mention this to and ask if she has any experience with MS treatments.",2,u4nuup,,datasets,2022-04-15 19:26:40,4
"[Table B05006 from the American Community Survey](https://data.census.gov/cedsci/table?q=place%20of%20birth&tid=ACSDT5Y2020.B05006) maybe what you are looking for.  This is for the nation.  If you want zipcodes, click on ""filter"" on the left hand side, then ""geography"".  A bunch of boxes should show up, one of them is ""Zip Code Tabulation Area"".  Click on it and choose the box at the top of the list for all zip codes.  Then click on the double arrow on the far right.  It will tell you that there are too many geographies, but you can download it.  [This is what I got](https://data.census.gov/cedsci/table?q=place%20of%20birth&g=0100000US%248600000) for all zip codes without downloading anything.  Hopefully it works.

No silver badge needed.  :-)",3,u51i9q,"Hi everyone, hope all of you are doing well;

I came across a direct request to find a Country-specific origin distribution of people by Zip codes in the United States. I had a dataset from 2010 I think which showed just this, from all countries of the world, but I do not know where this dataset came from.  


If the whole world countries are now available, only the Hispanic origin would be great.  


Thank you all for your time!",datasets,2022-04-16 09:32:26,3
Damn how is Denver not on here? In a housing search right now and it’s absolutely insane.,8,u3lzi9,,datasets,2022-04-14 09:54:45,13
"Pretty obvious guesses, at least for pointers: Scripps Institute, Monterey Aquarium.

Monterey publishes a widely-circulated grading of sustainability by species and catch location. I’d imagine they need to use the same data you are looking for.",1,u3xq9y,"I  want to do some research on overfishing so I'm wondering, does anyone have an datasets on the populations of different species of fish in different parts of the ocean over the last few decades?",datasets,2022-04-14 19:25:09,3
"[everypolitician.org](https://everypolitician.org) [https://github.com/unitedstates/congress-legislators](https://github.com/unitedstates/congress-legislators) is your best bet. Not sure how up to date it is, but the project is still being updated which is a good sign.",1,u3q8pg,"I am looking for a dataset that covers the information about historical and current congress members (senate and house), such as party, gender, etc. Thanks.",datasets,2022-04-14 13:12:22,2
"Hey insanelylogical,

I believe a `request` flair might be more appropriate for such post. Please re-consider and change the post flair if needed.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/datasets) if you have any questions or concerns.*",1,u384ok,"Basically as title says. For example I found that the 1964 Civil Rights Act was voted in with

63% of house democrats and 80% of house republicans voting yes, and 

69% senate democrats and 82% of senate republicans voting yes.",datasets,2022-04-13 20:33:44,2
"The link you provide shows very fat:

Source: International Monetary Fund

Which has a statistical database for online exploration, export and even an API.
Take it directly from there instead of indexmundi.",2,u3f37p,"[https://www.indexmundi.com/commodities/?commodity=energy-price-index&months=240](https://www.indexmundi.com/commodities/?commodity=energy-price-index&months=240) 

here is the link to fuel monthly price index, I was wondering if it's reliable to use this on my thesis? Or, where can I access the original source?",datasets,2022-04-14 04:15:22,1
"Hey Teosage,

I believe a `request` flair might be more appropriate for such post. Please re-consider and change the post flair if needed.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/datasets) if you have any questions or concerns.*",1,u3f7c0,"Hi everyone,

I am a postgraduate student. I am in dire need of dataset with at least a 1,000 rows that can be used to review financial and industry performance of a company.

&#x200B;

&#x200B;

thank you",datasets,2022-04-14 04:22:05,3
Let me know if you find anything,2,u3554z,"To make a long story short, I am trying to build a public tableau portfolio for my resume. Currently, one of my big hyper-fixations is the game 'Genshin Impact'. I figured focusing on the game's details will make the tableau portfolio build more of a passion project. 

Despite the game's popularity, I am struggling to find free datasets regarding revenue, characters, the character runs, active users, etc. I have looked at Kaggle and Google's dataset search. I have seen personal projects regarding the game before, but the owners of said data graphs said they obtained the information from the China Market app store. Unfortunately, I lack the knowledge to follow that same route. 

Any recommendations are appreciated. I am not opposed to paying for data through Statista. I am currently in lay-off status and would prefer to exhaust my free options first.",datasets,2022-04-13 17:53:39,1
"Hey! Not sure if there’s data on the suspect’s residency, but here’s a dataset on crime: https://databar.ai/source/20

Full disclosure I’m one of the founders of databar :)",5,u2xz9m,"In light of the recent shooting tragedy in NYC, I am curious to find out more about the relationship between crime location and suspect residency. For example: which US cities import the most crime from non-residents? Which cities export the most crime? etc. 

Not able to find anything on the public crime reporting datasets but curious if anyone here has thoughts.",datasets,2022-04-13 12:15:31,3
"Oh wow ! 👏👏👏👏
Where did you get training data from ?",1,u2tfyj,"Hi, all. I wrote a python package to automatically do schema matching on csv, json and jsonl files!

Here is the package: [https://github.com/fireindark707/Python-Schema-Matching](https://github.com/fireindark707/Python-Schema-Matching)

You can use it easily:

    pip install schema-matching
    
    from schema_matching import schema_matching
    
    df_pred,df_pred_labels,predicted_pairs = schema_matching(""Test Data/QA/Table1.json"",""Test Data/QA/Table2.json"")

This tool uses XGboost and sentence-transformers to perform schema matching task on tables. Support multi-language column names and instances matching and can be used without column names!

If you have a large number of tables or relational databases to merge, I think this is a great tool to use.

Inference on Test Data (Give confusing column names)

Data: [https://github.com/fireindark707/Schema\_Matching\_XGboost/tree/main/Test%20Data/self](https://github.com/fireindark707/Schema_Matching_XGboost/tree/main/Test%20Data/self)

||title|text|summary|keywords|url|country|language|domain|name|timestamp|
|:-|:-|:-|:-|:-|:-|:-|:-|:-|:-|:-|
|col1|1(FN)|0|0|0|0|0|0|0|0|0|
|col2|0|1(TP)|0|0|0|0|0|0|0|0|
|col3|0|0|1(TP)|0|0|0|0|0|0|0|
|words|0|0|0|1(TP)|0|0|0|0|0|0|
|link|0|0|0|0|1(TP)|0|0|0|0|0|
|col6|0|0|0|0|0|1(TP)|0|0|0|0|
|lang|0|0|0|0|0|0|1(TP)|0|0|0|
|col8|0|0|0|0|0|0|0|1(TP)|0|0|
|website|0|0|0|0|0|0|0|0|0(FN)|0|
|col10|0|0|0|0|0|0|0|0|0|1(TP)|

**F1 score: 0.889**",datasets,2022-04-13 08:52:34,6
"HUD has crosswalks between zip codes and census tracts,  and zip codes and counties. Not sure about municipalities, but tracts are pretty small.

https://www.huduser.gov/portal/datasets/usps_crosswalk.html",8,u2i3u5,"Hi guys, is there any free way to get the dataset with US information about States, Zip codes, Cities ?

any recommendation about good free sites for this ?

thanks in advance",datasets,2022-04-12 21:30:03,13
"Export to CSV then you can probably find tools to convert to .gpx

Maybe you need to learn a little SQL to get only the columns you need.

Useful tool: https://sqlitebrowser.org/

My go-to for adhoc twiddling with SQLite",1,u2v71r,"I went to bike ride and was recording the ride using proprietary maps app. After getting back home, the app failed saving the ride so I ended up extracting the unsaved data using ADB. Now I have .sqlite database file with coordinates + timestamps and I have no idea how to convert it to univeral GPX file that can be videwed or imported into any tracker. Any suggestions?",datasets,2022-04-13 10:10:12,1
"OpenWorm has this sort of data at several levels of granularity, including structural neuroml. It's not great to work with but maybe you'll be able to figure it out.

Note that the C. elegans connectome isn't as complete as it's advertised - the best datasets in adult are, afaik, still a Frankensteining of different animals. Even the Emmons paper comparing sexes ended up grafting two sexes' data together to get a complete wiring diagram.

EDIT: here is the morphology, of the axons at least https://github.com/openworm/c302/tree/master/c302/NeuroML2",2,u2qthw,"I'm looking for 3d neural data that shows the thickness as well as which neuron is connected to which one. 

I only found their connectome diagrams, but no luck with morphological data. 

Has anyone come across it?",datasets,2022-04-13 06:52:31,3
I’m interested but the sign up requires a 500wd narrative. Is the this hackathon hard to enter? I’m just an individual.,1,u22w3g,"Unique opportunity to explore real sensor and time-series data from 200+ New York buildings. In addition, you can compete for cash awards. The data set comes from building automation systems and connected devices, utility meters, elevators and IoT sensors.

The challenge is open to individuals and teams; students, professors, researchers and professionals. The event seeks novel use cases for the provided data set; encourages creativity and data fusion. Ideal submissions are those use cases that positively impact or accelerate the decarbonization of New York State buildings.

**Sign Up by April 15th.**

**Access given April 22nd.** 

**Submission by May 30th.**

**Full Event Details:** [www.rtemhackathon.com](https://www.rtemhackathon.com)

\*will post a free tutorial video series on the API at r/learnmachinelearning and r/learnpython",datasets,2022-04-12 09:18:45,2
I imagine probation/parole officers could get you that information. You might have luck looking at the ACLU website. They work with those type of issues so there is a chance they have a figure for you.,1,u2d0ql,"I'm trying to find information on employment status of felons by the felony committed. I've tried Bureau of Labor and Statistics, Department of Justice, even the FBI data tool. If I'm not supposed to ask this here could someone kindly direct me somewhere that will?",datasets,2022-04-12 17:01:59,2
Why not yfinance ?,8,u1vs0g,"A script that no one asked but is here, just in case, for future internet travelers to see how to scrape Google Finance Ticker data and time-series data using Nasdaq API.  

A gist to the same code below: https://gist.github.com/dimitryzub/a5e30389e13142b9262f52154cd56092


Full code or [code in the online IDE](https://replit.com/@DimitryZub1/Scrape-Google-Finance-Ticker-Quote-in-Python#main.py):

```python
import nasdaqdatalink
import requests, json, re
from parsel import Selector
from itertools import zip_longest

def scrape_google_finance(ticker: str):
    params = {
        ""hl"": ""en"" # language
        }

    headers = {
        ""User-Agent"": ""Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.60 Safari/537.36"",
        }

    html = requests.get(f""https://www.google.com/finance/quote/{ticker}"", params=params, headers=headers, timeout=30)
    selector = Selector(text=html.text)
    
    # where all extracted data will be temporary located
    ticker_data = {
        ""ticker_data"": {},
        ""about_panel"": {},
        ""news"": {""items"": []},
        ""finance_perfomance"": {""table"": []}, 
        ""people_also_search_for"": {""items"": []},
        ""interested_in"": {""items"": []}
    }
    
    # current price, quote, title extraction
    ticker_data[""ticker_data""][""current_price""] = selector.css("".AHmHk .fxKbKc::text"").get()
    ticker_data[""ticker_data""][""quote""] = selector.css("".PdOqHc::text"").get().replace("" • "","":"")
    ticker_data[""ticker_data""][""title""] = selector.css("".zzDege::text"").get()
    
    # about panel extraction
    about_panel_keys = selector.css("".gyFHrc .mfs7Fc::text"").getall()
    about_panel_values = selector.css("".gyFHrc .P6K39c"").xpath(""normalize-space()"").getall()
    
    for key, value in zip_longest(about_panel_keys, about_panel_values):
        key_value = key.lower().replace("" "", ""_"")
        ticker_data[""about_panel""][key_value] = value
    
    # description ""about"" extraction
    ticker_data[""about_panel""][""description""] = selector.css("".bLLb2d::text"").get()
    ticker_data[""about_panel""][""extensions""] = selector.css("".w2tnNd::text"").getall()
    
    # news extarction
    if selector.css("".yY3Lee"").get():
        for index, news in enumerate(selector.css("".yY3Lee""), start=1):
            ticker_data[""news""][""items""].append({
                ""position"": index,
                ""title"": news.css("".Yfwt5::text"").get(),
                ""link"": news.css("".z4rs2b a::attr(href)"").get(),
                ""source"": news.css("".sfyJob::text"").get(),
                ""published"": news.css("".Adak::text"").get(),
                ""thumbnail"": news.css(""img.Z4idke::attr(src)"").get()
            })
    else: 
        ticker_data[""news""][""error""] = f""No news result from a {ticker}.""

    # finance perfomance table
    if selector.css("".slpEwd .roXhBd"").get():
        fin_perf_col_2 = selector.css("".PFjsMe+ .yNnsfe::text"").get()           # e.g. Dec 2021
        fin_perf_col_3 = selector.css("".PFjsMe~ .yNnsfe+ .yNnsfe::text"").get()  # e.g. Year/year change
        
        for fin_perf in selector.css("".slpEwd .roXhBd""):
            if fin_perf.css("".J9Jhg::text , .jU4VAc::text"").get():
                perf_key = fin_perf.css("".J9Jhg::text , .jU4VAc::text"").get()   # e.g. Revenue, Net Income, Operating Income..
                perf_value_col_1 = fin_perf.css("".QXDnM::text"").get()           # 60.3B, 26.40%..   
                perf_value_col_2 = fin_perf.css("".gEUVJe .JwB6zf::text"").get()  # 2.39%, -21.22%..
                
                ticker_data[""finance_perfomance""][""table""].append({
                    perf_key: {
                        fin_perf_col_2: perf_value_col_1,
                        fin_perf_col_3: perf_value_col_2
                    }
                })
    else:
        ticker_data[""finance_perfomance""][""error""] = f""No 'finence perfomance table' for {ticker}.""
    
    # ""you may be interested in"" results
    if selector.css("".HDXgAf .tOzDHb"").get():
        for index, other_interests in enumerate(selector.css("".HDXgAf .tOzDHb""), start=1):
            ticker_data[""interested_in""][""items""].append(discover_more_tickers(index, other_interests))
    else:
        ticker_data[""interested_in""][""error""] = f""No 'you may be interested in` results for {ticker}""
    
    
    # ""people also search for"" results
    if selector.css("".HDXgAf+ div .tOzDHb"").get():
        for index, other_tickers in enumerate(selector.css("".HDXgAf+ div .tOzDHb""), start=1):
            ticker_data[""people_also_search_for""][""items""].append(discover_more_tickers(index, other_tickers))
    else:
        ticker_data[""people_also_search_for""][""error""] = f""No 'people_also_search_for` in results for {ticker}""
        

    return ticker_data


def discover_more_tickers(index: int, other_data: str):
    """"""
    if price_change_formatted will start complaining,
    check beforehand for None values with try/except and set it to 0, in this function.
    
    however, re.search(r""\d{1}%|\d{1,10}\.\d{1,2}%"" should make the job done.
    """"""
    return {
            ""position"": index,
            ""ticker"": other_data.css("".COaKTb::text"").get(),
            ""ticker_link"": f'https://www.google.com/finance{other_data.attrib[""href""].replace(""./"", ""/"")}',
            ""title"": other_data.css("".RwFyvf::text"").get(),
            ""price"": other_data.css("".YMlKec::text"").get(),
            ""price_change"": other_data.css(""[jsname=Fe7oBc]::attr(aria-label)"").get(),
            # https://regex101.com/r/BOFBlt/1
            # Up by 100.99% -> 100.99%
            ""price_change_formatted"": re.search(r""\d{1}%|\d{1,10}\.\d{1,2}%"", other_data.css(""[jsname=Fe7oBc]::attr(aria-label)"").get()).group()
        }


scrape_google_finance(ticker=""GOOGL:NASDAQ"")
```

Outputs:

```json
{
  ""ticker_data"": {
    ""current_price"": ""$2,665.75"",
    ""quote"": ""GOOGL:NASDAQ"",
    ""title"": ""Alphabet Inc Class A""
  },
  ""about_panel"": {
    ""previous_close"": ""$2,717.77"",
    ""day_range"": ""$2,659.31 - $2,713.40"",
    ""year_range"": ""$2,193.62 - $3,030.93"",
    ""market_cap"": ""1.80T USD"",
    ""volume"": ""1.56M"",
    ""p/e_ratio"": ""23.76"",
    ""dividend_yield"": ""-"",
    ""primary_exchange"": ""NASDAQ"",
    ""ceo"": ""Sundar Pichai"",
    ""founded"": ""Oct 2, 2015"",
    ""headquarters"": ""Mountain View, CaliforniaUnited States"",
    ""website"": ""abc.xyz"",
    ""employees"": ""156,500"",
    ""description"": ""Alphabet Inc. is an American multinational technology conglomerate holding company headquartered in Mountain View, California. It was created through a restructuring of Google on October 2, 2015, and became the parent company of Google and several former Google subsidiaries. The two co-founders of Google remained as controlling shareholders, board members, and employees at Alphabet. Alphabet is the world's third-largest technology company by revenue and one of the world's most valuable companies. It is one of the Big Five American information technology companies, alongside Amazon, Apple, Meta and Microsoft.\nThe establishment of Alphabet Inc. was prompted by a desire to make the core Google business \""cleaner and more accountable\"" while allowing greater autonomy to group companies that operate in businesses other than Internet services. Founders Larry Page and Sergey Brin announced their resignation from their executive posts in December 2019, with the CEO role to be filled by Sundar Pichai, also the CEO of Google. Page and Brin remain co-founders, employees, board members, and controlling shareholders of Alphabet Inc. "",
    ""extensions"": [
      ""Stock"",
      ""US listed security"",
      ""US headquartered""
    ]
  },
  ""news"": [
    {
      ""position"": 1,
      ""title"": ""Amazon Splitting Stock, Alphabet Too. Which Joins the Dow First?"",
      ""link"": ""https://www.barrons.com/articles/amazon-stock-split-dow-jones-51646912881?tesla=y"",
      ""source"": ""Barron's"",
      ""published"": ""1 month ago"",
      ""thumbnail"": ""https://encrypted-tbn1.gstatic.com/images?q=tbn:ANd9GcRlf6wb63KP9lMPsOheYDvvANIfevHp17lzZ-Y0d0aQO1-pRCIDX8POXGtZBQk""
    },
    {
      ""position"": 2,
      ""title"": ""Alphabet's quantum tech group Sandbox spins off into an independent company"",
      ""link"": ""https://www.cnbc.com/2022/03/22/alphabets-quantum-tech-group-sandbox-spins-off-into-an-independent-company.html"",
      ""source"": ""CNBC"",
      ""published"": ""2 weeks ago"",
      ""thumbnail"": ""https://encrypted-tbn1.gstatic.com/images?q=tbn:ANd9GcSIyv1WZJgDvwtMW8e3RAs9ImXtTZSmo2rfmCKIASk4B_XofZfZ8AbDLAMolhk""
    },
    {
      ""position"": 3,
      ""title"": ""Cash-Rich Berkshire Hathaway, Apple, and Alphabet Should Gain From Higher \nRates"",
      ""link"": ""https://www.barrons.com/articles/cash-rich-berkshire-hathaway-apple-and-alphabet-should-gain-from-higher-rates-51647614268"",
      ""source"": ""Barron's"",
      ""published"": ""3 weeks ago"",
      ""thumbnail"": ""https://encrypted-tbn3.gstatic.com/images?q=tbn:ANd9GcSZ6dJ9h9vXlKrWlTmHiHxlfYVbViP5DAr9a_xV4LhNUOaNS01RuPmt-5sjh4c""
    },
    {
      ""position"": 4,
      ""title"": ""Amazon's Stock Split Follows Alphabet's. Here's Who's Next."",
      ""link"": ""https://www.barrons.com/articles/amazon-stock-split-who-next-51646944161"",
      ""source"": ""Barron's"",
      ""published"": ""1 month ago"",
      ""thumbnail"": ""https://encrypted-tbn1.gstatic.com/images?q=tbn:ANd9GcSJGKk2i1kLT_YToKJlJnhWaaj_ujLvhhZ5Obw_suZcu_YyaDD6O_Llsm1aqt8""
    },
    {
      ""position"": 5,
      ""title"": ""Amazon, Alphabet, and 8 Other Beaten-Up Growth Stocks Set to Soar"",
      ""link"": ""https://www.barrons.com/articles/amazon-stock-growth-buy-51647372422"",
      ""source"": ""Barron's"",
      ""published"": ""3 weeks ago"",
      ""thumbnail"": ""https://encrypted-tbn2.gstatic.com/images?q=tbn:ANd9GcTxotkd3p81U7xhmCTJ6IO0tMf_yVKv3Z40bafvtp9XCyosyB4WAuX7Qt-t7Ds""
    },
    {
      ""position"": 6,
      ""title"": ""Is It Too Late to Buy Alphabet Stock?"",
      ""link"": ""https://www.fool.com/investing/2022/03/14/is-it-too-late-to-buy-alphabet-stock/"",
      ""source"": ""The Motley Fool"",
      ""published"": ""3 weeks ago"",
      ""thumbnail"": ""https://encrypted-tbn3.gstatic.com/images?q=tbn:ANd9GcQv5D9GFKMNUPvMd91aRvi83p12y91Oau1mh_4FBPj6LCNK3cH1vEZ3_gFU4kI""
    }
  ],
  ""finance_perfomance"": [
    {
      ""Revenue"": {
        ""Dec 2021"": ""75.32B"",
        ""Year/year change"": ""32.39%""
      }
    },
    {
      ""Net income"": {
        ""Dec 2021"": ""20.64B"",
        ""Year/year change"": ""35.56%""
      }
    },
    {
      ""Diluted EPS"": {
        ""Dec 2021"": ""30.69"",
        ""Year/year change"": ""37.62%""
      }
    },
    {
      ""Net profit margin"": {
        ""Dec 2021"": ""27.40%"",
        ""Year/year change"": ""2.39%""
      }
    },
    {
      ""Operating income"": {
        ""Dec 2021"": ""21.88B"",
        ""Year/year change"": ""39.83%""
      }
    },
    {
      ""Net change in cash"": {
        ""Dec 2021"": ""-2.77B"",
        ""Year/year change"": ""-143.78%""
      }
    },
    {
      ""Cash and equivalents"": {
        ""Dec 2021"": ""20.94B"",
        ""Year/year change"": ""-20.86%""
      }
    },
    {
      ""Cost of revenue"": {
        ""Dec 2021"": ""32.99B"",
        ""Year/year change"": ""26.49%""
      }
    }
  ],
  ""people_also_search_for"": [
    {
      ""position"": 1,
      ""ticker"": ""GOOG"",
      ""ticker_link"": ""https://www.google.com/finance/quote/GOOG:NASDAQ"",
      ""title"": ""Alphabet Inc Class C"",
      ""price"": ""$2,680.21"",
      ""price_change"": ""Down by 1.80%"",
      ""price_change_formatted"": ""1.80%""
    }, ... other results
    {
      ""position"": 18,
      ""ticker"": ""SQ"",
      ""ticker_link"": ""https://www.google.com/finance/quote/SQ:NYSE"",
      ""title"": ""Block Inc"",
      ""price"": ""$123.22"",
      ""price_change"": ""Down by 2.15%"",
      ""price_change_formatted"": ""2.15%""
    }
  ],
  ""interested_in"": [
    {
      ""position"": 1,
      ""ticker"": ""Index"",
      ""ticker_link"": ""https://www.google.com/finance/quote/.INX:INDEXSP"",
      ""title"": ""S&P 500"",
      ""price"": ""4,488.28"",
      ""price_change"": ""Down by 0.27%"",
      ""price_change_formatted"": ""0.27%""
    }, ... other results
    {
      ""position"": 18,
      ""ticker"": ""NFLX"",
      ""ticker_link"": ""https://www.google.com/finance/quote/NFLX:NASDAQ"",
      ""title"": ""Netflix Inc"",
      ""price"": ""$355.88"",
      ""price_change"": ""Down by 1.73%"",
      ""price_change_formatted"": ""1.73%""
    }
  ]
}
```

A basic example of retrieving time-series data using Nasdaq API:

```python
import nasdaqdatalink

def nasdaq_get_timeseries_data():
    nasdaqdatalink.read_key(filename="".nasdaq_api_key"")
    # print(nasdaqdatalink.ApiConfig.api_key) # prints api key from the .nasdaq_api_key file

    timeseries_data = nasdaqdatalink.get(""WIKI/GOOGL"", collapse=""monthly"") # not sure what ""WIKI"" stands for
    print(timeseries_data)

nasdaq_get_timeseries_data()
```

Outputs a `pandas` `DataFrame`:

```lang-none
                Open     High      Low    Close      Volume  Ex-Dividend  Split Ratio    Adj. Open    Adj. High     Adj. Low   Adj. Close  Adj. Volume
Date                                                                                                                                                  
2004-08-31   102.320   103.71   102.16   102.37   4917800.0          0.0          1.0    51.318415    52.015567    51.238167    51.343492    4917800.0
2004-09-30   129.899   132.30   129.00   129.60  13758000.0          0.0          1.0    65.150614    66.354831    64.699722    65.000651   13758000.0
2004-10-31   198.870   199.95   190.60   190.64  42282600.0          0.0          1.0    99.742897   100.284569    95.595093    95.615155   42282600.0
2004-11-30   180.700   183.00   180.25   181.98  15384600.0          0.0          1.0    90.629765    91.783326    90.404069    91.271747   15384600.0
2004-12-31   199.230   199.88   192.56   192.79  15321600.0          0.0          1.0    99.923454   100.249460    96.578127    96.693484   15321600.0
...              ...      ...      ...      ...         ...          ...          ...          ...          ...          ...          ...          ...
2017-11-30  1039.940  1044.14  1030.07  1036.17   2190379.0          0.0          1.0  1039.940000  1044.140000  1030.070000  1036.170000    2190379.0
2017-12-31  1055.490  1058.05  1052.70  1053.40   1156357.0          0.0          1.0  1055.490000  1058.050000  1052.700000  1053.400000    1156357.0
2018-01-31  1183.810  1186.32  1172.10  1182.22   1643877.0          0.0          1.0  1183.810000  1186.320000  1172.100000  1182.220000    1643877.0
2018-02-28  1122.000  1127.65  1103.00  1103.92   2431023.0          0.0          1.0  1122.000000  1127.650000  1103.000000  1103.920000    2431023.0
2018-03-31  1063.900  1064.54   997.62  1006.94   2940957.0          0.0          1.0  1063.900000  1064.540000   997.620000  1006.940000    2940957.0

[164 rows x 12 columns]
```

A line-by-line tutorial: https://serpapi.com/blog/scrape-google-finance-ticker-quote-data-in-python/",datasets,2022-04-12 03:24:07,12
Shouldn’t be easy to generate a synthetic dataset like this?,1,u2481i,"Hey there!  
Can anyone help me with searching such datasets? I tried searching on kaggle.com but couldn't find anything similar to my request. It kinda feels that I'm using the wrong keywords for search, so maybe someone can suggest correct ones.  
Here is some examples of what I'm looking for:  
https://i.imgur.com/vNz4tsX.jpg  
https://i.imgur.com/bVLjAcm.jpg  
https://i.imgur.com/57obxYS.jpg",datasets,2022-04-12 10:15:57,2
Link tot the actual dataset? Couldn’t find it.,1,u1ch1b,,datasets,2022-04-11 10:05:20,4
"I would look at the Financial Modelling World Cup website (basically an Excel modelling competition) - should have some worked examples that you can download, with relatively complex formulas.",4,u14hml,"Hi,
I'm trying out an excel alternative that claims basically full compatibility. I want to test that out. I'm not that good with Excel and I don't have any large, complicated files laying around to test out how far I can push the SW.

I've been looking on the internet, but I've fount only up to 1Mb files that contained only data, no formulas or anything - and that's the thing that I want to test. 

I haven't found a better place to ask. If this is not the place, I'm sorry, I don't know where else to ask.

So, do any of you have any xls/xlsx file lying around that you would be able to send me? The larger and more complicated, the better 

Thanks and sorry again, if this is not the right place to ask.",datasets,2022-04-11 03:22:04,4
"Hey Intelligent_Start288,

I believe a `request` flair might be more appropriate for such post. Please re-consider and change the post flair if needed.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/datasets) if you have any questions or concerns.*",1,u1fmhz,I am doing a research project using Stata and I am REALLY struggling to find the right datasets to explore the relationship between two variables. So far I have considered an investment in education on GDP or unemployment UK or EU but all the datasets seem to be very complex as I download them they are all presented in tables in excel and I am not sure how to convert them back into raw data to manipulate it in Stata. Any suggestion on how I can find the right dataset? Or maybe should I consider a different question?,datasets,2022-04-11 12:40:31,2
"You're trying to fit a square peg into a round hole, and they are *already* changing.  Within the last month, my residency became party of a different district.  However, here are some shapefiles which should get you on your way if you can use them: https://catalog.data.gov/en/dataset/tiger-line-shapefile-2019-nation-u-s-116th-congressional-district-national

I would imagine you'll need to perform an intersect with zip polygons.",1,u18y31,"I'm looking for a dataset listing each Congressional District and the zip codes that fall under that Congressional District. I realize there will be some overlap, but for these purposes it should be fine. I'm also aware that we're in the midst of redistricting and this will be changing in a few months. Any help would be appreciated.",datasets,2022-04-11 07:23:50,2
Nothing on Kaggle?,1,u19221,"I am looking for a dataset of corporate conversations (emails, chats etc) for a mini project that I'm working on as part of one of the modules for my Masters. I have already checked out Enron database but it's more personal and less business oriented.
Any help is appreciated.",datasets,2022-04-11 07:29:06,2
What have you tried already?  Have you searched the site for info on accessing their database?  Have you reached out to the site owners?,1,u14ctg,"[https://wigle.net/stats#ssidstats](https://wigle.net/stats#ssidstats)

 Sorry if this is the wrong sub to ask this, I don't know where else to post this other than r/techsupport.",datasets,2022-04-11 03:13:08,3
I've read this 5 times and still don't know what you want. Can you give an example or sentences?,1,u0zrfc,Anyone can help me to get mania -bipolar dataset,datasets,2022-04-10 21:53:05,2
Kaggle.com,2,u0vew2,"Basically, I have to choose a dataset from either FRED, US Bureau of Labor stats, or the census where the goal is to  ""Using multivariate regression, your empirical model should test the relationship between a dependent variable and two or more independent variables in a cross-sectional data setting""

Since this prompt is so vague and I have so much freedom, I don't even know where to begin. I was hoping some of you here would be able to point me in the right direction in terms of what to do. Any advice at all would be appreciated.",datasets,2022-04-10 17:47:14,2
"Hey Chops16,

I believe a `request` flair might be more appropriate for such post. Please re-consider and change the post flair if needed.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/datasets) if you have any questions or concerns.*",1,u0r34s,"I am not sure if this is the correct place to post this, but I thought I'd give it a go. I'm working on my first machine learning project, and I need to collect a big dataset of different handwriting styles relating to maths. I have created a website to make this easier, so I would appreciate if anyone could go to [https://draw.charliekingsland.me](https://draw.charliekingsland.me) and complete it, to help expand my dataset.",datasets,2022-04-10 14:08:08,4
Self promotion is when you link a website you own or work for. Usually to drive traffic to it or SEO etc. This isn't self promotion so you don't need the tag :),3,u0aj94,"I've made five sets of the data, each with a different filtering:

* All
* Government
* Media and Press
* Education
* NGO's and Non-profits

The records look like this:

`BR   28571   UNIVERSIDADE DE SAO PAULO`

I made these so that my web apps could tailor its content for e.g. visitors from educational institutions. I wrote [scripts to do the filtering](https://github.com/public-law/datasets/tree/master/Intergovernmental/Internet/scripts), which are in a sub-directory. The pattern matching leans towards being very accepting. And so, there are a few false positives in each file which don't belong.

https://github.com/public-law/datasets#internet-asns-country-codes-names-and-descriptions",datasets,2022-04-09 22:01:18,8
Probably need to comtact the UK met office. ERA5 only goes to 1950.,2,u0izck,"Hi! Im working on a project for my numerical analysis course. My group is hoping analyze WW2 weather data. Im having a hard time finding sources that have weather data that goes back that far, however, I have a strong sense that it exists out there somewhere. Does anybody have/know where I can find this data?",datasets,2022-04-10 07:41:24,4
"I’ve worked on this (sorry, propriety data set). It’s extraordinarily difficult to do because housing prices are so affected by external factors that you simply can’t represent in the data. COVID for instance will completely ruin any model by crashing house prices. It also affected different neighborhoods differently. 

Zillow tried this, bought a ton of properties and then folded the whole thing at a loss because the models even they built were so wrong.",13,u00mkj,"Does anyone know where to get raw data for residential houses/condos? For example, the historical prices, zip code, number of bedrooms/bathroom/balconies, neighborhood, etc. I am looking possibly create a model to predict housing prices in Chicago(preferred) or any city that has good data. I know it is extremely difficult to predict housing prices but I just wanted to give it a shot and it would be a good learning experience. Your help would be greatly appreciated!",datasets,2022-04-09 12:40:23,19
"Compare review score and review score. It will give r^2  = 1.

Joking aside, create a correlation matrix. Then plot feature pairs that have a high correlation and see how linear they are.",3,u0a14d,"I’m currently working on my first machine learning model, and the goal is to preform a linear regression on the Top 1000 movies dataset. I tested a few values in it for a linear correlation, of which there was none. Does anyone have any ideas on how I can best utilize this dataset to find 2 values that would correlate for a linear regression?",datasets,2022-04-09 21:26:41,17
https://irma.nps.gov/Portal/,1,u01l92,"Hello,

I am looking for national park data sets for all (or as many as possible) US national parks within the national park service.

Im looking for info such as:

- National park size and location
- Historical visitor count by day for multiple years
- Historical climate data such as temperature, precipitation, wind, UV index by day for multiple years
- Historical funding for NPS and each national park (even better if I can find budget info broken down within each park)
- Any other info that could be of interest such as all these stats by time of day, or visitor count by type of access (day pass, month pass, season pass) for each day

I’ve been digging through some resources but am looking for more creative approaches to finding official data for these parks.",datasets,2022-04-09 13:29:29,1
Maybe [this](https://github.com/leosmigel/analyzingalpha/tree/master/2019-09-18-sp500-historical-components-and-changes) will serve as a starting point.,1,u021ed,This is for a school project.,datasets,2022-04-09 13:52:53,1
"Hey cheetos_99,

I believe a `request` flair might be more appropriate for such post. Please re-consider and change the post flair if needed.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/datasets) if you have any questions or concerns.*",1,tzv3o9,Looking for datasets where for example there are 5 users and each of them rate a particular movie 1 and then the same users rate another movie 2 and so on. I am a college student with very little knowledge about datasets and would have to be finding out similarity index of users using jacards coeffecient and then map ant colony optimization using the similarity index. So any dataset which will be suitable and accessible to a student to perform these operations will be really helpful :').,datasets,2022-04-09 08:07:32,3
"I don't know where you live and if insurance companies work the same there. In my country some insurance companies have this data, as they are sending their customers to the right specialist. Problems:

Potential language barrier

I don't know if there is a company that lets you access the data

If they let you access it, I don't now if the data is stored in a way it couldbe used

Biased data, as older people tend to ask the insurance while younger ones tend to ask ""the internet""",1,tzdenf,"Hello everyone. 

I want to work on a dataset of online questions/searches that people ask when they are looking for medical help. More specifically when they want to see someone but they do not know where to go.

Made up search examples:

1) I've had a knee pain for two days. Who should I consult?  
2) Where should I go to get rid of my plantar wart?  
3) I am pregnant. What type of doctor can I go see  
4) Where can I go to treat my \_\_\_\_\_.  
5) Health specialist for back pain.

Can someone give me some advice as to how to gather this data? I know it may be vague, but I'm shooting my shot =) 

I've looked on kaggle, and I tried google keyword tool but I didn't find much that fit my needs. I'm thinking that I might have to request for an API access to some search engine or website, but I have had no success so far.",datasets,2022-04-08 14:08:25,1
"Hey Doomtrain86,

I believe a `question` or `discussion` flair might be more appropriate for such post. Please re-consider and change the post flair if needed.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/datasets) if you have any questions or concerns.*",1,tz04pp,"Hi, anyplace to get those?

 Like the email leak of data from the Democratic party in 2016, Panama papers, all of that stuff.",datasets,2022-04-08 03:04:31,16
"Physionet.org is a library by MIT containing all kinds of bmw related data. 

https://physionet.org/about/database/#open",2,tze51v,Please help,datasets,2022-04-08 14:44:20,2
I think the the UK Biobank might work for you.,2,tz1pad,I'm hoping to work on time series GAN application in biomedical datasets.,datasets,2022-04-08 04:47:39,2
[deleted],1,tz3uvb,"I know they basically give out their data for free and easy to use but my problem is that it's not complete, one of the most important and useful missing data for me is the ""top 1000 voters"" ratings, it helps to weed out all the ""controversial"" movies and series(looking at you India, Turkey and Bangladesh)",datasets,2022-04-08 06:43:43,1
So this doesn't help you at all (I'm sorry) but I have been looking for similar datasets for a week and can't find anything useful. Are you able to share the dataset you have for 2022?,1,tyzxv1,"Hi everyone,

I am looking for a dataset that shows the social media penetration by country over time. So far I only found one dataset about social media penetration in selected countries. However, it only has data from 2022. Do you have any idea where to collect the data or if there is already an existing dataset avaialble?",datasets,2022-04-08 02:51:36,2
"Doesn't have every one, but ballotpedia is a good start.",4,tyonkc,"Hello, I'm trying to find a resource or resources that would list the name and location (state, district etc) for every elected official in the USA. Including school boards and other local government positions. [USA.gov](https://USA.gov) is incomplete. [Niche.com](https://Niche.com) is just the same data but presented better. Is there a national index of people on government payroll perhaps? that might get me closer but for unpaid positions I'm not sure. If anyone knows where I should look or has any tips that would be appreciated.",datasets,2022-04-07 15:31:29,4
"Hey Complete_Egg_1822,

I believe a `question` or `discussion` flair might be more appropriate for such post. Please re-consider and change the post flair if needed.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/datasets) if you have any questions or concerns.*",1,typgww,"Looking for a dataset that contains images of stings/bites for various insects.

Closest thing I could find was a small Kaggle dataset that contained a few images of ticks and mosquito bites, but I am looking for something larger (more images, more variety in insect types).",datasets,2022-04-07 16:11:13,2
"What can we do with this data, come on guys ideas.",1,txr7qi,,datasets,2022-04-06 10:18:25,4
To my knowledge the census doesn't have that information below the state level. You can find industry info but not job titles. The methodology of the surveying isn't really statistically sound when you're sampling lower level geographies with that many options and also needing to protect personal information.,1,ty0or4,"Hello,

My background: I am familiar with SQL, Excel, and GIS platforms but am by no means an expert. 

I am trying to find US census data that shows the count of occupation titles (not employment industry) grouped by geography. I would like this geographic data down to the most granular level I can get but would need it as specific as zip code area or similar. 

&#x200B;

I've quickly learned that the preferred way of representing census data is by geoid or tract and not zip code but I have found a table on the census website that almost fits the bill; ""**B24124**  DETAILED OCCUPATION FOR THE FULL-TIME, YEAR-ROUND CIVILIAN EMPLOYED POPULATION 16 YEARS AND OVER"" found \[here\]([https://data.census.gov/cedsci/table?q=occupation&tid=ACSDT5Y2020.B24124](https://data.census.gov/cedsci/table?q=occupation&tid=ACSDT5Y2020.B24124)). However, this table only goes down to the state level. 

&#x200B;

Can anyone with more experience point me in the right direction to a table of set of tables that would allow me to have this same information with zip code detail? Might I be able to find still accurate data with a private data service? Any help you could provide would be appreciated.",datasets,2022-04-06 17:40:41,4
"Hey newsfromthefrontpage,

Sorry, I am removing this post because Youtube and associated domains have been restricted on this subreddit.

Please consider using a different format than video for your post.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/datasets) if you have any questions or concerns.*",1,txt2xc,,datasets,2022-04-06 11:42:33,1
"Certain states will have databases in the US. Pennsylvania has a big CVS file on its state data site that has all the millage rates for property taxes by school district, for example, but I have no clue if they have them for other types of property taxes.",1,txql1l,I'm making a mortgage calculator app and am looking for property tax data- either via an API or (preferably) a csv/json file. Thanks in advance.,datasets,2022-04-06 09:50:25,5
"Good starting place might be your local Chamber of Commerce.

Serious answer: they are much more likely to have a good answer to that than this sub is.

Start by refining what is really wanted (do they really want “legal entities”, or do they really want “businesses”, or “employers”?

A lot depends on specific jurisdictions. 

Most states probably have online databases of Corporations/LLCs but see above this might not really be what is wanted.

Most likely source for employee numbers would be your state’s Employment department.

See if your city, county, or state has a department charged with maintaining business statistics.

Not a simple or straightforward source fun complication like branch offices/store locations, etc.",2,txb1gs,I'm doing a research project for our local chamber of commerce and would like to find a list of all legal entities and their employee counts in our community. Where might I find this data?,datasets,2022-04-05 18:53:22,3
"https://fortune.com/fortune500/

It’s a list of companies.

Not a list of public companies.",2,txagfc,"I've got all US stocks, but can't seem to find a big list of stock tickers to match the remaining 96 that I'm missing. Any leads?",datasets,2022-04-05 18:23:02,3
"Youth Risk Behavior Survey (YRBS) data

https://www.cdc.gov/healthyyouth/data/yrbs/data.htm",1,tx3jlf,"I am currently teaching base R at our university. In 2 weeks the lesson will be about working with dataframes. I really just want to show some simple things. Calculating characteristic values, filtering, choosing rows and columns. But I would like to use a reasonably interesting dataset for it. The context is psychology, but anything would do - as long as it's interesting. Do you have any ideas?",datasets,2022-04-05 12:56:38,2
[https://www.reddit.com/r/place/comments/txjykg/has\_anyone\_mirrored\_httpsplacethatguyalexcomfinal/i3mqa9z/?context=3](https://www.reddit.com/r/place/comments/txjykg/has_anyone_mirrored_httpsplacethatguyalexcomfinal/i3mqa9z/?context=3) looks like it has a torrent soon,1,twovwe,"https://www.reddit.com/r/place/comments/twom9x/2022_dataset/?utm_source=share&utm_medium=ios_app&utm_name=iossmf

Do someone have infos ?

Edit : https://www.reddit.com/r/place/comments/txvk2d/rplace_datasets_april_fools_2022/",datasets,2022-04-04 23:56:55,2
"Hey Lexyr-Mod,

I believe a `request` flair might be more appropriate for such post. Please re-consider and change the post flair if needed.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/datasets) if you have any questions or concerns.*",1,tx6bg1,"Hey all.

We are a group of data engineers working to make Reddit a more accessible place for all.

There has been lost of discussion about where to get datasets for the unique April Fools event for Reddit, here are some we found.

[The Reddit /r/Place Dataset](https://socialgrep.com/datasets/the-reddit-place-dataset?utm_source=reddit&utm_medium=link&utm_campaign=theredditplacedataset) by SocialGrep contains all the posts and comments made on the /r/Place subreddit.

[Looking back at /r/Place](https://place.thatguyalex.com/) by /u/prosto_sanja contains picture data for this unique event.

Unfortunately, the data on who placed which pixels we couldn't find.

Happy analysis, everyone! We hope this will make something great.",datasets,2022-04-05 14:58:03,1
"If you have Python experience, you can write a scraper that selects each year in their form, then scrapes the whole table, inserts it into a pandas DataFrame, then goes to the next year. Could do this with beautifulsoup or selenium.

On this page: https://www.techpowerup.com/gpu-specs/",2,twuuky,"I have a GPU dataset I want to enrich with specs from Techpowerup's GPU database. But I can't find anywhere on their website I can download their database.

&#x200B;

I was trying to webscrape with [webscraper.io](https://webscraper.io) but I can't navigate to a page which includes all GPUs and you can't really make a proper selector from the popular GPU section. 

Is there a way I can have webscraper search for each processor I need by reading from a text file?

&#x200B;

I tried [databar.ai](https://databar.ai) but techpowerup's API is not available there. 

Would I need to interface with its API on my own?",datasets,2022-04-05 06:22:46,3
"You can serve up the data from the form/spreadsheet the same way using google apps script. Throw a password or key into your request from your server to the form data, and have the google script check the key.",2,twmadq,"I have a Google Form made up with a whole bunch of data collected, and I’d like to feed to through to my React web app to be visually represented. I’d like to keep it free as well, how might I go about this?

One method I’m considering is moving the data over to Google sheets, then publishing to the web as a .csv file. Once published, I’ll use the D3 module in JavaScript to fetch that data from the live .csv, and then use it. My only issue here is that I think the data would be public and anyone with an URL and see it, which wouldn’t be the worse because theirs no personal user data, but still if there’s any better or easier way to do this I’d love to know!

Thanks!",datasets,2022-04-04 21:11:04,1
"I host an application which allows you to visualize datasets but unfortunately right now it caps off at 100MB.

if yours is less than that i'd be happy to share. Note, that there is no working with the data on the app, just plots",2,tw76sy,"I have ran into a dilemma with numbers on Mac, where it only allows you to have 1 million columns. The issue is that I have 16 million to look at, cannot split the document, and cannot afford to pay for a different application. Does anyone know of any open-source/free applications I could use? (I be a broke student what can I say lol).  Thanks in advance :)",datasets,2022-04-04 10:06:00,9
"The dataset is available in this AIcrowd challenge:

https://www.aicrowd.com/challenges/spotify-million-playlist-dataset-challenge",3,tvwfjg,"Hello, I am currently working on a graduation project to build a playlist recommender system. I noticed that Spotify hosted a challenge along these lines a few years back and provided their Million Playlist Dataset for participants. I was wondering if anyone might still have this dataset and would be willing to share it? or recommend if there is any alternative dataset?

Thank you so much in advance!",datasets,2022-04-04 00:51:24,1
This data is so easy to generate by yourself. There are only a few irregular verbs in English. Cmon don’t be so lazy,1,twe198,i want to conjugate English verbs to the past and put them into sentence i want to use for a model. i found a library called mlconjug but i am facing a lot of bugs just instantiating a simple object so i thought of using a dataset in the form of a dictionary if it exists please let me know,datasets,2022-04-04 14:45:47,2
"I'm not aware of any such dataset, and I'd be highly skeptical of any analysis that described depression as ""simple"". Depression is a highly complex neurological issue that generally can't be boiled down to a simple model. 

If you're looking for approachable datasets, check out kaggle.",16,tvse7s,"Just what the title said, a depression dataset or anything related to depression. I'm trying to find a simple dataset that I can learn from. Doing a simple prediction and making it a website.",datasets,2022-04-03 20:45:47,6
The WHO list of essential medicines is going to cover most of this. https://www.who.int/groups/expert-committee-on-selection-and-use-of-essential-medicines/essential-medicines-lists,3,tv97j7,It could be better if I could get datasets of both commercial names (Trade names) and chemical names.,datasets,2022-04-03 06:25:07,2
"I've had to work with gas station GPS coordinates and that had to be purchased as it was considered competitive market data.

Wouldn't be surprised if it's the case with your project.",4,tuyl2k,"I'm not sure this dataset exists.
Interested to know the quickest method you guys would use to gather the data by oneself.
Typing in ""gyms"" on google maps gives pretty good local results. I wonder if there's a way to do this but without a limit on number of results or nearness.",datasets,2022-04-02 19:21:00,5
"Tatoeba is where i would start :-)

https://tatoeba.org/",3,tuioqb,"Hey there, I'm currently working on my  graduation work where I'm trying to construct some neuronal network and for this I really need a downloadable dataset with as many German words and their pronunciation as possible in it. I've searched for some time now and I really can't find one that satisfies my requirements, so I was wondering whether one of you could recommend something for me.

(If someone wants some more information about what I'm exactly doing in my graduation work, just ask)",datasets,2022-04-02 06:18:39,2
"Hey Neither-Tangerine-30,

I believe a `question` or `discussion` flair might be more appropriate for such post. Please re-consider and change the post flair if needed.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/datasets) if you have any questions or concerns.*",1,tudmf9,"Dataset for undergraduate students that shows thier major along with HS data such as HS GPA, AP, A Level, SAT, Gender,and sicial info that can help to predict thier Major at universities?",datasets,2022-04-02 00:56:25,1
"Hey Comedic_Guy,

I believe a `request` flair might be more appropriate for such post. Please re-consider and change the post flair if needed.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/datasets) if you have any questions or concerns.*",1,ttx4vm,"I am struggling to find data on energy prices per KWH for the past 20 years.

I only need the information for the UK and the world, so far I have found data on the UK and the OECD but global information remains elusive.",datasets,2022-04-01 10:58:52,5
"Hi, I use your platform to pull coingecko market data everyday. Thanks!",6,tt6yld,"Hi everyone!

My friends and I built [databar.ai](https://databar.ai), a free no-code API tool that lets you get datasets from all over the web. 

You don't need to know how to work with APIs to use our site (it's fully no-code). Basically all you do is pick an API (for example [Coin Gecko](https://databar.ai/source/7) or [WeatherBit](https://databar.ai/source/63)), customize your request with parameters, and get a clean, structured csv file in return. You can also schedule data pulls (with cron or just daily/weekly).

Some of what you can do right now:

\- Track crypto prices, volume, supply, OHLCs

\- [Scrape news articles](https://databar.ai/source/21)

\- Get [crypto social stats](https://databar.ai/dataset/135) (Twitter & Reddit followers & discussions)

\- Access public/government & crime data

\- Export [granular financial data](https://databar.ai/source/16) (IPO calendars, institutional holders, analyst ratings, multiples, ratios)

\- Get [COVID-19 data](https://databar.ai/source/2) (time series by continent/country/state)

\- Access anonymized [foot traffic data](https://databar.ai/source/43)

\- Analyze [Telegram usage](https://databar.ai/source/59) (post views, subscribers, mentions)

\- Scrape Google Maps reviews, photos, and locations

There's more that you can do, these are just a few that we use personally.

We're wondering if there are any features people would prefer - mostly posting for feedback/ideas. Please let me know if I'm posting in the wrong place. :)",datasets,2022-03-31 11:59:31,15
"https://www.ons.gov.uk/aboutus/transparencyandgovernance/freedomofinformationfoi/knifecrimedata

Try here",1,ttuvw6,"I'm conducting research on the severity of knife crime in London. Any data which relates to homicides, sentencing, and possession, etc. relating to knives is welcome. Data should be openly available to the public and fairly recently updated/recorded.",datasets,2022-04-01 09:26:56,1
"Hey aman167k,

I believe a `question` or `discussion` flair might be more appropriate for such post. Please re-consider and change the post flair if needed.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/datasets) if you have any questions or concerns.*",1,ttlsat,if anyones interested i'll send the download link.,datasets,2022-04-01 01:27:40,3
"Hey SunDropu,

I believe a `request` flair might be more appropriate for such post. Please re-consider and change the post flair if needed.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/datasets) if you have any questions or concerns.*",1,tt9lx8,"Hello, i'm working on a project where i need to make a web-app that predicts the performance of highschool student.  And based on that prediction, my university will put the student in the right class (with students that has the same level as him).

 Every student will have a rank from silver to diamond (silver, gold, platinum, diamond). That would help us to make classes with students that has the same rank.

It would be super helpful if the dataset is based on a highschool in France. If it's not the case,  then i would like to have a datasets of another country that has the same grades system as France. 

Thanks in advance.",datasets,2022-03-31 13:59:45,1
"Hey there,
You can try matching programmers (mostly INTJs, ISTJs, INTP, ESTJ) with this data in GitHub https://github.com/ianscottknight/Predicting-Myers-Briggs-Type-Indicator-with-Recurrent-Neural-Networks",1,tt00bc,"Hello, I am looking for a datasets where the feature is their hobbies, interest or personalities etc.  
Because I am planning to make an web app that recommend future programmers which path of their career they want to go based on their personalities, hobbies or interest. Specifically the web app will  analyze their personal data and predict or recommending which path should they take . 

  
for example: 

frontend - likes to design, draw etc.  
backend - less artistic more on logic personality  
fullstack - a bit of both frontend and backend  
datascience - likes data, or more on numbers and analysis.  


This are just some rough example hehehe.   
I also need some advice.. Please recommend me of somethings or if I am doing wrong.",datasets,2022-03-31 06:40:44,1
NOAA will likely have the data somewhere. Maybe here: https://coast.noaa.gov/slrdata/,2,tsd0bf,"The title is pretty much what I am looking for. I'm doing a project in a class of mine relating to how much the sea levels in the Gulf Coast are rising, I am having a hard time finding data detailing how the sea levels of the Gulf of Mexico have changed over time. If anyone knows where I can get data on that in csv or xlsx format, that would be highly appreciated. If anyone can also find a dataset on water surface temperatures in the area, that would also be appreciated.",datasets,2022-03-30 11:29:11,6
"What are you using, R or Python? I’d look into rtweet or twitteR",1,ts4jfw,"specifically, using graph neural networks to identify possible communities in these social networks",datasets,2022-03-30 04:43:02,1
"Liv-ex as already mentioned is the most reliable source. However, if you don't need perfect accuracy, you could try contacting Eric Levine, the owner of CellarTracker. The site collects data directly from users on prices paid for different wines. Eric started CT as a passion project and he's pretty awesome!",2,ts8nup,"Hi all, as the title suggests, I am looking for a dataset that contains wines (vineyard, year, type, etc...) and their historical sales price points. I have looked in various places but am having trouble finding a comprehensive dataset. Thanks in advance.",datasets,2022-03-30 08:13:19,4
"I kinda never understood these questions because… Dirty datasets are everywhere! Scrape Reddit, process Wikipedia pages/edits, download the anonymous leak of Russian databases/emails, stream Wordle tweets, etc…",2,trz14e,"I am building personal projects for my portfolio. My goal is to load data in the data warehouse like Snowflake, cleaning and transforming it using dbt(data build tool) into the Kimball schema, making it ready for analytics. The data I found on Kaggle is too clean to work and is ready for analytics. Where can I find some real-world datasets that need cleaning?",datasets,2022-03-29 22:15:52,5
"Don't know if you are looking for specific songs, but Kaggle has a lot of Spotify datasets (like [this one](https://www.kaggle.com/datasets/rodolfofigueroa/spotify-12m-songs), for example), which contain BPM, key, and other factors Spotify use.

I believe the Spotify API even enables you to gather this data for specific songs, but I don't have any experience on that.",4,trfqim,"I’m trying to find data on songs.  

The most critical data point I’m interested in is tempo (bpm) but I would also be interested in other details, like genre, key (or even just major/minor) and anything else which might be used to classify a song’s sound and feel.",datasets,2022-03-29 12:39:43,1
"Yeah, here's a free traffic dataset on bounding.ai : https://bounding.ai/en/listings/1739967-traffic-congestion-dataset",5,trcndn,Is there any public traffic footage dataset that can be used for vehicle reidentification?,datasets,2022-03-29 11:16:40,5
While not exactly what you asked for - have you looked at global reef watch? Geotagged photos of coral reefs,2,tr02q0,"Hello everyone,

I'd like to find a big dataset of geo- or location-tagged images that show streets or landscapes.Essentially a dataset which resembles a set of random locations from google street-view tagged with their respective geolocation as coordinates. I have not yet found such a dataset while researching and would much appreciate if someone knew where to find a dataset like this!",datasets,2022-03-29 06:43:58,4
Hi! Did you find anything yet? I'm looking for semiconductor chip info too and haven't find any,1,tqrrpm,"Trying to conduct an analysis of the current situation in the semiconductor industry. Im trying to see the supply and demand side of things. I need to find a large dataset or datasets of at least 400 rows and the variables are self-explanatory.  There are only a few datasets online that are on semiconductor shortages but the variables are hard to interpret. IF someone can help me find one, that would be awesome!!! Thanks!",datasets,2022-03-28 21:33:49,1
"Images most widely used in similar research: https://www.cancerimagingarchive.net/
  
genome map stuff for cancers: https://humantumoratlas.org/
 
 
Where to find more: https://datacommons.cancer.gov/repository/imaging-data-commons",2,tqm3u6,I'm looking to find a database that has statistics (ideally patient-matched) on transcriotion expression in health tissues vs tumors across the genome. Can anyone help me identify the best available dataset? I know there are large centers/projects devoted to things like this.,datasets,2022-03-28 16:19:19,4
Nice,2,tqewir,,datasets,2022-03-28 10:44:18,2
"Check out Adventure Works: https://github.com/Microsoft/sql-server-samples/releases/tag/adventureworks 

It’s a sample database from M$ that might help. Word of caution… don’t expect real world data to be this clean.",2,tqnf1a,Hello everyone. I am interested in CRM and also datasets about customers (I know getting this kind of data is really hard but I want to work on projects like these so if any one knows any good datasets for CRM etc please help.,datasets,2022-03-28 17:28:09,7
"Hey CuriousDevelopment9,

I believe a `request` flair might be more appropriate for such post. Please re-consider and change the post flair if needed.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/datasets) if you have any questions or concerns.*",1,tqbb6d,"Google is failing me - I’m after a database of (all) English synonyms I can work with.

Like if I look up “curiosity” you’d find “concern”, “inquisitiveness”, “interest” etc

A list of all synonyms for English words. Basically a thesaurus as a file to be analyzed.

Anything along those lines?

TIA",datasets,2022-03-28 08:03:25,2
"This is my moment!

I'm colorblind and I'll do this, if it weren't for being colorblind I'd be a cop now I'm a data engineer, it's time to use my super power",4,tqaaej,"Hi! I'm certainly working on a paper for college and for it I need to know about data concerning colorblind people or people who generally see colors differently. I'd do the survey amongst friends and colleagues but I doubt there's enough people who are colorblind to complete the survey. 

Also, if there already is some data that questions if colorblind people percieve movies and cartoons the same way when it comes to color psychology, I would love to know more about it, I just assumed there isn't much data considering it's pretty specific.",datasets,2022-03-28 07:16:36,7
"Hey swaddy375,

I believe a `request` flair might be more appropriate for such post. Please re-consider and change the post flair if needed.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/datasets) if you have any questions or concerns.*",1,tpyvj6,"Can anyone help me to find data on COBRA health coverage? I am trying to look the impact of covid on COBRA enrollment. Any lead would help, thank you so much!",datasets,2022-03-27 19:12:02,2
"As a learning project, this is nice, but for standard use, what would be the advantage of this over just loading a program into Pandas and calling df.describe()? And if you need more complete details on a data set, using the [pandas-profiling](https://github.com/ydataai/pandas-profiling) package?",17,tp584c,,datasets,2022-03-26 15:01:55,4
"Reminder to respect UKR op-sec by not sharing videos of UKR soldier locations or any other such classified intelligence you discover or witness online.

https://reddit.com/r/ukraine/comments/sy65wi/ministry_of_defense_of_ukraine_do_not_view_our/

News Sources: https://www.reddit.com/user/Ukraine_News_Bot/comments/tnadz3/news_sources/

Godspeed Ukrainians. 💙💛

Ways to help Ukraine (charities) https://reddit.com/r/ukraine/comments/s6g5un/want_to_support_ukraine_heres_a_list_of_charities/

Please message me if there are any translation errors, typos, or dead links.

This comment was made by a bot. Original comment from iamkunii on r/worldnews",2,tpbny8,,datasets,2022-03-26 20:38:18,1
https://data.gov.in/resources/national-hospital-directory-geo-code-and-additional-parameters-updated-till-last-month,1,tolcdy,Any database which contains atleast a list of all hospitals with their coordinates. Any additional information is a plus like list of doctors available and their speciality,datasets,2022-03-26 03:00:46,1
Do they need to be real people? There are some shockingly realistic ai generated ones around. Sites like https://generated.photos/faces allow you to filter by a number of things including for only front facing,2,toeuwu,"I'm looking for a dataset of images consisting of face positioned in this manner: https://imgur.com/a/fvOR08x

Any help would be appreciated!",datasets,2022-03-25 19:27:43,3
"Youtube has this functionality ... you can probably use [yt-dlp](https://github.com/yt-dlp/yt-dlp) to download the videos at different resolutions...

I.E.:

    yt-dlp -F https://www.youtube.com/watch?v=g_VYf8MdVSw
    yt-dlp -f 398 https://www.youtube.com/watch?v=g_VYf8MdVSw

would give you 1280x720 (mp4, av01.0.05M.08)

You could write a little script to download all needed resolutions for a list of videos.",3,to9xlx,So I just want a data set where I have some set of videos in 1080p and also in 720p. Or access to a data base where I can see the video in 1080p and the same video in 480p. Does this database exist? if not do  you guys know of any website where I can download 1080p videos and the same video in 720p?,datasets,2022-03-25 17:51:11,7
"Hey Careful-Start9199,

I believe a `request` flair might be more appropriate for such post. Please re-consider and change the post flair if needed.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/datasets) if you have any questions or concerns.*",1,tna2ci,"I found something on the census website, but I'm having problems using it. 

Anybody have anything",datasets,2022-03-24 17:46:14,5
Possibly FRED https://fred.stlouisfed.org/,1,tn2odc,"Hello! First time posting on here—I am working on a project that is aiming to visualize access to prime credit over time. Looking for data starting from 1989 (when FICO score was first invented) to 2022. I can only find 2003 to 2022 from here [https://www.federalreserve.gov/econres/notes/feds-notes/developments-in-the-credit-score-distribution-over-2020-accessible-20210430.htm#fig1](https://www.federalreserve.gov/econres/notes/feds-notes/developments-in-the-credit-score-distribution-over-2020-accessible-20210430.htm#fig1).

Can anyone here help find a data point in the 90s that indicate what % of the American population had access to prime credit? Thank you so much in advanced!",datasets,2022-03-24 13:32:18,2
I'm interested too,2,tm7zyz,Does anyone know where to find historic data (last 10 years or so) of betting odds of F1? Both races odds and championship odds.,datasets,2022-03-24 06:00:26,5
"Hey Upset-Customer-6715,

I believe a `request` flair might be more appropriate for such post. Please re-consider and change the post flair if needed.


*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/datasets) if you have any questions or concerns.*",1,tn7oxg,"Does anybody know where I could get a dataset with millions of song lyrics (current, if possible), that also includes information about the artist and genre. I can't find one on Kaggle thats large enough and recent enough, thank you!",datasets,2022-03-24 15:46:22,1
I am trying to be helpful. You can try out kaggle or github,1,tmiqgn,"Are there any existing datasets for keylogger and scalper bot detection?  
I'm doing my thesis on gpu scalper bot detection and I can't seem to find any relevant datasets to go of off. Could someone send some if they have them or at least point me in the right direction as I feel quite lost.",datasets,2022-03-24 08:51:15,1
"Looking for the same thing, did you find any?",1,tmcp2s,"I'm looking for a set of images to train a model to identify national flags. Hoping for a diverse set of countries represented and variety of situations (flying on a ship, hanging from flag pole, printed on a tshirt, etc). Any ideas on where to look?",datasets,2022-03-24 07:13:37,1
"Yahoo finance, search for the stock you are interested in and go to the 'historical data' tab and you can download price info",2,tm9nl7,"I am looking for Stock Data of as much different stocks as possible.

They should include price changes over the day not only open and close.

They also should be for 5+ years to current date.",datasets,2022-03-24 06:21:57,12
Your industry sounds fucked lmao,0,tm0pmr,"Edit: micro financing without including high interest rates! Coming more from a system change perspective... 



Maybe a bit of a dumb question, but I'm new to the field....
Where would you look for data and what data would you look for intended to predict the lending of micro credits to refugees.",datasets,2022-03-24 00:56:06,6
"""high quality""... ""Medium""... SMH.",13,tk3wdk,,datasets,2022-03-22 07:11:37,8
Can you let us into the demo quicker?,2,tjchvc," Hi everyone!

I’m Erik, Co-Founder of [subsets.io](https://subsets.io/) Subsets.io is a platform where you can upload datasets, and get matched with relevant external datasets which can be merged into your dataset with one click.

Our goal is to make it easier to pull in relevant external data. No more dealing with APIs and their rate limits, pagination, etc.

Check it out on [https://subsets.io](https://subsets.io/). Let me know what you think. :)",datasets,2022-03-21 07:01:19,7
I'm assuming you already tried Kaggle? Or [https://datasetsearch.research.google.com/](https://datasetsearch.research.google.com/),1,tjqbn0,"Hi all,

I'm looking for a patient-level dataset on infected individuals that would possibly include the types of symptoms they have and ""personal"" info (age, gender, country, etc). The target label I was hoping for would be hospitalized/non-hospitalized but any dataset with the following characteristics would be helpful. 

I tried the sticky sticky post but all of the datasets are aggregated up to the state or country level. Anything would be helpful and thank you!",datasets,2022-03-21 17:26:02,4
You could look through aidlab. May have something you can use https://www.aidlab.com/datasets,1,tjenbu,"Hi All,
Where can I get a dataset containing wearables data of 2+ users for many days. 
I require columns like Age, Weight, Resting heart rate, BMI, ECG, Steps, sleep hours, etc.

Kindly help me out. I intend to use this data for my Masters degree thesis.
Thank you very much.",datasets,2022-03-21 08:39:47,3
"I worked in the ticketing industry doing data science work and transaction data is held very closely by the event holders and brokers who own the tickets. Most of the platforms have really complicated database structures too due to building on old technologies and patching features on to handle returns, cost structures, etc. Might be hard to find a dataset and then parse the business logic.",3,tio8vi,"Is there any place I would be able to find resale ticket data from websites such as VividSeats, SeatGeek or Stubhub? Or a more general question, is there some sort of grand data repository (*cough* possibly one that is not legal) where large and niche datasets can be found? Most of my experience has been with financial data concerning just the stock market, so I am a bit inexperienced with navigating the Internet to find said niche datasets. Any help would be appreciated.",datasets,2022-03-20 09:03:06,4
"You could try [Databar.ai](https://Databar.ai) \- I am biased since I'm one of the co-founders haha, but it's built specifically for this use case + we let you use our API key for several APIs.

Sorry for the self-promotion folks :D",3,tikufj," Hi everyone,

I'm trying to get external data that may be relevant to improve my model. However, getting external data is harder than I expected.

I need to deal with APIs (quite technical), some of them have pagination and rate-limiting, which requires some additional loops or waits that I need to write.

I was wondering how you guys do it. Perhaps there's an easier way?

Thanks!",datasets,2022-03-20 06:16:59,14
"You can use transcripts of reality shows perhaps? Here's a good website for the transcripts of the Netflix reality show ""Love is Blind"" : [https://tvshowtranscripts.ourboard.org/viewtopic.php?f=1243&t=51203](https://tvshowtranscripts.ourboard.org/viewtopic.php?f=1243&t=51203)",12,ti775l,"I'm looking for romantic conversation or a flirting dataset that I can use for NLP text generation.

I found a couple websites with a large amount of pickup lines, but nothing for flirting. Anyone have any good resources?",datasets,2022-03-19 15:58:14,7
"Check out [Spotify Charts](https://charts.spotify.com/home). I think the data starts in Oct 2021, but you can take a look, maybe it goes back further.",1,tiabsz,"If it's not clear, I'm looking for 2 separate data sets, including the actual raw numbers of album sales/total artist streams

&#x200B;

If anyone knows where I can find this data, I'd appreciate it.

&#x200B;

Thanks!",datasets,2022-03-19 18:43:44,1
"Esri has either a LivingAtlas or Tapestry layer out there somewhere for most of these public stats and you can view or enrich a map with them in ArcGIS Pro or AGOL. A good place to start would be the People tab in LivingAtlas. (BusinessAnalyst data also includes Tapestry data)

https://livingatlas.arcgis.com/en/browse/#d=2&categories=People%3A1111111111",2,ti27xz,"I was wondering if there's something like this available? There's hundreds of counties all over the nation. I'm putting together a google spreadsheet with this info that includes appraisal data records, judicial court records, police, etc.. which are all publicly available. 

For example: 

* FTW - [https://www.tad.org/data-download/](https://www.tad.org/data-download/)
* Dallas County TX - 	[https://www.dallascad.org/DataProducts.aspx](https://www.dallascad.org/DataProducts.aspx)
* Austin TX	- [https://www.traviscad.org/reports-request/](https://www.traviscad.org/reports-request/)
* San Antonio TX	- [https://www.bcad.org/](https://www.bcad.org/)  


If there's something like this available, let me know! If not, is there any interest for this?",datasets,2022-03-19 12:01:56,1
I recommend [pushshift](https://www.reddit.com/r/pushshift/comments/bcxguf/new_to_pushshift_read_this_faq/?utm_medium=android_app&utm_source=share),21,thu2am,"Hello all,

I'm looking to learn how legal scraping reddit is (wihout the reddit api), what the limitations are, what I can do and so on. I would appreciate some links to read up on it.

&#x200B;

Disc: I'm using the data for a university project.",datasets,2022-03-19 05:21:42,12
This is more of a question than an answer. Can you pull the data with Twitter’s Rest API?,2,ti5fd4,"I   am searching a zerohedge tweet archive, does anyone have it? I would   like to run some NLP stuff on it. I would like to see how topic change   over time, top ones and related sentiment and magnitude.",datasets,2022-03-19 14:32:21,2
Check [Spotify charts](https://charts.spotify.com/home). There's also an 'old' website with similar information that has downloadable CSV files [here](https://spotifycharts.com/regional/us/daily/latest),2,tho2sk,"where and how can I find data about most stream songs in a specific country for a specific period of time.
I’m working on my thesis and it was going well, until my professor said I need to analyze the data of my own country, instead of worldwide. Thanks",datasets,2022-03-18 22:03:00,4
How big a dataset are you trying to make?,2,th7ep8,"Hi there,

I (and a team) are working on a new project which involves estimating the growth of a plant (vegetable/fruit/herb plant) through a camera. Basically, for this we would love a dataset of these plants and their growth in images, i.e. photos and characteristics of tomatoes overly ripe, or very young, etc etc.

This is quite a niche dataset I guess, but thought it would be worth asking! Googling has gotten us so close, but not quite there. 

We are also willing to synthetically create a dataset (i.e. through google images), but a pre-existing dataset is much preferable. 

&#x200B;

Thank you very much!",datasets,2022-03-18 10:21:45,2
Look on kaggle.com ! Lots of datasets there,8,tgw7ku,Hello. I’m working on an exploratory data analysis project and I’m looking for interesting and unique datasets. This is my first data science course and I’ve never done an EDA before so I’m pretty excited,datasets,2022-03-18 00:57:34,10
"SimpleRecon enables real-time 3D reconstruction without using 3D Convolutions.

Quick read on how it works: [https://www.qblocks.cloud/byte/simplerecon-real-time-3d-reconstruction/](https://www.qblocks.cloud/byte/simplerecon-real-time-3d-reconstruction/)

&#x200B;

Developed by Mohamed Sayed, John Gibson, Jamie Watson, Victor Adrian Prisacariu, Michael Firman, Clément Godard with Niantic, University of Oxford / College London and Google.",2,xigwzl,,deeplearning,2022-09-19 09:17:18,1
"This is a really interesting question!

Here are a couple thoughts based on my experience working with epilepsy data.

- fMRI studies generally need a ""baseline"" FMRI for contrast where the subject is performing some comparatively neutral mental task - but what what does that mean for epilepsy?

- Good quality fMRI generally requires that the subject stay in one place, but seizures make people move around unpredictably. (to correct for this you would likely need to perform some kind of image registration)

- fMRI has poor time resolution, which would be important if you're trying to pinpoint some kind of biomarker.

Is there a reason you wouldn't want to use EEG data (see ""Line Length""), which is abundant and doesn't suffer from these drawbacks?

Also note there are robust methods for detecting seizures in EEG data, so in an ideal world you would have multi-modal fMRI/EEG data and you could use one to tag the other.",4,xicmva,,deeplearning,2022-09-19 06:50:40,1
Tensorflow? You must pay me. Study pytorch or nothing.,-1,xi4v9r,,deeplearning,2022-09-19 00:17:03,1
"Link to huggingface space - [https://huggingface.co/spaces/krrishD/suitify\_v2](https://huggingface.co/spaces/krrishD/suitify_v2). 

Due to space constraints (huggingface doesn't provide you with a GPU), I ended up deploying the model to Banana Dev. And then used a gradio space to make an API call to the hosted model. 

Curious to understand how others are tackling model deployment/sharing",1,xi3706,"&#x200B;

https://preview.redd.it/twxum9bn6ro91.png?width=3056&format=png&auto=webp&s=2db3938da587cded44ec29a6c845e093ec19a001",deeplearning,2022-09-18 22:41:58,2
"Apparently, I need to share my email to some shady website to access this. Nah, I'm good.",13,xhf82r,"Hi all

I have prepared a set of 450+ practice questions that you can use to improve your data science skills in Pandas, SQL, and NumPy libraries.

These questions cover a wide variety of topics from each of these tools.

You can read more about practicing this notebook in my [Blog on Medium](https://medium.com/geekculture/450-practice-questions-that-will-make-you-a-pandas-numpy-and-sql-pro-1cd6f72ee330).

Thanks and I hope you'll have fun solving them :)",deeplearning,2022-09-18 05:20:56,3
Commenting for reach,1,xhg2xh,"Hi, so I am an undergrad studying data science. I am starting to apply for internships. Is there an interview checklist type thing, which has important points about neural networks and other things. Are there any questions/answer things that I should read before interviews?",deeplearning,2022-09-18 06:01:13,4
"It's impossible to help you without further details on your project.
Are you talking about your training or your validation loss? Because usually if the loss increase a bit past a certain point on your validation loss it is a sign your network is over fitting your data.",3,xhd5f9,"I'm using a self supervised pretrained ViT-B with frozen weights (pretrained on ImageNet). I was just training a 2 layer MLP on ~16k images from a completely different domain for 120 epochs. My training loss decreases in the first 10 epochs to a minimum of 1.7ish and then increases to eventually hover around 2 for the rest of the training (CrossEntropyLoss). Tried some hyperparamter tuning (increasing/decreasing lr, weight decay, lr scheduler, batch size), but my minimum loss went only from 2 to perhaps 1.7, which is still a dumb log loss (wild guesses). My accuracy starts at over 90 percent though and stays that way...?
Now, doing transfer learning from ImageNet to images from a completely different  domain was perhaps not a very good idea. I'm looking to unfreeze the backbone next, but I did that already for a few epochs and the loss shot up to 3 while hovering around there (i stopped the training early on).
Any suggestions as to why I'm terribly failing at this?",deeplearning,2022-09-18 03:28:29,5
"You can play with it on [HuggingFace Space](https://huggingface.co/spaces/daspartho/predict-subreddit)

I fine-tuned HuggingFace Transformers's DistilBERT on the dataset of titles of the top 1000 posts from the top 125 subreddits.

Notebooks for data collection and modeling are available on the [GitHub repo](https://github.com/daspartho/predict-subreddit). The [Dataset](https://huggingface.co/datasets/daspartho/subreddit-posts) and [Model](https://huggingface.co/daspartho/subreddit-predictor) are hosted on HuggingFace.

Limitations and bias-
- Because the model was trained on top 125 subreddits ([for reference](http://redditlist.com/)) therefore it can only categorise within those subreddits. I intend on increasing the count.
- Some subreddits have a specific format for their post title, like [r/todayilearned](https://www.reddit.com/r/todayilearned) where post title starts with ""TIL"" so the model becomes biased towards ""TIL"" --> r/todayilearned. This can be removed by cleaning the dataset of these specific terms.
- In some subreddit like [r/gifs](https://www.reddit.com/r/gifs/), the title of the post doesn't matter much, so the model struggles on them.

This was a fun project. I'd appreciate any ideas, feedback or suggestions for the project :)",10,xgik2i,,deeplearning,2022-09-17 03:05:25,6
"I have read (not tested) that SMOTE and Tomek links can be useful. I completed a commercial project in which the data was highly imbalanced but we had so much data that we just reduced the size of the over-represented case.

The imblearn Python library has an implementation, I think.",1,xgvbds,"What are some tricks/ loss functions/ state of the art techniques to train classifiers on significantly imbalanced data (multi class) ?

Other than:
i) Focal Loss
ii) Class Weighting 
iii) Oversampling 
iv) Data Augmentation",deeplearning,2022-09-17 12:27:50,1
"Hey man, I can't help you here but I'm also quite new to all this so if you want we can bounce ideas off each other",1,xg5ebw,"Hi there, I'm trying to learn to use the keras-unet-collection package. I created a simple unet model for segmentation of grayscale images with a shape of (256, 256, 1):

    models.unet_2d(input_size=(256,256,1), filter_num=[64, 128, 256, 512, 1024], n_labels=2)

The output of the model has a shape of (256, 256, 2), but my masks have a shape of (256, 256, 1). I tried using n_labells=1, because one neuron for the channel should be enough (either there is a 1 and the pixel is part of the mask or a 0 and it isn't) but that doesn't seem to work, the accuracy is almost 0 every epoch, and the network doesn't learn, so I guess I should use n_labels=2, which is what they use in the guide, but then, should I convert my masks? add a layer to change the shape of the output to match my masks? is there something else that I am missing?

I'm new to keras and deep learning, so sorry if this is a silly question. Any help is appreciated. Thanks!",deeplearning,2022-09-16 15:45:47,2
Vision transformer is available on huggingface,3,xg0tv8,"Hi,

Does anyone know if following models are available as pretrained in any library (pytorch, tensorflow, keras)?

COATNET

Vision Transformer

Coca",deeplearning,2022-09-16 12:39:02,2
What kind of data are you trying to fuse,1,xg3uku,Is there anyone can help me how to create an fusion model. I am working Image Processing Task. I just heard about fusion model but i don't get good study about it on internet.,deeplearning,2022-09-16 14:41:30,1
I know a few of people who use Roboflow for data management and MMDetection for the rest. They had it running themselves within 2 weeks with no Programming experience prior.,1,xfv86b,"We had written a small software for TF1 ( now ported to TF2 ) . It basically has the following features -

\- Collect Images and Organize Images

\- Annotate and Train models based on these annotation

\- Test The trained models and get API for these models.

\- Runs as a WordPress plugin , while backend can run anywhere.

It does the whole the whole cycle from collection , annotation , training and testing ( All GUI no Code ). At end we copy the models to our production machines.

We can not push our images to the cloud and have our own Training machines. The software has served us well , but as this is not the main project we are contemplating if putting effort in updating / maintaining it is best use of our time .

Now my question is that is it worth the time to update it , or there exist a  better software  which can help us do all the things in one place and is Non Developer Friendly ( Our Annotator's / clients etc are not Engineers ) . Looking for Suggestions  .

&#x200B;

Link : [https://github.com/ngcoders/deeppress](https://github.com/ngcoders/deeppress)",deeplearning,2022-09-16 09:00:41,1
What's the best pose-detector tool? I've seen that yolov7 has a builtin method.,1,xfcpfi,"[Real-Time Pose Estimation using AlphaPose, PyTorch, and Deep Learning](https://debuggercafe.com/real-time-pose-estimation-using-alphapose-pytorch-and-deep-learning/)

[https://debuggercafe.com/real-time-pose-estimation-using-alphapose-pytorch-and-deep-learning/](https://debuggercafe.com/real-time-pose-estimation-using-alphapose-pytorch-and-deep-learning/)

&#x200B;

https://preview.redd.it/21av346894o91.png?width=1200&format=png&auto=webp&s=dc2f05a282f6bae8ca46942fe5c1712e46047512",deeplearning,2022-09-15 17:35:31,2
"Unless you are a seasoned computer scientist, making distributed training work is almost impossible, at least in tensorflow.

TF does have good embarrassingly parallel tools, for idiots like me. But they don’t work on multiple nodes.",1,xfmqsf,,deeplearning,2022-09-16 02:37:47,1
Why doesn’t neural style transfer work though? You want to transfer the “feel of real photos” onto the simulated ones. This IS a style transfer problem. It’s just a matter of finding the right architecture and training the model well.,1,xfpkag,"hello everyone,

for an art project, i try to combine photographs of mine with simulated landscapes. this means, i have one stack of photos (a couple thousand) of \*very\* similar style and always the same subject and one the other hand, i have procedural landscapes (again thousands, but as they are procedural, i can generate as many as i want). i would now like to transfer the look and feel of the real photos onto the simulated ones (or vice versa). what type of deep learning approach can i take for this? i've looked into neural style transfer, but i don't think this is what i'm looking for, as i want the network to generate new images that are a mixture of both categories. do GANs work for this? 

&#x200B;

thanks for the help and recommendations!

best,

mm",deeplearning,2022-09-16 05:09:17,11
"Every post in this subreddit gets downvoted. Don't feel bad about it. It's a small sub, so even though you got an immediate for downvote, people will still see your post.

As for why exactly people downvote everything here, who knows.",19,xewjv3,"I will be buying a workstation and since NVIDIA has many 24GB GPU, which one is the best performing one? Budget is naot a problem. 

RTX A5000 and RTX A5500 are both 24GB. Any difference in performance?

Thanks .


Edit: what’s wrong with my question? It just got downvoted. If there is a better place to post this, please let me know.",deeplearning,2022-09-15 06:13:48,8
"subsets, AI > ML > DL",3,xfhilu,,deeplearning,2022-09-15 21:29:58,2
"this looks good, kinda similar to mine but I am waiting for the price drop/when needed to get a 3090 (currently got 3060 12GB + 5600x + 32GB RAM)

you might check the possibility to add a 2nd 3090 in due course if needed (check MB & may change PSU)

just check the prices of 5800x, they were getting lower, or may be a 12700k or so ?

isn't the casing is bit expensive ?

>Silicon Power A60 1 TB M.2-2280 NVME Solid State Drive

I don't know about this, if might useful to get faster drive if you are working with more data.

Also 3600 MHz RAM would be slightly better with AMD CPU, even I got 3200 MHz because it was cheaper",1,xfck4e,"Hello,

I am interested in purchasing a prebuild or building my own PC for deep learning. (I've been a lifelong Mac user so anything is an upgrade). My budget is 1.8k. I'm not sure if this build is good enough given my budget: [https://pcpartpicker.com/list/zzBtrD](https://pcpartpicker.com/list/zzBtrD). Please let me know what you guys think, any tips or suggestions are appreciated.",deeplearning,2022-09-15 17:28:23,3
Hrnet pose-estimation is pretty good for keypoint detection in general https://github.com/HRNet/HRNet-Human-Pose-Estimation,3,xeugcr,"If we consider the image of a face, I want to only localize the tip of the nose.

YOLO for example outputs a bounding box with 4 points. 

While I could simply use these coordinates, I am trying to find if there is not another architecture to predict only one point (i.e. the center) per image.

Any ideas or feedback appreciated :-).",deeplearning,2022-09-15 04:32:16,5
"In risk, we use binning, WoE and IVs to do preliminary feature selection for those cases",1,xeslcx,"So lets say you are having a dataset with labels and you underwent a complex preprocessing and you got 5000 features , but lets say i dont need all 5000 but instead take top 1000 features but we shouldn't use labes to rank features because at prediction time if you want to predict then we need to rank features right? So is there any technique as such or should i use dimensionality reduction techniques like pca ?

Thanks",deeplearning,2022-09-15 02:50:20,2
"You will not make money using these techniques. However, I thought the course was quite interesting anyways",0,xerl44,,deeplearning,2022-09-15 01:48:26,2
"This is a cool project and the neuron visualization makes it even better. Are you only visualizing the final two layers, or is the NN really only 8 neurons?!  


I would be interested in seeing how you did it if you felt like creating another video or sharing the code :)",2,xdvbwt,,deeplearning,2022-09-14 00:20:41,3
are you changing the learning rate?,10,xdd1fg,,deeplearning,2022-09-13 10:05:54,19
Wrong sub.,1,xdxmj8," Easy Tutorial on How To Create Brush Stroke Effect in Canva   
 

https://preview.redd.it/aync9tdkpsn91.png?width=1280&format=png&auto=webp&s=f0ac1c89b2519f30953c4277df71589d8374ecba

[Tutorial link](https://youtu.be/FtUIclM4fSU)",deeplearning,2022-09-14 02:45:57,1
What does this add on top of MONAI? The demos look just as complex.,1,xd6gqs,"We have released version 0.1 of fastMONAI, a low-code Python-based deep learning library for medical imaging built on top of fastai, MONAI, and TorchIO. We created the library to simplify the use of state-of-the-art deep learning techniques in 3D medical image analysis for solving classification, regression, and segmentation tasks. The entire library is written using nbdev, a tool for exploratory programming that allows you to write, test, and document a Python library in Jupyter Notebooks. You can install the library using pip and download the tutorial notebooks here: [https://github.com/MMIV-ML/fastMONAI](https://github.com/MMIV-ML/fastMONAI). If you decide to try it out and discover any issues, please don’t hesitate to reach out!",deeplearning,2022-09-13 05:39:52,1
"StableFace generates talking face videos with reduced motion jitters using 3D face representations.

Quick read on how it works: https://www.qblocks.cloud/byte/stableface-talking-face-generation/ 

&#x200B;

Developed by Microsoft Research Asia with Shanghai / Tsinghua University.",4,xcicfr,,deeplearning,2022-09-12 10:09:25,5
"Most products / services out there use similar architectures and fine tune them with domain specific data.  


Thats a fancy way of saying they take the same 'engine' and make it work for their task. They do this by providing specific examples of data that looks more like the data their customers will be using than the model originally has.  


You need to ask yourself does an off the shelf solution work well enough, or do you need to tune it for a specific task.   


Thats a hard question to answer if you're new to the space, so my advice would be to just pick one, and get it working, and try to do some things with it. You'll learn a lot about the space and can determine quickly if you need to learn more ML stuff, or just move on with an existing tool.  


The task you seem to be looking for is 'segmentation'",2,xd9xwq,"I do not know much about Machine learning, and I am not sure if I can ask question here. But if yes, I need help with either choosing best libraries to do Video Editing like Background Removal and similar. Some of the ones that I found is RVM: [https://github.com/PeterL1n/RobustVideoMatting](https://github.com/PeterL1n/RobustVideoMatting) (which currently seems like the best choice)  


I wonder is there better one and are there similar ones for things that [https://www.cutout.pro/](https://www.cutout.pro/) does? I am not new to coding, just Machine Learning things? Also is it possible to use any of those libraries including RVM to train over time or are training sets always pre determined?",deeplearning,2022-09-13 08:03:51,1
"You could probably make the first convolutional window bigger with double stride, as well as the last transposed convolution one, and reduce your network to 4 encoders and decoders.

You're not going to get further performance optimizations without hardware support on NNAPI generally",1,xd316n,"0

I have implemented an encoder-decoder architecture-based neural network with Neural Network API(NNAPI) Android Ndk.

There are 5 encoders and 5 decoders. The first encoder's input dimension is -> 1x160084 and the last encoder's output dimensions are 1x624. It takes 6 seconds to finish. Every encoder contains two convolutions.

The first Decoder's input dimension -> 1x624 and the last decoder's output dimensions are 1X160084. It takes 26 seconds to finish. Every decoder has a convolution and a transposed convolution.

The execution time is much slower in the decoder. But both work on the roughly same size of data. Why is there such a difference? I need to decrease the execution time for the decoder. I have found that transposed convolution is taking maximum time.

For the first transpose convolution input dimension is (1x624x1024) Output dimension is (1x2500x512),kernel size is (1x8x1024). Stride is 4. It is taking almost 5 seconds for this transpose convolution operation.

If we used a naive approach for transposed convolution it would take -> 1024 \* 624 \* 8 \* 512 \* 2 = 5234491392 operations. So it would be executed in 52s(if 1e8 operations are executed in 1 s). So there might be some optimization NNAPI implementation for transposed convolution.

Is there any way to see the implementation of NNAPI for transposed convolution and how to further improve it?",deeplearning,2022-09-13 02:35:32,2
Check it out: https://github.com/JoaoLages/diffusers-interpret,1,xci6dm,,deeplearning,2022-09-12 10:02:51,1
"It's so pedantic but I don't agree with your terms.   

>The key task of machine learning systems is to find patterns in existing data and then try to predict similar patterns in new data.

This is a good definition for ""pattern recognition"" and not really ""machine learning"".

The reason is because this definition excludes a lot of machine learning methods like generative methods (no new data, only generated data), genetic algorithms and reinforcement learning algorithms (no prediction based on new data, only rewards for the agent), unsupervised learning (clustering, dimensionally reduction, etc has no prediction of new data), and probably more that I'm not thinking about. In addition, your description includes things that I wouldn't consider machine learning like heuristic and rule-based algorithms (which I would still count as pattern recognition).

>Deep learning is a subset of machine learning. Whereas machine learning uses basic models relying on separate algorithms, deep learning models use neural networks. 

Yes, neural networks are normally deep learning, but as you state in the first sentence, deep learning is a subset of machine learning. Subsets don't work like your second sentence. All deep learning models are machine learning models. After all, ICML and JMLR are mostly neural network papers nowdays. Machine learning models are not just basic models. 

And anyway, some neural networks can be incredibly basic, even more so than classical methods.

>need to handle complex tasks requiring decision-making: image classification, speech recognition, NLP, and so

I would argue that most of these tasks are not complex. With exception to NLP, the other two are some of the more basic tasks of machine learning. If you had enough time, computational power, and a large enough dataset, you could probably throw most classification tasks at nearest neighbor (the most simplest classification method) and it will have an equal or better accuracy than a neural network. Of course there are huge advantages of a neural network like speed and better generalization, but that doesn't make the task ""complex"".


About your question. I would use ""machine learning"" when referring to the field.  I would use ""deep learning"" when I want a buzzword when referring to neutral networks.",2,xd02ls,"Machine learning is a subset of artificial intelligence that relies on the use of statistical learning algorithms. The key task of machine learning systems is to find patterns in existing data and then try to predict similar patterns in new data. To do so, machine learning systems continuously learn and improve themselves by processing structured data.

My team uses machine learning when we:

* have a small set of quality labeled data to train a machine learning model
* want to automate routine business processes (user identity verification, sales information analysis, medical diagnosis, etc.
* Deep learning is a subset of machine learning. Whereas machine learning uses basic models relying on separate algorithms, deep learning models use neural networks. In contrast to machine learning models, deep learning models show better performance on large datasets and allow for using already built and trained neural networks for new tasks. To make complex predictions, deep learning systems may use massive volumes of data.

Deep Learning would be a better choice in following cases:

* have large volumes of unstructured data
* need to handle complex tasks requiring decision-making: image classification, speech recognition, NLP, and so on

To learn more about the difference between Machine Learning and Deep Learning, as well as see examples of how these technologies are used in real life, [read the article](https://www.apriorit.com/dev-blog/472-machine-learning-applications?utm_source=reddit&utm_medium=organic&utm_campaign=blog&utm_term=ai).",deeplearning,2022-09-12 23:28:36,1
Nightmare fuel,1,xc4d78,,deeplearning,2022-09-11 22:30:40,1
"If you find literature related to the problem you are investigating, where NNs are trained/tested on datasets of similar size as yours, it makes sense to start from there. As a rule of thumb, start small and avoid bigger models for small datasets, or you might risk to overfit. I don’t think you should necessarily aim to publish your neural network architecture, especially if your field of research/application is cyber security. Maybe you could publish an applied paper related to the problem you are investigating.",1,xccfua,"Hi,

I am now researching the use of Neural Networks for cybersecurity and the goal is to study different datasets in the different scenario but my doubt is if should I use other NN already built and tested (and if so where can I find them? Tensorflow website or are any others?) or should I build my own NN and if the results are good should I publish that NN?

&#x200B;

Thank you",deeplearning,2022-09-12 06:09:42,2
"Try sampling frames. 200 and 600 frames for input size is too much. I don't know about your use case but sampled 64 frames was more than enough for a lot of projects. You can choose random, sequential or uniform sampling methods according to your needs.",1,xbbn0i,"I am attempting to use EfficientNet3D for a video classification task.

[https://github.com/shijianjian/EfficientNet-PyTorch-3D](https://github.com/shijianjian/EfficientNet-PyTorch-3D)

&#x200B;

I wanted to use a 3D-CNN that allowed me to input higher resolution images. I believe my project accuracy is currently suffering because my image inputs are 112x112. So much data is lost when resizing original images to 112x112, shapes and features are almost unrecognizable even by human eye.

I think that instead inputting images of a larger resolution; ex. EfficientNet 'b7': (633, 600), I would get much higher accuracy.

However I loaded up the model and when I try summary(model, input\_size=(1, 200, 200, 200)) , the model takes \~30 seconds to display torchsummary

When I tried summary(model, input\_size=(1, 333, 633, 600))  the model took >15 minutes. (I believe this could also be a memory issue, i'm not sure)Am I doing something wrong??

Has anyone else tried 3D-CNN with larger image resolutions?

Should I ditch 3D-CNNs all together and try a different route? If I cant practically input larger image resolutions then this is a dead end.",deeplearning,2022-09-10 23:54:20,1
"Not a fair comparison. Apples and oranges. oBERT != BERT-Large. oBERT is pruned so it is no surprise that it is faster. If you want to make an impression, give fair numbers.",3,xav2ef,,deeplearning,2022-09-10 10:33:10,10
"You can look at the official TF and PyTorch implementations, as well as the flax one for JAX.",1,xbcpd7,I’m working on processing 1024-length signal sequences. If you have any helpful info/paper/article on a similar matter or just to inspire myself on the attention-based structure i would be very thankful! 🥲,deeplearning,2022-09-11 00:59:42,1
"This is for benchmarking. There is likely not enough knowledge to create a competitive model. If you don't have resources, just use ImageNet Tiny.",1,xb3aql,"Within the scope of knowledge distillation, I proposed a novel idea. I don't have the computer power to train from scratch on ImageNet. Is it possible to train on a subset of Imagenet (see below) with a pretraining point as an initial point while using the entire validation set for testing? This subset is balanced and makes up around 1% of the whole training set. In addition to this, I will also support my results with training from scratch on small datasets like CUB200 and Standford Dogs. 
P.S. My approach is limited to high-resolution images. I can not use CIFAR10 or CIFAR100 or Tiny ImageNet.

&#x200B;

[https://www.tensorflow.org/datasets/catalog/imagenet2012\_subset](https://www.tensorflow.org/datasets/catalog/imagenet2012_subset)",deeplearning,2022-09-10 16:31:16,4
There is a bug in the code,4,x9zppn,,deeplearning,2022-09-09 09:19:38,4
unbalanced training set ?,1,xa7mmt,"I have trained Cifer 10 using resnet.I Got an accuracy of 98% in train set but still when I do prediction on images of the training set,I always get the same prediction every time, means the prediction is same for all the images I test on. What could be the reason behind it??",deeplearning,2022-09-09 14:51:32,1
https://paperswithcode.com/area/audio,2,xa1d1h,How to train mp3 audio in the model. Is there is any architecture to train audios ?? Give some clarification please,deeplearning,2022-09-09 10:27:29,1
"If you're using softmax at the output (with a log loss function), then you're scaling the outputs to have an aggregate of 1.0. But if your ground truth has multiple 1s in it, then the aggregate adds up to more than 1.0, so your gradients will never shrink to zero, no matter how much you train it.",2,x9739x,"  I came across an issue when writing my own neural network.  I was messing around with back propagation and noticed it seems to be limited to only being able to correct outputs as if they are the answer themselves. 

For example:  I was attempting to get my machine to do simple multiplication.  I decided to use a single normalized float output that represents 0-1000.  No matter how much I trained it on multiplication, it just returned complete trash.  But then I tried something else, I made the network have an output array of 100 floats and used those to represent integer answers to the questions. And it worked! 

Am correct that back propagation has this limitation?  Do you really need a different output for every answer?",deeplearning,2022-09-08 10:50:52,7
"View in your timezone:  
[Saturday, September 10th, at 10AM CEST][0]  

[0]: https://timee.io/20220910T0800?tl=How%20to%20detect%20fraud%20with%20Memgraph%3F%20-%20Live%20stream%20on%20Saturday%2C%20September%2010th%2C%20at%2010AM%20CET


^(_*Assumed CEST instead of CET because DST is observed_)",1,x9158q,,deeplearning,2022-09-08 06:54:52,1
"This article is just promoting your startup, it doesn’t explain how you fixed the labels.",10,x9ais9,"Hi folks! I've made a new technique for finding errors in semantic segmentation datasets, using new explainable AI techniques from my PhD. I was frankly pretty surprised to be able to find over 50 different error patterns , and 7% of total pixels labelled incorrectly, in MIT ADE20K (one of the most widely used segmentation datasets). In the corrected dataset, some of the less common classes are tripled in size.

While there's been some work on improving ML datasets generally, as far as I know this is the only work on semantic segmentation, where each pixel in an image is given a label.

I would love to get the communities thoughts on this. I am also building a company, so if you're interested in using this in your work feel free to DM me.

To learn more about the results (and see more pictures), check out my article:

[https://medium.com/@jamie\_34747/how-i-found-over-50-label-issues-in-a-popular-semantic-segmentation-dataset-95b025a6f1b5?source=friends\_link&sk=59763b5bcd810230bcf20b2ca4e6fa0e](https://medium.com/@jamie_34747/how-i-found-over-50-label-issues-in-a-popular-semantic-segmentation-dataset-95b025a6f1b5?source=friends_link&sk=59763b5bcd810230bcf20b2ca4e6fa0e)

If interested, I also did similar work on object detection in MS COCO, where I found nearly 300,000 annotation errors: [https://medium.com/@jamie\_34747/79d382edf22b?source=friends\_link&sk=d36ad07c074818c48d8f421f6ed104cd](https://medium.com/@jamie_34747/79d382edf22b?source=friends_link&sk=d36ad07c074818c48d8f421f6ed104cd).

[Example labels from MIT ADE20K. When trees overlap with the sky, it is sometimes labeled as “tree”, sometimes labeled as “sky”](https://preview.redd.it/faf9s6yf0pm91.png?width=875&format=png&auto=webp&s=fa5884b3ed41c1d7c5c3afbf0602e71e3e90243f)",deeplearning,2022-09-08 13:15:53,1
🙄,1,x8wm1p,"  Easy Tutorial on How to Blur Edges of an Image in photopea , it's an easy guide to feather and blur edges     
 

https://preview.redd.it/24ipl8jf1mm91.png?width=1280&format=png&auto=webp&s=e4c328838723b716c10ec3ec179cc001972a55c1

[Tutorial link](https://youtu.be/FkqGM5m7nPQ)",deeplearning,2022-09-08 03:15:49,1
Check out Deeplizard series on YT.,6,x83yoa,"Here's an example StatQuest video: https://www.youtube.com/watch?v=J4Wdy0Wc_xQ

I'm looking for YouTube channels or other online resources that do fairly high-level but intuitive explainers of how various DL architectures work. A pretty good example is 3b1b neural network series, but he only goes through MLPs. It'd be awesome to have videos like that but for more architectures (RNNs, CNNs, GANs, GNN, transformers etc.)

Feel free to suggest more than one channel, as I doubt any singular channel will cover everything.",deeplearning,2022-09-07 05:33:57,5
"References:  
►Read the full article: https://www.louisbouchard.ai/general-video-recognition/  
►Ni, B., Peng, H., Chen, M., Zhang, S., Meng, G., Fu, J., Xiang, S. and Ling, H., 2022. Expanding Language-Image Pretrained Models for General Video Recognition. arXiv preprint arXiv:2208.02816.  
►Code: https://github.com/microsoft/VideoX/tree/master/X-CLIP  
►My Newsletter (A new AI application explained weekly to your emails!): https://www.louisbouchard.ai/newsletter/",2,x8nc6t,,deeplearning,2022-09-07 18:58:38,1
"Thanks for making this. I've been meaning to try out some machine learning, and this might be a good place to start.",2,x7dr89,"## Tutorial Series: Intro to Computer Vision with PyTorch

Hey all. I'm a PhD student in experimental physics and I needed to learn how to apply Mask RCNN (or some instance segmentation model) to a custom dataset for my research to detect physics signals in spectrograms. While learning the material over the last several months, I built an Introduction to Computer Vision tutorial series using pytorch and pytorch lightning. 

Summary of the [Tutorial Series](https://www.youtube.com/playlist?list=PLS84ypkqWiQ8-TL0AmTRynkzK0v-d4C5m): By the end of this series, a PyTorch computer vision novice should have the tools to train any of the models we cover (object counting with CNNs, image segmentation with UNET, Faster RCNN, Mask RCNN) on a custom dataset (Part 1 - Part 4) and also quickly apply a trained Mask RCNN model to their own images and videos (Part 5). Enjoy!

For each video, there is an accompanying colab notebook so one can easily follow along and do one's own exploration. The youtube tutorial series can be found [here](https://www.youtube.com/playlist?list=PLS84ypkqWiQ8-TL0AmTRynkzK0v-d4C5m). The full documentation for the project and links to the notebooks can be found on [GitHub](https://github.com/drewbyron/pytorch-tutorials/blob/main/README.md). 

I also made a PyPI package that contains all of the classes and methods created in the colab notebooks so that people can easily create their own sandboxes to play around in and start to build intuition. The full documentation for the package is [here](https://github.com/drewbyron/pytorch-tutorials/blob/main/pytorch_tutorials/intro_to_computer_vision/README.md).

I hope this is useful for those new to applying deep learning to computer vision tasks! 

For deep learning veterans, I would really value your feedback on the project.",deeplearning,2022-09-06 08:40:20,5
"I think this is kinda cool !
This is a great project to get a feel of what NLP models can do, and get a bit of practice in.
May I suggest trying to expand the model to other types of plot ? Pretty much having a model able to classify between genres like horror, thriller and sci-fi along anime.
You could even get it to output different values for how much a plot fits every category, that would be pretty neat and a nice exercise.",3,x76i6o,"You can play with it on [HuggingFace Space](https://huggingface.co/spaces/daspartho/anime-or-not)

Simply write a plot and it will determine whether or not it is an anime story.

I fine-tuned DistilBERT on a dataset of plots labeled as anime or not. To make the dataset, I scraped IMDb lists for anime and vice-versa and then extracted plot details using IMDbPY.

Notebooks for data collection and modeling are available on the [GitHub repo](https://github.com/daspartho/anime-or-not).

I've recently been experimenting with NLP and the fantastic HuggingFace Library (still learning things) and I used what I've learned to make this project.

I think it's kinda cool and I'm proud of it. I'd appreciate any feedback or suggestions on the project and would love to see what you guys come up with using it :)",deeplearning,2022-09-06 02:56:38,2
"You can use them, but you'll likely have to specify which apps run on which card. You can't use them simultanously for a task.

But honestly I do not see why you couldn't just use the nvidia one as a display card. You might lose around 300-500 MB of VRAM in doing so, but it is not like the power draw and heat of the 570 justifies using it as a display adapter only.",2,x7fdx3,"I have a desktop with AMD 3950x cpu and amd Rx570-4GB built 2 years back. Used to run okish with tensorflow+rocm till few months back. Now, With recent linux kernel update to 5.15, the setup is broken totally. I am looking forward to order a nvidia rtx3060 12 gb. My question is, can I put both amd GPU and nvidia GPU in the same machine. Plan is to use rx570 solely as a video card (as the 3950x cpu doesnt have internal graphics) and use the nvidia card solely for deep learning. Will this work?.",deeplearning,2022-09-06 09:46:42,5
"🚀 Find out how to get started using Weights & Biases 🚀  
[http://wandb.me/ai-epiphany](http://wandb.me/ai-epiphany)

  
In this video I cover 3 publicly shared LLM 🚀 projects/papers and the pain they experience training them (🍿🍿🍿):  
  
1. ""What Language Model to Train if You Have One Million GPU Hours?"" introducing BLOOM 176 billion parameter model by BigScience!  
  
2. ""OPT: Open Pre-trained Transformer Language Models"" introducing 175 billion parameter model OPT-175B!  
  
3. ""GPT-NeoX-20B: An Open-Source Autoregressive Language Model"" introducing, well, GPT-NeoX-20B, a 20 billion parameter LLM!  
  
All 3 projects shared their weights, code, and papers, so it's a great way to dig into large language models and understand them better.   
  
I walk you through the papers and the chronicles/logs they've shared sharing the pain they experienced training at these scales! :))  
  
Cluster deletion, 2 million consecutive backslash symbols in the dataset, and much more - it's fun!",2,x78kop,,deeplearning,2022-09-06 04:51:42,1
https://www.amazon.com/Production-Ready-Applied-Deep-Learning-TensorFlow/dp/180324366X,0,x7j0sm,,deeplearning,2022-09-06 12:12:11,1
"Indeed, it'll be slower than an Nvidia GPU, and much slower than an Nvidia RTX GPU. That said, you shouldn't really be **training** DL models on a laptop. You'll be fine **running** models, especially small models that fit within your 8 GB of VRAM. I think Stable Diffusion should fit, but I can't say for sure.

Similarly, training deepfakes would be painful. Running some deepfake models that someone else has trained should be fine, just don't post your results.",1,x7i45t,"So as the title suggests I recently got this laptop but unfortunately It is  past the return window, 4 days past the return window. I wanted to tinker around deep fakes and stable diffusion but amd is still not supported for these models yet.

 
I am also a computer science grad who is looking to get into deep learning concepts. I know amd graphic cards have a lot of limitations with respect to DL and  ROCM  is still lagging behind CUDA.  How much of deal breaker is it? Should I sell it for a Nvidia powered GPU? 

Sorry for the formatting, I am on my phone.",deeplearning,2022-09-06 11:35:56,1
"I got a better one for ya:  
Can humans super-impose our consciousness and experiences to things that aren't human any more?",2,x785x5,,deeplearning,2022-09-06 04:30:34,2
"Hierarchical deconstruction followed by synthesis in the latent: identified beats are clustered and projected onto a unit sphere.  
  
You might be asking is identifying beats part of the deconstruction or synthesis process. The idea is to generate data redundant at one scale and abstract in another, before inverting the process, arriving at abstract data in the first scale. Call this the latent.  
  
From there it's a matter of projecting on to whatever medium/plane preferred.",-1,x70rqe,,deeplearning,2022-09-05 21:16:14,1
How many free courses do we need?,1,x6cnsw,,deeplearning,2022-09-05 03:16:31,1
"The task you're describing is called denoising. It's usually done with autoencoders or GANs, or more recently various transformer networks.",3,x6p216,"Hi all, I'm relatively new to deep learning, but I've been doing various machine learning things for a few years now. I'm writing my master's degree dissertation on using deep learning to remove reverb from human speech. 

I've made quite a nice large dataset (about 200gb) consisting of two folders; one with normal dry speech, one with the same speech with various reverb effects applied. 

My issue is that for the life of me I can't find anything on this type of learning, where you have two sets of data and want to make one set as close as possible to the other (in this case, make the reverberant sound files sound like the anechoic sound files). Everything I've done so far has been classification like classifying images of fish or flowers. 

My guess is that this will end up being image based learning by using fourier transforms to turn each clip into a spectrogram image (or similar visual representation of sound data).

Any clues as to what type of deep learning I should be looking into? Any example projects I might find useful?

Many thanks for any help that can be offered, I'm scratching my head trying to figure out what this type of learning is",deeplearning,2022-09-05 12:34:24,1
"Those are fairly weak arguments- 

First off, the fact that innate behaviors are present in animals doesn't imply anything about them being necessary or even beneficial for an organism's ability to learn.

Secondly, brains not encoding the architecture of their neural network are basically analogous to the fact that NNs don't encode which edges the network is going to use in the learning process. I can think of lessons to take from animal neural networks, but this isn't one of them.

Lastly, comparing birds to planes is a bit silly- the fact that their goals are different is obvious to anyone at a glance, and when we say ""planes fly better"", we say it in the context of what humans want from flying objects, not in an absolute sense.",3,x69mll,,deeplearning,2022-09-05 00:04:15,2
"I’d say it’s best to dive right into the code. My  current favorite is the Colab notebooks by Phillip Lippe for the University of Amsterdam course, which are very good in terms of getting you up to speed *and* showing and explaining enough details. There also are videos available, if you prefer that. https://uvadlc-notebooks.readthedocs.io/en/latest/",2,x6as5s,"I have dabbled a bit in neural networks in college ( just passed out with my undergrad in electronics). It was always some quick fix by pasting codes from different sources, using stack overflow and modifying existing code. So it is safe to say I only have a very vague idea on anything or maybe I am a complete newbie. 

So I would like to start all over again. Where should I begin, what must I focus on at first? Do I go hardcore math at first to learn the more basic machine learning models and implement them from scratch or jump to deep learning models within the tensorflow package and how to better use them? Every day I see newer models being released.

I would be grateful if you guys could give me suggestions on what to do or how to proceed. Sorry for the vagueness of the post since I myself have pretty clouded thoughts on the matter. Thank you again.",deeplearning,2022-09-05 01:16:10,13
"OP this isn't *that* kind of ""deep learning"" sub.",10,x6f41w,"Learning quickly  can give you an even greater advantage. If you want to learn faster, you need a new approach to the process that allows you to understand the essence of matter and relate it to the new concepts you encounter.

The following  9 tips will help you become a fast learner:

1.Analyze your learning style:
Before you can start experimenting with different learning methods, you need to understand what type of learner you are:

-Is your memory linked to sounds? Maybe you can remember what you read when a certain song was played? If so, then you fall into the category of auditory learners.


If you want to start your studies more efficiently, it is advisable to record and listen to the lectures instead of reading textbooks. Does it relate the information to the visual content?


If you are a visual learner, you need to implement images, charts, infographics, colored lists ,tables, flashcards …

2-Create a useful environment :
Your environment plays an important role in learning. Decide what is the best environment to learn faster and more appropriate for you. For most people, a very quiet place is ideal. In this case, you need a quiet environment dedicated to study. if you want to give your brain the best chance to learn the information. The truth is that a quiet, distraction-free environment means...[Read more ](https://viemina.com/top-9-secrets-to-fast-learning/)",deeplearning,2022-09-05 05:34:12,2
GPU use always results in non-deterministic computation because the order of operations is non-deterministic and precision errors change with different order of operations.,3,x5giij,"My results are not reproducible while the global seed is set. Am I doing something wrong or is this an issue of the macbook M1?

I am using:

&#x200B;

* MacOS 12.5
* Python 3.10.6
* tensorflow-macos 2.9.2

Besides, the model is also unusually bad.

The datasets are from [https://www.kaggle.com/competitions/dogs-vs-cats](https://www.kaggle.com/competitions/dogs-vs-cats)

Code is at: [https://stackoverflow.com/questions/73597382/tensorflow-results-are-not-reproducible-while-the-global-seed-is-set-python](https://stackoverflow.com/questions/73597382/tensorflow-results-are-not-reproducible-while-the-global-seed-is-set-python)",deeplearning,2022-09-04 00:04:57,6
"CPUs are not suitable for training deep neural networks, at least for now. You definitely want a GPU, but do not limit yourself to your equipment on hand especially in a laptop.

I'd suggest cloud services like Colab, GCP, AWS, or Lambda over a laptop-grade GPU. The latter is usually underpowered yet super hot and loud.",14,x54tkr,"
I m going to buy a new laptop and the only thing that concerns me is whether I am overspending for laptop in GPU. I am doing my BTech in Artificial intelligence. And my task includes running heavy softwares like visual studio, Android studio, matlab, etc. simultaneously. And I am also going to deep dive into deep learning.

So the question is, do i need to buy a laptop with dedicated GPU for long run? 

If not, which intel or ryzen processors will do the job?",deeplearning,2022-09-03 14:06:05,28
"I would wager also everyone who is building/developing ML solutions in a business context faces these problems to a degree.

For 1: In a business context there is so much data it must be stored in different ways/servers/databases/etc with varying degrees of granularity and new data streams are being added all the time.  Furthermore different teams/people are constantly adding new fields/features and deprecating old ones (which need to stay on the books for backwards compatibility).  This makes keeping complete and updated documentation a Sisyphean task.  

People are rewarded for writing something that works and gives an immediate boost to the companies revenue/profit.  If they happen to document it well that is nice to have, but when is the last time you saw someone brag about how well documented their project was on their resume or internal release memo?  This problem can be addressed partially by company/team culture, but you are fighting against the Nash equilibrium of the prisoner's dilemma.  Most people can maximize their career earnings by forgoing documentation, building fast/dirty solutions for nice ROI, repeat until you have spaghetti code, then jump to a new team/company with a nice bump in compensation (and run this cycle every 3-4 years).

For 2: ML in a business context is not about building SOTA models, it is about building a tool that solves a business problem.  The goal is not a model with the best test scores, but to maximize ROI for the cost of the tool.  Most business problems are interdisciplinary regarding the types of ML they need and models get chained together to improve the final results.  Eg a forecasting task for 100k concurrent times series could be significantly improved by clustering those time series so you can use ""similar ones"" as additional features in the forecast of each individual series.  

Unless you want to just sit in a silo maintaining and tweaking parameters of a base level model (that is plugged into other models), you need to be interdisciplinary.

----

That being said if you can build a user friendly solution to either of these problems that requires minimal user effort to maintain and easily integrates with companies current systems, then you will be a billionaire.  Also you probably will have built an AGI...",2,x4p10h," 

Dear community,

I wanted to explore and understand and see if the community is facing the same problems while developing ML solutions as detailed below. Also, how are they solving these problems:

1. **Hard to identify and discover features:** As a data scientist, it used to take me 10 days to find the right features for my models. I used to solve this problem by talking to domain experts or searching the database for features. Then finally writing from scratch the sql queries to extract the data.
2. **Time consuming search for techniques/ML algorithms:** As a data scientist, it is  painful for me to search for new techniques relevant for the problem at hand. Say, I might be a Computer Vision specialist, suddenly I am asked to work on forecasting problem. The pain of searching over the web for solutions and then learning it and finally coding it up is extreme.

Eager to hear your views on this",deeplearning,2022-09-03 01:05:11,3
"Admittedly I haven't looked at your code at all, but if your first epoch never ends and you have a custom data generator, your generator likely never terminates.

Add prints as needed and reduce the number of actual samples per epoch to a small debug subset (e.g., 2 batches' worth of it, or maybe 2.5 so you cover the incomplete batch case too), then debug through your generator and see what happens when it runs out of data.",1,x4r7fc,"I'm training a unet, however the model seems to not go past the first epoch on training. Any help appreciated!

I have a dataframe, which contains the path to the image & its mask. 

The custom data generator function : 

 

`def custom_generator(dataframe,image,label,batch_size,input_size):`  
  `list_images = dataframe[image].tolist()`  
  `list_masks = dataframe[label].tolist()`  
  `ids_train_split = range(len(list_images))`  
  `print`  
 `while True:`  
 `for start in range(0, len(ids_train_split), batch_size):`  
      `x_batch = []`  
      `y_batch = []`  
      `end = min(start + batch_size, len(ids_train_split))`  
      `ids_train_batch = ids_train_split[start:end]`  
 `for id in ids_train_batch:`  
        `img_name = list_images[id]`  
        `mask_name = list_masks[id]`  
        `img = cv2.imread(img_name)`  
        `img = cv2.resize(img, image_shape, interpolation = cv2.INTER_AREA)`  
        `mask = cv2.imread(mask_name)`  
        `mask = cv2.resize(mask,image_shape, interpolation = cv2.INTER_AREA)`  
        `mask = preprocess_mask_image2(mask, 50)`  
        `x_batch += [img]`  
        `y_batch += [mask]`  
 `break`  
      `x_batch = np.array(x_batch)/255.`  
      `y_batch = np.array(y_batch)`  
 `yield x_batch, np.expand_dims(y_batch,-1)`

This is my training code block :

 

`batch_size = 2`  
`image_shape = (128,128)`  
`train_gen = custom_generator(data_train,'image','mask',batch_size,image_shape)`  
`valid_gen = custom_generator(data_test,'image','mask',batch_size,image_shape)`  
`start = time.time()`  
`history = model.fit(`  
    `train_gen,`  
    `epochs = 10,`  
    `steps_per_epoch = 554,`  
    `verbose = 1,`  
    `validation_data =valid_gen,`  
    `validation_steps=1,`  
    `class_weight=None,`  
    `max_queue_size=10,`  
    `workers=1,`  
    `use_multiprocessing=False`  
 `)`  
`end = time.time()`  
`print('/n/t Time Taken : ',end-start)`

The training seems to not move past the first epoch. 

&#x200B;

[took 50s to train for one epoch, waiting since the last 5 mins.](https://preview.redd.it/g6qzmy6yeml91.png?width=576&format=png&auto=webp&s=8b32778ffdf1c883f0ad080e3455928d4d37df6e)

I looked up online and added a `use_multiprocessing` but that didn't fix this issue.

There's 8k samples in training and 1.4k in validation.",deeplearning,2022-09-03 03:29:01,4
So whats your question? Why would anyone do your homework for you?,3,x54beu,,deeplearning,2022-09-03 13:43:52,3
"Thank you, will try it out",2,x3x6xn,"🐋 For researchers and data scientists familiar with Docker, I open sourced this setup for Stable Diffusion.

&#x200B;

Features:

\- one command install and run

\- uses the GPU (nvidia)

\- jupyterLab preinstalled

\- diffusers library preinstalled

\- notebooks ready to download and try Stable Diffusion

\- text to image, image to image and (coming soon) inpainting and outpainting

&#x200B;

Let me know if you try it out, contributions welcomed!

🔗 [https://github.com/pieroit/stable-diffusion-jupyterlab-docker](https://github.com/pieroit/stable-diffusion-jupyterlab-docker)",deeplearning,2022-09-02 02:39:02,4
"My first small project was a CNN layer visualizer. It visualizes the patterns or shapes that maxiamlly activates a layer in a CNN network. Helps you to better grasp what each layer is doing, and a great exercise to practice some tf workflows",2,x3yo14,"I would like to learn CNN and I think I understand basic concepts, but no interesting ideas. Does anyone know a list of exercises for beginners with gradually increasing complexity? Ideally exercises focus on common mistakes/problems and come with datasets",deeplearning,2022-09-02 04:05:53,1
"What I really want is to be able to send in an image, and get the prompt that would have generated it. Often times I know what style I want, but i don't have the words to describe it.",2,x3pi3i,,deeplearning,2022-09-01 19:14:48,3
"These frameworks, like TensorFlow and PyTorch, are written in C++, for speed, but expose an **API** for developers to use.

Then developers can use these frameworks' APIs by writing code in their language of choice, which is often Python. This way their code is better, but they still get the massive performance advantage of offloading the heavy-duty performing to the C++ framework.

So it depends on what you mean by ""implement machine learning projects"". If you're building a framework, use something performant like C++. If you're using a framework that someone else has built, use a more convenient language like Python.",6,x3uwxn,,deeplearning,2022-09-02 00:08:03,1
Any info on the architecture? How does it scale for several objects interacting with one another?,2,x2w6f1,,deeplearning,2022-08-31 19:44:43,6
"References:  
►Read the full article: https://www.louisbouchard.ai/psg/  
►Yang, J., Ang, Y.Z., Guo, Z., Zhou, K., Zhang, W. and Liu, Z., 2022. Panoptic Scene Graph Generation. arXiv preprint arXiv:2207.11247.  
►Code: https://github.com/Jingkang50/OpenPSG  
►Project page (PSG dataset): https://psgdataset.org/  
►Try it: https://replicate.com/cjwbw/openpsg, https://huggingface.co/spaces/ECCV2022/PSG  
►My Newsletter (A new AI application explained weekly to your emails!): https://www.louisbouchard.ai/newsletter/",1,x2taex,,deeplearning,2022-08-31 17:27:18,1
"What’s being predicted, and what is the input?",3,x2iuul,"For example, there is data like,

* Person1 // time 0 : time 1 : time 2 : time 3 : ~
    info 1     //  info1 values over time
    info 2   //   info 2 values over time
    info 3   //   info 3 values over time
* Person2 // time 0 : time 1 : time 2 : time 3 : ~
    info 1     //  info1 values over time
    info 2   //   info 2 values over time
    info 3   //   info 3 values over time
* Person3 // time 0 : time 1 : time 2 : time 3 : ~
    info 1     //  info1 values over time
    info 2   //   info 2 values over time
    info 3   //   info 3 values over time
	~
	~
	~

and if I want to train a RNN model with this kind of data to predict one of another person’s info, 
Which way can I make each person’s data distinct for model to train?

I think One-Hot Encoding is best to represent the distinction between each person, but only in ideal way.
because there will be curse of dimensionality inevitably.

I’ve been looking for a way to represent it, and found several methods, usually representing by figures, 
but I’m still not sure which way to use. Using any kind of figures may also misguide the learning possibly(even if very little).

so which way is the best way to represent the distinction between people for grouping the data to train a RNN model?",deeplearning,2022-08-31 10:06:04,3
Hot take: The fact that we can all appreciate this and get fooled for a second means “the AI” did a good job,69,x1gxfa,,deeplearning,2022-08-30 04:45:38,9
which version of yolor are you using?,1,x20oi3,"Hello everyone,

&#x200B;

I am doing an object detection task(Concrete/rock) on a moving conveyor (speed 0.3m per second). The main issue I am having is very bad mAP. When I saw the dataset I didn't expect more than 60% mAP. But it's hardly touching 50% which is pretty less compared to what I want to achieve. At the moment, I just wanna push it further. Here is a sample image from the dataset:

&#x200B;

https://preview.redd.it/tzk8ccuhcyk91.jpg?width=640&format=pjpg&auto=webp&s=ceab8d1205776ae73c15b580660af223fc072a4e

If you look closely, the picture quality isn't good, resolution isn't that high. Lots of other staff on the conveyor belt with the rocks. Even the belt itself has dirt that looks like rocks. 

&#x200B;

Things I have tried:

YoloV5, YoloR with some extra Augmentation, Hyperparam tuning, image resolution up/down scaling. But still there is no improvement. Any ideas/suggestions will be really appreciated given that the detection will have to be done in real-time.",deeplearning,2022-08-30 18:35:53,2
"Happy to share that our study on designing DNA sequences to control gene expression using generative deep learning is just out!

This is a continuation of our previous work, where we learned to 'read' regulatory DNA using deep neural nets, accurately predicting gene expression levels in multiple organisms and finding predictive regulatory grammar across whole gene regulatory regions.

[https://www.nature.com/articles/s41467-020-19921-4](https://www.nature.com/articles/s41467-020-19921-4)

Here we combine the predictive models with advanced generative models in an architecture termed ExpressionGAN that can be used to 'write' (design) de novo regulatory DNA with target gene expression levels.",4,x1lkxv,,deeplearning,2022-08-30 08:10:23,2
"When you say you have 240 data points, what do you mean exactly?

Is that the total number of samples you have, or is that the size of each training sample in your data set?

Because to train an LSTM I would normally be looking for >100,000 training samples.

If you have only 240 samples, I'm not sure if any ML technique will apply.",0,x1n3xx,"Hi guys, 

I'm working on a time-series problem (with \~240 datapoints) and every LSTM I train keeps predicting the same value for different inputs.

I tried every suggested solution on stackoverflow and similar pages (changing activation functions, dropout, lag, layers, amount of nodes, different lags) but it still keeps predicting the same value (with a few models I got a repetitive pattern that was predicted with three different values).

The last possibility I read is, that I have to few data, which makes sense. Still, wanted to check what you guys think - thanks in advance.",deeplearning,2022-08-30 09:11:46,1
"I’m not familiar with the model, but if there’s a latent space that’s much smaller than the input space, wouldn’t it be more efficient to randomly sample in that latent space?  I’m thinking that with each sample you could ‘train’ the input to best reproduce that latent state and manually label the emotion if necessary, like the way Google’s Deep Dream Generator works.",1,x1ahy7,"I was recently reading a research paper on the task of emotion manipulation, the task being to take in an image of a face and an emotion, and to output the original face augmented to the specific emotion.  The paper in question describes a model called GANmut ([https://openaccess.thecvf.com/content/CVPR2021/papers/dApolito\_GANmut\_Learning\_Interpretable\_Conditional\_Space\_for\_Gamut\_of\_Emotions\_CVPR\_2021\_paper.pdf](https://openaccess.thecvf.com/content/CVPR2021/papers/dApolito_GANmut_Learning_Interpretable_Conditional_Space_for_Gamut_of_Emotions_CVPR_2021_paper.pdf)) and does a great job based on the pretrained model on their github page ([https://github.com/stefanodapolito/GANmut](https://github.com/stefanodapolito/GANmut)). 

I was looking to try and implement this model for a project I am thinking about, which will involve inputting a face and an emotion, as one would expect. The issue here is that it is tough to figure out how to input an emotion. In the paper they describe two methods of inputing emotions into the 'learned conditional space', one being a cartesian plane with an x axis of \[-1,1\] and a y axis of \[-1, 1\]. Choosing some coordinate on this plane results in an output of a specific emotion. According to the paper, the emotions can be mapped on this plane as individual groups, however the boundaries and shapes of these groups are slightly random during training (figure 3 in the paper shows an example of this). I am trying to map out where exactly each emotion corresponds on the coordinate plane and came up with a crude approach, and am looking for if there is a better one. My approach is simply to take images and alter them iterating through the whole plane with a single image, saving the augmented faces, and running a facial emotion recognition model on the outputs. Then using the emotion output I map out the emotions on a scatter plot to get a rough estimate of where each emotion lies on the plane. 

I feel like I may be missing something that is a much better approach to figuring out where emotions correspond on the plane, and due to my relative inexperience I don't happen to know of any solutions this issue. It would be great if someone knew or came up with a better methodology for the task at hand.",deeplearning,2022-08-29 22:16:57,3
[deleted],3,x1bh43,"I was trying to run the code where I got the error like this 

Invalid argument:  No OpKernel was registered to support Op 'CudnnRNNV3' used by {{node cond_40/then/_0/cond/CudnnRNNV3}} with these attrs: [dropout=0, seed=0, input_mode=""linear_input"", T=DT_FLOAT, direction=""unidirectional"", rnn_mode=""lstm"", is_training=true, seed2=0, num_proj=0, time_major=false]

This is my final year project and I m really stuck into it for like 7-10 days. Can anyone help me out of this ?",deeplearning,2022-08-29 23:13:56,8
"🚀 Sign up for Cohere using my link and get 150$ credits (2x-ing Shakespeare lol) 🚀  
https://os.cohere.ai/?utm\_source=influencer&utm\_medium=&utm\_campaign=ALEKSA  
  
My code is here: https://github.com/gordicaleksa/stable\_diffusion\_playground  
Feel free to contribute and watch out for the deep dive into stable diffusion over the next couple of days. :)",2,x0u6bk,,deeplearning,2022-08-29 10:13:54,3
"I have a question though, can you use the GTX series cards with the PyTorch cuda? I thought you couldn’t..",1,x0qo3y,"Benchmarked two cards I have at the moment and I was kinda surprised by the results. I was looking around the internet for a benchmark to test these two cards as they cost me the same price. They have same number of CUDA cores and nearly same VRAM size. I used the benchmark by [AI Benchmark](https://ai-benchmark.com/alpha.html). I noticed the benchmark only uses 9.4GB of VRAM. 

**Is there any other benchmark that I can run on these cards?**

|Metric|RTX 3060 (avg of 2 runs)|GTX 1080 Ti (avg of 2 runs)|
|:-|:-|:-|
|Device Inference Score|8534|9744|
|Device Training Score|9141|9387|
|Device AI Score|17673|19128|",deeplearning,2022-08-29 07:49:09,5
"*ODD Platform* is the first tool to provide truly end-to-end data discovery, observability and trust from ingestion to production. Based on ODD Spec for metadata collection, it removes barriers and lets you add any tools to your stack.

&nbsp;

It is designed to meet the needs of various users (Data Scientists, Data Engineers, ML Engineers, BI Engineers, Analysts, Managers), to help make data more discoverable, manageable, observable, reliable, and secure. It addresses the inefficiencies of conventional data catalogs through standardized data collection, improved data catalog compatibility, end-to-end data lineage, and advanced data quality and data observability practices. 

&nbsp;

The platform is designed to accelerate time to value (TTV) and reduce the costs of building and maintaining data products for enterprises of all sizes.

&nbsp;

**Key wins:**

* Shorten data discovery phase

* Have transparency on how and by whom the data is used

* Foster data culture by continuous compliance and data quality monitoring

* Accelerate data insights

* Know the sources of your dashboards and ad hoc reports

* Deprecate outdated objects responsibly by assessing and mitigating the risks

&nbsp;

Everything is thoroughly explained on their [Github page](https://github.com/opendatadiscovery/odd-platform), and you can visit their [blog](https://blog.opendatadiscovery.org/) for the use case scenarios.",2,x07rfw,,deeplearning,2022-08-28 15:17:56,2
"It depends on what you want to do, what is the input dimension, why do you want to use a VAE, etc... It also depends on the amount of data that you want to use for training. Can you clarify your question?",1,x0ism6,"I’d like to find the best model parameters such as number of layers, number of neurons (channels) in hidden layers, or kernel size in CNN of VAE model. What is the best way of this? -especially the decoder part makes things difficult",deeplearning,2022-08-29 00:45:50,2
"I will consider that by machine learning, you mean ""machine learning that was done before deep learning ~~was invented~~ became accessible"". Else u/lamnguyenvu98 is right.

In machine learning you control the operations that compute the features from the image (see scikit-image library for example) while in deep learning you expand the neural network to the features part so the network figures out how to create meaningful features by itself. You increase the need for data and computational power doing so.

EDIT: not ""invented""",3,x0jp3y,,deeplearning,2022-08-29 01:46:26,2
Perhaps onnx runtime is a more lightweight solution that offers faster inference and also has a C++ API,1,x02q54,"All I'm finding is stuff about online deployment on some cloud. The transformers library seems to perform a lot of lazy module loading which makes it terrible for deployment modules like pyinstaller and py2exe.

Any recommendations on how to send an application that uses a HuggingFace model to a user that doesn't have Python installed?",deeplearning,2022-08-28 11:44:44,3
My guess is that this is a server (enterprise) product. Enterprise products are often expensive. You'd better off buying a 3090 for that price,1,wzru8l,,deeplearning,2022-08-28 03:02:35,11
"Yes.

But that ""random"" is wrong, even after reading the explanation. 
There is a clear direction that things are set and they evolve. In standard DL models there is a Loss function which minimizes the error, for example.",6,x01do2,,deeplearning,2022-08-28 10:47:47,10
Why wouldn't you be able to?,2,wzqz2b,,deeplearning,2022-08-28 02:06:23,6
"References:  
►Read the full article: https://www.louisbouchard.ai/latent-diffusion-models/  
►Rombach, R., Blattmann, A., Lorenz, D., Esser, P. and Ommer, B., 2022. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 10684–10695), https://arxiv.org/pdf/2112.10752.pdf  
►Latent Diffusion Code: https://github.com/CompVis/latent-diffusion  
►Stable Diffusion Code (text-to-image based on LD): https://github.com/CompVis/stable-diffusion  
►Try it yourself: https://huggingface.co/spaces/stabilityai/stable-diffusion  
►Web application: https://stabilityai.us.auth0.com/u/login?state=hKFo2SA4MFJLR1M4cVhJcllLVmlsSV9vcXNYYy11Q25rRkVzZaFur3VuaXZlcnNhbC1sb2dpbqN0aWTZIFRjV2p5dHkzNGQzdkFKZUdyUEprRnhGeFl6ZVdVUDRZo2NpZNkgS3ZZWkpLU2htVW9PalhwY2xRbEtZVXh1Y0FWZXNsSE4  
►My Newsletter (A new AI application explained weekly to your emails!): https://www.louisbouchard.ai/newsletter/",1,wz4ufo,,deeplearning,2022-08-27 07:55:18,1
I did 2 courses from Jose Portilla. One was  boot camp python for machine learning. Another was Tensorflow and keras for deep learning. These courses are highly rated. Personally I found them highly practical but low on theory. I would highly recommend you do just these 2 courses and then figure out where you want to head,2,wyplze," 

I purchased 7 courses from Udemy to go deep in python,R and ia. Could someone help me to say where should i start, then pass and what should i take last and if i have to return one this courses for a refound by redundant information? please. Well later i would like to learn GO, any courses that you can recomend?

[https://www.udemy.com/course/100-days-of-code/](https://www.udemy.com/course/100-days-of-code/)

[https://www.udemy.com/course/python-the-complete-python-developer-course](https://www.udemy.com/course/python-the-complete-python-developer-course)

[https://www.udemy.com/course/all-about-python-learn-to-code-and-make-multiple-apps](https://www.udemy.com/course/all-about-python-learn-to-code-and-make-multiple-apps)

[https://www.udemy.com/course/tensorflow-developer-certificate-machine-learning-zero-to-mastery](https://www.udemy.com/course/tensorflow-developer-certificate-machine-learning-zero-to-mastery)

[https://www.udemy.com/course/learn-data-science-deep-learning-machine-learning-nlp-r](https://www.udemy.com/course/learn-data-science-deep-learning-machine-learning-nlp-r)

[https://www.udemy.com/course/data-science-machine-learningtheoryprojectsa-z-90-hours](https://www.udemy.com/course/data-science-machine-learningtheoryprojectsa-z-90-hours)

[https://www.udemy.com/course/the-full-stack-data-scientist-bootcamp](https://www.udemy.com/course/the-full-stack-data-scientist-bootcamp)",deeplearning,2022-08-26 18:09:35,4
Excited to try this out,1,wy5wde,"🐳 Here is a docker containing everything you need to download, save and use the AI #StableDiffusion on your machine. It contains JupyterLab as notebook environment and the diffusers library ready to go.

You can fire it up with a simple ""docker-compose up"", but:  
\- It's necessary to have docker and docker must see the GPU (via nvidia-docker).  
\- It's also recommended a GPU with at least 6 GB VRAM

I'm planning to add notebooks for image2image and inpainting   
Let me know if you try it out!  
Contributions welcomed 😉

[https://github.com/pieroit/stable-diffusion-jupyterlab-docker/](https://github.com/pieroit/stable-diffusion-jupyterlab-docker/)",deeplearning,2022-08-26 03:45:09,4
Make them hold a small baby putin.,11,wxsg7n,,deeplearning,2022-08-25 16:06:27,6
![gif](giphy|1uwqJK9S8ilC8),2,wxad87,,deeplearning,2022-08-25 03:18:37,1
Skyrim,1,wxmb1t,"I want to run a new project that would be based around analyzing the event industry prior and post COVID in each country. I focus mainly on sector in Nordic, can anyone guide me to some good datasets from Denmark, Sweden, Norway and Finland? I focus on festival, conferences and events over 1000 attendees.",deeplearning,2022-08-25 11:54:47,2
"google colab: [https://colab.research.google.com/drive/1NfgqublyT\_MWtR5CsmrgmdnkWiijF3P3?usp=sharing](https://colab.research.google.com/drive/1NfgqublyT_MWtR5CsmrgmdnkWiijF3P3?usp=sharing)

demo built with gradio: [https://github.com/gradio-app/gradio](https://github.com/gradio-app/gradio)

hosted web demo for stable diffusion: [https://huggingface.co/spaces/stabilityai/stable-diffusion](https://huggingface.co/spaces/stabilityai/stable-diffusion)",1,wwv1o4,,deeplearning,2022-08-24 14:16:26,2
This is really great! Do you have other such bots too?,5,wwhru9,,deeplearning,2022-08-24 05:04:14,8
I need help understanding this too. Lmk when a suitable response is found.,2,wx8qyd,"Hello, I learn to implement Single Shot Detector (SSD) from scratch. However, I cannot understand the output in SSD. SSD tries to predict the prior/anchor box position offset and scale: 

&#x200B;

[output of SSD \[source: sgrvinod\/a-PyTorch-Tutorial-to-Object-Detection: SSD: Single Shot MultiBox Detector | a PyTorch Tutorial to Object Detection \(github.com\)\]](https://preview.redd.it/7t0flx6altj91.png?width=2768&format=png&auto=webp&s=dd3f857163f841b653c6be37722b7a7f81cc6270)

My question is that why position offset need to be divided by width/height of prior, why cannot model directly output the ""pure"" offset ( groundtruth's center\_x - prior's center\_x ). Another question is that why we take log on scale of prior, I think model can learn to predict ""pure"" scale ( groundtruth's width / prior's width ).

I really confused about this, thank you.",deeplearning,2022-08-25 01:38:43,1
"Just what I needed, Thank you",5,wwflc8,,deeplearning,2022-08-24 03:09:38,5
"\- [Andrew Ng's Artifical Intelligence course on Coursera](https://www.coursera.org/learn/ai-for-everyone) (free)

\- [Fast.ai](https://Fast.ai) Deep Learning course for Coders ([course.fast.ai](https://course.fast.ai)) (free)

\- [Udacity's Artificial intelligence nanodegree](https://www.udacity.com/course/ai-artificial-intelligence-nanodegree--nd898) (paid)

\- To put your skills to the test sign-up to [Kaggle](https://kaggle.com) and enter a competition",2,wwqt7x,  I want to start with my personal projects so I need resources to get some ideas.,deeplearning,2022-08-24 11:23:32,1
"I have an auto-neural-network-generating tool that’s intended for tinyML applications, but I haven’t added the ‘tiny’ bit to the interface yet (a max-weights field).  If you decide it would be useful for your IoT project, I’ll go ahead and wire that in ASAP.  URL is:  cdeeply.com/dnndoodler

NNs are a good option for tinyML because they’re fast and code-light if you keep the weight count down.  Both memory and runtime are roughly proportional to the number of weights.",2,wwgd74,"Heyo, We are starting a IoT/ML project at work and the idea is to do a live time tabular binary classification. The data will be fed from an array of sensors and based on this a model needs to be trained and be able to classifying in a streaming pattern. Some things to note, the complexity is incredible because the sensors collect data every 0.5 ms and the model needs to be able to make a prediction within 30 ms. Looking for a starting palce/ research papers that might be related to these. I've already started reading a bit about tinyML but the resources are not very standardized. If someone has already worked on a similar problem would love to have a chat with you. Thanks for your time.",deeplearning,2022-08-24 03:53:11,1
"Diffusion models seem to be the reigning models, but in general, GANs should be a good baseline model to implement",9,wvte49,"Deeplearning has been a bit of a hobby when I find the time.  I've been out of it for a while.  Are GANS still worth focusing on (for all applications, i.e. text, visual, audio, etc.)?",deeplearning,2022-08-23 09:34:31,12
"Code for https://arxiv.org/abs/2108.13041 found: https://github.com/abanitalebi/auto-split

[Paper link](https://arxiv.org/abs/2108.13041) | [List of all code implementations](https://www.catalyzex.com/paper/arxiv:2108.13041/code)



--

To opt out from receiving code links, DM me",2,ww92y1,"Just read Auto-Split: A General Framework of Collaborative Edge-Cloud AI by a group of Huawei researchers ([https://arxiv.org/pdf/2108.13041.pdf](https://arxiv.org/pdf/2108.13041.pdf)). How feasible is it to break up the models and  serve them to the edges and the cloud?

If it is possible, is this task easy to implement? Does Tensorflow, PyTorch or cv.dnn have guides or APIs to achieve the task? I am speaking from the perspective of a small development team that are not experts at machine learning. I can only imagine splitting the model at a non-functional binary level, not at a functional subgraph levels.",deeplearning,2022-08-23 20:49:49,1
You should probably make this a proper python package,2,wvytyp,"Hello, my new paper is out: [https://arxiv.org/pdf/2208.09399.pdf](https://arxiv.org/pdf/2208.09399.pdf), ' Diffusion-based Time Series Imputation and Forecasting with Structured State Space Models ', have a look, and perhaps our proposed models might be useful for you. I'm actually looking for a review of my repo: [https://github.com/AI4HealthUOL/SSSD](https://github.com/AI4HealthUOL/SSSD). 

I will appreciate if someone could tell me what to include/change there to improve, thank you",deeplearning,2022-08-23 13:15:01,3
How is this related to deep learning,1,ww0c9d," Hello everyone,

I and my friends are currently in the senior year of our university. We are currently looking for opportunities that we could do tourism-related business together. Therefore, we developed this questionnaire to explore about how one's daily life choices can influence one's travel choices as well as how one attached meaning to one's preferred theme of tourist attraction. The questionnaire composed of in total 19 questions. All of these questions were developed through various lens by different members of our team who majoring in Psychology, International Relations and Business Management. Each of these questions would allow the respondent to self-reflect his/her current situations, and think of how particular places/tourist attraction could play a role as a comfort zone in one's life. Having these questions responded would allow us to have a better idea of how we could accommodate customer's psychological needs in a form of either services or products.

If you are interested, please find our survey link attached below

[https://docs.google.com/forms/d/e/1FAIpQLScR-jfpsm0KdAIW28ddBx71cNTBjuVEAPueNEL\_9jCuOy6Oqg/viewform?usp=sf\_link](https://docs.google.com/forms/d/e/1FAIpQLScR-jfpsm0KdAIW28ddBx71cNTBjuVEAPueNEL_9jCuOy6Oqg/viewform?usp=sf_link)

Thank you in advance for your participation!",deeplearning,2022-08-23 14:16:21,1
"Following!

I am in the same situation! My current Asus ROG GL503 has worked beautifully for 4 years but is now showing wear and tear. I really need to upgrade my laptop. Moreover I am going into a very graphics and 3D intensive master’s program. So I need a laptop that gives me good battery life, is not too heavy and can maintain a good level of high load performance especially with deep learning and neural networks.my masters program deals with Unity, graphical programming, computer vision and deep learning. 

I have done some surfing and found that going for M1 max MacBook Pro 16 is our best bet but I need more credibility for that. I also don’t think I can afford an M1 max so I’m planning to go for the next step down version: M1 pro MacBook Pro 16 with 10/16 cpu and gpu cores respectively. 

However, I recently read that OpenGL support got deprecated for MacOS and that MacBooks aren’t generally good for graphics and 3D related tasks. Again, I need some more verification on this from people working with these. 

I have also looked at the Asus ROG Flow X16 laptop with 3070 Ti which seems like a solid option. But the caveat is that it can only maintain 5-6 hours of battery life whereas a M1 pro MacBook Pro can maintain 17 hours, that seems like a deal breaker for me.",1,wvt21n,"Requirement for: Data scientist / bioinformatics / computational chemistry / machine and deep learning 

Current utility: datasets from clinical trials, gene expression datasets for 20000 genes, structural biology: crystallography data/pdb/pubchem data/drug bank data/chemoinformatics data(rdkit).

Standard libraries used so far: numpy sklearn pandas seabon matplotlib statsmodels scipy tensorflow and then some. 

Interested in: cuda development, using clara discovery (a drug discovery platform by nvidia), 3D molecular modeling and simulations, generative models (GAN) on 3d molecular descriptors and crystallographic data to simulate molecular configurations/poses/protein atructir prediction 

I see many people claiming that the new MacBook can do such things but then I also hear how people say that making python work on macbook is cumbersome as is. But at the same time I am not interested in carrying a super heavy hard to carry device that's gonna cook at high heat anytime I use it. 

Price being no issue can you provide macbook vs nvidia or any laptops which will do well with loading massive datasets, execute code with low latency for python libraries I mentioned, perform machine learning well, have an ability to handle docking/3d simulations etc, have a decent battery life, not have a cheap fragile body, won't get too hot or be too heavy to carry.  

Also, to leverage gpu computing, any egpu setups you might recommend that I can plug and play with said laptop 

Thank you",deeplearning,2022-08-23 09:21:03,4
Also check out https://www.reddit.com/r/StableDiffusion/,6,wv1lbv,,deeplearning,2022-08-22 11:41:57,8
"You're broadcasting wrong shapes.

W.x multiply by the transpose of x

Edit: Transpose of W sorry I was high",7,wvfh8n,,deeplearning,2022-08-22 21:45:25,6
"A couple notes from my experience:

* don't trust a single train/test split - do a rolling cross-validation.
* the biggest hang-up is going to be your external regressors, because you'll need the future values of those in order to forecast. I'm assuming you're plugging in actual values for these instead of estimated values, which just isn't close to reality.

&#x200B;

In general I wouldn't trust the Prophet algorithms, either - they don't have any auto-regressive terms and so they basically just fit a curve through the points. Fine if your demand is very consistent, but terrible otherwise.",2,wuz1eh,"Hello,

&#x200B;

Just a really short question. Has anybody had any experience with forecasting models in production? Any success stories?

&#x200B;

&#x200B;

We have a new venture at my PhD in which we try to predict time series data(demand forecasting). And we started with basic models like TBATS and ARIMA and then ventured into Prophet, Neuralprophet, LightGBM and vanilla Pytorch LSTMs.

&#x200B;

Hence, their performance is decent on an MSE/RMSE/MAE base, I am not trusting the fact that it works this well with our data (6 years worth of weekly data demand for that product).

&#x200B;

Any advice? Does anybody work with forecasting on a daily basis ? Has anybody looked at Google\`s TFT? Or NBEATSx?

&#x200B;

Any help is deeply appreciated.",deeplearning,2022-08-22 10:01:54,8
"For an easy fix: Solutions to Cold start problem. 

For a deeper fix: Continual learning.",1,wucbkq,"For example, for a recommendation system with user embeddings that gets trained incrementally (say daily) how can we learn embeddings for new users without retraining from scratch?",deeplearning,2022-08-21 15:31:29,1
Whats the benefit to paperwithcode?,2,wtl6s1,"Your suggestions, comments, and candid feedback would be highly welcome!  

Here's what it looks like in action:

**Input (with code filter on):** ""photo style transfer""  
[https://www.catalyzex.com/search?query=photo%20style%20transfer&with\_code=true](https://www.catalyzex.com/search?query=photo%20style%20transfer&with_code=true)  
**Output**: list of all ""photo style transfer"" papers with corresponding code implementations linked  


https://preview.redd.it/3xl29sb6byi91.png?width=2894&format=png&auto=webp&s=bd728b3c519c97cec89abb5e697103d42998da38

Video of it in action:

&#x200B;

https://reddit.com/link/wtl6s1/video/cb5ugi6bgyi91/player",deeplearning,2022-08-20 16:56:54,2
"google colab with full code for app built using gradio and diffusers: [https://colab.research.google.com/drive/1NfgqublyT\_MWtR5CsmrgmdnkWiijF3P3?usp=sharing](https://colab.research.google.com/drive/1NfgqublyT_MWtR5CsmrgmdnkWiijF3P3?usp=sharing)

(academic access needed to use model [https://stability.ai/research-access-form](https://stability.ai/research-access-form), public release coming soon)

gradio: [https://github.com/gradio-app/gradio](https://github.com/gradio-app/gradio)

diffusers: [https://github.com/huggingface/diffusers](https://github.com/huggingface/diffusers)",7,wsqr58,,deeplearning,2022-08-19 15:22:22,15
r/lostredditors,2,wt377d," Canva Tutorial on How to Blend Multiple Images , it's a creative way to merge two photos in Canva or more .   
 

https://preview.redd.it/eew9visnaui91.png?width=1280&format=png&auto=webp&s=8b84e2542899f00581d0e075de93c576a6834a60

[Tutorial link](https://youtu.be/gLarJn2oDp4)",deeplearning,2022-08-20 02:45:24,1
"thanks for sharing this, it is so interesting :)",2,ws800l,,deeplearning,2022-08-19 00:51:11,1
"0) Post your paper on arXiv.

1) It is very strange you are going about this by first picking the publisher (and in particular a for-profit one).

2) To accurately advise you on this manner, someone extremely plugged into the current research would need to really understand the results/method of your paper.

3) People who can do this well face a large demand for their time, so are unlikely to do this for a complete stranger on the internet.  This is (one of) the power of personal relationships in research.  You need to ask someone you know personally.",2,ws7x0n,"Hey everyone,

I have never published a paper before and I'm looking to publish my project in springer. It is about using deep learning for image inpainting in skin lesion images. Can someone tell me which journals in Springer should I try for and a median time of how long it takes to get published?",deeplearning,2022-08-19 00:45:50,1
When you advertise it as data science but it turns out to be data analytics,24,wqwvmr,,deeplearning,2022-08-17 11:52:40,5
model(x),2,wrh802,"So I have one requirement that tells me to train a model and add cells for inference, to showcase it in a good way. I'm a junior ML so I've never stumbled upon this phrase before nor have done something like it. I watched a bunch of videos and read a bunch of text about what is inference in ML models but it's so confusing and I don't think I got it right. 

For now, I believe that inference is doing predictions, I don't know if I'm right but if I'm not please ELI5. Also, for now the last cell of my code is [`model.fit`](https://model.fit) so what should I do after this in order to showcase the inference?",deeplearning,2022-08-18 04:34:33,1
Why would you need 12 courses ?,1,wrf6gv,,deeplearning,2022-08-18 02:41:30,1
so cool! ty!,2,wqhn67,"Dear all,

Here is **Computer Vision News** of August 2022.

Many great articles about **AI, Deep Learning, Computer Vision** and more (with code!)...

[HTML5 version (recommended)](https://www.rsipvision.com/ComputerVisionNews-2022August/)

[PDF version](https://www.rsipvision.com/computer-vision-news-2022-august-pdf/)

Dilbert on page 2. Free subscription on page 60.

Enjoy!

https://preview.redd.it/mns8p1eix7i91.jpg?width=400&format=pjpg&auto=webp&s=7250587d8dee96d0d64764d4d76975c1818f8b81",deeplearning,2022-08-16 23:32:34,2
https://www.kaggle.com/getting-started/78482,2,wqkibg,"I want to start doing projects on Kaggle, to becomes an MLE, so I can get a good grasp over the entire field. 

However, I'm not sure which projects to do and in which order. Something like Neetcode but for Kaggle would be very useful, does anyone know of anything similar?",deeplearning,2022-08-17 02:30:47,3
"If sequence order is irrelevant, you want a permutation invariant (or equivariant) model. A transformer without positional embedding is essentially just that. I believe the broader field is called deep sets",2,wqriz5,"I'm looking for a differentiable model that operates on sets of scalars and outputs a scalar. Anyone has a suggestion?  
Thanks :)",deeplearning,2022-08-17 08:14:39,2
Hey there! Please Check Your DM.,2,wqq7s3,"I have a semantic segmentation project, and I would like to pay a company to make annotations for my dataset.

The issue is the price for the image, so I'm asking for the minimum dataset size that can get me working. 

Model : MobileNet v2 Unet

Pretrained : yes, on camvid and cityscape and on a custom dataset that I made on my own (but was a living hell to make)

Classes : 3 (including other)

Thanks for your help !",deeplearning,2022-08-17 07:20:16,2
"sucks that it took you a week to get the quota raised, but it’s likely there to prevent abuse. if someone could create an aws account and quickly spin up a large number of expensive instance types, they could perform a denial of service attack, preventing other customers from using those instances for legitimate reasons",2,wqplq7,"Is it just me or are AWS service quotas completely stupid? 
Everytime i am trying to train a model on sagemaker i run into the problem that i cannot use the selected instances. I mean, they are way more expensive than the basic ones, why deny the extra revenue. And the application form for extra quotas is surprisingly cumbersome too. It took me a whole week to convince support that i need that shiny fancy graphic accelerated instance with extra memory.

At least they could provide you a prefiltered list of instances i can actually use to train my models. 
I already checked out the service quotas menu, they tell me „for instance xyc for training purposes you have a quota of 1“ and back in sagemaker it basically tells me „no your not“

Are there any best practices i am not seeing?",deeplearning,2022-08-17 06:54:44,1
"Did you already apply basic contrast image preprocessing?
Like in
https://danielmuellerkomorowska.com/2020/06/27/contrast-adjustment-with-scikit-image/",2,wqhp91,"Hello there.   
I'm working with medical imaging using GAN models, when it comes to compare true and fake images I'm using SSIM, however, contrast plays a big role in my project and SSIM can give me a high value (\~.95)  when both images have the same shape but the contrast estimation is completely wrong in the fake image. 

So, does anyone know a good metric to quantify the difference in contrast in true and fake images?",deeplearning,2022-08-16 23:36:02,5
"The 12GB on the 3060 is going to probably be best, given that the base 3070/Ti are 8GB cards (albeit with GDDR6X so better bandwidth). At least the extra VRAM leaves room to increase batch sizes, rather than not running them at all. IMO The only decent 30xx options for ML are the 3060 and 3090 at their price points, 3070/TI too small, same for 3080, and the 12GB card is pricey. 3060 12GB should be good.",13,wpq7yr,"I am looking to buy either one of these cards due to their VRAM size and price. I cannot afford the RTX 3080 12GB or 3080 Ti 12GB. I am not sure if going for GPU with 8GB VRAM is a good idea either. There is RTX 3060 Ti, 3070, 3070 Ti within my budget.

I noticed RTX 2060 12GB has 272 Tensor Cores whereas RTX 3060 has 112 Tensor Cores. I tried comparing other specs too but it is difficult to decide. Not sure what specification should I give more importance to. I tried googling which GPU to buy but articles were old. 

&#x200B;

https://preview.redd.it/8h7s0wb9u1i91.png?width=955&format=png&auto=webp&s=855e6c1d8e4fccfc1ccfcc5461087a457316d3a4

I would really appreciate your opinion. Feel free to share other options. I tried AMD RX 6800 16GB but it was not possible to run Tensorflow GPU on Windows.",deeplearning,2022-08-16 02:57:21,3
"Code for https://arxiv.org/abs/1608.00859 found: https://github.com/yjxiong/caffe

[Paper link](https://arxiv.org/abs/1608.00859) | [List of all code implementations](https://www.catalyzex.com/paper/arxiv:1608.00859/code)



--

To opt out from receiving code links, DM me",1,wq1sdl,"I'm new to deep learning, and am learning about the Temporal Shift Module paper, because a dataset I am interested \[[Assembly101](https://assembly-101.github.io/)\] in recently released pre-trained weights for it on their network. 

As I went down the rabbit hole, I came to the paper on Temporal Segmented Networks, on which the TSM module is attached to in the project's code. I understand that it is a two-stream network, and so have the following questions. I've linked the paper below. 

* The paper mentions sparse sampling as a method of modeling the temporal information, so why is a two-stream network being used? As I understood it, two-stream networks were used because one stream did the visual recognition through CNNs, and the second-stream is meant to model the time dimension, through measures such as optical flow etc. 
* Also the paper itself mentions four types of input modalities:  a single RGB image, a stacked set of RGB images, stacked optical flow frames, and stacked warped optical flow images. What does this mean? 
   * How can we have a two-stream network that runs on a single RGB frame? Doesn't that just mean that one-stream is not running? 
* I've seen optical flow being considered as an important heuristic for video action recognition, and many papers make use of them. However, given that it needs to be precomputed, do optical flow networks make sense for real-time use cases? Can someone please explain why or why not in detail? 

Temporal Segmented Networks: [https://arxiv.org/pdf/1608.00859.pdf](https://arxiv.org/pdf/1608.00859.pdf)

TIA",deeplearning,2022-08-16 11:30:58,1
"Probably the best ressource for deep learning in the image domain to start of with for me was Deep Learning (MIT Press book) from Goodfellow et al. 2016. Then I would probably start to implement a few basic examples in Pytorch, like building a CNN and an autoencoder. With that you can already do a lot, although this is already far from the state of the art. But in my oppinion it is important to start from a strong base where you have implemented and understood these methods before using more sophisticated methods like Resnet (which i would advise to use as a standard backbone for real projects)",1,wpp9g6,"I need to get my grip strong on deep learning for computer vision. Professor in my university recommends to get familiar with framework like tensorflow or pytorch or mxnet. Though he didn't specify it clearly, so here I'm looking for your suggestions and additional resources to look into, if possible.",deeplearning,2022-08-16 01:57:19,4
"Well, the formula for f1 score is: f1 = 2*precision*recall / (precision+recall). 

If the confidence interval (0-1) is low, so will the precision be, because there will be a lot of false positives (FP) (precision = TP / (TP + FP) ). This leads to a low f1 score.

When the confidence interval is high, recall will be low (recall = TP / (TP + FN) ) because a lot of actual positives will be falsely classified as negatives (FN). This also leads to a low f1 score.",1,wpouky,"I trained a deep learning model  using YOLO-V5 Model and got a weird looking F1-Curve. 

Can someone please explain me what's going on here?

https://preview.redd.it/iwn704fid1i91.png?width=2250&format=png&auto=webp&s=a45df9b8caae227f9a63b351be6ac1080dd39320",deeplearning,2022-08-16 01:30:38,1
"I once uploaded a python script on github repo and then had to download it using wget, maybe you can try that",1,wpj3b0,"Right now, I want to use Colab’s Host Runtime, but to also have access to files from my local machine. However, if I connect to a “local runtime”, then I end up running code directly from my local machine, which I don’t want.

Is there a good way to run on Colab’s machines (Host Runtime), but have direct access to your local files? Linking to Google Drive could be an option, but this is not ideal since I want to directly reference scripts from my local machine rather than uploading them to Drive each time.

If there’s another subreddit that this would also be helpful to post in, please let me know.

Thank you for any insights!",deeplearning,2022-08-15 20:06:34,1
"Hello, 
You can process medical images with CNN working on 3D data. The architecture you should use will depend of your goal but you can use and modify already existing network. « Paper with code » can be a good start to look at what have been done: https://paperswithcode.com/task/3d-medical-imaging-segmentation

For the image format, you should be able to found a Python libraries to open your images and then you can transform them into tensor you your deep learning library (Pytorch , tensorflow or other)",2,wpcc4y,"I would like to know how to process Medical images in 3D dimensions,

what are the possible packages I will need to deal with, and what format of images ( nib , docm )

thank you in advance",deeplearning,2022-08-15 15:02:23,3
"So typical normalization subtracts out the mean of the batch and divides by the standard deviation of the batch. It means that the batch will be 0 mean and  most samples will lie on the interval -1 to 1 (depending on the stats of the batch). 

The bias of a neuron will be able to add back in to position the inputs into the relu activation region. So it is really left up to the bias weight to position it in the activation region.",1,wp52rs,"Hello,

I am reading the book Deep learning with python 2nd edition from Francois Chollet and on chapter 9.3.3 (p. 257). Introduces the batch normalization and it recommends to write like that:

x = layers.Conv2D(32, 3, use\_bias=False)(x)

x = layers. BatchNormalization() (x)

x = layers.Activation(""relu"")(x)

instead of

x = layers.Conv2D(32, 3, use\_bias=False, activation = ""relu"")(x)

x = layers. BatchNormalization() (x)

And explains it liek that:

""Doing normalization before the activation maximizes ther utalization of the relu.

I understand this because relu at 0 has 0 value, but has someone little more detailed explanation?

my second question is

he recommends leaving these (BatchNormalization layers) frozen whn he does fine tuning. Otherwise they will updating their internal mean and variance, which can interfere with very samll updated applied to the surreound conv2d layers.

This puzzles because if we have 4 hidden layers let's say 2 3 4 5 and all of the have BatchNormalization . and i want to fine tune the 4 and 5 for me would be natural to unfronze the batchnormalizaiton layer between 4 and 5.

thanks for the time to read that.",deeplearning,2022-08-15 10:11:46,3
"I would recommend option 1. 

My reasoning has very little to do with workloads tho tbh. This is just general advice. Be it gaming or DL or video editing etc. 

Don't buy high performance windows laptops which have H or HX series processors with dgpus. 

They aren't as portable, if u push them hard they wear out quite fast, battery life is atrocious. I've gone through 3 in 6 years. 


Buy a thin and light , since a macbook is out of the question just get whatever is the most portable with a relatively fast low watt cpu. Use cloud for small experimental training on the go if u need to.",1,wozf61,"Switching to a new job now. The new company offers me a new laptop purchase. The available options are all ThinkPads and will be running Linux and Linux only, no windows and no mac.

1. Thinkpad x1 carbon (gen 10). Can top it to i7-1280P and 32gb ram. Ultralight at \~2.5lb with a 14"" display. Extremely portable which works flawlessly with a 65w GAN charger. Bad thermal and weak performance at ultrabook level.
2. Thinkpad p1 (gen 5). 32gb ram, i7-12800h CPU without a dGPU. 16"" footprint at at 4lb and can work with a \~1lb 135w GAN adapter (so \~2lb extra travel weight than the x1 carbon). Much better thermal and thus much more robust CPU performance for my local coding, compiling, dockers, multiple vms, etc..
3. Thinkpad p1 (gen 5). 32gb ram, i7-12800h CPU with RTX A4500. The machine itself weights about the same as option 2, but with a heavy 230W adapter which is more than 2lbs to get the GPU (RTX 3070ti level of performance, but with 16gb VRAM). Battery life is supposed to be way worse than the option 2 and x1 carbon, so not that much portability.
4. Thinkpad P16. 64gb ram, i9-12950HX CPU and RTX A4500. 16"" display, much better thermal than the P1 options but weights 6.5lb and kinda must be run docked with the 230W adapter. Absolutely no portability and will be kinda bad for my back.

I was previously a computational scientist developing simulators on MPI and CUDA systems as well as doing a bit classical machine learning methods. So I do get much experience messing with these libraries as well as Linux and HPC stuff. But I am mostly a new starter in the deep learning world and will need to learn quite a lot extra. 

I get a workstation in my office as well as access to the company servers to really train the models if necessary. So the dGPU in these laptops will mostly supposed to be used for light prototyping and debugging purposes only and no hard training. I also own a home pc with a RTX 3080 for all my personal projects, gaming and a bit freelancer work, so the new laptop will be for work only. But I am going to travel a lot (and portability definitely matters) in the near future and may have issues remoting to that workstation or my home pc fulltime under certain circumstances. I am not sure if I would still be able to play with very small batch sizes locally on pure CPU mode in a timely fashion in said scenarios if I go the x1 carbon ultrabook or P1 without dGPU.

As I am expected to give out my choice soon, I decide to come to this subreddit and ask for help. Any suggestions and/or experiences will be more than welcome.",deeplearning,2022-08-15 06:15:34,4
"Code for https://arxiv.org/abs/1811.02629 found: https://github.com/christophbrgr/brats-orchestra

[Paper link](https://arxiv.org/abs/1811.02629) | [List of all code implementations](https://www.catalyzex.com/paper/arxiv:1811.02629/code)



--

To opt out from receiving code links, DM me",1,wol6ci,"Was reviewing the BraTS dataset for research and stumbled upon this paper. I thought the Foundation Models paper had the record for most authors listed on a paper before this lol

[https://arxiv.org/pdf/1811.02629.pdf](https://arxiv.org/pdf/1811.02629.pdf)",deeplearning,2022-08-14 17:27:13,1
"I don't have time to write a full reply right now, but the short answer is that you are correct. Alpha zero (bar multithreading) with always be deterministic if you disable the root node noise and always pick the node with the most number of visits.

I believe it was in a follow up ama, or some sort of forum replied that they mention that the temperature still is set 1 at the beginning of evaluation games. That way the opening is randomized, but the rest of the play out is deterministic. So you get results of optimal play given slight variation to the opening. Which is no longer deterministic.",2,worct9,"Hi all, after poring over the AlphaGoZero papers, I have one fundamental issue that I can't seem to understand. It seems that selecting a move given a board state and a certain neural network is a deterministic process. Yet the papers will say things like ""this net B is considered better than net A if it wins 55%+ of the time"", implying there is variance. Where is this randomness coming from?

The way I understand it, when selecting a move given a board state, each network runs a Monte Carlo Tree Search on the current state, then selects the action/edge with the highest visit count.

There are two points where I could see randomness appear, but from my understanding neither is actually random. Perhaps someone can help clarify?

1. Is the randomness in the tree returned by the MCTS? I don't think so, as each action is selected based on a combination of its prior distribution (deterministic by model) and exploration value (deterministic as well)
2. Is the randomness in the action selection once the tree is returned? I don't think so, as the paper is quite clear that during evaluation the ""temperature"" is -> 0, and the move is selected purely by visit count in the tree.

After looking through some GitHub implementations, I also cannot find where they introduce variance when playing an official non-training game. I am certain my understanding is wrong somewhere; any help would be appreciated!",deeplearning,2022-08-14 22:32:42,22
"I think this question can not be answered generally. If I'd be you I'd just try and compare validation/test loss.

It could f.e. help improving this weakness or overfit on those samples and loose generalization capability.",1,wofanv,"I’m training a large dataset, the average loss has come down to 0.03 but there’re 50k samples with loss ranging from 0.1 to 1. Should I train on these 50k samples specifically(maybe it’ll save some time?) or should I keep training on the entire dataset?",deeplearning,2022-08-14 13:02:35,3
"Code for https://arxiv.org/abs/2110.15032 found: https://github.com/Oneflow-Inc/oneflow

[Paper link](https://arxiv.org/abs/2110.15032) | [List of all code implementations](https://www.catalyzex.com/paper/arxiv:2110.15032/code)



--

To opt out from receiving code links, DM me",3,wo3o9l,"Hi all,

We are thrilled to announce the new release of [**OneFlow**](https://github.com/Oneflow-Inc/oneflow)**, which is a deep learning framework designed to be user-friendly, scalable and efficient.** OneFlow v0.8.0 update contains 523 commits. For the full changlog, please check out: [**https://github.com/Oneflow-Inc/oneflow/releases/tag/v0.8.0**](https://github.com/Oneflow-Inc/oneflow/releases/tag/v0.8.0).  


**Paper:** [https://arxiv.org/abs/2110.15032](https://arxiv.org/abs/2110.15032);  
**Code:** [https://github.com/Oneflow-Inc/oneflow](https://github.com/Oneflow-Inc/oneflow)

Welcome to install OneFlow v0.8.0 for a new user experience. Your feedbacks will be much appreciated!

Highlights and optimizations in this release:

**1. PyTorch API compatibility**

OneFlow v0.8.0 provides more and better PyTorch compatible APIs. In v0.8.0, a series of new features and interfaces that are compatible with PyTorch 1.10.0 are in place, including 68 new APIs that are aligned with PyTorch; 84 bugs are fixed to ensure better compatibility between operators and interfaces, allowing users to transfer more PyTorch models to OneFlow with just one click.

&#x200B;

**2. Wider support of global operators**

All operators support Global Tensor more widely and efficiently. Fixed 28 bugs related to Global Tensor and added 180 Global operator unit tests, making the development of distributed models with Global Tensor faster and easier.

&#x200B;

**3. Better performance**

The advanced features of Graph have been improved for better performance:

In addition to the original ZeRO-DP, ZeRO can be used in parallel with MP, 2-D, and 3-D to further reduce memory overhead.

Added a new pipeline parallelism API for Graph to simplify the configuration for pipeline parallelism and accelerate training when using pipeline parallelism and 3-D parallelism.

Added debugging features in multiple dimensions, including logical graphs, light plan physical graphs, memory analysis, and Python stack information, to further improve efficiency of Graph.debug.

The combination of OneFlow v0.8.0 and LiBai v0.2.0 enables higher computation speeds of GPT and BERT under 3-D parallelism on multiple dimensions, surpassing those of Megatron-LM with the same configurations. (For more details, see: [https://libai.readthedocs.io/en/latest/tutorials/get\_started/Benchmark.html](https://libai.readthedocs.io/en/latest/tutorials/get_started/Benchmark.html)).

&#x200B;

**4. OneEmbedding component**

OneEmbedding is an extended component specifically designed for large-scale recommender systems. It boasts excellent performance, extensibility, and flexibility.

API Documentation: [https://docs.oneflow.org/en/master/cookies/one\_embedding.html](https://docs.oneflow.org/en/master/cookies/one_embedding.html)

&#x200B;

**5. Multi-Device adaptation**

OneFlow v0.8.0 provides a neat, efficient, and easily extensible hardware abstraction layer EP (Execution Provider) to adapt to different hardware. With the introduction of the hardware abstraction layer, no modifications are needed for any module of the framework to adapt to new hardware devices, regardless of the implementation details of any underlying hardware or framework.

To make the new hardware devices work, users only need to implement a series of interfaces based on the protocols of the hardware abstraction interfaces and the status quo of the hardware devices.

EP also defines a set of basic computing interface primitives, allowing the reimplementation of kernels. Primitives provide interfaces that are more flexible than the runtime interfaces provided by EP. Different interfaces are independent of each other, and each interface represents a kind of computing capability that can be provided by a certain hardware device.

**6. Debugging tool stack**

New debug tools: OneFlow-Profiler and AutoProf.

OneFlow-Profiler is a tool used to collect performance information during framework execution. It can keep records of the execution time of operators and system components, the allocation of memory, and the corresponding input and parameters of operators. All this information helps developers find out the main source of overhead in framework execution and thus implement targeted optimization.

AutoProf is a framework for testing the performance of OneFlow and PyTorch operators. It provides an elegant and efficient method to detect the alignment between OneFlow APIs and PyTorch APIs, allowing users to conveniently compare the performance of OneFlow APIs and PyTorch APIs.

**7. Error message**

Improved error message with more details. Refactored exception handling.

&#x200B;

**8. API documentation**

Made over 20 revisions to the OneFlow API documentation, restructured the documentation based on features, and added further elaboration of modules and environment variables including OneFlow oneflow.nn.graph, oneflow.embedding, and oneflow.autograd, in addition to the general operator APIs.",deeplearning,2022-08-14 03:58:04,1
This is amazing ty for sharing!!,2,wnrfhx,"Announcing the Youtube release of one last special lecture for [**CS25: Transformers United**](http://cs25.stanford.edu/) held at Stanford University given by the Godfather of AI, **Geoffrey Hinton** 🤩!! 

See our watchlist here 👉: [Youtube Link](https://www.youtube.com/playlist?list=PLoROMvodv4rNiJRchCzutFw5ItR_Z27CM)

Original thread: [Here](https://www.reddit.com/r/MachineLearning/comments/vuw77a/n_firstever_course_on_transformers_now_public/?utm_source=share&utm_medium=web2x&context=3)

Happy Learning!!

&#x200B;

https://preview.redd.it/ly4t0ym3dkh91.png?width=350&format=png&auto=webp&s=fdf6301d7d2d6c101f84be0e034c9cb1d665de94",deeplearning,2022-08-13 16:17:02,4
Yes. You pass the final state of the LSTM into a softmax layer.,6,wnia1f,"

Hi all, I’m new to deep learning, wanted to ask, can we use LSTM for multiclass classification like random forest where the outcome will be “low”, “medium”, “high” for example. Thanks",deeplearning,2022-08-13 09:17:32,3
"Can you elaborate on what you mean by ""elimination singularity""?",3,wnn9vu,"Batch normalization (BN) can smooth loss landscape and avoid elimination singularity. However, in some tasks such as computer vision having large number of sample in one batch is not possible. 
In this video, I will walk through a paper discussing micro batch training that can outperform BN.
Please subscribe, leave a comment and share with your community.

https://youtu.be/9mZ0FhBCYI8",deeplearning,2022-08-13 13:01:55,3
"References:  
►Read the full article: https://www.louisbouchard.ai/banmo/  
►Project page: https://banmo-www.github.io/  
►Paper: Yang, G., Vo, M., Neverova, N., Ramanan, D., Vedaldi, A. and Joo, H., 2022. Banmo: Building animatable 3d neural models from many casual videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 2863–2873).  
►Code: https://github.com/facebookresearch/banmo  
►My Newsletter (A new AI application explained weekly to your emails!): https://www.louisbouchard.ai/newsletter/",1,wnew2o,,deeplearning,2022-08-13 06:45:13,1
"Personally i'd still stick to Cloud Services like Google Colab when using M1. The support still is very much work in progress performance wise.

There was a reddit thread 3 months ago about it, although i don't know how much it has developed by now, haven't tried it since then tho. https://www.reddit.com/r/MachineLearning/comments/ut30ck/d_my_experience_with_running_pytorch_on_the_m1_gpu/?utm_source=share&utm_medium=mweb3x",3,wn88e3,"Hi there, 
I'm relatively new to Machine Learning (ML) and I'm looking to learn pytorch on my m1 macbook air. I was wondering if it is fully supported on apple silicone and if it works perfectly fine just like on intel macs. I saw online that pytorch recently started supporting m1 macs but does it work perfectly in its current state?
Thank you in advance!",deeplearning,2022-08-13 00:14:46,3
"AvatarGen is a 3D Generative Model that enables the generation of non-rigid human avatars with full control over poses.

Quick read on this awesome project: https://www.qblocks.cloud/byte/avatargen-3d-generative-model-for-animatable-human-avatars/

Developed by Jianfeng Zhang, Zihang Jiang, Dingdong Yang, Hongyi Xu, Yichun Shi, Guoxian Song, Zhongcong Xu, Xinchao Wang, Jiashi Feng",2,wmkv3w,,deeplearning,2022-08-12 06:07:52,2
Learn the libraries,3,wn74ll,"Hi, so I am trying to deep learning from the new fast.ai course. It is really confusing to code along with the videos. I have no prior knowledge with the fastai and pytorch library. Can you guys suggest how to tackle this",deeplearning,2022-08-12 23:07:21,8
I long for the day when ML subs won't be spammed with half-assed listicle blogposts anymore.,2,wn3zha,,deeplearning,2022-08-12 20:16:47,2
"You can use the normal L2 norm while training, that is differentiable, and then the outputs can layer be multiplied by p to get the suitable values. Not entirely sure if that would work out though.",1,wmhw6j,"Hi, I have to normalize a neural network output to have a certain L2 norm. I know that this can be done in a differentiable fashion. 

Say the input to the NN is \[x, p\], where x is a vector and p is a scalar less than or equal to some constant P. The output y should have a L2 norm less than or equal to p.

My main question is whether this is a learnable function? I haven't seen this structure and I am wondering if this has some design/logical/math flaws.",deeplearning,2022-08-12 03:28:38,6
"In theory you can use grid search for finding the best hyperparameters for a neural network, though as it takes a lot of time to train an ANN it's not a good use of computational resources. If you want to do a more intelligent search through parameters then Bayesian Search is an option.

One of the most important things to get correct is the learning rate and you can use [cyclic learning](https://arxiv.org/pdf/1803.09820.pdf) for that

In terms of the number of layers and neurons, you can always create a network that is larger than you need then use regularisation of some kind (e.g altering the cost function to reward lower weights or adding drop-out layers) to prevent overfitting.

You can also use look at papers of people solving similar problems to give you a good idea of sensible choices to make. And quite often it will make sense to use transfer learning and just retrain a model that solves a similar problem for a short period of time.",1,wmfyej,"I have previously worked on xgboost where we can run a grid search to optimize the parameters. In case of neural networks we can manipulate the nodes, layers, optimize, loss function etc... Is there any other way other than manual trial and error to optimize these inputs for a neural network?",deeplearning,2022-08-12 01:26:51,1
"I've had to deal with maixduino/maixpy for 2 month last year, and let me tell you something

Maixduino is utter trash, their cards die for no reason and their codebase is a clusterfuck of stolen code and random comments that somehow made it to production

The only good thing about the cards I used was that there was an esp32 that could be salvaged

Don't think you can ever achieve something other than some hobby projects with their products",1,wlr7td,"Can you imagine running DeepLearning Model on classical Arduino ATmega328 (32KB Flash, 2KB RAM)?

**TinyMaix** make it come true\~

TinyMaix is an Ultra Lightweight **TinyML** infer lib, only **400 lines C code, 3KB Flash**(.text), easy to port, even ATmega328 2KB RAM can run MNIST (handwritten digit recognition) with TinyMaix\~

It also Support multi architecture accelerate, not only **ARM SIMD/NEON/MVEI** instructions, but also **RISC-V P/V extend** instructions\~

（RV32P use smaqa instruction, support INT8 model; RV64V (rvv0.7.1, but use intrinsic 1.0), support FP32&INT8 model ）

(most tinyML lib like tflite-micro,uTVM need at least tens of KB Flash and RAM,

it is impossible to run even MNIST model on ATmega328's 2KB RAM,

and they usually only optimize for ARM instructions, not for opensource RISC-V instructions.)

&#x200B;

Here is the project demonstrate Arduino Mini(Atmega328) run MNIST successfully\~

Try it out: [https://github.com/sipeed/TinyMaix](https://github.com/sipeed/TinyMaix)

TinyMaix is very easy to port, you can easily port it to any chip, enable TinyML for your platform\~

&#x200B;

&#x200B;

https://preview.redd.it/tcjep2krz2h91.jpg?width=1084&format=pjpg&auto=webp&s=96665f210f309cd77e58640c3e8045699bc0041b",deeplearning,2022-08-11 06:18:37,4
You don't really run DL models on laptops if you're planning to do that.,7,wlw7uv,"
I want to learn deep learning and Machine learning and i want to buy a new laptop. Should I buy any laptop i find or  buy a powerful computer with high specifications and features ?",deeplearning,2022-08-11 09:51:51,6
"Thanks for sharing this. 

I once wrote a tensorflow code for the original google paper. My feeling is that it only works well with a very specific set of parameters and it is terribly slow... You have to train one network per image... 

I will have a look at the double dip but I'm afraid that similar problem will be obtained (slow and difficult to tune).",3,wl9x5r,"Deep Image Prior, a deep learning-based image denoising method, is a very interesting idea as it disentangles deep networks with big data (it doesn't require a training set). I wrote a tutorial on how to implement it in PyTorch:

[https://taying-cheng.medium.com/deep-image-prior-in-pytorch-e6edf666a480](https://taying-cheng.medium.com/deep-image-prior-in-pytorch-e6edf666a480)",deeplearning,2022-08-10 14:52:45,3
"It is just personal choice of using sequential for testing. You can use random sampler for testing too.
For training you have to use random sampler so that you are not loadind continously images. This is what I observed in medical images. You care about test images only. And if you want to save those images after testing you have to save ground truth, pred image. But if you loading testing data sequentially then you can save only pred images just to save some memory and cross check them easily.",3,wlh2wn,"When I read some PyTorch codes of others, I found most people use this: `RandomSampler(train_data)`  rather than `SequentialSampler(train_data).` Any reasons behind it?

    train_sampler = RandomSampler(train_data)
    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)
    
    valid_sampler = SequentialSampler(valid_data)
    valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=BATCH_SIZE)

Thanks!",deeplearning,2022-08-10 20:24:23,5
Found some [here](https://github.com/deepmind/deepmind-research/tree/master/alphafold_casp13) and [here](https://github.com/Urinx/alphafold_pytorch).,1,wktnx0,"I'm looking for notebooks or code repositories of AlphaFold 1 (or similar derivatives thereof). AlphaFold 2 had me curious, but I find the AF1 architecture much more instructive from a machine learning standpoint and I'd like to tinker with it myself. 

May I kindly ask you to point me to any relevant resources? Thanks!",deeplearning,2022-08-10 03:12:23,1
How is this different than using a filter?,7,wk4wo1,,deeplearning,2022-08-09 07:34:48,8
"I think you can use Supervisely to annotate video dataset. Maybe refer to this [https://supervise.ly/videos/](https://supervise.ly/videos/)  


Good luck!",1,wk9jk5,"Hello everyone, 

For my PhD, I'm building an accelerometer data decoder using covnets. But first I need to annotate videos of animal behaviour, and I'd like to have some software or code (R, python or julia) that allows me te predefine the classes of behaviours to annotate, assign them to numbers on my keyboard, and then just press those buttons as I watch the video to annotate it. And ideally that would yield a csv with the times of when each behaviour happened. There's nothing on google that I could find that fits this description. 

 

Any suggestiones are greatly appreciated!

&#x200B;

Thanks",deeplearning,2022-08-09 10:40:15,3
"Getting drivers, cuda and cudnn is always a painful experience.   
1. Check if your tensorflow version and CUDA version are compatible  
2. cuDNN files should go somewhere inside the CUDA folders ([https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html](https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html))

  2.1. you might also need zlib  
3. You should only need CUDA on your PATH this way  
4. Check if your nvidia drivers are up-to-date",1,wk2zms,"Hey guys,

Training does not commence. I'm using a very lightweight MNIST convnet example, batch size is 1, CUDA 11.7 and it does train on CPU.

I'm getting the usual message:

 I tensorflow/core/common\_runtime/gpu/gpu\_device.cc:1616\] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3981 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5 

But on epoch start, bang it crashes without any information.

Thank you for the help!

  
P.S. Do I need CuDNN to train with keras? I've downloaded the files and added to PATH, it didn't make any difference (there is a possibility I didn't get the ""installation"" right). 

https://preview.redd.it/fxcn8uq2jog91.png?width=130&format=png&auto=webp&s=49824148279ad6f149f43635b8d3d131d2f4e9df",deeplearning,2022-08-09 06:12:40,4
Maybe enabling op determinism is your solution. Note that it will have an impact on performance. https://www.tensorflow.org/api\_docs/python/tf/config/experimental/enable\_op\_determinism,1,wjt7o7,"Hello all, Has anyone faced the issue of reproducing the same results for independent trials using‏‏‎‏‏‎‏‏‎‏‏‎­TF? I possess already used all the tricks mentioned in stack overflow like setting seeds for numpy and TF and python hash seed. I even used shuffle as false between the epochs. Still the results are not consistent. However training the model using a CPU and tricks above can give same results. It is only that gpu operations are non deterministic. I understand one can do averaging over multiple trials. However the standard deviation of accuracy over the multiple trials is around 2% which is feel is quite high. This problem is more of an issue when we try to compare models and data augmentation strategies. Has anyone faced such issues? How did you deal with it?",deeplearning,2022-08-08 20:51:12,1
"Streamlit is pretty easy to get up and running and you can write it all in python.

If you don’t care about learning react/flask or Django and want something you can get up and running fast it’s perfect.

[streamlit](https://streamlit.io/)",11,wj827h,,deeplearning,2022-08-08 05:49:57,5
"I think its a good idea, but you should do a thorough literature search that such a thing doesnt exist yet.

From the model side, there is one main difference between transformers and recurrent encoder-decoder models that is relevant to you: recurrent models encode the input into a single vector, while transformer encoders output a vector per token. So you would have to figure out how to do the interpolation there",1,wj4tw5,"Hey everyone, I'm currently figuring out the topic of my master thesis. And I wan't to know if my idea is stupid / feasible  since I have not really worked with transformers before:

**Data:** The data I have is 10 years of medical data from a fairly big hospitals ICU. It's about 100 biomarkers at a spacial resolution of 30 minutes per patient with an average ICU hospitalization time of 5 days.

**Method:** I found a paper that describes how they perform data augmentation in the latent space of an encoder-decoder model. They do this by interpolating between the latent spaces of two samples, and generate a new sample with this. \[**DeVries,** [source](https://openreview.net/pdf?id=HJ9rLLcxg)\]

Now my idea. Since transformers are basically special encoder-decoders (from how I understand) they also create a latent space (or feature space). I wan't to try if the data augmentation technique used by  DeVries also works in transformers and if it performs better or worse compared to the normal encoder-decoder they used.

For those of you that have a better understanding of transformers then I do: Is this possible in theory?

Cheers in advance and please ask any questions if I didn't explain myself properly.",deeplearning,2022-08-08 02:51:50,10
Take a look at [https://streamlit.io](https://streamlit.io) and [https://www.gradio.app](https://www.gradio.app),2,wjeibo,"The technology must be industrially popular. My tech stack includes HTML, CSS, Javascript, and data visualization. I am more into machine learning and data science.",deeplearning,2022-08-08 10:19:33,1
You could always create your own image inpainting dataset by manually degrading common face datasets.. That way you also have the ground truth to compare your model’s output against.,3,wj9vzj,"Hello folks, I am looking for a dataset with image pairs where the conditional image has missing features ( an example would be a face dataset where parts of the eyes or nose or ear are degraded). Can you help me by pointing out any such datasets. This is for a small research work for school. 
Thank you.",deeplearning,2022-08-08 07:13:00,1
"I believe simpler tasks such as your problem statement are good benchmark sets. Academics want to improve the model regardless of the task because we can always do better. Simpler tasks of image segmentation, denoising are easier for analysing the robustness, learning capacity, etc of your model. The comparisons are made because deep learning is a fairly new field and research is building on top of whatever was regarded as state of the art.(Models that achieve better results at COCO,VOC,PASCAL... ) And comparisons make it easier to discover faults of the previous models and gives an insight to us for our next modeling choice!",3,wigfpj,"The exam was to make a DL model that takes an image of 2 superimposed images as an input and gives the 2 separated images as an output and the grading depends on the metric which was the MSE loss (without pre-processing).

All I did was going through the blogs, github, anything I can get my hand on and use all models and use the model that yields the lowest MSE.

Sure some models were better than others but to a certain degree, they all worked. I mean it's not a classification problem so yeah, it should 'work'. But what I'm trying to say is they all worked at the end. I used models for image denoising, image segmentation, image resolution, etc and all of them worked.

Which made me wonder: for a problem such as that, what's the point of academics trying to improve models for tasks like that?
Plus: why do they try so hard to compare each other when you're already at results good enough, you can't tell the difference with the naked eyes (a 0.0003 MSE vs 0.0004 MSE, for example)?

It was something I was just asking myself while doing the project (or searching for a code to copy/paste lol).

Btw I studied the theory and I get it, but it was just easier to do that - especially that the grade is stupidly inversely proportional to the MSE: the lower the MSE, the higher the grade.",deeplearning,2022-08-07 07:01:53,3
This is so cool!,5,whwgtk,Here is the link to the repo https://github.com/Shreyz-max/Doodle-to-Image-Generator,deeplearning,2022-08-06 12:42:16,1
Have a look at how the metrics classes in keras are implemented. Personally I'd remove the compute word and just have the functions called saliency() and activation().,2,whry7j,"I am trying to restructure some code for a explainable AI product to follow solid principles. In the code there are methods that compute [saliency](https://analyticsindiamag.com/what-are-saliency-maps-in-deep-learning/),  and activation. Both of these methods, have few common functionalities.

&#x200B;

So I decided I will create an abstract class called metric\_computer which would have the common functionalities listed and child classes called saliency\_computer and activation\_computer would implement these. The class names metric\_computer, saliency\_computer and  activation\_computer does not even look good to me, does any of you have suggestions on any better class names ?",deeplearning,2022-08-06 09:14:06,2
"How did you split your two datasets? Randomly or they are from different sources? 

I would do cross fold validation and randomly sample and train at least 10-100 times depending on how long your models take. Then you’ll get a better idea of the range of performance across random folds. 

It could just be this one random time you split it, A was great and B was shit.",3,wh95md,"I have fine tuned pretrained trained ResNet50 for road crack detection. I have two different sets of training and validation datasets, lets call them A and B. the testing dataset is the same.

When trained on training validation **dataset A**, I got 92% accuracy and f1 score on the test set.

When trained on training and validation **datasest B**, I got 59% accuracy and 51% f1 score.

The model and hyper parameters are the same. 

I understand there is something wrong with one dataset. 

What are the potential issue with the datasets that is performing worse?

**I have tried to ensure that dataset B doesn't have mislabeled samples.** 

Looking for different possible explanations.",deeplearning,2022-08-05 16:09:43,10
"Data Science is the booming technology in the present world. You can learn Data Science in many ways like watching Youtube Videos, Learning from reading books like the [**Best Data Science Books for Beginners**](https://codingvidya.com/best-data-science-books-for-beginners/) to Advanced, and Courses both paid and free.",1,wgrlyq,,deeplearning,2022-08-05 03:10:18,1
You could try using the ONNX Runtime​​ or TensorFlow C++.,7,wgvi5r,"I'm building an open source video surveillance application. I'd like to start adding some deep learning to my project (specifically, I'd like to do people detection (possibly with bounding boxes)). The application is entirely a C++ codebase and it works on Linux and Windows.

Ease of deployment of my application matters a lot to me. A header only inference engine implementation would be my preference... Second best would be a static library. In a perfect world this inference engine would not itself have a ton of dependencies. Ideally, this system would be able to make use of a GPU if available, but to be honest I'm curious if I can get good enough inference performance on the CPU so that this feature could be available to everyone. Does such a thing exist yet?",deeplearning,2022-08-05 06:28:26,8
Good video but I don't think this is the right sub for this,1,wg8lzu,"it's very simple canva tutorial , and no need for canva pro

&#x200B;

https://preview.redd.it/z87l73g5qqf91.png?width=1280&format=png&auto=webp&s=2666c43a0a32cb6117f34ad61d989ea4bda1b8b4

&#x200B;

here is the tutorial [link](https://youtu.be/gOej6z40L_w)",deeplearning,2022-08-04 11:33:35,2
"Do you have results for evaluation?

How firm of a grasp do you have of the model?

Do you have a background for the research you could elaborate in the intro?


Really hard to do so, especially with no proper double check, but you didn't mention it has to be good",1,wh6u6f,,deeplearning,2022-08-05 14:26:00,1
"but once we trained the weights, how to use it later for our usecase?",1,wggxr5,,deeplearning,2022-08-04 17:30:45,1
Rudimentary?! Your brain is rudimentary cause it produces nothing's fine,-1,wg4cu9,"So I’ve been getting tired of googling and getting stackoverflow when I already know what library I want, and not being able to search those libraries docs because of their rudimentary keyword based searches. Thus, I decided to make a search tool for open-source python libraries (with a focus on ML libraries, since that’s mostly what I work on) thats curated for actual developers and permits natural language queries.

I’m gonna keep this free as long as I can, so it'd be wonderful to get feedback from anybody who'd be up to give it a try.  Check it out at [https://www.pysearch.com](https://www.pysearch.com) and please feel free to share with anybody else you know who might benefit from this!",deeplearning,2022-08-04 08:38:49,2
"HMU, souds like a good plan/thing to give it a shot, I am up for bi-weekly thing :)",4,wfazqr,"Would anyone here be interested in starting a little discord community where people can vote on weekly/ biweekly papers to read and discuss as a group? I’m looking for a way to engage with complicated material and share the ideas with other people for better understanding. I’ve got a big back log of papers I want to read and I’m trying to come up with ways to enforce a personal reading schedule. Thanks in advance for any feedback.

Edit: Link to new server

https://discord.gg/GdX7nbTu",deeplearning,2022-08-03 09:26:14,38
You can take a look at FaceNet: https://ieeexplore.ieee.org/document/7298682,1,wg0fnf,I want to create a face detection network for a dataset of around 520 people. I have the code ready for the face detection and all the data loaders but I am struggling with which model/approach to go for. I have roughly about 25-30 pictures per person so what would be the most accurate way to go about this?,deeplearning,2022-08-04 05:49:20,1
I will suspect the data imbalance problem,2,wfemwo," If  the deep learning model predicts class 1 samples with 100% correction  (class 1 predicted as class 1 ) while class 2 (25% of class 2 samples  predicted as class 2 while 75% of them were predicted as class 1) with  25% correction then what is the potential problem?

Is it because that the training and testing datasets are not correlated enough

or there are some mislabeled samples in the training datasets

or some other issue?

What is the potential problem?",deeplearning,2022-08-03 11:48:38,6
"> How should I prepare the dataset only with images of 3 people?

No need to prepare anything (except calculating the embedding of one photo of each person) if you use this library: https://pypi.org/project/deepface/ .  It'll give you the choice of leading pre-trained models that can easily differentiate between 3 (or even a few thousand) people with a single sample.   To scale to 100,000s with decent accuracy you'll need to either provide multiple examples of each target person, or need carefully posed/cropped/contrast-balanced/etc images. 

> For the above project, I would like to train my own deep learning model, rather than using OpenCV.

You could - but expect to get worse accuracy than the pre-trained facial recognition models.  It'll take a lot of different facial expressions of each person; different combing of hair; different angles.  Otherwise you're more likely to accidentally train a model that thinks ""oh, smiling and just-woke-up-hair must mean person #1"".",1,wewd5d," Project description-:

A single-page <HTML, CSS & JS> website (hosted on *Amazon S3*) with access to the laptop's camera will send the live video stream to *Amazon Kinesis* which will trigger the Facial Recognition code <Python (Keras)> on *AWS Lambda*. It will recognize the person in the feed and respond with 2 numbers, one stating the *fan's RPM value* and the other being the  *RGB value* for the *led*. This data will somehow be sent to the *FPGA board*  connected to the cloud (and the laptop) and the fan and light will act accordingly.  Initially, the facial recognition code will be explicitly built only for 3 people. The option to create a new profile will be added later.

For the above project, I would like to train my own deep learning model, rather than using OpenCV.

Issues-:

1. Is the architecture appropriate for my project idea or should I change something the architecture or the workflow?
2. How should I prepare the dataset only with images of 3 people? Even if I  augment the data how would it scale to 1,00,000 images enough for the model?
3. Which model would perform best for the same?
4. What should be the FPS value for the video feed?

I would really like you all to provide your insights on this and any improvements if needed. Thanks and regards.",deeplearning,2022-08-02 21:02:56,1
">Do you guys take notes while you take online course?

Yes.",14,wea13s,"I am currently doing Deep Learning specialization by Andrew Ng. 
Should I make hand written notes 📝 for each slide? 
I found a bunch of github links where learners have posted their notes. 
Do you guys take notes while you take online course?",deeplearning,2022-08-02 05:10:08,4
"It comes from the term “Variational Inference” or “Variational Bayesian Inference”.  

Let us assume that your data comes from a probability distribution p(x). Let’s also assume you want to learn this distribution. If this distribution is too complicated, you can try to find an approximate (simpler) distribution q(x). That’s what variational inference is all about. Since many integrals are too intractable to solve. 

Example: When training a variational autoencoder (say to generate images or to de-noise images), you assume that your training data (your images) already come from a complex distribution p(x). Your autoencoder will approximate that distribution by learning how to generate these images (given the parameters). So, think of your neural network as the simpler distribution q that approximates the true distribution p. 

P(x) is too complicated, but q(x) is not. I have often gotten the question: How do I sample from the true distribution p(x) if I don’t have it? Well, the answer to that is that you’re assuming that your training data represents samples from this distribution. So picking 10 random datapoints from your training data is equivalent to sampling 10 data points from your complex distribution p(x).",16,we8piq,"Was looking into VAE's and couldn't think of an obvious reason why they are called so. Is there a reason, variational autoencoders are called so?",deeplearning,2022-08-02 04:01:21,5
"To carry a general conversation about anything? That's far too large and expensive for someone. The large language models of today that are capable of this, like Google's Lambda, used a training data set of 1.56 trillion words and 137B parameters.

If there's a specific conversation you want your bot to have you need a training set of words or ""corpora"". If that data set doesn't exist, then you create your own through a generative model.",1,we8gr6,"So, I'm new to DL and ML in general and I want to make a AI chatbot that can maintain a conversation. There's tons of datasets out there, but I want to know what is the ""best"" dataset you would use for a chatbot like that one I'm trying to make...

&#x200B;

Any suggestions?",deeplearning,2022-08-02 03:47:24,6
"Dang. From the title I was imagining a different project: From a non-sexual video clip, predict whether future parts of the video are porn or not.

I think that would also be a cool project!",19,wdtya3,"Hi all, 

Here is one of the few NSFW posts in this sub. I am wondering that it would be a cool personal project if I could train a deep learning model to predict the sex act (oral, or different positions) being performed in a particular X-rated video. I've seen different projects out there predicting different human activities in a video, but I haven't come across something like this. The way I'm thinking of approaching this problem is:

1. Label videos and store individual frames corresponding to those acts.
2. Train a CNN model to predict these categories.

I'm sure this problem isn't this straight forward but I'd love some pointers from you all as to what my approach here should be. For example, each act can be filmed from a variety of different angles and thus would need lot of data capturing all those angles.",deeplearning,2022-08-01 15:05:47,8
"~2k$ ?
Have you been rejected for a new job because you did not have this training ?
Higher pay in your company with it ?",2,we4sup,,deeplearning,2022-08-02 00:00:59,1
"Not much information. I've used festival for voices before.

https://www.cstr.ed.ac.uk/projects/festival/

I believe that the above is its link. I prefer the female British voice. I wish they had a Dalek voice.",1,we6mud,How to improve the voice modulation more like humans in the Chatbot and recommendations and suggestions please share,deeplearning,2022-08-02 01:56:56,2
"Your model just classifies everything as label 0. It has good accuracy because you have an imbalanced dataset. For a better measure of performance use F1. In this case it will be NaN or 0, depending on your settings, which would mean your model didn't learn anything other than the general distribution of your data.",5,wdmhwn,"The model is giving 0.9 acc, and 0.3 loss, but when I draw the confusion matrix, all the anomalies are labeled FPs, it does not make sense to me and ROC curve is straight line thru the middle. How do I interpret it? Loss and Acc curves look normal though. Could someone point me in a good direction?

&#x200B;

`testing_datagen = ImageDataGenerator(rescale=1/255)`

`testing_generator = testing_datagen.flow_from_directory(`

`TESTINGDIR,`

`classes = ['anomaly', 'normal'],`

`batch_size=1,`

`class_mode='binary',`

`shuffle=True`

`,color_mode=""rgb"")`

&#x200B;

`model.evaluate(testing_generator)`

`STEP_SIZE_TEST=testing_generator.n//testing_generator.batch_size`

`testing_generator.reset()`

`preds = model.predict(testing_generator, STEP_SIZE_TEST)`

\#Confution Matrix and Classification Report

`Y_pred = model.predict_generator(testing_generator, STEP_SIZE_TEST+1)`

`y_pred = np.argmax(Y_pred, axis=1)`

`print('Confusion Matrix')`

`print(confusion_matrix(testing_generator.classes, y_pred))`

`print('Classification Report')`

`target_names = ['normal', 'anomaly']`

`print(classification_report(testing_generator.classes, y_pred, target_names=target_names))`

&#x200B;

Confusion Matrix

\[\[23075     0\]

\[22226     0\]\]",deeplearning,2022-08-01 10:00:36,12
I think that node / subgraph / graph embeddings are the terms you are looking for,5,wdgmz8,"Hi there y'all

I am writing a report on graph theory and need some help with some terminology as i am not really an expert.

I don't know which term would be best used for the following:

Clustering similar nodes together to form a single node with a feature vector that represents the internal nodes (the ones that the cluster represents) and preferably can reconstruct from this vector.

&#x200B;

Also, are there any papers I can reference to check out the state-of-the-art?",deeplearning,2022-08-01 05:47:18,6
"When it comes to deep learning, VRAM per GPU is King. If you are using multiple GPUs, their memory pool do not add up.

Let's say you are training a model over two GPUs, then same copy of model will exist over both GPUs and they will train batches of data in parallel and exchange notes on backpropagation after each step.

Now a model can also be split between two GPUs, such that first half layers are in one GPU, later half are in second GPU. This is feasible but painful to do in terms of programming.

I'd suggest go for highest VRAM GPU you can afford, then start adding up more of them.",1,wdj2tb,"Hello, I'm a 3d artist that got into machine learning recently, I am particularly interested in gpt and NLP in general, I am building a new workstation and would love to get some clarifications here.

* Can someone please explain the difference between using multiple gpus with nvlink and multiple gpus without nvlink in deep learning?
* For fine tuning big models like the gpt neox 20b is it mandatory to have a single gpu with 48gb or can you do with multiple gpus that collectively meet the requirement and if so do they need to be connected with nvlink or to be physically on the same node or what? 
* How important is the role of ram (clock-speed and capacity and cpu here?
* I havent touched image generation at all, but if I am to experiment with serious works using image generation networks do the same answers apply?",deeplearning,2022-08-01 07:38:04,1
"1. Nope, you need to pad the encoded sequences to an equal length

2. On position only, probably not. Normally data is shuffled before training and split into training and validation sets. And even without that, it seems a bit more on the unlikely side. To make the model aware of dates you would need to add that into the model, preferably in a separate sequence than the text. This will then be processed and concatenated somewhere in the network to combine the two, and make the model aware of both the text and dates.

3. You are right with the encoding part. Your strings will be split into a sequence of classes / integers (which are mapped to the corresponding string).",2,wdo7gu,"Hi.

We have a sequence of products, arranged from oldest product taken to newest. A customer can take one or multiple products at different periods. I want to create a model that's trained on a sequence of products, and is able to 'predict'/recommend what a customer's next best offer/product. 

It was suggested using a LSTM model but I had a few questions/concerns regarding this. Still very new to neural networks.

\- Can the input be of different lengths?

\- If my input will be a sequence of products of varying lengths, would the model somehow know which product is the most recent based on position only? We might weigh more recent products (and maybe the time between two products) more. We have the date when a customer takes the product. 

\- I assume I have to apply encodings/vectorization on my products, so we'll assign a number (or float?) for each product. So the sequence of products will be a sequence of product arrays?

&#x200B;

Any direction or references would be appreciated. Please feel free to suggest how you would approach this.

Thanks.",deeplearning,2022-08-01 11:10:17,6
"This guy is just amazing! Loved all his videos, it's a go to stop for learning something new, which hardy any other yt videos have..love it!",1,wcp26w,,deeplearning,2022-07-31 06:43:37,1
What's the data like? Only images and detection GTs?,2,wd9ige,Hey Everyone. I was recently tasked(by my university) with creating a AI based speed detection system for Vehicles using deep learning. I'm a beginner in this field. Can someone help me and guide me on how to do this?,deeplearning,2022-07-31 22:44:26,5
"I’m not a js or ui guy, but I’ve previously seen handoffs where you have the ML guy upload the model artifact somewhere and then there’s an api that uses the model and handles cache/predictions. The UI is just interacting with said API",2,wciid7,"Hi, is it possible for me to create a website that uses ONNX.js to serve the model and create a frontend using React? If yes, please point toward some resources.

Also, what other ways can a Pytorch model be served into production in conjunction with a react frontend, any suggestions are welcome.",deeplearning,2022-07-30 23:54:17,1
"Do you want discrete latent variables? If so, check out:

https://blog.evjang.com/2016/11/tutorial-categorical-variational.html?m=1

https://jxmo.io/posts/variational-autoencoders

The VQVAE paper",1,wcc1yy,"Hello community. I'm currently implementing a VAE solution. In essence I am trying to decode into categorical variables.  (I have a fixed sequence/vector of ints)

In TensorFlow 2.0 examples , I saw that they used sigmoid\_cross\_entropy\_with\_logits as the reconstruction loss. 

AFAIK, this is useful for binary reconstructions but I wondered if I could use a cross-entropy loss instead. Also, I saw the derivation of a VAE has log p(x|z) as the reconstruction term. I always wonder if I could use the NLL loss using the mean and variance parameters instead. But that probably only works if x is gaussian distributed.",deeplearning,2022-07-30 17:49:01,1
"NVIDIA always.  There is no question that NVIDIA's CUDA (CUDNN, CUBLAS, etc.) deeplearning ecosystem is superior to AMD's equivalent.   AMD's offerings are fine for gaming, but you'll get better integration with deep learning platforms, better performance, and have a wider user-base to help with questions with NVIDIA hardware.",8,wbz8wp,"Hi There, I'm planning to get a GPU for AI and Deep Learning.   

Prices for the GPUs are as follows:

&#x200B;

\[6700xt: $563\]([https://mdcomputers.in/asus-dual-rx6700xt-o12g.html](https://mdcomputers.in/asus-dual-rx6700xt-o12g.html))

&#x200B;

\[3070: $675\]([https://mdcomputers.in/inno3d-rtx-3070-twin-x2-8gb-n30702-08d6-171032lh.html](https://mdcomputers.in/inno3d-rtx-3070-twin-x2-8gb-n30702-08d6-171032lh.html))

&#x200B;

\[6800XT: $750\]([https://mdcomputers.in/asus-tuf-rx6800xt-o16g-gaming.html](https://mdcomputers.in/asus-tuf-rx6800xt-o16g-gaming.html))

&#x200B;

Which will give a better VFM? It would be better if someone compares with actual Benchmarks instead of just saying XYZ product is better just because that XYZ manufacturer is better in AI.",deeplearning,2022-07-30 07:51:18,6
Yes,8,wayr2c,"I feel like this is a stupid question but here goes. RelU for example is a non-linear activayion function. From my current understanding, wouldnt the output still be composed of straight lines joined together? How can relu networks approximate polynomial functions for indtance? Is it sort of like joinning together many lines to approximate a curve?",deeplearning,2022-07-29 00:54:29,16
Do you have deep learning experience? You can try fine tuning YOLO and see what happens.,1,wbavny,"I'll approach the following problem next week and would love to get some advice. I am considering modeling it as a regression problem for x,y,r or as an object detection problem.

I have microscopy images coming from multiple sources(different contrast, brightness, zoom, focus), and each image has a circle in it that I want to detect.

An image can have multiple circles in it(of which I am interested in a specific one) and it can also have a single circle. Ultimately, I am interested in the area of that circle.  

I have around 4k of annotated images(bounding box around the circle) from 3 sources, I have gotten good results with using hugh transform but it doesn't generalize properly to the other image sources since they have different amounts of circles in them.

I am starting to think that using deep learning with heavy augmentations(contrast, brightness, etc...) will help me generalize  to all the different sources.

Any input would be appreciated.",deeplearning,2022-07-29 10:36:11,3
"Really depends on if want to get into the theory, which would require graduate level math at least",2,wbf34d,"I want to understand deep learning as fast as possible so that I can contribute to one of my research projects on computer vision. I will give 5-6 hours per day, and I want to get decent in 1 month. I define decent as reading and understanding deep learning papers and codes.",deeplearning,2022-07-29 13:37:40,8
"It will depend on what implementation you use. Detectron2 can accept COCO for Mask RCNN, it’s pretty common. A free account on hasty.ai is the best way to label instances in my opinion. Check it out",2,wb3t81,"Hi, I will have to label some instances from micrograph images and understand that Yolact accepts COCO-style annotations. Are the annotation formats the same for U-Net and Mask R-CNN? Is there some labelling tool which can be used for both?",deeplearning,2022-07-29 05:42:03,3
I’ve come across using multi rater agreement. Multiple humans and your algorithm score the same dataset then each gets a score. If the algorithm has a value comparable to humans then you have human level performance. https://en.m.wikipedia.org/wiki/Inter-rater_reliability,1,waqqlr,"Hi, I would like to get some advice on how to go about measuring human-level performance (HLP) for an object detection task.

What kind of experiments should I design to measure this, because my ground truths also come from human annotators. Does this mean I am comparing one human annotator against the other to measure the HLP?

How about measuring HLP for image classification?",deeplearning,2022-07-28 17:58:00,2
"It's not likely to give you results you'd expect, but it depends on the optimizer. Whether or not a model performs good is also dependent on the initialization of variables.

Essentially when you do hyperparameter tuning, you are trying to eliminate the influence of random chance. You create multiple differently initialized models and you take multiple measurements to try and see what the real behaviour of the model is.

If you were to tune hyperparameters on a model that is already trained a bit, you are still trying to eliminate random chance. But now your model is at the very least dependent on ALL the hyper parameters used to train it. So your initial hyperparameter search does not apply necessarily, you only empirically sort of proved that for the initial conditions your best model has the optimal hyperparameters. There is NO GUARANTEE these are optimal parameters for the rest of the training.

The takeaway here is that for hyperparameter tuning, you should train the model fully. Then you could perhaps fine tune it on some data if you're pretraining it, but keep in mind that your optimal model is NOT GUARANTEED to train optimally when fine-tuning data. This is a separate task. If you want to tune hyperparameters for a fine-tuning task, then you should consider model pretraining and finetuning a single task.

So you should look at what your end goal is, and fine-tune hyperparameters by measuring performance on each instance that reaches that goal. You cannot guarantee that some optimal checkpoint will result in the optimal goal, that would be the definition of a greedy algorithm, and you know that greedy algorithms do not always find the optimal solution.",1,wah86t,"Hello,

It's one of these ""I'm pretty sure I should know that"" moment. :-)

I'm using Optuna to tune some hyper parameters (but the model architecture is not one of them). I use an EarlyStoping monitoring the validation metric (IoU for semantic segmentation if that matters) and save the corresponding checkpoint.  

I wonder if it's possible to start trial N with the ""updated best model"" meaning the best model found among trial 0 to N-1. That way I would see if the new parameters can do better right from the start. 

Is it an efficient way to save training time or does using different starting point for each trial is not a valid/fair way to compare each parameters set?

The only thing I see is the learning rate momentum (if there's any) will be different. But I'm not sure it would have such a  big impact…

Thank you!",deeplearning,2022-07-28 11:22:59,5
"It's available here:

[https://github.com/xinntao/Real-ESRGAN/blob/master/inference\_realesrgan\_video.py](https://github.com/xinntao/Real-ESRGAN/blob/master/inference_realesrgan_video.py)",1,wa6vao,,deeplearning,2022-07-28 04:00:50,6
No overlap. What do you want to do in ML? Python will be the lingua Franca even if you’re doing related things like MLOps/platform engineering/MLE/DE.,2,wamy0w," 

My main skills were solidity and web development. Obviously, I don't think my solidity skills will be useful here, but will my web development skills have some form of value in my journey? Or is it all python?

Also, any recommendations or thoughts of moving from Blockchain to ML/DL and AI are welcomed.

Thanks",deeplearning,2022-07-28 15:10:56,5
"Oh, BTW, there are non image files in the folder does it matter? It didn't throw an error while training!",0,wabjfc,"**(Edit: SOLVED! Thank you!)** Any idea what could be going on? Do you think it couldn't identify an image?

    PIL.UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x7fa170d2f410>
    [[{{node PyFunc}}]]
    [[IteratorGetNext]] 
    0 successful operations. 
    0 derived errors ignored. [Op:__inference_predict_function_20292]

TB:

    2022-07-28 09:18:38.79: W tensorflow/core/framework/op_kernel.cc:1733] UNKNOWN: UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x7fa170d2f410> Traceback (most recent call last):    
    
    File ""/home/w/anaconda3/envs/detection_keras/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py"", line 270, in __call__     ret = func(*args)    
    File ""/home/w/anaconda3/envs/detection_keras/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py"", line 642, in wrapper     return func(*args, **kwargs)    
    File ""/home/w/anaconda3/envs/detection_keras/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 1030, in generator_py_func     values = next(generator_state.get_iterator(iterator_id))    
    File ""/home/w/anaconda3/envs/detection_keras/lib/python3.7/site-packages/keras/engine/data_adapter.py"", line 831, in wrapped_generator     for data in generator_fn():    
    File ""/home/w/anaconda3/envs/detection_keras/lib/python3.7/site-packages/keras/engine/data_adapter.py"", line 957, in generator_fn     yield x[i]    
    File ""/home/w/anaconda3/envs/detection_keras/lib/python3.7/site-packages/keras/preprocessing/image.py"", line 110, in __getitem__     return self._get_batches_of_transformed_samples(index_array)    
    File ""/home/w/anaconda3/envs/detection_keras/lib/python3.7/site-packages/keras/preprocessing/image.py"", line 342, in _get_batches_of_transformed_samples     keep_aspect_ratio=self.keep_aspect_ratio)    
    File ""/home/w/anaconda3/envs/detection_keras/lib/python3.7/site-packages/keras/utils/image_utils.py"", line 394, in load_img     img = pil_image.open(io.BytesIO(f.read()))    
    File ""/home/w/anaconda3/envs/detection_keras/lib/python3.7/site-packages/PIL/Image.py"", line 3031, in open     ""cannot identify image file %r"" % (filename if filename else fp)  
    
    PIL.UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x7fa170d2f410>",deeplearning,2022-07-28 07:33:15,5
Maybe you can add regularization techniques and metrics,6,wa39m3,"Hi,  
im working on paper that sumarizes the commonly used training techniques in deep learning. I would like to ask for tips on topics i am missing and if you have tips on sources which take insight into these specific topics it would be really helpful.  
So far, i've looked into

* gradients and back-propagation.
* error function such as BCE,MSE,MAE.
* Optimizers such as gradient decent, conjugated gradients, SDG, ADAM

&#x200B;

But i still feel like im missing a lot of stuff that would be helpful for others to understand the training processes.

Please help",deeplearning,2022-07-28 00:31:31,6
This is way too complicated if you haven't coded ML projects before. Start with something simple.,2,w99crl,I read this paper ( [https://arxiv.org/pdf/1612.07411.pdf](https://arxiv.org/pdf/1612.07411.pdf) ) and want to implement it but I dont have much experience on how to start the coding part. I have read the paper thoroughly and understood it as much as I can but I face problems in understanding how to implement the complex equations used in the paper. I looked on GitHub for related codes but could not find useful resources. Can someone suggest to me how can I start coding?,deeplearning,2022-07-27 01:40:57,4
Tip for the future - post your errors or Google them.,1,w9gsij,"I am trying to train a custom dataset with yolox algorithm for object detection, I didn't find any solution, I followed all the youtube videos, the Roboflow colab, I still get errors while training
I want someone to really help me
This is the output after I train
2022-07-29 18:55:16.364 | INFO     | yolox.core.trainer:before_train:125 - args: Namespace(batch_size=16, ckpt='/content/yolox_s.pth', devices=1, dist_backend='nccl', dist_url=None, exp_file='exps/example/yolox_voc/yolox_voc_s.py', experiment_name='yolox_voc_s', fp16=True, local_rank=0, machine_rank=0, name=None, num_machines=1, occupy=True, opts=[], resume=False, start_epoch=None)
2022-07-29 18:55:16.366 | INFO     | yolox.core.trainer:before_train:126 - exp value:
╒══════════════════╤════════════════════════════╕
│ keys             │ values                     │
╞══════════════════╪════════════════════════════╡
│ seed             │ None                       │
├──────────────────┼────────────────────────────┤
│ output_dir       │ './YOLOX_outputs'          │
├──────────────────┼────────────────────────────┤
│ print_interval   │ 10                         │
├──────────────────┼────────────────────────────┤
│ eval_interval    │ 10                         │
├──────────────────┼────────────────────────────┤
│ num_classes      │ 3                          │
├──────────────────┼────────────────────────────┤
│ depth            │ 0.33                       │
├──────────────────┼────────────────────────────┤
│ width            │ 0.5                        │
├──────────────────┼────────────────────────────┤
│ data_num_workers │ 4                          │
├──────────────────┼────────────────────────────┤
│ input_size       │ (640, 640)                 │
├──────────────────┼────────────────────────────┤
│ random_size      │ (14, 26)                   │
├──────────────────┼────────────────────────────┤
│ train_ann        │ 'instances_train2017.json' │
├──────────────────┼────────────────────────────┤
│ val_ann          │ 'instances_val2017.json'   │
├──────────────────┼────────────────────────────┤
│ degrees          │ 10.0                       │
├──────────────────┼────────────────────────────┤
│ translate        │ 0.1                        │
├──────────────────┼────────────────────────────┤
│ scale            │ (0.1, 2)                   │
├──────────────────┼────────────────────────────┤
│ mscale           │ (0.8, 1.6)                 │
├──────────────────┼────────────────────────────┤
│ shear            │ 2.0                        │
├──────────────────┼────────────────────────────┤
│ perspective      │ 0.0                        │
├──────────────────┼────────────────────────────┤
│ enable_mixup     │ True                       │
├──────────────────┼────────────────────────────┤
│ warmup_epochs    │ 5                          │
├──────────────────┼────────────────────────────┤
│ max_epoch        │ 300                        │
├──────────────────┼────────────────────────────┤
│ warmup_lr        │ 0                          │
├──────────────────┼────────────────────────────┤
│ basic_lr_per_img │ 0.00015625                 │
├──────────────────┼────────────────────────────┤
│ scheduler        │ 'yoloxwarmcos'             │
├──────────────────┼────────────────────────────┤
│ no_aug_epochs    │ 15                         │
├──────────────────┼────────────────────────────┤
│ min_lr_ratio     │ 0.05                       │
├──────────────────┼────────────────────────────┤
│ ema              │ True                       │
├──────────────────┼────────────────────────────┤
│ weight_decay     │ 0.0005                     │
├──────────────────┼────────────────────────────┤
│ momentum         │ 0.9                        │
├──────────────────┼────────────────────────────┤
│ exp_name         │ 'yolox_voc_s'              │
├──────────────────┼────────────────────────────┤
│ test_size        │ (640, 640)                 │
├──────────────────┼────────────────────────────┤
│ test_conf        │ 0.01                       │
├──────────────────┼────────────────────────────┤
│ nmsthre          │ 0.65                       │
╘══════════════════╧════════════════════════════╛
2022-07-29 18:55:16.855 | INFO     | yolox.core.trainer:before_train:132 - Model Summary: Params: 8.94M, Gflops: 26.76
2022-07-29 18:55:23.159 | INFO     | apex.amp.frontend:initialize:328 - Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.
2022-07-29 18:55:23.160 | INFO     | apex.amp.frontend:initialize:329 - Defaults for this optimization level are:
2022-07-29 18:55:23.160 | INFO     | apex.amp.frontend:initialize:331 - enabled                : True
2022-07-29 18:55:23.160 | INFO     | apex.amp.frontend:initialize:331 - opt_level              : O1
2022-07-29 18:55:23.161 | INFO     | apex.amp.frontend:initialize:331 - cast_model_type        : None
2022-07-29 18:55:23.161 | INFO     | apex.amp.frontend:initialize:331 - patch_torch_functions  : True
2022-07-29 18:55:23.161 | INFO     | apex.amp.frontend:initialize:331 - keep_batchnorm_fp32    : None
2022-07-29 18:55:23.162 | INFO     | apex.amp.frontend:initialize:331 - master_weights         : None
2022-07-29 18:55:23.162 | INFO     | apex.amp.frontend:initialize:331 - loss_scale             : dynamic
2022-07-29 18:55:23.162 | INFO     | apex.amp.frontend:initialize:336 - Processing user overrides (additional kwargs that are not None)...
2022-07-29 18:55:23.163 | INFO     | apex.amp.frontend:initialize:354 - After processing overrides, optimization options are:
2022-07-29 18:55:23.163 | INFO     | apex.amp.frontend:initialize:356 - enabled                : True
2022-07-29 18:55:23.163 | INFO     | apex.amp.frontend:initialize:356 - opt_level              : O1
2022-07-29 18:55:23.164 | INFO     | apex.amp.frontend:initialize:356 - cast_model_type        : None
2022-07-29 18:55:23.164 | INFO     | apex.amp.frontend:initialize:356 - patch_torch_functions  : True
2022-07-29 18:55:23.164 | INFO     | apex.amp.frontend:initialize:356 - keep_batchnorm_fp32    : None
2022-07-29 18:55:23.165 | INFO     | apex.amp.frontend:initialize:356 - master_weights         : None
2022-07-29 18:55:23.165 | INFO     | apex.amp.frontend:initialize:356 - loss_scale             : dynamic
2022-07-29 18:55:23.171 | INFO     | yolox.core.trainer:resume_train:297 - loading checkpoint for fine tuning
2022-07-29 18:55:23.287 | WARNING  | yolox.utils.checkpoint:load_ckpt:27 - Shape of head.cls_preds.0.weight in checkpoint is torch.Size([80, 128, 1, 1]), while shape of head.cls_preds.0.weight in model is torch.Size([3, 128, 1, 1]).
2022-07-29 18:55:23.288 | WARNING  | yolox.utils.checkpoint:load_ckpt:27 - Shape of head.cls_preds.0.bias in checkpoint is torch.Size([80]), while shape of head.cls_preds.0.bias in model is torch.Size([3]).
2022-07-29 18:55:23.288 | WARNING  | yolox.utils.checkpoint:load_ckpt:27 - Shape of head.cls_preds.1.weight in checkpoint is torch.Size([80, 128, 1, 1]), while shape of head.cls_preds.1.weight in model is torch.Size([3, 128, 1, 1]).
2022-07-29 18:55:23.288 | WARNING  | yolox.utils.checkpoint:load_ckpt:27 - Shape of head.cls_preds.1.bias in checkpoint is torch.Size([80]), while shape of head.cls_preds.1.bias in model is torch.Size([3]).
2022-07-29 18:55:23.289 | WARNING  | yolox.utils.checkpoint:load_ckpt:27 - Shape of head.cls_preds.2.weight in checkpoint is torch.Size([80, 128, 1, 1]), while shape of head.cls_preds.2.weight in model is torch.Size([3, 128, 1, 1]).
2022-07-29 18:55:23.289 | WARNING  | yolox.utils.checkpoint:load_ckpt:27 - Shape of head.cls_preds.2.bias in checkpoint is torch.Size([80]), while shape of head.cls_preds.2.bias in model is torch.Size([3]).
2022-07-29 18:55:23.315 | INFO     | yolox.core.trainer:before_train:152 - init prefetcher, this might take one minute or less...
2022-07-29 18:55:23.532 | ERROR    | yolox.core.launch:launch:90 - An error has been caught in function 'launch', process 'MainProcess' (1124), thread 'MainThread' (140241892104064):
Traceback (most recent call last):

  File ""tools/train.py"", line 125, in <module>
    args=(exp, args),
          │    └ Namespace(batch_size=16, ckpt='/content/yolox_s.pth', devices=1, dist_backend='nccl', dist_url=None, exp_file='exps/example/y...
          └ ╒══════════════════╤═════════════════════════════════════════════════════════════════════════════════════════════════════════...

> File ""/content/YOLOX/yolox/core/launch.py"", line 90, in launch
    main_func(*args)
    │          └ (╒══════════════════╤════════════════════════════════════════════════════════════════════════════════════════════════════════...
    └ <function main at 0x7f8c99fd7200>

  File ""tools/train.py"", line 104, in main
    trainer.train()
    │       └ <function Trainer.train at 0x7f8b9d6a5710>
    └ <yolox.core.trainer.Trainer object at 0x7f8b97a87790>

  File ""/content/YOLOX/yolox/core/trainer.py"", line 69, in train
    self.before_train()
    │    └ <function Trainer.before_train at 0x7f8b97fcc050>
    └ <yolox.core.trainer.Trainer object at 0x7f8b97a87790>

  File ""/content/YOLOX/yolox/core/trainer.py"", line 153, in before_train
    self.prefetcher = DataPrefetcher(self.train_loader)
    │                 │              │    └ <yolox.data.dataloading.DataLoader object at 0x7f8b7e43d410>
    │                 │              └ <yolox.core.trainer.Trainer object at 0x7f8b97a87790>
    │                 └ <class 'yolox.data.data_prefetcher.DataPrefetcher'>
    └ <yolox.core.trainer.Trainer object at 0x7f8b97a87790>

  File ""/content/YOLOX/yolox/data/data_prefetcher.py"", line 26, in __init__
    self.preload()
    │    └ <function DataPrefetcher.preload at 0x7f8b9bf39290>
    └ <yolox.data.data_prefetcher.DataPrefetcher object at 0x7f8b9abd2850>

  File ""/content/YOLOX/yolox/data/data_prefetcher.py"", line 30, in preload
    self.next_input, self.next_target, _, _ = next(self.loader)
    │                │                             │    └ <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f8b90065710>
    │                │                             └ <yolox.data.data_prefetcher.DataPrefetcher object at 0x7f8b9abd2850>
    │                └ <yolox.data.data_prefetcher.DataPrefetcher object at 0x7f8b9abd2850>
    └ <yolox.data.data_prefetcher.DataPrefetcher object at 0x7f8b9abd2850>

  File ""/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py"", line 517, in __next__
    data = self._next_data()
           │    └ <function _MultiProcessingDataLoaderIter._next_data at 0x7f8ba5239710>
           └ <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f8b90065710>

  File ""/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py"", line 1199, in _next_data
    return self._process_data(data)
           │    │             └ <torch._utils.ExceptionWrapper object at 0x7f8b7eef7050>
           │    └ <function _MultiProcessingDataLoaderIter._process_data at 0x7f8ba5239830>
           └ <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f8b90065710>

  File ""/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py"", line 1225, in _process_data
    data.reraise()
    │    └ <function ExceptionWrapper.reraise at 0x7f8c97bb3320>
    └ <torch._utils.ExceptionWrapper object at 0x7f8b7eef7050>

  File ""/usr/local/lib/python3.7/dist-packages/torch/_utils.py"", line 429, in reraise
    raise self.exc_type(msg)
          │    │        └ Caught KeyError in DataLoader worker process 0.
          │    │          Original Traceback (most recent call last):
          │    │            File ""/usr/local/lib/python3.7/...
          │    └ <class 'KeyError'>
          └ <torch._utils.ExceptionWrapper object at 0x7f8b7eef7050>

KeyError: Caught KeyError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File ""/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py"", line 202, in _worker_loop
    data = fetcher.fetch(index)
  File ""/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py"", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File ""/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py"", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File ""/content/YOLOX/yolox/data/datasets/datasets_wrapper.py"", line 121, in wrapper
    ret_val = getitem_fn(self, index)
  File ""/content/YOLOX/yolox/data/datasets/mosaicdetection.py"", line 91, in __getitem__
    img, _labels, _, _ = self._dataset.pull_item(index)
  File ""/content/YOLOX/yolox/data/datasets/voc.py"", line 145, in pull_item
    target = self.load_anno(index)
  File ""/content/YOLOX/yolox/data/datasets/voc.py"", line 126, in load_anno
    target = self.target_transform(target)
  File ""/content/YOLOX/yolox/data/datasets/voc.py"", line 64, in __call__
    label_idx = self.class_to_ind[name]
KeyError: 'celtia'",deeplearning,2022-07-27 07:51:57,2
"Thanks for sharing this very interesting work!

One point would interest me that I don't yet see addressed in the blog post: What are the error rates of the approach itself? How many things are flagged that turn out not to be labelling errors and how many labelling error remain unflagged?",3,w8o9m5,"Hi folks! I've made a new technique for finding errors in object detection datasets, using new explainable AI techniques from my PhD. I was frankly pretty surprised to be able to find about 275k errors in MS COCO's training set (which has around 700k labels). This includes things like incorrectly drawn bounding boxes (shown below, about 55k), missing background labels (178k), and missing labels that overlap with existing labels (40k).

While there's been some work on improving datasets, as far as I know this is the largest number of errors found on any public ML dataset, by a wide margin.

I would love to get the communities thoughts on this. I am also building a company, so if you're interested in using this on your work feel free to DM me.

To learn more about the results (and see more pictures), check out my article: [https://medium.com/@jamie\_34747/79d382edf22b?source=friends\_link&sk=d36ad07c074818c48d8f421f6ed104cd](https://medium.com/@jamie_34747/79d382edf22b?source=friends_link&sk=d36ad07c074818c48d8f421f6ed104cd)

&#x200B;

[ COCO label \(solid line\) and FIXER correction \(dotted\) in MS COCO. The COCO label cut off the baseball player’s legs ](https://preview.redd.it/k4vix5vcwxd91.png?width=1440&format=png&auto=webp&s=b9f28c2a2c62abd3b69e6fe6ee587f1fb2b56a25)",deeplearning,2022-07-26 09:31:34,2
"From my experience i have found that continuing the pre-training of BERT with your data (conversation data) works most of the times. However, if the volume of data you have matches the volume of data that was used to pre-train BERT, pre-training BERT from scratch using your data might be worth a shot.",4,w91fa5,"Hi guys, I am now trying to pretrain a language model(probably BERT, ELECTRA, or roBERTa).

Our domain would be limited to ""conversation"" and we are expecting colloquial language.

We are now collecting the data.

I have few questions.

1. Would it be OK to pre-train just with conversation domain and colloquial data?  
Should I include some text data from news or wiki?
2. Would it be better to start from other pre-trained weights? (ex) bert-base-cased, bert-base-uncased) or is better to start from random initial weight.",deeplearning,2022-07-26 18:37:29,4
This is pretty cool,3,w8cfd2,"**Pro tip for machine learners:** Instead of searching around on Google or elsewhere for code implementations for AI/machine learning techniques/methods/tasks, you can now directly find them on CatalyzeX — we’ve rolled out a search filter toggle that allows you to see only papers that have code available! https://www.catalyzex.com/s/photo%20style%20transfer?with\_code=true 

Do check it out live, and your feedback and constructive criticism is highly welcome anytime! 🙏 (Disclaimer: I am one of the creators of CatalyzeX)

https://reddit.com/link/w8cfd2/video/x352u33mxud91/player",deeplearning,2022-07-25 23:36:26,6
"Chain of thoughts prompting and bayesian reasoning can be used to implement any arbitrary planning algorithm by a llm. The trouble is that models are too big and expensive to run, with a bias toward zero-shot or few-shot learning. Recursion and repetition is necessary to account for comparison of a range of outcomes when planning and predicting the results of different scenarios.  

It's not reasonable to expect LLMs to be magically capable of simultaneous reasoning over complex scenarios, but any given planning method can be broken down into steps LLMs are able to handle.",2,w8igau,,deeplearning,2022-07-26 05:26:02,2
"Not only does the Air throttle after 5 minutes of load, the compute improvements over last gen are relatively small, and even that was only comparable to budget discrete laptop GPUs in speed, rather than high end GPUs.",2,w8s13f,,deeplearning,2022-07-26 12:02:30,10
"Transformer models can be exported to ONNX, TensorRT, whatever, you name it. I am not sure why you claim they are more difficult to deploy, I've never experienced that.",3,w8cawj,"I've heard many people saying that self attention mechanisms are not as deployment friendly as conv, linear layers etc. Yolov5 author said mechanisms like Squeeze and Excitations are difficult for exporting models to deployment friendly formats. Transformer based models are also facing challenges to deployment due to their attention modules (at least this is what I was told)

Why is this? For example, SE is really just a pool and  linear layers. Both are very commonly deployed components, what makes the combined SE module not friendly any more?",deeplearning,2022-07-25 23:29:11,9
"My main concern is what are your covariates, and do they contain enough information to predict the squat increase? I assume that there are a lot of relevant factors that are difficult to capture such as nutrition, genetics, stress, and so on.

Anyways, that wasn't the question - just wanted to point that out before you go on and collect massive amounts of data. Let's assume that you manage to gather relevant information and the problem is indeed solvable (in the sense that you have enough data and the covariates contain enough signal to predict your target). For your first case I'd start with collaborative filtering and if that doesn't work move into more complex methods like factorization machines. For the second case you could start with a transformer or RNNs (those contain the inductive bias that you need which is exploit of temporal information), if that doesn't work you could try again a factorization machine, giving it the representations learned by the RNN/transformer, which should be capturing the relevant time-related features.",1,w8i1qp,"I am building a machine learning model that recommends the best powerlifting training method to individuals in different environments.

Simply, for example,

Before training with method Alpha, A person was 6 feet tall and weighs 200 lbs including 100 lbs of muscle and can squat 400 lbs. (+ other quantifiable values )

After training with method Alpha, the person is still 6 feet tall but weighs 203 lbs including 102 lbs of muscle and can squat 420 lbs. (+ other quantifiable values )

The above information can be vectorized as:[ ‘Before’data : (6 feet, 200 lbs, 100 lbs, 400 lbs, other values . . .) ][ ‘After’ data : (Method Alpha, 6 feet, 203 lbs, 102 lbs, 420 lbs, other values . . .) ]

If I got a lot of data like this from a lot of people, I would be able to build a machine learning model for 2 cases.
* Case 1: Recommending the method that ended with the most increased squats in similar environments.
* Case 2: Recommending the method that ended with the most increased squats, but considering not only each environment separately but also time-series data for each person, so automatically the model could find some time-series data similar to the environments that the person requesting recommendation has gone though. based on it, may recommend better.

In each Case 1, and Case 2, what are the proper machine learning algorithms?(like CF, MF, RNN, CNN, etc )",deeplearning,2022-07-26 05:05:49,2
"I hope this help.
https://youtu.be/fMzaRptc1AA",2,w7zhqn,,deeplearning,2022-07-25 13:33:45,1
"You should probably start by learning the basics of ML and the math required first (Linear Algrebra, Stats and Probas etc..), it is pointless to look at code examples and not understand the inner workings. If you don't have a strong math/CS background it's gonna be difficult.

My reference book is https://www.deeplearningbook.org/",2,w8mub2,"Hi guys,

I am trying to learn Deep learning and for theory I have found many books as Deep Learning for Coders with fastai and Pytorch, Hands on ML, etc…is there any youtube channel or course( payed or free), which shows python code on variete of example in deep learning? Like explaining code from scratch or making real project, end to end? Thanks in advance",deeplearning,2022-07-26 08:35:05,8
Right click in anaconda prompt to paste your api key,2,w8dioo,"I am using Yolov5 and while trying to run train command, this wandb login keeps popping up and won't even let me use an existing account (option 2) nor let me opt out (option 3). It doesn't accept the correct API also even if I paste it. Please help!

https://preview.redd.it/mfmojl6d9vd91.png?width=979&format=png&auto=webp&s=2a6745e0f5e5e8cb1ee750a8002c8adf70027ddc",deeplearning,2022-07-26 00:41:33,2
"Maybe, but you're not doing yourself any favors by combining these tasks in one step.  You'd be better off training one model to differentiate between the two sounds, and a different model to classify each of the sounds.  Each of these tasks are relatively simple to accomplish; combining them would be relatively difficult.

To specifically answer your question: I'm not aware of a single model that does what you're describing.  Anything that resembles that capability is likely using an ensemble method like how I described.",2,w8dsek,"Hi, is there a way to make a multi label model, that is capable of classifying the labels/classes from audio files in chronological order? For example if the audio contains a dog barking and afterwards sounds of birds singing, the model will output these two classes and also tell the order of them",deeplearning,2022-07-26 00:58:45,3
"For some reason I could not find a concise list like this anywhere on google. I decided to make this repo in order to get my thoughts together on the different DL model types and their use cases. It was a really good learning experience for me but now that it's finished I'm sure others will be able to make use of it also.

Let me know if you see anything major missing from the list or incorrectly written in the narratives. Thanks.",1,w7r113,,deeplearning,2022-07-25 07:55:55,1
Looking to learn AI and searching for the [**best Artificial Intelligence Books for Beginners**](https://codingvidya.com/best-artificial-intelligence-books-for-beginners/) to Advanced then I found the best AI books here?,1,w8chgm,,deeplearning,2022-07-25 23:39:57,1
"First order motion model does this if you want to make a reference video of yourself singing the song. It transfers face movement from a video to an image. 

Wav2lip will match mouth movement of a video to an audio clip. But you need a video of whoever you want to sing the song. 

Both of these produce low quality outputs though, I think 256x256.

My repo does exactly what you're looking for but I haven't tried it with song lyrics, only regular speech. All you need is an image of a person and upload the audio you want them to speak. Produces 1024x1024 output. If you try it, let me know how it works. 

https://github.com/johnGettings/LIHQ",2,w8illy,,deeplearning,2022-07-26 05:33:17,3
i have successfully run it via jupyter notebook but it's all text base no form like in google colab. i want to have gui and good ux like google colab is it possible? i have also tried in vs code but it seem similar like jupyter notebook only text no gui form,1,w88gbh,,deeplearning,2022-07-25 20:06:37,4
https://web.stanford.edu/class/cs25/,2,w876oy,"Hi! I am relatively new to deep learning but have a decent statistics/classic ML background and finished Andrew NG's Deep learning Specialization. I am starting to work on a research project in which I will try to use RNN mechanisms with Transformers. Do you have any recommendations for a course that will help me to get more practical usage of RNNs and Sequence Models in general?

Will appreciate any help, Thank you!",deeplearning,2022-07-25 19:08:16,2
"It definitely sounds useful, allowing you to use implementations from all sorts of frameworks.

Why wouldnt it be feasible? It sounds like they already did it?",1,w7ucez,"What do you think about unify AI 
https://lets-unify.ai.


It’s a project to create an abstraction over existing ML libraries 
so they can be used from a single interface 

Do you think it’s feasible 
or useful",deeplearning,2022-07-25 10:08:08,2
"Only if you *need* them

*need*: actually using it for deep learning",14,w773vp,"Apologies for asking again but prices have plummeted even further since the last post I found (2 months ago).

Prices have almost reached $1000 on the better AIBs. The Zotacs have gone under the figure recently. 4090s will be another 4 months and are likely to be priced at $1499 which is about 1.5x the price you can get a 3090 for these days. Performance gains might just involve tossing more power at it. VRAM size looks to be the same.

Thoughts?",deeplearning,2022-07-24 14:57:01,13
"Usually you don't need to save augmented data and add it to your training data on disk. You can just use data augmentation on the fly, where data are augmented automatically before being fed to the model. If however you need to save them for some reason, then you'll have to write your own script to save the output of the augmentation functions. Alternatively, some data augmentation libraries like those in keras offer an argument to save output to disk.",4,w6rfn3,"I am trying to increase my training size using data augmentation. However, the augmentation functions are only changing my input images (but I want to add augmented images to training set). How do I do that?
I am using tensorflow datasets. So, I am not downloading the images, rather using prefetch dataset.",deeplearning,2022-07-24 02:10:50,7
"Ah yes, an age old problem in the field of DL",3,w78zjk,,deeplearning,2022-07-24 16:20:18,2
is there any tutorial or documentation to how we can integrate custom model in to this?,1,w60hy9,,deeplearning,2022-07-23 03:06:42,4
"There is no ""best image-to-image"" architecture, depending on your task, dataset, or metric one approach would work better than another.

[www.paperswithcode.com](https://www.paperswithcode.com) is a great place to check the performance of different models on different benchmarks, also worth taking into account the availability of pretrained models, model size, implementation license, and how easy is it to understand and modify the code etc.",1,w6414y,"I was wondering what is the best image-to-image architecture out there in terms of performance (low MSE, for example)? 

Obviously, I'm talking about applications such as image deblurring/denoising, semantic segmentation, image resolution, etc.",deeplearning,2022-07-23 06:27:33,2
would you share the prompts?,5,w5jk8v,,deeplearning,2022-07-22 12:48:05,7
Looking to learn Machine Learning and searching for the [**best Machine Learning Books for Beginners**](https://codingvidya.com/best-machine-learning-books/) then I found the best Books on ML here?,1,w5td4l,,deeplearning,2022-07-22 20:16:13,2
"You should check out kaggle, they have some solid intro courses. Personally, I use them to brush up from time to time. Be sure to make use of their datasets/competitions as well. 

The tricky thing is, the course will give you the basics, and there’s no “one way” to approach or solve a problem. So use the datasets to explore, develop, and grow your own repertoire of DS solutions.",2,w5huhw,"According to your own experiences, what's the best free online course for being to get into DL ?",deeplearning,2022-07-22 11:35:48,2
I thought he was switching to swift or something? That still looks like python.,2,w4x6m8,,deeplearning,2022-07-21 18:04:25,2
"Dataset size isn't really relevant on its own for your choice of optimizer.

As a rule of thumb ADAM is more robust to bad hyperparameters initialization  and will often achieve convergence fast enough, but SGD can be much faster if you understand what you are doing.",6,w4rim7,,deeplearning,2022-07-21 13:55:56,7
"I made this illustration because I've often seen the bullseye/dartboard graphics used to explain bias and variance, but it was never quite clear *how* those estimates were made with ML models and how it related.

A model is an ML architecture (e.g. a linear regression curve) with a specific set of hyperparameters. To estimate the bias and variance, you split the dataset up into several, independently sampled datasets, retrain the model on each one and measure the error / metric on the same, separate test set for each training subset.

This gives you a distribution of the error / performance metric, which allows you to then quantify the bias and variance, as visualized on the bullseye/dartboard.

This illustration is from my upcoming book [Zefs Guide to Deep Learning](https://zefsguides.com)",1,w3s7zq,,deeplearning,2022-07-20 10:29:30,1
A quick [DDG search](https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet.pdf) shows the A100 SXM interconnect is rated for 600GB/s,1,w4fno9,"Our research group will be buying 8-10 A40 GPUs and we were thinking of buying Nvlinks for interGPU connections. 

However, I have read that A40 has 112.5 GB/s memory bandwidth through NVLink (per slot I presume).

Does this mean that, regardless of whether we use A100s instead of A40s, the run-time of the parallel jobs running on multiple GPUs will always be limited by the memory bandwidth of the interGPU connection, 112.5 GB/s?

&#x200B;

if so, I have seen benchmarks claiming that the run-time scales down linearly with the #GPUs used. How can this linear scaling be true?",deeplearning,2022-07-21 05:38:24,2
"Singular value decomposition and principal component analysis for sure will. I'm not familiar with the terminology for the other areas you mentioned.


Courses or material in optimization would certainly help. Linear programming, quadratic programming, covex optimization, mixed integer programming, interior point methods, gradient descent, stochastic gradient descent, subgradient methods, proximal gradient. Deep learning is all about optimization and familiarizing yourself with any of these will help you.


Additionally regression of various types would be benenficial to familiarize yourself with too. Linear regression, ridge regression, lasso regression, elastic net regression, logistic regression. Regression is just optimization too so some of the techniques listed in the previous paragraph can certainly be used to solve these problems.",3,w44eaq,"Title is the core of what I’m asking. I know basic scalar/vector/matrix/tensor notation and operations. I’m familiar with the concept of linear spaces and determinants. What I want to know is the advanced topics of linear algebra required for a better understanding of deep learning. 

Singular value decomposition?
Canonical forms of matrix operators? 
Bilinear Forms?
Manifolds or isomorphisms? 
Representations and categories of linear algebras? 

I’m vaguely familiar with these concepts but I don’t *know* them. Will studying these help me understand deep learning on a more fundamental level or am I wasting my time? 

I know that there’s a huge probability aspect to it also. I’ve taken multiple probability/statistics classes, but they’re undergrad level so topics on those would also be helpful.

Thanks!!",deeplearning,2022-07-20 19:08:40,1
"Just an idea:

Looking at the dataset images, it looks like the image from MNIST and Fashion MNIST have been superimposed and then normalized.

Assuming you have that datagen() function, you can create a dataset with x as the appended set of images from the function and y as the corresponding number from MNIST. You can then run a model(any model, CNNs usually work good) on the dataset that your professor has provided to identify the number in the appended image. Now since you have the number, if you want the exact MNIST image in the appended image, you can compare the appended image with the number images in the dataset and you get both your images from that.

Alternatively you could also add a parameter during datagen() which is a unique ID for each image in MNIST and Fashion MNIST to y and then use a model and thus get it directly 

(Just starting out, I might be wrong)",1,w3iook,"Blind Source Seperation: so the professor made that datagenerator() that takes a random image from MNIST and a random image from MNIST FASHION, stack them together and we should create a model that takes the 32x32 generated image and output the correct 32x64 image of the seperated images in the input or 2 images 32x32 seperated. We should do it without preprocessing.

How do I make the model? I desperately need ideas, I'm completely confused. Thank you!

&#x200B;

&#x200B;

https://preview.redd.it/x2mnfc4z6pc91.png?width=1907&format=png&auto=webp&s=12283841b11c1335715db9f9dc4097ee32570564",deeplearning,2022-07-20 03:10:40,7
"Well the heat is not much, but prolonged loads will hurt its life expectancy, yes. You'd have to get into 70-80°C temperature ranges, though",2,w3q2jw,"Hey, I have ASUS FX505DD 1050 ti 3 gb laptop, I am using it for small projects, but the fans are always running loud , one day I was messing around and disabled my internal GPU from the device manager then I started training a model and it's so quiet I turned on MSI afterburner  the 1050 is being used and the temps are in 60 range.

If I continued turning off my internal GPU when training models will this hurt my laptop in a way?",deeplearning,2022-07-20 09:03:52,1
"DALL-E2 was trained on the entire internet of images, so it already does comic book art.",2,w3manp,So Dall-e works really well for certain illustrations. Will it be possible to create an equivalent model trained on comic book art? Will this be feasible?,deeplearning,2022-07-20 06:25:26,4
"For any new person trying to get into GANs, avoid them. That one advice will save a lot of your time. 

You can thank me later!",2,w387lv,,deeplearning,2022-07-19 17:30:37,3
"Maybe do k-cross validation 

Also compare parameters, no of operations and time taken?

You could also abalation studies for individual classifieds if they have different modules

Top 10% accuracy is standard so I'm assuming you did that, maybe try comparing accuracies with a limited training set? It'll compare how much the model learn with an increase in dataset size. 

(I'm just starting out so maybe don't have that much experience sooo please feel free to correct me)",2,w3dz6l,"Greetings all,
Could anyone please guide me how to compare different classifiers on the same dataset? I do not think that training on the whole dataset and calculating the accuracy would be a valid enough comparison. For now I have taken different training dataset sizes and compared their accuracies on each of these sizes for a fixed test dataset size. I have also calculated the precision, recall, fscore and confusion matrix for a particular dataset size for all of these classifiers but I do not feel it would be correct to generalise the obtained observations for all training dataset sizes.",deeplearning,2022-07-19 22:16:19,2
"Your dataset consists of points (x,y), where y is soft labels rather than the traditional one hot encoded ones, correct? 

If so, cross entropy is good here, as it can be used to compute how similar one distribution is to another. In PyTorch, they have this built in starting from PyTorch 1.10. See the second bullet point here https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html",3,w2qq9w,"I want to Build a model where I have the target discrete probability distribution for each sample. 

&#x200B;

I was planning on using a  Softmax on final layer with MSE loss function or will Cross entry work just as well?

Any existing papers that have tried to do something similar?

Thank you in advance.",deeplearning,2022-07-19 04:45:39,7
Looking to learn AI and searching for the [**best Artificial Intelligence Books for Beginners**](https://codingvidya.com/best-artificial-intelligence-books-for-beginners/) to Advanced then I found the best AI Books here?,1,w2m892,,deeplearning,2022-07-19 00:00:16,1
"Last time when I came across this issue, I first created a mask <-600 hu. Then you are left with few objects in image lungs and outside air. Skimage has a method called clear boundaries which will remove the outside air. This should take care of your issue.
https://www.kaggle.com/code/ankasor/improved-lung-segmentation-using-watershed/notebook
This notebook explains it better.",1,w2c190,"Hey, 

I was actually on a program extracting data on CT's dicom but on some volume I've got a lot of artefact entouring the body (in lung scan) in air . Is there any easy way with preprocess to remove it with some scikit library or torch ? I research and only find with halo/metal artefact thanks in advance !",deeplearning,2022-07-18 15:22:27,1
"Looks like your transforms are a set, not a list or tuple. Sets are un-ordered, so its not  iterating it in the order you listed. Change your `{}` to `[]` in the `transform.compose`",7,w2f78o,"I'm creating a vgg16 program through pytorch but keep running on the error: TypeError: pic should be Tensor or ndarray. Got <class 'PIL.Image.Image'>.

I've tried everything I can to get it to go away but have no idea. Any ideas? Here's my full code:

`import time`

`# import numpy as np`

`import torch`

`torch.tensor(1)`

`import torch.nn as nn`

`# import torch.nn.functional as F`

`# from torchvision import datasets`

`from torchvision import transforms`

`from` [`torch.utils.data`](https://torch.utils.data) `import DataLoader, Dataset`

`from torchvision import models`

`from PIL import Image`

`import os`

`os.environ[""CUDA_DEVICE_ORDER""]=""PCI_BUS_ID""`

`os.environ[""CUDA_VISIBLE_DEVICES""]=""0""`

`if torch.cuda.is_available():`

`torch.backends.cudnn.deterministic = True`

&#x200B;

`path = './data/'`

&#x200B;

`# Device`

`DEVICE = torch.device('cuda:0')`

&#x200B;

`# Commented this out because it won't work`

`#DEVICE = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")`

`print('Device:', DEVICE)`

&#x200B;

`#DEVICE = torch.cuda.device(0)`

`#print('Device:', DEVICE)`

&#x200B;

`# hyperparameter`

`random_seed = 1`

`learning_rate = 0.001`

`num_epochs = 15`

`batch_size = 10`

&#x200B;

`num_classes = 2  # class number`

`IMG_SIZE = (512, 512)   # resize image`

`# IMG_MEAN = [0.485, 0.456, 0.406]`

`# IMG_STD = [0.229, 0.224, 0.225]`

&#x200B;

`transforms = transforms.Compose({`

`transforms.ToPILImage(),`

`transforms.Resize(IMG_SIZE),`

`transforms.ToTensor()`

`#     transforms.Normalize(IMG_MEAN, IMG_STD)`

`})`

&#x200B;

&#x200B;

`class MyDataset(Dataset):`

`def __init__(self, root, datatxt, transform=None, target_transform=None):`

`super(MyDataset, self).__init__()`

`with open(root + datatxt, 'r') as f:`

`imgs = []`

`for line in f:`

`line = line.rstrip()`

`words = line.split()`

`imgs.append((words[0], int(words[1])))`

`self.root = root`

`self.imgs = imgs`

`self.transform = transform`

`self.target_transform = target_transform`

&#x200B;

`def __getitem__(self, index):`

`f, label = self.imgs[index]`

`img =` [`Image.open`](https://Image.open)`(self.root + f).convert('RGB')`

&#x200B;

`if self.transform is not None:`

`img = self.transform(img)`

`return img, label`

&#x200B;

`def __len__(self):`

`return len(self.imgs)`

&#x200B;

&#x200B;

`train_data = MyDataset(path + 'train_data/', 'train_label.txt', transform=transforms)`

`test_data = MyDataset(path + 'test_data/', 'test_label.txt', transform=transforms)`

&#x200B;

`train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)`

`test_loader = DataLoader(test_data, batch_size=batch_size)`

&#x200B;

&#x200B;

`""""""`

`initial model`

`""""""`

`model = models.vgg16(pretrained=True)`

`for param in model.parameters():`

`param.requires_grad = False`

`# model = models.vgg16(pretrained=False)`

&#x200B;

`model.classifier[6] = nn.Linear(4096,num_classes)`

`model =` [`model.to`](https://model.to)`(DEVICE)`

`criterion = nn.CrossEntropyLoss()`

`optimizer = torch.optim.Adam(model.classifier.parameters(), lr=learning_rate)`

`# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)`

&#x200B;

`""""""`

`train`

`""""""`

`for epoch in range(num_epochs):`

`start = time.perf_counter()`

`model.train()`

`running_loss = 0.0`

`correct_pred = 0`

`for index, data in enumerate(train_loader):`

`image, label = data`

`image =` [`image.to`](https://image.to)`(DEVICE)`

`label =` [`label.to`](https://label.to)`(DEVICE)`

`y_pred = model(image)`

&#x200B;

`_, pred = torch.max(y_pred, 1)`

`correct_pred += (pred == label).sum()`

&#x200B;

`loss = criterion(y_pred, label)`

`optimizer.zero_grad()`

`loss.backward()`

`optimizer.step()`

&#x200B;

`running_loss += float(loss.item())`

`end = time.perf_counter()`

`print('epoch {}/{}\tTrain loss: {:.4f}\tTrain accuracy: {:.2f}%'.`

`format(epoch + 1, num_epochs, running_loss / (index + 1), correct_pred.item() / (batch_size * (index + 1)) * 100))`

`print('Time: {:.2f}s'.format(end - start))`

`print('Finished training!')`

&#x200B;

`""""""`

`test`

`""""""`

`test_loss = 0.0`

`correct_pred = 0`

`for _, data in enumerate(test_loader):`

`image, label = data`

`image =` [`image.to`](https://image.to)`(DEVICE)`

`lable =` [`label.to`](https://label.to)`(DEVICE)`

`y_pred = model(image)`

&#x200B;

`_, pred = torch.max(y_pred, 1)`

`correct_pred += (pred == label).sum()`

&#x200B;

`loss = criterion(y_pred, label)`

`test_loss += float(loss.item())`

`print('Test loss: {:.4f}\tTest accuracy: {:.2f}%'.format(test_loss / 12, correct_pred.item() / 120 * 100))`",deeplearning,2022-07-18 17:47:45,2
"I might be very wrong, but your model seems underfitted you can try training it for more Epochs or adding more data? (Again I might be wrong so this was just something from top of my mind )",5,w1s7gn,"Hi All,

I'm trying to fit the following time-series data ( in blue ) using LSTM networks  but prediction are not coming right ( in green ).

So, at input I'm using normalize value of daily visit with 7 day window and in output same window with 1 day increase. 

I can see the pattern that i have down spikes on weekend. So I'm giving input (weekday, visits) but my accuracy is not coming down.

Can you guys suggest me any idea?

https://preview.redd.it/leued569r9c91.png?width=1235&format=png&auto=webp&s=1820a4bf718b94295c7350b786a1109e4e871614",deeplearning,2022-07-17 23:18:59,15
"Well you are just training your network to recognize that a classification in the class you removed is always an error
And btw, you probably don't need a softamx layer if you are using CE loss",2,w1czer,"I am developing a CNN model to classify images into 5 classes. I was performing some tests and noticed that one of the classes was affecting the results negatively, so I removed that class to see if without it the model would perform better. 

When I removed it the results improved. Previously I was getting around 88/89% validation accuracy and after removing the class I reached 93%. 

The thing is: I forgot to change the final Dense layer units. Initially I was using 5 units, one for each class, and softmax activation. After one of the classes removal I should have changed this to 4, and once I did the results were much worse. 

I have no idea why I am getting better results with a misconfigured model. Any ideas on what should happen if you pass more units to a Dense layer with softmax than the number of classes?",deeplearning,2022-07-17 10:47:50,10
"Its probably because using distances would make it rotation-invariant rather than equivariant. What you want is a transformation that, given rotated inputs, produces equally rotated outputs.
Distances remain the same values under rotation",2,w157qj,"Hi,

I have just finished reading the SE3 transformer paper ([SE(3)-Transformers: 3D Roto-Translation Equivariant Attention Networks](https://arxiv.org/pdf/2006.10503)) by Fuchs et-al and although I'm sure I understand less than 50% of the math involved, I have a lingering question:

Is there a reason one can't use distance matrix representation (such that M\[i,j\]=d(i,j) for all i > j) as a valid rotation translation equivarient representation for point cloud data? Why is there a need for a neural network to encode such representation? (of course, it is not a one - to - one representation, inverted/flipped point clouds can still map to the same distance matrix, but I haven't seen how the SE3 transformer handles this case)",deeplearning,2022-07-17 04:19:58,3
"Have you checked nvidia-smi to see the load on the GPUs? Is the batch size too small? For optimal performance you need to max. out the VRAM in both GPUs. Last, you are only training on one GPU,  (“/GPU:0”); you have to chose a distributed training strategy.",6,w0yszp,"I have gotten a machine with 2 Nvidia GeForce GTX 1080 Ti Gaming OC 11G. I have some machine learning projects that I am working on. I have never used this GPU before. Therefore, I started by checking its computation capabilities. To start, I have prepared and installed everything to start working with the GPUs and do some coding. I have started also by benchmarking the GPU using the code mentioned below.  Surprisingly, Not as expected! The training took more than 90 seconds ( about 9s each epoch and 6ms/step), which is almost equivalent to my GTX 1650 MXQ. To make sure that everything is correctly configured, I have checked any running processes on the GPUs but didn't find any. In addition, tried to change which GPU the two GPUs each time, but the same results. I also checked RAM, CPU, and Hard Drive loads but couldn't find a remarkable load!!

I sent the code to a friend with the same GPU, and he mentioned good enough results than mine (33 seconds for the whole training,  3s each epoch, and 2ms/step), which is really surprising to me, and I don't know where exactly the problem.

Any help, suggestions, or further discussion on the topic will be appreciated.

&#x200B;

My PC is a Dell precision 7920

RAM: 16G

Cpu: Xeon silver 3204.

&#x200B;

The code used for the benchmarking:

    import tensorflow as tf
    from tensorflow import keras
    import matplotlib.pyplot as plt
    import numpy as np
    print(tf.config.experimental.list_physical_devices())
    print(tf.test.is_built_with_cuda())
    (X_train, y_train), (X_test,y_test) = tf.keras.datasets.cifar10.load_data()
    X_train_scaled = X_train / 255
    X_test_scaled = X_test / 255
    y_train_categorical = keras.utils.to_categorical(
        y_train, num_classes=10, dtype='float32')
    y_test_categorical = keras.utils.to_categorical(
        y_test, num_classes=10, dtype='float32')
    def get_model():
        model = keras.Sequential([
                keras.layers.Flatten(input_shape=(32,32,3)),
                keras.layers.Dense(3000, activation='relu'),
                keras.layers.Dense(1000, activation='relu'),
                keras.layers.Dense(10, activation='sigmoid')    
            ])
        model.compile(optimizer='SGD',
                      loss='categorical_crossentropy',
                      metrics=['accuracy'])
        return model
    print(""Num GPUs Available: "", len(tf.config.list_physical_devices('GPU')))
    import time
    st=time.time()
    with tf.device('/GPU:0'):
        gpu_model = get_model()
        gpu_model.fit(X_train_scaled, y_train_categorical, epochs=10)
    print(""----%.2f----""%(time.time()-st))

&#x200B;",deeplearning,2022-07-16 21:20:41,11
BERT-Large (345 million parameters) is now faster than the much smaller DistilBERT (66 million parameters) all while retaining the accuracy of the much larger BERT-Large model! We made this possible with Intel Labs by applying cutting-edge sparsification and quantization research from their [Prune Once For All paper](https://arxiv.org/abs/2111.05754) and utilizing it in the [DeepSparse engine](https://github.com/neuralmagic/deepsparse). It makes BERT-Large 12x smaller while delivering 8x latency speedup on commodity CPUs. We open-sourced the research in [SparseML](https://github.com/neuralmagic/sparseml); [run through the overview here and give it a try](https://neuralmagic.com/blog/bert-large-prune-once-for-distilbert-inference-performance/)!,4,w0fp7z,,deeplearning,2022-07-16 05:35:58,1
"Like, get in the car and drive?  Maybe.  Visit a website?  For sure.",1,w0qysp,"Where normal people get to play around with newish models. Like speaking with a chatbot such as gpt3, making images with Dalle 2, playing games against AI like alpha zero and so on. **Do you know of any such place?** I think this would make normal curious people how only read about AI in media understand the technology better.",deeplearning,2022-07-16 14:31:28,1
Just use semantic segmentation to extract the person and mask in your own background.,7,w0b7io,,deeplearning,2022-07-16 00:45:55,4
"If A is the set of data, can you np.roll(A) and set last element to the prediction, and repeat?

I'm confused though what you mean you can ""predict"" the value in test.",2,w0i1az,,deeplearning,2022-07-16 07:33:35,4
Is this free?,1,vzojmv,"One day to immerse yourself in technology that is a first for companies and engineers around the world!

To help you begin your immersion in AI as effectively as possible, we've prepared experts to assist you all the way.

Not without a competitive component, the winners will receive worthy prizes that will help them successfully use advanced technologies for their projects.

So come join us and learn everything you need to know about RL!

[Register here](https://lablab.ai/event/reinforcement-learning-openai-gym?utm_medium=23&utm_source=Reddit&utm_campaign=RL1&utm_term=Hackathon)

[Reinforcement Learning OpenAI Gym Hackathon](https://preview.redd.it/vm4gf3pviqb91.png?width=1920&format=png&auto=webp&s=bb3ba85c6e4d5b3069c742a3e38e6c3d70d6843a)",deeplearning,2022-07-15 06:18:45,1
[github link]( https://github.com/Shreyz-max/Memes-Generator),1,vzyvvd,,deeplearning,2022-07-15 13:52:57,2
It might be easier to work with your images/predictions and use geometry/math to generate lines from your detections. If you have georeferenced imagery you can generate polygons from an instance segmentation model and convert to vector using a simple geotransform,2,vznpfd,"I am trying to find a model/architecture that will allow me to train a model to draw a line between certain points. The specific scenario I am trying to solve is to delineate crop rows (lines) from a set of input plant locations (points). 

I have done quite a bit of work with CNN/image based models, but am having a hard time finding the right kinds of models for this type of task. I haven't been able to find much in google searches for working with vector type data (points, lines, shapes etc.). After seeing the amazing results possible from image models, it seems to me like the above scenario should be a pretty simple thing to train a DL model to do. 

In my search I have come across Pytorch geometric and Graph Neural Networks however these seem mostly focused on classifiying and linking nodes based on attributes rather than spatial position. Can anyone suggest where I could look for some details on models working with points/lines, or some keywords that could help me find what I'm looking for? Are Graph Neural Networks what I should be delving into? Are there any examples of them being applied to 2D spatial data?",deeplearning,2022-07-15 05:38:33,12
could you pit the link here not in the title please ?,1,vzo4vj,,deeplearning,2022-07-15 05:59:58,3
Why not actually feed them to each neuron in the first layer,2,vzjvx2,"Hi All,  
I'm completely novice in deep learning. I need to develop an application, which gets an image and 2 real numbers as inputs, and based on the partially mirrored part of the image spits out a real number as result. The 2 input numbers are essential to deduce the result.  
A human could do some calculations and visually select the important region of the input image based on the numbers, and then only the cropped image would be the network input, but I would like to rely on the network and have it perform the full job.  
I plan to implement a CNN for the job in Keras. The interesting region in the image can vary in position and size.  
However, I can't find any hint or idea about encoding the input numbers or any way to feed them into the network. I thought about two vivid-colored bar graphs on two sides of the image, each proportional to an input number, but don't know if it is efficient enough. I have a feeling that a more direct feed to the network would be better.  
Thank you in advance.",deeplearning,2022-07-15 01:53:27,11
"[1] Jacob Solawetz, ""Computer Vision Use Cases in Healthcare and Medicine"",  https://blog.roboflow.com/computer-vision-in-healthcare/, 2021.",1,vzj2kk," Probably not the best subreddit to post but I am out of ideas...

How do you cite a website (e.g. [roboflow.com](https://roboflow.com/)) in a research paper?",deeplearning,2022-07-15 00:57:57,2
"Heyy.. I see you are trying to extract gradiant maps (aka saliency maps) for your models. To answer you questions:

1. It is possible to capture the saliency maps for any model, provided minimal code modifications are done.

2. I have used GradCam in past and I believe it does the job very well. It has a good documentation, and provides neat instructions to integrate it into existing code.

Do check it out if it helps. Happy coding!",2,vyx1fc,"Quick question for the research field. Im now getting a grasp of how saliency maps work but I got some questions: 

1. Can I get a saliency map out of every CNN model ? (Particularly interested in Clarifai)

2. Would you recommend a certain saliency map technique? My research goal is to see what prompted a classification output.",deeplearning,2022-07-14 07:20:40,1
"If you want it in one screen,  I would say you have to combine both of them together as one script and make a single python code",5,vz5epd,"I have a yolov5 traffic signs detection program and I use this command to run it

python [detect.py](https://detect.py) \--source 0 --weights [model.pt](https://model.pt) 

and a lane detection program

python [lane.py](https://lane.py) 

What I want to do is 

run the two programs at the same time to detect both traffic signs and lane on a single screen.

Can someone helps please?",deeplearning,2022-07-14 13:23:08,11
"Deep learning is a branch of machine learning where artificial neural networks and algorithms learn from large amounts of data. Inspired by the human brain, deep learning systems can improve their performance with access to more data and after gaining enough experience.

Deep learning has been particularly effective in Virtual Assistants. The virtual assistants of online service providers like Alexa, Siri, or Cortana use deep learning to help understand our speech and the language humans use when they interact with them.

For more information, you can visit the following website:  
https://ml-concepts.com/  
  
Feel free to reach out to me for any help.  
  
\[Full Disclosure: I am a part of the https://ml-concepts.com/ team\]",1,vz14h2,,deeplearning,2022-07-14 10:16:16,1
"By duplicating data, do you mean you double the size of your dataset by duplicating each item? If so, there are a couple of things that could be driving the increase in performance depending on how you specifically duplicated it. 

If you duplicated the data before doing a train/val/test split, then you would have a good portion of the val and test set in the training set. This would cause your performance to look better than it actually is. If you instead duplicated your data after splitting, this would be (almost) the same as simply doubling the number of training epochs. That is because each epoch would be running through the training set twice. You can try increasing the number of epochs without duplicating your data to see if you see the same increase in performance.",5,vyb2y8,"Hello everyone, 

I’m trying to create a model that predicts the gender based on first name. 
When I train the model on non-duplicate data the accuracy is very low 77%. But when I increase the data by duplicating the data I get above 90%. 

I need your advice on: 
1- Is it ok to train the model on duplicated data? 
2- what hyperparameters can be tuned to achieve a good accuracy? 
3- Other algorithms suggestions to build a model that can predict gender.",deeplearning,2022-07-13 12:04:53,9
"System76 is all in on linux but not sure if they sell in the EU yet https://www.omgubuntu.co.uk/2022/06/system76-is-opening-a-european-distribution-center

[https://system76.com/laptops](https://system76.com/laptops)

worth a look",1,vye7ec,"I am currently looking for a laptop with a NVIDIA card, which also runs nicely on Linux. Maximum budget is 2000 Euro, as I am still as student. I really want a NVIDIA card, since it is super useful for prototyping and inference. Any hints are much appreciated.   


So far I was thinking of some versions of a ThinkPad or any other Lenovo. However, the 'nice' GPUs are all out of budget for me, so I would end up with a T1000 and I am not entirely sure if that makes a lot of sense.   


Thank you!",deeplearning,2022-07-13 14:18:57,8
It is common to use kalman filter to track objects over time. Check out SORT algorithm for a simple fast but naive solution. There are also learning based trackers (e.g. deepSORT) which are slower but more robust against occlusion and better at matching objects between two consecutive frames in general.,2,vybe90,"Hello, 

CNN detectors (Efficientdet, YOLO, etc) will provide single-frame detections of objects, but the accuracy could be improved by looking at multiple frames over time.  For example, you could feed their detection outputs into a filter that looks at multiple frames over time to see if (1) there was a likely detection, (2) which detection class was most likely, and (3) its movement over time, which can also provide a more accurate count of ""individuals"".  

What I'm describing might be a naive approach to something that's already done... I want to see what's out there already (Keras, OpenCV?) that I can use more or less out of the box.  If I have to adapt some code that's fine too-- I'm just coming up blank for what to look for and could use some suggestions on where to look.  

&#x200B;

thanks!",deeplearning,2022-07-13 12:18:23,4
"&#x200B;

For prediction I am using this:

`newdata = np.asarray(to_append.split())`

`new=newdata.reshape((1,30))`

`newArray = new.astype(float)`

`model_xgb_2 = XGBClassifier(random_state=1289564,use_label_encoder=False)`

`model_xgb_2.load_model(""XGBmodel.txt"")` 

`model_xgb_2.fit(newArray)`",1,vyecxb,"I have a csv file on which I have used StandardScaler, PCA and fit\_transform to train the data. So when I am trying to predict it for a single value do I use the standardscaler and do PCA? I am confused on this one, could someone throw some insight, please?",deeplearning,2022-07-13 14:25:37,1
Do you need more than 1 future prediction? Are you doing seq2seq or simply seq2value?,1,vyd6fq,"Hello Everyone,

I would like to ask a question and seek your wisdom here. 

For my project, I need to write a multivariate LSTM network, which given the sequence with sensor data (like in the figure below) predicts our current location. I am planning to convert it to a supervised learning problem by taking ""Location"" values as ""y"" and all the sensor data as ""x"" with a moving window of size 10. 

But the problem I have encountered is that normally this method is used to forecast the next value/s in the sequence, whereas I would like to use this sequence to classify the location. I believe this problem can be solved if I use normal feed-forward networks, however, I need an LSTM solution for this.

Do you have any suggestions, maybe a different method to feed the data to the model or any other which comes to your mind, my fellow researchers?

Thanks in advance.

[Raw Data](https://preview.redd.it/7q1b1u8p8eb91.png?width=1219&format=png&auto=webp&s=98ad3f733ec924d1e0c66123b7c58505fa6d01b8)",deeplearning,2022-07-13 13:34:46,1
"For your particular usage, just do a UNET or any variation (i say your particular usage because beating a deadline is your top priority)

But in normal scenarios you'd try a couple of model families and document the results then report the one that brings high consistent results.

If it gets too overwhelming, just pay someone and beat the deadline, then sit down with a calm mind after and learn",5,vxsmu1,"Hi.

I am new to deep learning and pretty bad at coding.
My project requires me to segment lung x-rays to predict lung diseases. I was thinking of using Mask-RCNN but so far the codes i have tried didn't work at all and i keep running into one problem after another. Then I saw a comment here that indicated that mask-RCNN may be too sophisticated for tasks like that. So before I go down that spiral of making bad decisions again, I have some questions. 

1. Please share your advice regarding which architecture should i use for this particular task?

2. Also if you know of any resource that may explain its working and code as simply as possible (consider me a child learning a complex concept if it helps), do share it please.

3. Due to resource issues, I can currently work in google colab. I have tried jupyter notebook but due to my own lake of skills and resources, it didn't work out for me either. 

4. Any other guidance or advice that you think may help me will be much appreciated.

Sincerely,
An extremely stressed out student who can't sleep due to the anxiety of not being able to do this and the deadlines just keep coming closer.",deeplearning,2022-07-12 19:48:35,17
"Generally, backbones trained on ImageNet will do the trick, so just import torchvision.models and get the one you want 😅",2,vy0kno,,deeplearning,2022-07-13 04:05:52,2
"Yes, it should ideally. Would be better than the old hardware but it makes more sense getting a newer card since the GPU prices are going down.",3,vxcg4u,"Quick question. I am doing a deep learning project with a custom trained yolov5s/yolov5m model, and i need some decent inference time for calculations. I already get about 1.5 fps on a Celeron J4125 at full throttle. Would an upgrade to a laptop with a GTX 1650 Mobile be good enough for decent inference? I am more than happy with 5-10 fps, but i know 10x performance increase is asking a lot, even with CUDA and cuDNN. 

P.S. I'm using torch.hub for inference, thanks",deeplearning,2022-07-12 07:23:40,8
"Quick read on this project: https://www.qblocks.cloud/byte/text2live-text-driven-layered-image-video-editing/

Developed by Omer Bar-Tal and Dolev Ofri-Amar and Rafail Fridman and Yoni Katen and Tali Dekel",2,vwk673,,deeplearning,2022-07-11 07:47:35,3
"I'm from mobile but those are 2 properties of the logarithm:

Log(a*b) = log(a)+log(b);

Ln(exp(a)) = a; (log of base e)",1,vxf44t,"The following is a derivation of the cross entropy loss from logistic regression:

https://preview.redd.it/zbof767kx5b91.png?width=1618&format=png&auto=webp&s=d1d426fa90ad6d7affe69ec02f746fa8a582f265

I don't understand how to go from the 1st RHS to the 2nd RHS. Can someone please explain this to me?

Any help will be much appreciated, thanks in advance!",deeplearning,2022-07-12 09:20:02,2
"As far as I know time.time measures wall time, so yeah it does give you approx what time takea inference. If you are interested in optimizing it you could try onnx runtime, make sure gpu util is high and you are not bounded with io",1,vxelvx,"Currently trying to get a proper inference time on a obj detector. I run the same image about 100 times to get a good average. I am running these inferences on a GPU, however, I am using time.time() to get my inference times. I understand that CUDA performs operations asynchronously, so I used torch.cuda.synchronize() to wait for all processes to finish, however I am getting a much slowing inference time. Like two times. Does that mean that is the true inference time of my net?",deeplearning,2022-07-12 08:58:38,2
When your model does really well on the training data but poorly on validation or test data.  Usually you care more about how your data performs on stuff it’s never seen before so you want to find the balance that gives you the best performance on test data.,5,vxe4up,"I am confused of this term. i searched about it in google, all i see is how to prevent it. what exactly is it?",deeplearning,2022-07-12 08:37:55,3
Are you looking to learn computer vision and searching for the [**best Computer Vision Courses**](https://codingvidya.com/best-computer-vision-courses/) then I found the best Courses on Computer Vision here?,1,vx4nfh,,deeplearning,2022-07-11 23:43:54,1
"Did you read any tutorials about training neural nets?

&#x200B;

The usual approach is the following:

* split the data into batches. For example, if you have 1600 images, you could have 100 batches of 16 images each;
* within one epoch you iterate over batches until your model ""saw"" each image once;
* then you repeat the process in the next epoch;
* then you train for N epochs until the training stops;",3,vx0sp3,"I am planning to train the Neural Network for a simple model. As such, I have few data. I already have splitted the dataset for training and testing. My question is how do you pass the dataset in order to train the Neural Network efficiently. In other words, I am confused on whether to pass the whole dataset, and then reiterate. Or, to at first pass one data, reiterate for some time, then move onto next data, reiterate and so on. I already have implemented the latter method, but I realized that training got stuck for another data later. I think I am also having similar issues with the former method. Also,if there is more efficient way other than what I have mentioned, please suggest me?",deeplearning,2022-07-11 20:03:10,2
I didn't know this was possible. Great find.,1,vwq1yd,"A re-implementation of the famous 2020 paper - ""Extracting Training Data from Large Language Models"" by Nicholas Carlini, Florian Tramer et al. 

Code -  [https://github.com/shreyansh26/Extracting-Training-Data-from-Large-Langauge-Models](https://github.com/shreyansh26/Extracting-Training-Data-from-Large-Langauge-Models)

The official implementation is great and I definitely learned a few things from it. In the re-implementation, I have also included the temperature-decay sampling and sliding-window-based minimum perplexity metric which was not present in the official implementation. 

I checked the extracted Samples (refer to the Github repo)  and they surely contained some memorized information.",deeplearning,2022-07-11 11:58:42,1
"Computer Vision, with applications in manufacturing (e.g., error detection, sorting, etc.)",4,vwhaw2,,deeplearning,2022-07-11 05:31:39,2
"Owning a GPU is great if you are running workloads 24/7, it will easily pay for itself, or if you play video games. I have a 3090 for deep learning (read: playing Overwatch).",27,vvwgdv,"&#x200B;

I am going to purchase a new PC and i need to know if it is worth spending that extra money.

I'll use it for training of CNN, RNN and LSTM.",deeplearning,2022-07-10 10:29:31,28
"I think you potentially can consider following as part of architecture:

\- how parameters initially initialized

\- parameters can be frozen/non trainable, their values probably be part of architecture",2,vvzwei,"I have believed until now that the architecture refers only to the types of layers and layers sizes, the structure/graph itself.
Parameters (weights) are part of a model that is an instance of the architecture, is that correct? Obviously the architecture defines the number of parameters but does not imply any values for them. I am having doubts now, please enlighten me:D",deeplearning,2022-07-10 13:09:34,4
"You don't even always need a lot of the above for some papers. For some, knowing summations is enough; for others, you need to know matrix notation and basic linear algebra operations; and for a smaller set, you need to know some basic calculus.

Instead of learning all of stats, linear algebra and calculus beforehand, try just reading some papers in the SOTA section of paperswithcode. Note what you understand and what you don't understand. Look up what you don't understand--make a list of mathematical topics and look them up. It's easier to learn in context this way. Spending a year on math alone can be a serious hindrance.

The biggest thing you need to understand a paper is to know deep learning itself, and the papers that came before it. So read a lot of papers and become familiar with the concepts and how they map to code. Some basic resources here: *Grokking Deep Learning* and the classic deep learning book by Ian Goodfellow. 

As for creating new DL algorithms: Learn how to do research. Usually, this means a PhD, but not always. Again, read papers and try to grasp the insights that led them to new architectures.",28,vvrcms,"Hey guys besides linear algebra, calculus, probability theory and few other must know math topics for ml/dl ,what are the other math topics that I must know if I want to understand most ml/dl/rl research papers or want to make an algorithm of my own.",deeplearning,2022-07-10 06:19:45,2
"Nice blog, I’ll definitely be referring to that for some practice attempts. I wonder if you came across anything on short text during your research?",1,vvail0,"Transformers are awesome for so many things in 2022, but one thing I've found them to struggle with is generating embeddings for long documents.

I put together a blog post going through some interesting techniques. Let me know if it helped you!

[Blog post](https://www.notia.ai/articles/clustering-long-documents)",deeplearning,2022-07-09 13:34:07,2
"I would think it would depend on whether you've achieved a global or local maximum. If the model has found a global maximum, the weights should be quite similar between runs. The actual weight values slightly differ, but the weight distribution should be the same. If the model is converging, but with different weight sets, I'd think that would be indicative of a local maximum.",0,vvmrnh,"I have seen in many papers, specially in Deep learning applications in medical imaging, that they interpret attention weights as something like interaction between features (ie. Feature Interaction). But, every time you train the model wouldn't you get new weights? Then, how does this interoperability holds any value if the weights keep changing everytime you run it?",deeplearning,2022-07-10 01:08:05,1
"The amount of samples in a batch you are using is too large, you should lower it down. Also maybe check if that gpu is currently being used by other people/tasks.",7,vvmqiv,I included all the code if that helps,deeplearning,2022-07-10 01:05:49,19
Thanks for sharing. Will the entire lecture be available at some point?,1,vuw4ed," 

**CS 25: Transformers United**

https://preview.redd.it/n7ohmir2sha91.png?width=350&format=png&auto=webp&s=912733504c5dcef94e3ee3eae66bec39f1b007d9

Did you grow up wanting to play with robots that could turn into cars? While we can't offer those kinds of transformers, we do have a course on the class of deep learning models that have taken the world by storm.

Announcing the public release of our lectures from the first-ever course on **Transformers: CS25 Transformers United** ([http://cs25.stanford.edu](http://cs25.stanford.edu)) held at [Stanford University](https://www.linkedin.com/school/stanford-university/).  


Our intro video is out and available to watch here 👉: [***YouTube Link***](https://www.youtube.com/playlist?list=PLoROMvodv4rNiJRchCzutFw5ItR_Z27CM&fbclid=IwAR2mJd868IzGp8ChykBBRTxq7RQh-KICfnAg8rLQ-qsekbhnUcd_z4-4E7g)

Bookmark and spread the word 🤗!

 [(Twitter Thread)](https://twitter.com/DivGarg9/status/1545541542235975682?s=20&t=_Ed9dpjD9Qpx4svpMNDIKQ&fbclid=IwAR2tnSQROnkOQl15aa6nkfNFaJdrnZQHDbidooDaQRJALlWsYMiQU_37dn4)

Speaker talks out starting Monday ...",deeplearning,2022-07-09 00:11:39,2
"Ok firstly I’d like to say I don’t know much about ai and haven’t studied it in any detail. 

is this actually ai in this video or is it just programmed to do this and musk just acts along with it.?

 Also I personally do not believe in any religion but one thing I do believe is that we’re here for a reason and if ai was the smartest thing created it would know that humans have a reason to be here just sayin",0,vvf5fr,,deeplearning,2022-07-09 17:26:05,1
"I found a similar paper don't know how different they are, I just read the title and tried searching similar problems. I always start with codes from paperswithcode.com and then change accordingly for my problem statement. Here is a link for similar project may be this helps https://paperswithcode.com/paper/regularizing-face-verification-nets-for-pain",4,vunb8x,"Does anyone know of any video tutorials or clear directions for building a deep neural network to detect pain, similar to those who did in this paper?

[https://www.sciencedirect.com/science/article/pii/S0933365720312197?casa\_token=BlNYy0tYr1IAAAAA:MorIeX6In0-SYD\_n37svx2YBdhXoHg1Qk3h1f7MizXYyIgMZdV0gr3cHGAPYZxE70G28mNA8aw](https://www.sciencedirect.com/science/article/pii/S0933365720312197?casa_token=BlNYy0tYr1IAAAAA:MorIeX6In0-SYD_n37svx2YBdhXoHg1Qk3h1f7MizXYyIgMZdV0gr3cHGAPYZxE70G28mNA8aw)

I think a deep learning model like this has real practical use. Does anyone know of any good resources or methods that could help me build something similar to this or help me get starting in **building a deep learning model for making predictions based off of images?** I have access to a database of pain images so I am looking for help in getting started with model building!",deeplearning,2022-07-08 15:59:52,2
"How well does it work on downstream tasks? I wonder how pretraining can work for timeseries. 

After all, pretrained nlp models learn basic natural language understanding and generation skills. Pretrained cv models learn feature extractors that are useful for photos of the world. What is the common denominator in timeseries? Timeseries can be arbitrary and behave totally differently between domains.",2,vuz19m," Hi, I've just published my latest medium article in [**Towards AI**](https://www.linkedin.com/feed/#).

""How to Design a Pre-training Model (TSFormer) For Time Series?""

There have been numerous attempts in NLP (Natural Language Processing) tasks recently, and most of them take advantage of using pre-trained models. NLP tasks’ feed is mostly the data created by human beings, full of fertile and excellent information that almost can be considered a data unit. In time-series forecasting, we can feel a lack of such pre-trained models. Why can’t we use this advantage in time series as we do in NLP?! This article is a detailed illustration of proposing such a model. This model is developed by considering two viewpoints and has 4 sections from input to output. Also, the Python code is added for a better understanding.

[https://pub.towardsai.net/how-to-design-a-pre-training-model-tsformer-for-time-series-c2a177ebb51d](https://pub.towardsai.net/how-to-design-a-pre-training-model-tsformer-for-time-series-c2a177ebb51d)",deeplearning,2022-07-09 03:45:47,3
1.2k stars in 2 days...,4,vu4t7k,"[https://github.com/WongKinYiu/yolov7](https://github.com/WongKinYiu/yolov7)

[https://arxiv.org/abs/2207.02696](https://arxiv.org/abs/2207.02696)",deeplearning,2022-07-08 00:15:40,3
"I know that a couple of other DNN models have also become popular for instance segmentation / classification of 2D (esp. medical) images.  That said, I like Mask R-CNN (or Detector2) for 2D segmentation especially, and think you really can't go wrong with it.  (My work cares more about segmentation than classification.)  Pretraining first with your base classes followed by subclasses seems likely to work in a predictable fashion w/ few surprises.",2,vub9ue, After having obtained segmentation masks and cropped the image parts of  interest I wish to classify the obtained crops into certain categories  (classifying them right away is not an option). For such a task is  MaskRCNN the best option/practice?,deeplearning,2022-07-08 06:51:29,1
You get a P100 on Google colab. Works fine for smaller projects and uni work,3,vuawt2,Debating between a NVIDIA P100 vs 3080. P100 does not have tensor cores. What do you think?,deeplearning,2022-07-08 06:33:46,4
ashutoshkumar56007@gmail.com email me maybe we can work on that together.,1,vu9pft,"Hello guys, i have some question about how can i create a ML  code that parses the data send by MQTT topic. I wrote a script in java that receives information from an iot device and forwards it to another platform,but now I need to create something that parses and creates a default behavior for that data. and if the values ​​are greater than this standard, 
I started learning ML this week so I don't have much knowledge about it yet, but I need to develop it for work",deeplearning,2022-07-08 05:31:25,1
[removed],0,vtx80s,,deeplearning,2022-07-07 17:18:53,2
Awesome. I've been waiting for this. Adding it to my watch list for thr weekend,1,vte1ur,,deeplearning,2022-07-07 01:55:22,5
I heard diagnosis is a good,1,vtj1cy,What medical issues do machine learning can solve...how I choose the medical problem that I work on it...what problems can I search by myself that ....please donnot say for diagnosis...make it clear and specific about the problem?,deeplearning,2022-07-07 06:43:49,1
"https://pytorch.org/hub/  
https://www.tensorflow.org/hub  
https://huggingface.co/models  
https://rwightman.github.io/pytorch-image-models/",5,vtikdl,,deeplearning,2022-07-07 06:19:58,5
"With over 7000 languages on the planet, leaving none behind with only 200 of them seems a little hyperbolic...",14,vsq411,,deeplearning,2022-07-06 06:31:20,6
"The general problem you're talking about seems to be ""clustering"", just to put a name to it.

Very broadly speaking, there are a couple of approaches to clustering.  Distance-based clustering puts a metric on the space of features and seeks to find groups of observations that are very close together while simultaneously being far apart from other observations.  Model-based clustering assumes a data generation process - like a mixture of Gaussian distributions - and then tries to optimize the parameters in the model.

Using deep learning for clustering seeks to optimize a measurement on produced clusters.  Google ""deep learning for clustering"" and you'll find a lot out there.  In the past I've worked primarily with mixture models, because I've been working in areas with smaller data sets (ecology, animal behavior).  One tremendously nice feature of mixture models is that we can often identify which features impact the resulting clusters the most.  The downside is that mixture models are models, and for anything even reasonably complicated require a bit of background in probability.",2,vsy33w,"Hi there! I'm kind of looking for some general guidance on how to go about creating a chart that shows similar things, somewhat like this Google experiment with bird sounds: [https://experiments.withgoogle.com/ai/bird-sounds/view/](https://experiments.withgoogle.com/ai/bird-sounds/view/) or this font map: [http://fontmap.ideo.com/](http://fontmap.ideo.com/)

I'd like to be able to create/take a dataset that has a variety of attributes for each item, for example:

* Size
* Color
* Purpose
* Price

and train the network to group them based on similarity and some natural patterns would emerge such as Hammers are near each other because they have a similar price, size, and purpose. Or maybe some fruits could be ranked based on their sweet/sour levels.

Please let me know how I could explain this better. Is this something that deep learning would even be good for? I'm thinking that I would train it for some datasets that have even more attributes than that but hopefully that example is illustrative of my goal.

I've recently been experimenting with Pytorch and YOLOv5 but this is kind of outside the realm of where I even know what to start with.

Any suggestions are welcome! Thanks in advance",deeplearning,2022-07-06 12:21:12,2
I think one cannot perform KD for object detection simply like this. The output of last layer is not just the class probability. It also consists of localization infos.,2,vsmx5c, I am aware of some variants of knowledge distillation (2015 paper by Hinton et al.) being used on object detection. Let's say some papers have used the intermediate feature maps of teacher model for training the student. I haven't found out any that uses the original concept of using the logits of last layer(softmax output) to guide any student object detector.,deeplearning,2022-07-06 03:37:13,2
"You can see this video for reference. 

https://youtu.be/qWGgK4IrH-s",1,vsmkrq,Has anyone tried using an external NVIDIA GPU for machine learning on a MacBook Pro?,deeplearning,2022-07-06 03:15:01,2
LayoutLM is good for this. [https://towardsdatascience.com/fine-tuning-layoutlm-v2-for-invoice-recognition-91bf2546b19e](https://towardsdatascience.com/fine-tuning-layoutlm-v2-for-invoice-recognition-91bf2546b19e),2,vrv49y,I doing a project on document analysis try to extract text information from document image. I am using  a crnn (cnn+rnn+ctc) to do this.However I would like to know other deep Learning Algorithms that i can apply and compare my with.,deeplearning,2022-07-05 03:54:13,3
"Try asking this in r/MachineLearning , this sub doesn't actually do any real deep learning, probably full of kids who pretend to do it so their parents will buy them an nvidia gpu.",5,vrvqwn,"If you were working on your Ph.D. today, what aspect of neural networks/deep learning would you choose as your research topic and why?",deeplearning,2022-07-05 04:34:12,5
You should repost the link as a comment.,4,vrqzyj,,deeplearning,2022-07-04 23:00:44,3
Just a guess (since I’m not a TF user) that it’s l2.,2,vrnlmt,"I'm doing the Deep Learning Specialization from Dr Andrew Ng, and on a video, he mentions L2 regularization on an NN as: 

(lamba/2m) \* (sum(w\[l\])\^(2))

Then you add this to the cost function.

While  [Tensorflow](https://www.tensorflow.org/api_docs/python/tf/keras/regularizers/L2) defines it as:

\`loss = l2 \* reduce\_sum(square(x))\`

I don't know if TF applies that lambda factor after when calculating the cost function, or if the factor is the l2 parameter itself, then you would manually compute (l2/2m) before passing it as a parameter.

Thanks!",deeplearning,2022-07-04 19:35:38,8
"There are a lot of options in terms of medical data; it comes in many forms. What interests you? Tabular data of vitals, family history, etc? Image data in the form of x-rays, retina scans, etc? Volumetric Data from CT scans (brain scans)? These are just a few types of data in the medical field to choose from. I would recommend picking one that you find engaging and starting a small project. Websites like Kaggle have a lot of interesting datasets to comb through but there is more out there.

It’s worth noting that most medical data has to be anonymized for consumer privacy.

What experience do you have with Deep Learning? Maybe I can help point you in the right direction.",1,vruw7l,"Hey, guys. I'm new to the field, and trying to start analyzing data, as fast as possible.
What are the resources that I can use, to pull medical data, for Data Science analysis and Deep learning?",deeplearning,2022-07-05 03:38:36,3
"Ask yourself why you need a neural network for this?

You should just use old school procedural computer vision, do a tutorial on OpenCV or whatever and learn about thresholding, chroma key / luma keying, feathering masks, and be done with it.",2,vr20wj,"First of all I would ask if somebody retrained [Robust Video Matting](https://github.com/PeterL1n/RobustVideoMatting)  model on own data? I am trying to, but with all the models I end up getting bad quality result as the ones attached to the post.   
So my data is some objects rotating on 360 and with white backgrounds, The task seems to be pretty simple as the model just has to remove white bgr and keep colorized object. I have masks on every 10th frame of my videos. The masks are 0 - bgr, 255 - fgr. I have tried Robust Video Matting model, MODNet, PaddleSeg and several segmentation models and every of them failed to show consistent results on that data. What should I do in the case?

https://preview.redd.it/0zr3pthe9i991.png?width=2000&format=png&auto=webp&s=21890ad48bfc14914029bb98795c7225a58ff2f3",deeplearning,2022-07-04 00:40:27,2
any help?:-),1,vre17c,"I am foucsing on image captioning these days, I found that, in most of recent works,  they just use cached  features  produced by other object detection models.

Why they don't optimize the detector end-to-end since there are some end-to-end detectors such as detr?",deeplearning,2022-07-04 11:33:05,1
"Just follow a pytorch tutorial using libtorch, which follows the python API pretty closely",4,vr7esd,,deeplearning,2022-07-04 06:26:15,2
"You can use already existing datasets like flikr8k/ flikr30k or if you absolutely want to destroy your computer, go for mscoco or conceptual captions.

I would recommend reading ""show, attend and tell"" paper, if you are new.

If i remember correctly, there is a blog post on tensorflow's website about how to build your own caption generator.

[Here](https://www.tensorflow.org/tutorials/text/image_captioning)",3,vqz4we,"Hey there,

I'm pretty new to Deep Learning, recently trying to build a model to predict caption for a given image using CNN and RNN. But I don't know how to code these things and put them together for my own dataset.

So is there anyone who can help me to build a model which can generate Image Captions?  
I would like to use the TensorFlow library.  
Thank You. :)",deeplearning,2022-07-03 21:29:17,2
"It really depends on what you're doing. For computer vision, for example, any CNN or vision transformer can be used as a backbone. Just need to skip the classification (and maybe pooling) layer. Using the timm library makes that pretty easy if you're using pytorch.",5,vpzlxp,What networks can be used as a backbone for feature extraction? What are examples of ones you can and can’t use and why? I am trying to create a HydraNet but need to pick a backbone. I just don’t know what the criteria is.,deeplearning,2022-07-02 12:48:52,2
Vast.ai is exactly that. It's like a p2p cloud gpu platform and pretty easy to use. Way cheaper than normal cloud providers.,26,vpledc,"Is there like an Airbnb for GPUs?

Want to run something that is too computational heavy for my Mac but don't need all that large cloud GPU providers offer.",deeplearning,2022-07-01 23:30:11,21
"Project-oriented is best. I did the FastAI course (project based, not math based), Full Stack Deep Learning, and then focused on my own projects (including paper implementations). I learned far more that way than from any of the university ML courses I had encountered before.",2,vpd54a,"Hey!
I have basic knowledge of Python and some knowledge of Pandas. 
My plan is to follow along a cats and dog classification with pytorch tutorial. After that I'll do another classification project with different dataset.
Is it a good idea to go forward? I tried learning math, theory and everything, but after a week, I lose all hopes and give up. So I thought this project oriented approach might work.  What do you think?
Lastly, what will be the tutorial you’ll suggest for cats&dogs classification with pytorch. 
N.B. Pytorch is a must.",deeplearning,2022-07-01 15:43:42,4
"so what, its still software on a disk , ants life is more valuable than some data on somme disk that can be cloned bazillion times by ayone and their granny if one drive dies

these pseudo philosophical theories about sentience are cringe AF dood",2,vppcqm,what i would do is convert the videos into a sentences. then see how similar those sentences is to each group of sentences in the database that contain activities.then have the ai respond with it's answer.,deeplearning,2022-07-02 04:03:40,1
"U-Net is the most popular architecture used for semantic segmentation tasks (afaik)

U-Nets encode the input image data via several convolutional blocks and decode it with the help of transposed convolutions and skip connections. It's essentially an Auto Encoder with skip connections between encoding and decoding blocks at the same level.

There's a lot going on in this illustration!

- The name U-net comes from the U shape when drawn like this
- The encoding arm uses conv blocks with max pooling
- The decoding arm also has conv blocks, but uses data from the skip connections from the encoding arm
- The data from the skip connections is concatenated with the incoming data to the decoding blocks
- This skip data allows the network to more effectively recreate the original image data
- The output of the decoding blocks are upscaled with transposed convolutions

All of this makes for very powerful semantic segmentation and can be used as the basis for other tasks, e.g. colorizing, super-resolution, in-painting, image generation, etc

This is an illustration I made for my book, Zefs Guide to Deep Learning (zefsguides.com)",3,vokp2h,,deeplearning,2022-06-30 15:24:40,4
"No code... yawn, when  i see no code it pretty much means that  its juts a publicity flex so others would release their flex vids as well ( dalle 2 vs imagen case ) and impressive sum of money exchanged hands to make this unavailable for lOooSers who dont pay.",8,vo6sqf,,deeplearning,2022-06-30 05:06:46,12
"This is why I consider leaving academia. It’s a joke, really. Even the peer review process is not as rigorous or, I dare say, ethical as I thought it would be. It is really disappointing.",20,vo2aoq,"   I work as a project assistant in a research lab at a university. In last 2 month I have observed one thing that everyone just wants to publish a paper as fast and as much as they can. In the process, I feel we always show the numbers which make our research look good and hide the ones that are not.  This whole thing just feels weird to me. In the end instead of tweaking the idea and making changes in your hypothesis, we always end up making changes in our experiment that make our hypothesis look valid. My question is , does this happen with others too  ?  PS: I work in deep learning research.",deeplearning,2022-06-30 00:18:13,11
"no autocorrelations analysis, stationary statistics, data transformation (besides minmaxscaler), were there outliers?",3,voj7kb,,deeplearning,2022-06-30 14:19:22,3
"So you are promoting your website in a bunch of subs now, when the content can easily be found through google or there are official tutorials on the Pytorch website",3,von67x,"[Automatic Mixed Precision Training for Deep Learning using PyTorch](https://debuggercafe.com/automatic-mixed-precision-training-for-deep-learning-using-pytorch/)

[https://debuggercafe.com/automatic-mixed-precision-training-for-deep-learning-using-pytorch/](https://debuggercafe.com/automatic-mixed-precision-training-for-deep-learning-using-pytorch/)

&#x200B;

https://preview.redd.it/mv1qfbknou891.png?width=1200&format=png&auto=webp&s=a1f9b93a964060e24ce14c624909bd5ce25059bd",deeplearning,2022-06-30 17:22:11,1
"For some specific use cases there is research that suggests, that training from scratch improves the performance. I remember two papers in relation to Remote Sensing, but I don't have the links at hand. This is especially relevant if the number of input channels changes",2,vo2miq,"Hello, I am currently working on a network which should be detected defects such as scratches. So far I have always used pre-trained networks to classify different defect types. Now I wondered if it might be more useful for my case to initialize the network (I use GoogLeNet) with random weights instead of the normal transfer-learning approach with pre-trained weights.

Or can one say in good conscience that pre-trained networks actually perform better for all classification tasks than networks with random initialization?

Would be happy about some opinions or maybe some literature :) thanks!",deeplearning,2022-06-30 00:39:53,4
"As I understand it, the encoder half of the network basically compresses the representation of the input images. By the time you reach the center of the network, you have a much smaller matrix than you started with.

Now you ask, given this small matrix (set of numbers) what function can we apply that minimizes the difference with some larger label matrix?

If you attempted to do this without convolution, with a fully connected network with the same width at every layer, you'd end up eating the costs of not compressing the data by learning convolutional filters, which reduce the image down.

The real answer is - data compression",9,vnco7p,"I am trying to understand what is the meaning of convolutional layers means by autoencoders. In the encoder part, we are using it to extract features, yes, but in definition, the decoder part is used for the reconstruction of the image. Why are we using convolution in the decoder part then?

(Same question for U-NET too.)

https://preview.redd.it/jg74p87zfj891.png?width=829&format=png&auto=webp&s=a2d1faa521f5b6bbfa580833561037dae083aa2f",deeplearning,2022-06-29 03:33:55,60
If it works on your test set then it is a good practice,1,vnnh5d,"I hope everyone is enjoying their day. I am working on a project for my final year of university that uses deep learning and transfer learning, specifically VGG16.

And I personally gather data from the retinas of eyes; sadly, my dataset is just about 350 photos of retinal images divided into 5 stages.

Stage 0 : 93 images

Stage 1 : 37

Stage 2 : 57

Stage 3 : 77

Stage 4 :122

I must apply deep learning despite the fact that my dataset is too small to do so.

I consider data augmentation and employ customary data augmentation techniques such as ( rotation flip gray scale...)

Right now, everything is wonderful.

However, I believe that it would be beneficial to supplement the data in other ways, such as with RGB image separation, which will triple the number of images, and with other filters, such as gaussienne weights.

Do you think using the most recent procedure to enhance datasets is a smart idea?

Although my team may not agree with me, I believe that this technique is equivalent to grayscale conversion. However, in my case, I apply custom filters and separation.",deeplearning,2022-06-29 12:05:40,5
"https://paperswithcode.com/task/video-segmentation

https://paperswithcode.com/task/video-object-segmentation/latest",1,vnnx5p,"Hi! wish you having a great time.  
I wanna research about a problem that is segmenting objects in a video and classifying them in predefined categories. I don't know what area of research to look and would be grateful if you can introduce me to one and offer any paper to read, specially a review paper.",deeplearning,2022-06-29 12:25:02,1
Does it show accuracy and results without paying?,1,vncz0a,,deeplearning,2022-06-29 03:53:40,4
https://www.deeplearningbook.org/,5,vnjxdj,"I've been messing around and have done a few projects in ML, but now I wanna go further and learn DL as well. Are there any resources y'all could point towards or any tips on how to progress on learning it. I dunno but for some reason it just seems intimidating to start and learn.",deeplearning,2022-06-29 09:32:09,4
">my primary concern is VRAM  
>  
>Are 2 \* 3060 12GB cards better for the above-mentioned purpose? or a single card with 16GB RAM?

24GB > 16GB, so if your primary concern is VRAM, then I think the answer is straight-forward.

There's certainly a lot that can go into this equation, but if you don't have any specific requirements, then maximizing VRAM is a good target.  You may face some difficulties dealing with two video cards vs one, but it shouldn't deter you from making that decision and it'd make for a good learning experience.  Just make sure the rest of your system is compatible with such a set up.

You may also want to look at 3090s, which come with 24GB VRAM.  Currently, you'd probably be able to get the two 3060s for less than one 3090, but *potentially* not by much.  I know *I'd* rather spend a few extra hundred dollars to get a 3090 over two 3060s, but going with the 3060s gives you the flexibility to buy one, try it out, and if you end up needing another one, you can get it later (which is probably what you should do regardless).

I wouldn't get the RTX A4000 16GB.  For the price, I'd rather either get to 3060s or pay a bit more for a 3090.  But, again, that's just looking at it from only the VRAM perspective.",5,vnhb7w,"Hello Experts,

I have been lurking around the corner for getting a GPU for my work purposes. For the last couple of weeks, I feel comfortable buying a graphic card. Since this is my first time getting a personal graphic card (especially with my own money), I need this purchase to last 3 years at a minimum. Currently, I am torn between a Dual 3060 12Gb and a Single RTX A4000.

**DISCLAIMER**: I know 40XX series is just around the corner, but rumored specs are saying RAM == 10 GB for 4070, and 4080 is likely to be out of my limit.  Plus the power consumption is off the charts. I am going to wait for the release anyway as it is likely to drop the prices of 30XX models.

**My Usage:**

I am planning on building Deep learning models across NLP, CV, and mixed-use cases. I love dirty prototyping and brute force searching for better models. This allows me to visualize the changes in performance with changes in parameters. Consider this as GridSearchCV from Sklearn.

So my primary concern is VRAM, I can compromise a little on training velocity as long as I am able to fit my model and even parallelize it. I am looking to create an open source package for the same in the future.

**Following are my concerns:**

Are 2 \* 3060 12GB cards better for the above-mentioned purpose? or a single card with 16GB RAM?

I wish to learn from your experiences in building deep learning machines.

Thank you in advance. Please feel free to ask questions and I will elaborate more.",deeplearning,2022-06-29 07:37:36,12
You will likely have to mix in context information because there is no way you will be able to fit the whole turn sequence into any transformer model.,2,vncuny,"Has anyone here fine tuned a hugging face Bart model on CRD3 dataset, i am working on it and i am getting really confused how to use CRD3 dataset in summarization task, there are som many turns and only a small chunk of summary, should i just append all the turns corresponding to a chunk and use that as dialogue, but that giving a very large input and max token length will be 40000, and even on GPU i can't take more than 1024 input token, please someone help me",deeplearning,2022-06-29 03:45:35,4
"I'll assume you only have one camera (unlike say Apple's [Measure app](https://support.apple.com/en-us/HT208924) which relies on the iphone's stereo cameras) your best bet would either be to use a monocular depth estimation model - [random example I happen to already have open](https://www.youtube.com/watch?v=gRqrYJWyXyI) - or to compute depth from motion (meaning you have to move the camera around). The former is not going to be super accurate since the model is literally just guessing, and the latter is probably even harder to get working in production due to the involvement of inertial sensors and unpredictable motion paths. In either case you will probably need a database of lens calibration data in order to provide accuracy across different devices. This isn't a problem for Apple since they also control the hardware.

Considering the application, where a single inch could make or break a furniture decision, I'm not sure the technology is good enough without relying on something beyond a single phone camera. If you had an object of known size in the photo, you might be able to calibrate off that (e.g. if you ask the user to put a ruler somewhere) but at that point they might as well just use a tape measure.",3,vn5tco,,deeplearning,2022-06-28 20:30:17,3
"They should probably look at more than one paper before writing something,

language models can do reasoning just fine with 'chain of thought' prompting,

https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html

The problem appears to be that reasoning isn't the default behavior of language models and so unless explicitly directed to do so, will take shortcuts.  Sounds a lot like humans.",8,vmlik8,,deeplearning,2022-06-28 04:58:03,1
You will need to code up your own interpolation technique and use that. You can start by looking at the source code of that function in scipy and copying and pasting. Pytorch also has a version of this function if you wanna use that.,3,vmqucg,"The first line

https://preview.redd.it/lnrqduzqyd891.png?width=468&format=png&auto=webp&s=ae469896127e6bb0e0b62328c59dec3a336cdef0",deeplearning,2022-06-28 09:08:28,3
"I don't have it, but I can absolutely guarantee the real thing is far less fantastical than what you describe. Nevertheless, I do have a faint memory of a press release along those lines. I'd guess that maybe it was being used for chip design and came up with one where transistors communicated with each other in an odd but more efficient way. In any case, it doesn't appear to have been good enough to result in actual production of said design.",3,vmjcvo,"I remember reading an article or seeing a video a few years ago about a deep learning model that ""discovered"" a way to send signals faster than ""physically"" possible by using some transistors that are near other transistors that are near the goal and sending the signal ""magnetically"" over an empty space instead of via the normal gates. Or something like that.

Does anyone remember and know what I am talking about? Could you link it here?",deeplearning,2022-06-28 02:47:49,2
Thanks for the tutorial :),1,vlxh0k,"View the tutorial here: [HERE](https://formatswap.com/blog/machine-learning-tutorials/how-to-generate-images-from-text-prompts-with-vqgan-clip-python-and-tensorflow/)

This tutorial teaches you how to convert any text prompt to an image using VQGAN-Clip.

For  example you could use the prompt ""A spray painting of a waiting computer and a bedroom in the style of Edgar Degas and Art Nouveau"".

This would generate the following image:

&#x200B;

https://preview.redd.it/97bb9nmjh6891.png?width=336&format=png&auto=webp&s=4f98d06e938f8bde8a47c5b8293b064af6068448

Let me know if you have any questions or comments.",deeplearning,2022-06-27 08:00:03,3
"Homomorphic encryption is a real thing.  I can also absolutely, 100% guarantee that it'll be easier working on the remote server than implementing homomorphic encryption for deep learning.  

Why don't you code and debug locally, then train the model remotely?",5,vm2rt4,"Hi all, I am developing a deep learning model, but the data is sensitive and can only be stored on a remote server. However, coding and debugging on that server is inconvenient.  So, is it possible to use the data in my local machine without data leakage? (make sure I can't download the real data from my side otherwise the person who manages the data won't allow me to do it.)

Really appreciate your help!",deeplearning,2022-06-27 11:48:31,6
"One factor to consider is that while GANs produce the most perceptually pleasing images, they may do this at the cost of deviating from the training data. A dataset only contains so much information about the target output, so a system must decide whether realism is important or whether 'image quality is important.'  


Not to say you cannot use GANS, or some other kind of neural network, just that you should watch out for this. This is known as the distortion-perception trade-off (I read about it in the context of image upscaling: https://www.youtube.com/watch?v=3Oetx8DCUCA).   


If you go forward with this project, it would be good to discuss things like this in your writing as justification for your chosen system. Good luck.",5,vltet2,"&#x200B;

1. Increase the number of abdominal CT images through GAN

2. Create a segmentation model to determine where the cancer is in this image

3. The data accumulates every time a doctor examines it, so increase the data through GAN and let the model learn it

&#x200B;

do you guys think it can be possible?",deeplearning,2022-06-27 04:33:45,3
"1.) to just save them back to a folder you could do this with CV2  - you don't need Keras for this.

2.) you could use Keras if you wanted to, KerasCV offers RandomContrast and RandomBrightness: https://github.com/keras-team/keras-cv",1,vm4m8t,I want to learn how I can do data augmentation on all images of entire class using keras and save the augmented images in the same class folder also i dont want to rotate or change their size just increase or decrease brightness and contrast can someone tell me how it can be done with Keras ?,deeplearning,2022-06-27 13:08:11,3
"Usually, they are all vectors. You would have a small linear layer after h_t to get y_t",1,vlxt4a," 

Hi Guys! I have a problem in doing LSTM Manual Calculation. As for the introduction, my name is Kafie and im undergoing a thesis with LSTM Prediction bitcoin price as my topic. My lecturer asked me to do a manual calculation with 10 data samples to predict a price for 1 day for 2 types of input(Univariate - 1 feature, and Multivariate 2 feature).  
My problem here is that i dont know how the formulas work with multivariate (2 input, in this case i use bitcoin close price and tweet volume as my 2nd input feature)

&#x200B;

[Lstm Forget gate formulas](https://preview.redd.it/7vmbkqz2k6891.png?width=1826&format=png&auto=webp&s=7c5301ef65b6c7d05e25398edc761d171fea69db)

f\_t=σ(W\_f.\[h\_(t-1),x\_t \]+b\_f)

As for the example, in this forget gate formula, For a single input (Univariate), i only need to put normalized data of bitcoin close price as the Xt . But how does it works for 2 input ? do i do \[0.252, 0,345\] as the Xt? and if yes, how does did that work to have a single output value for the forget gate?

Also in the end im only expecting 1 value of h\_t because im expecting a single output as my prediction (bitcoin close price). Any idea how to work this out? How can the second feature influence the value of h\_t?

Sorry for messy explanation, hope anyone can help me with this.",deeplearning,2022-06-27 08:14:42,1
Indian Institute of Science has a AI masters program. The question is whether you can actually get into it given how fierce the competition is,2,vlyido,"I have been thinking about my future education. India prioritizes education very much. Although I don't think there are any good AI masters degrees in India. I searched through the internet and found that Carnegie Mellon university and Stanford university offer this AI masters degree, but I am not sure about it. Are there any universities offering good AI masters degree? Are they in India or outside? I am pursuing bachelors in Data science currently.",deeplearning,2022-06-27 08:46:26,5
Are you searching for a method that does not require manual labelling?,1,vlnwpl,"Hello.

I am a college student who is interested in GAN

I am studying about it, but I have an one simple question.

&#x200B;

i want to generate medical image by GAN and improve segmentation model.

but if i want to learn image created by GAN, It needs a label!

&#x200B;

Can you give me advice how to label an medical image created by GAN?!

&#x200B;

THANK YOU

HAVE A GOOD DAY",deeplearning,2022-06-26 22:23:42,13
"I mean, the RX6600 is mostly unusable for DL, so the GTX 1070Ti wins by virtue of existing",4,vllmld,"Title says it all. I swapped the 1070 Ti out awhile back and I'm curious if bringing it back would get better performance.

If it matters, I don't train neural networks very often, I mostly just use pre-trained ones for upscaling images (and now video, which takes a long time) and I'm wondering if I should dust off the old 1070 Ti to speed things up.",deeplearning,2022-06-26 20:13:57,7
"Wow, impressive. I don't mean to be picky but there are a couple of faces in there that are still in Uncanny Valley terrritory for me. The substructures (eyes, eyes + eyelids etc) move around a little within the face itself. Very close, though.",3,vl7mi7,,deeplearning,2022-06-26 08:49:01,12
[deleted],2,vl32v8,"Hi!

For my deep learning class I need to design a CNN that can detect Plants on a field, similar like in this video [https://www.youtube.com/watch?v=ijAfvd6S6-A](https://www.youtube.com/watch?v=ijAfvd6S6-A)

I designed a working CNN, based on transfer learning with a mobilenetV2 architecture, which works fine on the dataset. One of the questions, however, is to explain how such a CNN could exploit the (almost perfect) regular placement of plants in the field.

Do you have any ideas what my professor would like to hear as an answer to this question?

I'm not sure, but I only thought about the model's receptive field that may be a little smaller, as we know what size object to expect. Thanks in advance, I would be glad to hear some thoughts of yours :)",deeplearning,2022-06-26 04:53:18,3
"There is one I know of, they discovered a new formula through symbolic regression on a trained GNN model: https://arxiv.org/abs/2006.11287",6,vktu06,"To clarify my question with a (semi) made up example: say we trained a DL model to take some input data to predict some outcome (eg. amino acid sequence to predict protein conformation). Say we also have very little idea how to relate the outcome to the predictors. Then we did some form of interpretability on the DL model and this led to novel theory/insight (eg. certain amino acid subsequences are highly likely to appear on the surface of the protein once it is folded). 

Are there good case studies of this sort of thing happening? Links to papers would be super appreciated.

(Disclaimer: I know very little biochemistry so my example might be wrong/nonsense/known without DL)",deeplearning,2022-06-25 18:52:21,6
"The swin Transformer was released in March 2021, more than a year ago. By comparison, ViT was released in October 2020, so between Swin and now is about twice the time than between ViT and Swin.
This post should not be called new. And it's not even the real title of the video linked",4,vl5ijv,,deeplearning,2022-06-26 07:08:36,1
Yes,2,vkjvk3,"Hello,

I have worked lately on quantizing trained models and I noticed that the models I get work only on CPU, is there a way to make them work on GPU?",deeplearning,2022-06-25 10:24:58,6
"There is no rule for this. DL is sometimes more art than science. If the original problem is similar to the transfer, remove the last layer(s). If they are different, you gotta cut at earlier layers. I would at least try training only the classification layers and training some more in different experiments.",14,vjv0r1," I am trying to make generalizations about which layers to freeze. I know that I must freeze feature extraction layers but some feature extraction layers should not be frozen (for example in transformer architecture encoder part and multi-head attention part of the decoder(which are feature extraction layers) should not be frozen). Which layers I should call ""feature extraction layer"" in that sense? What kind of ""feature extraction"" layers should i freeze?",deeplearning,2022-06-24 11:26:44,7
"classification and regression are not fundamentally different: just change the output layer, the loss and here you go.",4,vjyo2d,"I am doing a PhD and I would like to use point cloud data (X, Y, Z coordinates) to predict forest biomass. This is a regression task (i.e., multiple input points, a single output value). However, based on my initial research, almost all point cloud deep learning applications are applied to classification problems (e.g., is this point cloud a chair or a stool, etc. etc.).

&#x200B;

Question: Are people aware of examples where point cloud data is used for a regression task? I am having a very hard time finding examples of code, blog posts, or research.

&#x200B;

Follow up question: why is there a lack of applications of point cloud deep learning regression?

[Example of proposed point cloud deep learning regression.](https://preview.redd.it/r7elodcvym791.png?width=995&format=png&auto=webp&s=41df0f7f995e911d7004c27fb2725dcf758cbe61)",deeplearning,2022-06-24 14:16:34,5
"I guess this is personal taste, but I wouldn’t consider most of what’s on this list as breakthroughs",1,vjon4y,,deeplearning,2022-06-24 06:36:20,1
"Make sure that you save the best checkpoint you have in terms of your target metric. Early stopping always takes some time after the optimum to stop training so the model will already be slightly overfit.

In fact, I usually use only checkpointing because you never know what might happen after training for much longer. Loss curves usually arent textbook U shaped",2,vk0kap,"I recently started to use early stopping as the default method for my trainings.

I have a couple of questions for more experienced folks:

1. Is there any situation in which using ES is a very bad idea? 

2. What metrics should I watch? I used to compute ES based on the loss (this is the advice I got) but I had at least two occasions in which the loss started getting higher from one point on (in a actual parabolic manner) while my target metrics continued improving. The first case was a very unbalanced classification problem (so I suspect the imbalance was the issue) while in the second case it was a summarization task and the target metrics was ROUGE (I have no idea why this happened in this case)

3. Are there any general good practices I should be aware of when using ES?",deeplearning,2022-06-24 15:46:58,6
Use the existing examples and change the input dimensions may be?,3,vjnnuy,"I'm trying to find a keras example of inception v3 with 2000x2000x1 images, but I may be using the wrong keywords, because I can't find anything. Is there an example I could look at to try to understand how to use it? Thanks!",deeplearning,2022-06-24 05:47:31,11
[nvtop](https://github.com/Syllo/nvtop),16,viwgfx,,deeplearning,2022-06-23 06:02:50,13
You need to train both a classifier and a segmentation network.,1,vj4vqm,"Hi guys,

I am a Deep Learning noob and I have a question for you guys. I would be extremely thankful if someone could help me with this.

I am wondering how can I use a CNN to detect a variable number of ""interest points"".

Let's say I want to train a CNN model to detect the location of fingertips. So, given an input image of a hand, I want the model to detect the coordinates of all five fingertips.

According to what I know, to solve this problem I would train a simple CNN model, used for classification, such as an AlexNet or a ResNet, with an output layer of size 10, with two coordinates for each of the five fingers.  I would use a loss that takes into account the distance of the predicted point to the ground truth location of the fingertip, something like a Minimum Squared Error Loss. I think this would work, but if you think there is a better way to represent pls tell me.

Now, the problem I have is that I want the model to handle images where only 4 or 3 of the fingers are visible, but I don't know how to. If I train the model how I was describing above, the model would always output 5 pairs of coordinates and always try to find the 5 fingertips.

Can anyone help me understand how can I make the output size variable so that the model can handle the three and four fingertips situations? Or can you direct me to a paper or example with a similar problem with a good solution?

Thanks for reading and sorry if my English is not the best.",deeplearning,2022-06-23 12:24:45,1
"its pretty instulting to call ai sentient as well, its a piece of code on the disk, dont be a clowns people, no ai will ever be sentient, you can just copy it over again and itll never die caus it will never be alive",1,viy4fi,,deeplearning,2022-06-23 07:24:14,1
"Sure! Anything to tell the story that you have more experience than just pen and paper academic work. Try to focus on what you actually accomplished. My eyes glaze over when I see something on a resume like ""trained an object detection model for ML class"". That literally says nothing. You could have just turned the key on someone else's dataset and model training code. Try to explain what unique thing you brought to the project, if you did anything from scratch yourself, if you improved on a baseline according to some metric, etc...",6,vivjcb,Is it okay to add projects you did in a course in your resume?,deeplearning,2022-06-23 05:15:24,2
Short answer: yes.,1,vixcvf," Can someway some deep learning model or something tell the difference when the image is blurred(the human eye is also not able to see numbers)or not? 

https://preview.redd.it/fbib9nj4ld791.jpg?width=825&format=pjpg&auto=webp&s=132e7331df2882d5c1b6b83cd8c2e49375cbb598

https://preview.redd.it/3p4zooj4ld791.jpg?width=562&format=pjpg&auto=webp&s=2184676631b64029c79edbe35419ef3bfcbdff9c",deeplearning,2022-06-23 06:47:57,3
Yes in a convolution layer the parameters being learned are the values in the filter,1,viqst8,I was reading up about CNNs from [here](https://poloclub.github.io/cnn-explainer/). I got the basic idea about filters that they are a square matrix and we just perform dot product with the values overlapping in the input tensor. Although I am confused as to how does the neural network determine theses weights in the filters. Does this also use the optimizer function too?,deeplearning,2022-06-23 00:02:44,3
My idea but if it has to be github.com maybe you can try compressing and splitting it into 25mb chunks.,1,vimtgv,"I made an **Dogs Vs Cats Classifier** using Pytorch :)

The sad part is , the model size is 129 mb.

To deploy a model it has to be uploaded to github (25mb max limit) 

Hence i decided to retrain the model But i dont know how to reduce the size in Pytorch !!!!! :(

CAN ANYONE HELP ME OUT WITH CODE !!",deeplearning,2022-06-22 20:04:26,2
"Square: if you make the kernel square you let the model learn whether a given filter should actually have a more rectangular kernel or not. Effectively you could think of a rectangular kernel as equivalent to a square one (that the rectangular one would fit inside) where you constrained some of the values to zero. Maybe that would be reasonable for some tasks, but regardless that would be you choosing to add some additional inductive bias to the model. If you're going to add more bias you should have a justification. 

Odd sizes: if you have an odd-sized kernel then it has a center voxel that you can imagine mapping onto a voxel in the output tensor. If you have an even-sized kernel then it's offset by a half voxel. You would also have to pad asymmetrically if you wanted a same-style convolution. Historically filter kernels such as for gaussian blurring, laplacian edge detection, or Law's filters were all odd-sized. 2x2 isn't really enough to get a proper feature detection kernel, though it is good for up- or down-sampling. So 3x3 is kind of like the smallest filter that actually does something interesting. And while you could scale the kernel up, a 3x3 and then another 3x3 is kind of like the same thing as a 5x5 (without an activation function I believe it's identical since they're linear operations). So why not just do 3x3, activation, 3x3 and get some more interesting nonlinear behavior as compared to a single 5x5?",9,viguyk,"Is there a scientific explanation why kernels usually are square matrices with odd sizes? (3x3,5x5,7x7 etc.)",deeplearning,2022-06-22 15:06:34,2
Hey Y’all - I work for a start up called Applica. We are using deep learning to be the best in document understanding. Figured we might as well start talking about it on Reddit,2,vi5sws,,deeplearning,2022-06-22 06:53:25,7
"Code for https://arxiv.org/abs/1904.12356 found: https://github.com/ondyari/FaceForensics

[Paper link](https://arxiv.org/abs/1904.12356) | [List of all code implementations](https://www.catalyzex.com/paper/arxiv:1904.12356/code)



--

To opt out from receiving code links, DM me",1,vif6e9,"Hi All

I have a question on this paper -  Deferred Neural Rendering: Image Synthesis using Neural Textures. I would be very grateful if anyone can help me out, if you are familiar with the techniques described in it.

[https://arxiv.org/pdf/1904.12356.pdf](https://arxiv.org/pdf/1904.12356.pdf)

This paper details a neural rendering technique, which learns a neural network based rendering pipeline from image data, plus matching 3D mesh geometry.  The data which the neural renderer uses is in the form of texture maps, mapped onto the 3D mesh. However these maps are learned feature maps, which are trained alongside the renderer. 

My question is about the production of these maps. How are they trained? They are inputs, rather than network weights, so how does backprop update them? Are they just momentarily treated as being weights? Each feature texel swapped in one by one? I feel like there must be a more straightforward way, but the paper slightly glosses over this.

Many thanks for any help or insight you can give me

Best Regards

Matt",deeplearning,2022-06-22 13:52:43,1
"The ease of applying those generated images to 3D models would be one reason they're so popular. I have previously used just OpnCV and Python to generate images although it was slow.

Perhaps the game engines' extremely optimized texture processing capabilities are what draws people in?",1,vi3s7r,"I'd like to fool around with making synthetic data for computer vision/object detection/image classification. There's a lot of options, and I can't find any comprehensive description of why I'd use one tool over another. 

I'm experienced with Unity, so I would definitely be comfortable working in it. However, it doesn't really make a ton of sense to me - I don't need real time performance, physics, AI, etc. - so I would think it would make most sense to use something that's a bit more focused on pure rendering - so something like blender would make more sense to me. But it seems like Unity/Unreal engine are both quite popular for generating synthetic datasets, so I might be missing something.

Intuitively, I'd think that using Blender, or ideally something that used in the movie industry (Nuke? Houdini?) would make the most sense, yet these don't seem to be used as frequently. What am I missing? Why are Unity/Unreal so popular?",deeplearning,2022-06-22 05:12:17,2
"Invert your cross validation split (train on 20%, test on 80%). See how it looks.",30,vheki4,"I have a CNN neural network that makes disease predictions after ingesting respiratory audio files.  I'm getting 100% test data accuracy.  Now the response variable / label is not fed into the model ... instead the neural network is fed a numpy array representation of the audio file (array extracted from audio file features).  

I'm worried that I did something wrong in my code that caused bad cross validation (or other issue) ... leading to an erroneous 100% accuracy.  Would anyone be willing to take a look at my code and provide some thoughts?",deeplearning,2022-06-21 07:25:34,23
Try torchserve,9,vhjx02," Hello,

I'm deploying a Machine Learning model (Named Entity Recognition) with Django. In short, the user chooses a field (Politics or Science for example) and writes a text in a search area. Then the model identifies the named entities in the text.

My problem is that the ML model (encoder) is loaded each time the view is triggered, which slows down the process. Any idea how to optimize this and load it only once ?

My views.py :

    def search_view(request):
      if request.POST:   
        field = request.POST['field']
        query = request.POST['query']
        encoder = load_encoder(field)
        results = Ner_model(query,encoder)
        context['result'] = results 
      return render(request,'ner/results.html', context) 

Thanks!",deeplearning,2022-06-21 11:24:36,9
https://www.coursera.org/learn/linear-algebra-machine-learning?specialization=mathematics-machine-learning,1,vh9gg7,,deeplearning,2022-06-21 02:40:28,2
well now I don't want it anymore,52,vgqwg2,,deeplearning,2022-06-20 10:29:33,15
"You might want to look up invertible neural networks. What you're describing is hard in general, but invertible NNs are designed to make this feasible.",2,vhcbn0,"Hello Folks! I'm gathering information on how to obtain the scope of inputs (it can be more than one) required for a given output. Let's suppose I'm using a vanilla 1 hidden layer fully connected network with non linear activation function/

I've come across a few options like, numerically solving the inverse equation (given its non linearity, not sure how one would solve analytically, but we can analytically end up with multiple equations from relu's..), using backpropagation with a defined cost on a small perturbation from the desired output.

So, I wanted to know if you guys know of any literature on this or opinions or tricks or anything that might prove itself useful!

Thanks in advance!",deeplearning,2022-06-21 05:35:07,1
"Looks like you already found the error. Cuda is for Nvidia sadly. 

What you can do is just run on cpu instead of your graphics card. There are some ways to get cuda working on amd, but it's a bit more technical",2,vhceyp,"I am trying the implement this paper [https://github.com/xingyizhou/CenterNet](https://github.com/xingyizhou/CenterNet) 

In one of the steps to install it, I am getting an error -

\`\`\`

Traceback (most recent call last):

  File ""[build.py](https://build.py)"", line 21, in <module>

raise ValueError('CUDA is not available')

ValueError: CUDA is not available

Traceback (most recent call last):

  File ""build\_double.py"", line 21, in <module>

raise ValueError('CUDA is not available')

ValueError: CUDA is not available

\`\`\`

I have installed the dependencies as stated.

I have an AMD GPU and I read that CUDA works only on NVIDIA GPUs.

Can I run this somehow and is there a way to solve this?",deeplearning,2022-06-21 05:40:12,5
"For models that output sequences, most of the time, your model will output padding. This is done so that all examples in a batch output to the same length. When computing loss (or, more importantly, performance metrics), you should mask off this padding so it doesn’t affect your training or evaluation.",9,vgll4a,"I am trying to understand the idea behind having masked loss functions or masking a loss function. 

For instance, looking at L1_loss functions and I see something called 'Masked L1 Loss'. Any ideas what these are about? Thanks.",deeplearning,2022-06-20 06:31:22,5
"Have a look into Transformer, especially how BERT is trained using Masked Language Modeling. Here they try to automatically complete a sentence with some words masked out.

Example:
  INPUT: I like the [MASK] Pizza.
  OUTPUT: I like the eat Pizza.

In general they mask something like ~15% of their dataset, so ~3 picks in your case",1,vghvjf,"I want to make an AI in python tensorflow that predict league of legends drafts and I have the dataset already.  


A draft if a selection of 20 (5 picks and 5 bans for 2 teams) different characters encoded in one hot vectors.

What I want to make is a model that is able to predict a (or several) missing vectors. For example, if a team select 4 characters, what would be the most fitting 5th caracter ? Or if nothing is selected yet, how should I start ?

I thought I should look for LSTM but I know nothing about it and I'm worried I can't use them because my 20 vectors don't have the same meaning but they're in 4 different categories. And actually I have no idea how I should go about it.",deeplearning,2022-06-20 02:58:04,2
How many labelled photos do you have?,1,vgmwh8,"What would be a good modelling choice if I wanted to classifiy let's say, 10 different types of rocks from pictures containing mixes of these rocks?

The rocks have different colors, textures...etc. Also, I just want to classify 10 specific rocks, every other one not belonging to one of these classes should be identify as an ""impurity"". Imagine I have plenty of labelled data.

Thank you in advance!",deeplearning,2022-06-20 07:34:13,4
the code looks fine ! whats wrong ?,1,vgla96,"Apologies if this is the wrong forum to ask something like this - I can't figure out for the life of me why my backprop implementation is wildly off from autograd's implementation.  Any help would be majorly appreciated!

[https://datascience.stackexchange.com/questions/111961/why-isnt-my-backprop-matching-autograd](https://datascience.stackexchange.com/questions/111961/why-isnt-my-backprop-matching-autograd)",deeplearning,2022-06-20 06:15:43,2
PyTorch lightning is basically that,1,vgg9ed,I am a PhD student working in Deep Learning based NLP methods. I am trying to develop a boilerplate code of my own. Looking for inspirations or ideas?,deeplearning,2022-06-20 01:03:38,2
"I'm not 100% sure of how this loss is calculated, but from what I understand of the code, you're calculating the mean gradient over X and y on the whole image (I'm not entirely sure about the meaning of the 2* at the end), so you're dividing by the number of gradient values (the number of X and y gradients is different because you remove once a row and once a column, which in general have different shapes)",5,vfsg04,,deeplearning,2022-06-19 03:40:38,13
"Nice one! 

Check the input dimensions in the end. Seems like all the images in the batch got flattened into a single dim.",1,vfsj9d,"So i have been wondering , what does a neural network see ??

while studying and researching i came across a term called ""FEATURE MAPS""!!

Here I present a notebook about visualising feature maps using pytorch !!

[https://www.kaggle.com/code/shyamgupta196/feature-map-quick-book/](https://www.kaggle.com/code/shyamgupta196/feature-map-quick-book/)

https://preview.redd.it/rgf9bq2v4k691.png?width=500&format=png&auto=webp&s=f187ab16620e5971a9752057ed96347585b316fd

hey guys 🙋‍♂️

if you get to learn something from this notebook

PLEASE LEAVE AN UPVOTE ! ⬆️⬆️😇

And

If you think something missing in the notebook !

you can definitely share it in the comments

\#kaggle 😇",deeplearning,2022-06-19 03:46:51,3
"Yolo does object detection, not image classification. 

Check this : [https://www.tensorflow.org/tutorials/images/classification](https://www.tensorflow.org/tutorials/images/classification)",4,vg3emo," I have project which is image classification.  
now the project requirement is that I have to do it using yolo algorithm  
but as far is I have searched Yolo is only used for object detection  
Does anyone here knows if yolo can be used for multi class image classification ?",deeplearning,2022-06-19 13:06:09,6
"Might help:  
[https://deci.ai/blog/jetson-machine-learning-inference/](https://deci.ai/blog/jetson-machine-learning-inference/)  


These are different hardwares and TensorFlow uses runtime optimizations when you load a model for inference. This might be caused by a tensorflow optimization, plus the GPUs are different.  
Inference frameworks in general tend to compromise accuracy in favor of reduced latency.  
Tensorflow does it - [https://pdfs.semanticscholar.org/e37b/58bb5c056fa972a0a502d46b5abcfd7da5e1.pdf](https://pdfs.semanticscholar.org/e37b/58bb5c056fa972a0a502d46b5abcfd7da5e1.pdf)",2,vft6eq,"Hi everyone,

This is my first time seeking deep learning help on reddit but Im desperate so plz help out!

1. I decided to create a face recognition system and deploy it on two edge devices. For this purpose, I used the FaceBoxes model for face detection and FaceNet model for creating 128-D embeddings on the detected faces. For classification, I used the MLP classifier which I trained on Google Colab.
2. I took the trained Colab MLP model and deployed on Jetson Nano and Jetson TX2. All the major packages (Python, OpenCV, Tensorflow, Numpy etc) used the same versions on both devices. Even the Jetpack on both devices was same (4.4.1).
3. The recognition results on each device, individually, were constant. Like if I ran face recognition on a video on jetson nano, it would always give the same accuracy : 98%.
4. Same for Jetson TX2, constant accuracy result: 99%.
5. BUT I have to justify in my course, why do the two devices show different accuracy results on the SAME TEST VIDEO, using the SAME MODEL, trained on COLAB.

Unfortunately, I am not a hardware expert. I thought maybe it could be a difference of quantization or FP16/FP32 or something. But I dont even know what these terms mean. So some help in justifying why the accuracies are different on the two platforms, would be HIGHLY APPRECIATED. Please guide me. Thanks!

Edit: BTW, I used the sci-kit library for my implementation of the MLP classifier. And I used Tensorflow 2.3.1 for running the models.",deeplearning,2022-06-19 04:29:58,2
"non-image:

google translate, maps, siri, alexa, anything openAI, anything deepmind (alpha fold)",5,vfd8ie,"So much of the success of deep learning has been with convolutional neural networks for image recognition and similar applications. There are many famous architectures available (some even built into deep learning APIs like Keras) that work well for these applications.

But what about for non-image applications? Are there any success stories for deep learning regarding something else? Something like predicting individual income from a set of demographic characteristics in the U.S. census?",deeplearning,2022-06-18 12:19:38,4
Please stop spamming this sub with these images.,9,vfvrtr,,deeplearning,2022-06-19 06:58:11,2
Also probably used AI in [this project of trying to predict which of their users were regulators trying to regulate the industry](https://www.zdnet.com/article/uber-faces-criminal-investigation-over-greyball-spying-program/),1,vf65hv,,deeplearning,2022-06-18 06:22:43,1
https://arxiv.org/abs/2201.00650,7,vfdzfc,"Hi, I am learning deep learning and wanted to see what do these FAANG companies expect us to know in deep learning. Is there any resource for that. I don't mind machine learning interview questions too.",deeplearning,2022-06-18 12:57:30,1
Paperswithcode is good,7,vewu23,"I am an RL guy, I found that many RL papers adopt/add some DL techniques on existing RL algorithms.

I want to know that if there is any resource that summarizes the progress of the SOTA DL algorithms? I want to take that as a reference of what I could adopt or add on the RL algorithms.",deeplearning,2022-06-17 20:00:41,3
[Yes](https://paperswithcode.com/task/voice-conversion/latest).,3,veyq1o,I have seen so many papers doing image to image translation. Where image of one domain is transfer to other eg. Horse to Zebra and vice versa. Is it possible to do something like this on voices of people. Like input voice is tranfer to someone else's voice. Eg. Input is a sentence in my own voice and the model gives output in Optimus prime voice. Is something like this possible using CycleGAN or anyother Deep Learning Framework. Just a wild curosity..,deeplearning,2022-06-17 21:56:28,3
"Don't create the wandb.Artifact and save it? You have to explicitly do that so if you don't then you won't, y'know?",4,vf0kpr,"I'm doing a sweep on a relatively large model and WandB is uploading more than 1 GB of weights at the end of every run. I don't really need to restore the models at any point, is there a way I can disable the model upload?",deeplearning,2022-06-18 00:03:00,2
"I would check memory consumption, could be running out",7,velfjz,"My script is running on a directory that has 30k sound files, that is converting the sound into spectrograms. Not sure, what is going on. Any thoughts?",deeplearning,2022-06-17 10:32:51,14
"I would choose neither given that they employ paywalls. If that is not important to you, I heard TDS is a bit better, even though it's more or less the same thing.",4,velswa,"I'm  considering writing a deep learning tutorial, but I'm not sure where  to post it, whether in TowardsDataScience, directly in Medium, or  choosing other alternative. I understand that TowardsDataScience is a  Medium publication, but, does it provide more visibility or something?  Is there any clear advantage/disadvantage for choosing one over the  other? Should I choose another alternative instead?",deeplearning,2022-06-17 10:49:07,2
"Since it uses Transformer, this may be of help

https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/reports/custom/15742249.pdf",3,ve8oiq,"Hello,

I have a project where I need to quantize models using pytorch. I managed to find a way to quantize a resnet model but it seems like it doesn't work on a Swin Tranformer model since this model has a different architecture. Has anyone any ideas on how to quantize a Swin transformer model?",deeplearning,2022-06-17 01:55:53,16
Yes if the imgs arent that size it will resize.,1,vebclt,"the command to train is

!python train.py --img 640 --batch 64 --epochs 100 --data /content/datasets/cc-2/data.yaml --weights [yolov5s.pt](https://yolov5s.pt) \--cache

&#x200B;

Does the --img 640 means that yolo is resizing the dataset training images to 640x640? 

If so, then resizing images at preprocessing stage is not necessary?",deeplearning,2022-06-17 04:49:35,2
I've been trying to get it running without a gpu on VM and can't seem to get there. Any suggestions on a minimum viable for running playground/mini in the cloud without breaking the bank on a gpu instance?,1,vdpyyq,"DALLE-Mini has become really popular, so much so that the public access points are commonly overrun with traffic. Using the video below, one can run DALL-E Mini on their own Windows computer, with or without a GPU! No more waiting on some overrun third-party service!

[https://www.youtube.com/watch?v=OqEuEe-xSKk](https://www.youtube.com/watch?v=OqEuEe-xSKk)",deeplearning,2022-06-16 09:30:59,3
… and so busy that it is impossible to try,2,vdbqlp,,deeplearning,2022-06-15 19:51:25,6
"Whatever batch size your gpu can fit, and however many epochs it takes for your validation metrics to stop improving",7,vdvx8w,,deeplearning,2022-06-16 14:03:39,7
nyc explanation..,1,vdpazd,,deeplearning,2022-06-16 09:01:31,1
This is awesome! I will definitely plug it into my current DL projects and see how much faster I can make my models. Thanks!,3,vcx8dy,"Hi everybody,

I have been working for a while on improving AI inference efficiency/speed, and many times I have been asked if the same could be done to make training faster as well. Training can be a bottleneck, often costly and slowing development.

The answer is clearly a yes, many well know that. Training can be streamlined at different levels.

1. One can change the way training is performed (**algorithmic optimization**) by trying to achieve faster or earlier convergence. You can change the learning rate, the scheduling policy, the training recipe or replace one level with another that requires less computational resources.
2. Another option is to apply **precision reduction techniques**, which involve a trade-off of some precision to achieve a smaller memory footprint and a faster model. For example, one could ""prune"" the non-critical or redundant parts of the neural network graph (pruning), take advantage of the properties of sparse matrices (sparsity), reduce the size of the activations and model weights from 32 or 16 bits to 8 bits or even to 4 or 1 bit (quantization).
3. Moving down, closer to the hardware, you could optimize the way the model is mapped to the hardware (**compilers**) and the way the model accesses data in memory (**data loading**), making better use of computer resources. This can be achieved by storing the data closer to the processor and converting the neural network calculations into compiled binaries so that the CPU or GPU can execute them readily.
4. Also, you can increase the amount of computing resources used for training and further accelerate training by parallelizing computations across multiple machines (**distributed computing**).

Along these lines, I decided to create an open-source that works on optimizing the full computing stack. This allows people to benefit from the **compound acceleration** provided by these 4 levels of optimization techniques, and early results are promising. Moreover, the library also aims to make training acceleration very ""accessible,"" so that everyone can use these optimization techniques, which are often complex to be implemented today. This is achieved through the use of **class decorators**, a Python constructor analogous to Java's @annotations. In short, you can simply insert these annotations (e.g. @accelerate\_model() ) into your code and the decorator will make sure that you use your computing resources to the fullest.

This library is called nebulgym and so far it **accelerates training by 30%-50%** and I definitely believe there is room to reach 70%-90% or more. The library will evolve over time and support other use cases. And if you would like to contribute or just give feedback, it will be super appreciated! [https://github.com/nebuly-ai/nebulgym](https://github.com/nebuly-ai/nebulgym).

Here's a snippet of training with nebulgym decorators (`@accelerate_dataset` and `@accelerate_model`)

```
@accelerate_dataset()
class MyDataset{…}

@accelerate_model()
class MyModel{…}

#Train your model as you usually do
```

About the **tech behind the open-source**, as of now the library works on 3 building blocks: acceleration of the data loading process, and acceleration of both back and forward propagation through sparse strategies and efficient compilations.

Regarding the latter aspect, nebulgym leverages Rammer \[1\], a DNN compiler design that optimizes the execution of DNN workloads on massively parallel accelerators. It generates an efficient static spatio-temporal schedule for a DNN at compile time to minimize scheduling overhead. It maximizes hardware utilization by holistically exploiting parallelism through inter- and intra- operator co-scheduling. It achieves this by proposing several novel, hardware neutral, and clean abstractions for the computation tasks and the hardware accelerators. These abstractions expose a much richer scheduling space to Rammer, which employs several heuristics to explore this space and finds efficient schedules.

On top of this, nebulgym computes only a small subset of the full gradient to update the model parameters in back propagation \[2\]. The gradient vectors are sparsified so that only the elements with top magnitude are kept. As a result, a smaller fraction of the weight matrix is modified, leading to a linear reduction in the computational cost.

Nebulgym also changes the way data is loaded, with the goal of eliminating any time when the processor is not processing but waiting for data to load. Indeed, a default data loader reads the data from your storage and performs some user-set preprocessing (e.g. converting the data to normalized tensors, removing biases, resizing images, etc.), and then transfers the data to the model. This process is repeated for each data and for each epoch. The data loader introduced in nebulgym at first epoch performs the same tasks (data loading and preprocessing) but writes/saves the preprocessed data (in parallel) to a fast access memory, which is usually SSD memory if available. This \[a\] slows down the first epoch slightly (\~20% slower during testing), but starting with the second epoch thereafter \[b\] preprocessing will not be computed again and \[c\] data will be transferred from fast-access memory to RAM (in parallel) to make maximum use of memory bandwidth. This speeds up all the following epochs and prevents data loading from becoming a bottleneck for the entire training process, which happens in many cases.

And that's it. Give it a try, and leave a star on [github](https://github.com/nebuly-ai/nebulgym) if you like the concept :) Also feedback would be very much appreciated! And if you want to discuss/chat about AI optimization, with other AI researchers we are organizing reading groups and other knowledge-sharing activities on [this channel](https://discord.gg/RbeQMu886J) launched recently.

[\[1\]](https://arxiv.org/pdf/1706.06197.pdf) Xu Sun, Xuancheng Ren, Shuming Ma, Houfeng Wang. meProp: Sparsified Back Propagation for Accelerated Deep Learning.

[\[2\]](https://www.usenix.org/system/files/osdi20-ma.pdf) Lingxiao Ma, Zhiqiang Xie, Zhi Yang, Jilong Xue, Youshan Miao, Wei Cui, Wenxiang Hu, Fan Yang, Lintao Zhang, and Lidong Zhou. Rammer: Enabling Holistic Deep Learning Compiler Optimizations with rTasks with Reduced Overfitting.",deeplearning,2022-06-15 08:33:57,4
Multiply with a number to increase the amplitude?,2,vd6iyr,"I have a sound that I loaded using `librosa` but I want to change the volume of that array before I send for further processing. For this particular application, I want to change it only after converting that into an array for other `transform`ations, not when it is in the *.wav* form. Any resources or ideas?",deeplearning,2022-06-15 15:28:52,6
You can assign other number for padding,2,vcztfy,"Hello Reddit,

&#x200B;

For a school project that I am doing I will be predicting on data points that have a variable input. An example array could be:

\[

   \[ 15,16,20,10\] # this will be 30 numbers long

   \[ 49, 3, 54, 99\]

\]  
The shape will be (N,30). The number N depends on how long the sequence is and will be different everytime. I have seen some options to use padding to add \[0,0,0....\] to the sequence but 0 is for me a significant number that could represent something in a 'real' array. How would I fix this problem with LSTM? 

I am using tensorflow 2.8",deeplearning,2022-06-15 10:28:21,3
"Curious. What countries use revise instead of review? Are they synonyms? If not, what dictionary are they using? Does this imply that studying something again is similar to changing/ammending something you have learned?",2,vc7ao9,,deeplearning,2022-06-14 09:23:48,2
"It's pretty easy to set up WSL (Windows Subsystem for Linux).  Unfortunately, even though it's running under Windows, it's still going to be Linux.  There is a Colab notebook, how about that?",2,vcwera,"I've seen some videos about this, but those were strictly Linux

Has anyone here managed to do it?",deeplearning,2022-06-15 07:57:21,2
"It sounds like you would benefit from reading up on using Shapley Values for model explainability.

If your goal is to figure out why the model is classifying a sentence one way or the other, the SHAP values of each word would tell you how much the output score shifts in one direction or the other as that word is changed out with other words. 

If you're trying to do this for model explainability, you'll get much farther identifying which words have the highest SHAP value in a sentence than you would trying to train a bespoke generative model that can both understand this information and generate a natural language explanation that contains it. 

Check out the [SHAP library docs](https://shap.readthedocs.io/en/latest/example_notebooks/overviews/An%20introduction%20to%20explainable%20AI%20with%20Shapley%20values.html),  it has a good introduction to the topic, and the library is pretty easy to use.",2,vck7gg,"I'm trying to build a model that will give a reason for why the sentence is classified, for eg:

Input : I recently purchased this product as it was cheap and affordable. The reviews deceived me to try this. But as soon as i got it, I found that the product is damaged and I am not satisfied with the support. 

( assume that this sentence is classified as ""Negative feedback"" )

Output : Damaged product and user is not satisfied with the support.

So here, once the sentence is classified, I am trying to give a reason on why it has been classified as ""Negative feedback"".

It might look like Text summarisation but it's not exactly what I'm trying to do. Because the reason might be in a particular sentence and that's all i need. Because I'm assuming if the model classified it as negative feedback, then there might be a reason for it and it must be in the context.

Can i get any insights on what kind of data i should gather? Should i manually label the data? Also I'm thinking of using T5 but maybe a causal model like gpt-2 will work? 

It'd be great if i could get some suggestions from the forum on how to proceed with this problem. Thanks.",deeplearning,2022-06-14 19:40:40,2
"I use unet regularly for most of segmentations. Yes you can contact paper authors, and most of the researching give replies unless you are asking pretty basic question. As per your project use unet patch based segmentation. Try looking into torchio and monai for easy pipelines. If you need more help go to paperswithcode.com. you will get many unet implementations.",2,vcan5u,"Hi, I'm an undergrad student trying to learn dl and segmentation using u-net. I have a database with black and white images and masks with regions of interest, but they have different sizes (between 6000x5000 and 3000x1500). All the tutoriasl I've seen used same-size images, and they were all small images, so I'm not sure how to proceed.

Also, I'm trying to follow a paper I found, but the researchers don't really go too deep into implementation details, and I thought that maybe I could write an email asking them for some directions... I guess it depends on each researcher, but are they usually ok with people asking them about their papers? Would it be ok to ask them for the specific unet implementation or the trained model?

If anyone could help me with this, or point me in the right direction (maybe with a tutorial, book or other resources), I would greatly appreciate it.

Thank you!",deeplearning,2022-06-14 11:53:28,8
"I don't think it makes sense to break your data up into three dimensions. The first two are both the time dimension but you're preventing the RNN from seeing the time steps sequentially.

You've got 6000x3000 samples of a 15 channel signal",2,vc3on7,"I have a signal that is 6000 seconds long, it has 15 channels, and 3000 samples per second. The shape is (6000, 3000, 15). However, I am not able to make sense of the RNN input shape that would work here. I looked at many resources (including [official examples](https://keras.io/guides/working_with_rnns/) and [blog posts](http://philipperemy.github.io/keras-stateful-lstm/)) and also got some intuition from answers like [this one](https://stackoverflow.com/questions/48491737/understanding-keras-lstms-role-of-batch-size-and-statefulness/48506964#48506964), but I am unsure how to account for the 3000 samples per second in the data. How do I reshape the data to fit with [batch, timesteps, feature]? The anomalies according to the labels are ~20 seconds, therefore it seems I should use timestep=20, I see that a batch size of 32 is often suggested. I understand these two, but would the feature itself be a matrix of shape (3000, 15)? I am unable to figure this out.    


If anyone has any pointers or ideas, I would greatly appreciate it. I am trying to learn DL over the summer and any opportunity to learn more would be great!",deeplearning,2022-06-14 06:36:52,5
… why… would you be surprised that different seeds lead to different results?,8,vc17kn,"I am working on a model where I am trying to find whether weights of a particular layer are conveying any relationship. But everytime I initiate my weights with different seeds, I get different weights after training (even if my loss is identical to three float points). 
So Is there any way to stabilize the weights so that I don't get new each time I train the model?",deeplearning,2022-06-14 04:19:36,5
"I'm not familiar with the dataset, but in the context of OCR ""detection"" is finding text in an image, while recognition is actually turning the text image into text. I suspect this is similar: in the first you're interested in the location of the signs, but not their specific semantic. The second instead is concerned with reading out the signs themselves.",3,vbz1r5,Whats the difference between the german traffic sign detection benchmark (GTSDB) dataset and the german traffic sign recognition benchmark dataset (GTSRB) ?,deeplearning,2022-06-14 01:50:00,3
The linked video is about OpenAI's DALL-E version 1. DALL-E Mini is not from OpenAI. [Here](https://www.reddit.com/r/dallemini/comments/vbqh2s/how_dalle_mini_technically_works_a_tweet_thread/) is a Tweet thread about how DALL-E Mini works.,1,vbhywi,,deeplearning,2022-06-13 10:49:29,2
"This is amazing. I saw this post a month ago and can't believe there's still barely anything online about this.

Are you one of the developers?",1,vb6vvc,,deeplearning,2022-06-13 00:52:37,1
And why would you want to use an LSTM?,6,vbah8j,"I want to build a binary classification model using LSTM for my custom datasets. Unlike the MNIST dataset, I want to feed RGB images as input but can't figure out how to reshape my inputs.",deeplearning,2022-06-13 04:57:02,7
"Paper: https://www.nature.com/articles/s42256-020-00237-3  
Code: https://github.com/mlech26l/keras-ncp",1,vb03ks,,deeplearning,2022-06-12 17:59:58,1
Search stuff about student learning,1,vb8emn," 

Hello,

I'm practically a beginner and I'm working on a project to minimize a model's size (a model that's already trained for a specific task that includes color detection) using Pytorch while keeping a certain level of accuracy maintained. During my research on this topic, I found out that the most efficient way is using quantization and pruning. Concerning pruning, the best advice I got is to access the model's layers  and modify the number of neurons on each of them and then retrain the pruned version on same dataset. Is this the best idea to decrease the number of parameters (which leads to decreasing the model's size normally)  while keeping a certain level of accuracy?",deeplearning,2022-06-13 02:45:11,2
"Well, for one thing, you don't keep up with the entire ""field of deep learning."" You choose a specialty within that (somewhat more specific than just ""computer vision"" or ""NLP"") and you focus on papers in that area. You don't need to read every paper. If your focus is on the papers in your sub-specialty, you can still read the more significant, groundbreaking papers in other fields, but you can filter by importance. Many papers won't be relevant at all to you; you can just read their abstracts. The wider you go, and the further from your field you go, the higher your filters should be.

With that said, there are good newsletters out there with curated lists of papers (like The Sequence, The Gradient, or The Batch). There's also arXiv sanity and paperswithcode.

When it comes to getting used to reading papers, you mostly just need practice. There's a large number of ""chunks"" (concepts or sub-concepts) and a particular way they're expressed. Over time, you learn these ""chunks"" and abstract them, so that when you see them in a paper you already know exactly what it means. The first time you see a chunk (e.g., positional encoding for vision transformers) it might slow you down a bit as you learn about and try to understand it. But the next time, you'll already know it and integrate it into your mental model of the paper quite quickly. The same goes for mathematical notation--as you see it repeated, you develop a mapping from it to the DL concept it relates to. In this way, you'll get faster and faster with time as you develop a rich network of mental abstractions.",18,vam6v4,"New research papers are published daily with new architectures, algorithms, techniques and whatnot. Keeping up with the state of art models is quite overwhelming and tedious. Professionals or veterans in this group can you tell me how you keep up with this? How do you deal with new research papers and more importantly how do you read them and understand them? Is there any efficient way of doing this without burning out?",deeplearning,2022-06-12 06:39:06,5
What a bizarre way to solve that problem,10,vaqvud,"I have to decide to accept or decline a master thesis offer. The topic is the following: there is a robot arm that feeds people with a spoon, however the arm sometimes occludes the camera. So the idea is to use generative adversarial network (GAN) to generate the person face to eliminate occlusion, but i feel like this thesis is just collect data, create an architecture and tune the hyperparameters, so no thinking and deep knowledge is required. Can someone with more experience tell me otherwise?",deeplearning,2022-06-12 10:30:50,26
"Hey, I would like to join the group.",1,vaq4tw,"I found the course in [https://github.com/maziarraissi/Applied-Deep-Learning](https://github.com/maziarraissi/Applied-Deep-Learning) to be the best for me, as it seems fit for the scattered knowledge I have in ML, CNNs and some tensorflow knowledge.

&#x200B;

I think if I have someone/group to run along the course with, it will be more effective. So hit me up if you are interested.",deeplearning,2022-06-12 09:55:19,7
"Try prompt-engineering GPT-3 to do it. I have recently found that to work surprisingly well in some tasks. The prompt could be something like:

Invoices converted to structured format: 

Invoice: <example input>

Structured: <example output>

###

Invoice: <example 2 input>

Structured: <example 2 output>

###

Invoice: <test input>

Structured:",2,vao353,"I am trying to embark into a personal project that attempts to parse invoices in a text format into an structured output with the extracted information.

Each invoice is different in terms of format and structure, but all of them have similar information, some more than others.

For instance, you could have an invoice with a text representation as follows:
 
    **field_a:**            some_val
    **field_b:**            other_val

and others like:

    **field_X**           **field_Y**
     SomeVal            OtherVal

Each invoice would have distinct fields, spacing, industry related terms, products information, rates of services, discount information, etc.

I have been looking into several NLP papers, including Named-Entity resolution and Text summarization. But those apply to regular text, not to semi-structured text where spacing and position matters.

Can you recommend/suggest some research papers that tackle a similar problem? 

Thank you in advance",deeplearning,2022-06-12 08:16:05,2
"have you heard of the hough transform? not to knock on your current approach, but you could use some pretty basic computer vision techniques to differentiate between circles/rectangles",3,vagymz,,deeplearning,2022-06-12 00:43:46,5
Are you installing through conda?,2,val3kg,"So I wanted to setup an environment for tensorflow and it needed Visual Studio 2019. I accidently downloaded 2022. Installed the C++ distributable. When I installed CUDA it showed in the note that it didn't install Nsight for 2019 Visual Studio. Will that be a problem? below is the youtube video I have been following.
[Video](https://www.youtube.com/watch?v=19LQRx78QVU&ab_channel=NicholasRenotte)",deeplearning,2022-06-12 05:37:52,10
"I would say RTX3060 (\~ $400) would be the most cost effective solution, cheap but with 12 GB VRAM and just 170 W max.",7,v9vav1,"Hi,

Sorry if this is a stupid question, but I need your advice.

But first a little bit about me. About 10 years ago I graduated in multimedia and artificial intelligence. Unfortunately, they didn't teach us about deep lerning at the time. Years go by and I wanted to stay up to date with these topics, I installed TensorFlow for Python, OpenCV for Python and tried to assemble some image recognition myself. It even worked out, but with my current equipment, the performance was at the level of one frame per second. And now the question.

&#x200B;

What kind of graphics card should I buy in order not to overdo the costs, but at the same time to be able to squeeze a lot from it, learn more about deep learning and implement some amateur project?",deeplearning,2022-06-11 04:06:56,12
"
I see you've posted a GitHub link to a Jupyter Notebook! GitHub doesn't 
render large Jupyter Notebooks, so just in case, here is an 
[nbviewer](https://nbviewer.jupyter.org/) link to the notebook:

https://nbviewer.jupyter.org/url/github.com/arjun-majumdar/Autoencoders_Experiments/blob/master/Conditional_VAE-MNIST_TF2.ipynb

Want to run the code yourself? Here is a [binder](https://mybinder.org/) 
link to start your own Jupyter server and try it out!

https://mybinder.org/v2/gh/arjun-majumdar/Autoencoders_Experiments/master?filepath=Conditional_VAE-MNIST_TF2.ipynb



------

^(I am a bot.) 
[^(Feedback)](https://www.reddit.com/message/compose/?to=jd_paton) ^(|) 
[^(GitHub)](https://github.com/JohnPaton/nbviewerbot) ^(|) 
[^(Author)](https://johnpaton.net/)",1,v9tjq1,"Implemented Conditional-VAE on MNIST dataset using TensorFlow-2.8 and *tf.GradientTape()* API. You can refer to the full code [here](https://github.com/arjun-majumdar/Autoencoders_Experiments/blob/master/Conditional_VAE-MNIST_TF2.ipynb).

For generating synthetic data using trained network, there seems to be two ways:

1. Use **learned latent space**: *z = mu + (eps \* log\_var)* to generate  (theoretically, infinite amounts of) data. Here, we are learning 'mu'  and 'log\_var' vectors using the given data, and, 'eps' is sampled from  multivariate, standard, Gaussian distribution to add stochasticity.
2. Use multivariate, standard, Gaussian distribution = N(0, 1) as *z* which is then passed through VAE's decoder.

***What is the ""the standard way"" to generate data?*** (from the two options above), or,  how can we find that. Neither the original [Auto-Encoding Variational Bayes](https://arxiv.org/abs/1312.6114) paper nor the [β-VAE](https://openreview.net/forum?id=Sy2fzU9gl) paper seem to specify the best way to generate images. The latter does say: ""The most informative latent units *zm* of *β*\-VAE have the highest KL divergence from the unit Gaussian prior"", confirming at least that the posterior distribution is not *N*(0,*I*) and the difference matters - [reference](https://www.deepmind.com/publications/beta-vae-learning-basic-visual-concepts-with-a-constrained-variational-framework).",deeplearning,2022-06-11 01:58:00,4
"It's worth doing if you're finding that a single epoch takes so long that it's worth you spending your time up front fiddling around with learning rate convergence curves to find the optimum hyperparameters for the one cycle. Bear in mind you still technically will have to decide a bunch of hyperparameters somewhat arbitrarily (or else do a grid search anyways).. most salient of which is how many epochs to use in each half cycle and how many to use in the convergence cycle. 

In my experience if you have access to reasonable amounts of compute power and your task isn't that intense relative to your resources it's easier and more efficient to just use early stopping and do some kind of hyperparameter grid search with constant learning rate (and other optimizer hyperparameters constant also)

For every task and scientist there is an optimum balance of your own time investment in optimizing the process vs just letting the computer compute while you do something more productive. One cycle learning policies have their applications and it's up to you to figure out when they are or aren't worth doing.",4,v9hnx3,"I recently learned about [super convergence](https://arxiv.org/abs/1708.07120). It looks like something that will greatly speedup my workflow. I am planning to use the [OneCycleLR](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.OneCycleLR.html) for this. However, my friend cautioned that it didn't work for him and there aren't many papers using it for any standard algorithms. 

Has any of you found it useful? Why isn't it mainstream? Why aren't people using it more?",deeplearning,2022-06-10 14:08:57,5
They really need to add a cacheing service to store popular requests.,2,v99o5e,"DALLE-Mini has become really popular, so much so that the public access points are commonly overrun with traffic. Using the video below, one can run DALL-E Mini on their own computer, with or without a GPU! No more waiting on some overrun third-party service!

[https://www.youtube.com/watch?v=eWpzLIa6v9E](https://www.youtube.com/watch?v=eWpzLIa6v9E)",deeplearning,2022-06-10 08:00:08,14
"What you're referring to is called ""numerical differentiation"": https://en.wikipedia.org/wiki/Numerical_differentiation

While numerical differentiation is conceptually simple and doesn't require explicit knowledge of derivative functions, it's not considered to be the same thing as automatic differentiation.  For further reading, see section 2.1 of this paper: https://arxiv.org/pdf/1502.05767.pdf

To summarize, numerical differentiation is less efficient (O(n) evaluations of a function f are required in n dimensions) and more subject to instability introduced by truncation and rounding errors.  Contrast this with automatic differentiation over a composition of functions with explicitly defined derivative functions where those O(n) evaluations required for numerical differentiation happen ""all at once"".  The so called ""cardinal sins"" of floating point are also less likely (though certainly not impossible) with autodiff; those being 1) adding small numbers to big numbers and 2) subtracting numbers which are approximately equal.

*Note*: Some of the above comments are paraphrased from the linked paper.",1,v9eldp,"Hi, so I just started calculus for deep learning on youtube ([link](https://www.youtube.com/watch?v=4iT2Bzc4EIc&list=PLRDl2inPrWQVu2OvnTvtkRpJ-wz-URMJx&index=42&ab_channel=JonKrohn)). So I am guessing that we use 'backpropogation' to get the partial derivative of the cost function with respect to the model parameters. Can we use the [delta method](https://www.web-formulas.com/displayImage.aspx?imageid=242) for it. Don't know much about NN, is this correct?",deeplearning,2022-06-10 11:45:12,1
are they good though?,1,v90zlz,,deeplearning,2022-06-09 23:04:34,1
Are you making sure that you include every feature in the importance plot?,1,v9bj4v,"I was training a Xgboost model for a Kaggle competition and after getting the feature importance  chart I noticed that only 80% of the features are being used for split, does it mean that my model is only using those 80% features and ignoring others?",deeplearning,2022-06-10 09:24:59,2
"I wouldn't go overboard with your laptop choice for college. For machine learning and Python, you can use https://colab.research.google.com/, which provides you with free computation power. I survived college with just a cheap notebook.  
Now, to answer your question about AMD machines, I can see it comes with a NVIDIA GPU, which you should use for machine learning. Training with CPU is very time-consuming. According to [this three year old post](https://www.reddit.com/r/MachineLearning/comments/9qo1o2/d_is_amds_ryzen_cpus_good_choice_for_machine/), Tensorflow is a bit slower on AMD CPUs compared to Intel, however, as long as you use the GPU, it should not be a problem.",6,v93djl,"I am interested in purchasing a laptop for college, I am working on Matlab , tensor flow and Pytorch. I am interested in this model : [https://support.hp.com/it-it/document/c07904480](https://support.hp.com/it-it/document/c07904480) . I have never worked on a Amd machine before and wanted to know if issues would potentially arise post purchase in terms of software compatibility and computation time. The laptop has 5800 amd cpu and nividia 3060 gpu. I would appreciate ask insights shared on this configuration.",deeplearning,2022-06-10 01:54:40,7
"Use the tools that windows offers, so, basically, use linux, because windows for machine learning is a bitch, but you have the container option inside windows to use linux without installing it, not even on a pendrive/thumbdrive.

I am serious, i work with this and every time people just end up working in linux.

As a workaround, if it is for school or university, google colab.",3,v90h8d, Been struggling to get the tensorflow object detection api installed on windows. I'm willing to pay anyone who is able to help me successfully install it. Assuming I don't figure it out on my own.,deeplearning,2022-06-09 22:31:49,16
I think it's more accurate to call it self supervised instead of unsupervised.,1,v8uyc7,"[Introduction to Generative Adversarial Networks (GANs)](https://debuggercafe.com/introduction-to-generative-adversarial-networks-gans/)

[https://debuggercafe.com/introduction-to-generative-adversarial-networks-gans/](https://debuggercafe.com/introduction-to-generative-adversarial-networks-gans/)

&#x200B;

https://preview.redd.it/z6z5fud5to491.png?width=1200&format=png&auto=webp&s=545a5bdcd6c40faf9e0044313315c62e57987682",deeplearning,2022-06-09 17:20:26,1
"Looking at the documentation for PyTorch linear function you can see that the weights are multiplied after a transpose operation. 

https://pytorch.org/docs/stable/generated/torch.nn.Linear.html",1,v8t94x,"I  have a training FFNN for MNIST with a batch size of 32. My first linear layer has 100 neurons, defined as  nn.linear(784,100). When I check the  shape of the layer using model\[0\].weight.shape I get \[100,784\].  
My input is of the shape \[32,784\].  
It was my understanding that there are matrix multiplication Weights with the input, however, I cannot see how to do that between the weight tensor of shape \[100,784\] and input tensor of shape \[32,784\]. Shouldn't the weight tensor be \[784,100\] ? Or does PyTorch transpose the tensor before multiplication?

Thanks!!",deeplearning,2022-06-09 15:54:50,1
"If you aren't light on time you could go through Berkeley's Deep Learning course (282A):

Lecture Videos: https://www.youtube.com/watch?v=J0oTHK8NRco&list=PLki3HkfgNEsK1PNSxxPi1YuUKi2cvz5S5

Problem Sets/Discussions/Solutions: 
https://cs182sp21.github.io/
https://cs182sp22.github.io/",2,v8mb38,"Hello,

I am trying to learn Deep learning, for theory I have bought the book Deep Learning for Coders with fastai and Pytorch is there any youtube channel or udemy course or from coursera. Which shows some examples with python code on variete of example in deep learning?  
I have found only one  Nicholas Renotte .

&#x200B;

thank you",deeplearning,2022-06-09 10:40:26,3
"Well you’re at the right place. If you don’t want to learn from books, online forums with other people are some of the best you can get.",8,v8a7nv,Where can I ask theoretical questions about deep learning? I have trouble comprehending the technicalities on my own. Understanding the terms from other papers or books leads me down a rabbit hole every time.,deeplearning,2022-06-08 23:12:14,5
"I really don't recommend doing this in league of legends if you are a beginner because it is extremely hard. However, yes it is possible, and in some video games that are close to vision-based robotics it is even realistic and highly interesting from a broader perspective.

If you are a total beginner I recommend you start with the same simple environments as everyone, but once you are getting more versed into this, I recommend you check the [TMRL](https://github.com/trackmania-rl/tmrl) repo where we do exactly that in the TrackMania video game.",2,v8e3o9,"Hello there, im trying to study DL/Ai, im a complete beginner..  and i was wondering if it is possible to not create an enviroment but use an existing ""Game"" that AI scan frame by frame. To give an example, i would like to create an AI for League of legends, could i create it without recreating a moba enviroment but scanning an existing one?  
(sorry if my english isn't quite good)",deeplearning,2022-06-09 03:51:08,4
"Really cool. I'm a noob, but it seems to me that the main advantage seems to be saving CNN/kernel operations? So would the bottleneck be the framerate of the camera feeding the chip optical data, or does the chip somehow not need another device to feed it input? If whatever is on the receiving end of the chip's output is digital, wouldn't that also mean most of the chip's throughput is wasted since the output data has to be converted to electrical signals?",3,v7p8pq,,deeplearning,2022-06-08 06:07:13,3
"I don't have practical experience with the same task, but I'd try to fine tune transformers on the sequence size you have, it should be relatively easy to train the meaning of larger positional encoding values.

If you can fit the model to VRAM with the batch size of one, you can use the gradient accumulation to achieve the necessary batch size. Otherwise, you can use the model parallelism to train different transformer layers on different GPUs.",2,v8av9v,"How to train GPT or BERT for large context where context length is more than 1024 tokens. Truncating the context is not an option as the complete context is important. 

One approach that I can think of is breaking/dividing the context into multiple chunks.

What are my other options?t",deeplearning,2022-06-08 23:57:22,3
You'll need a lot of dick pics,25,v7a8jc,"I'm new to Machine learning and just learning Tensorflow/Keras. I'd like to make a model to detect dick pics posted by spammers. I know how to train a model for 'Cat vs Dog', but what about my case where I have only one category to detect? Should I dump whatever image that is NOT dick into one category, and the dicks on the other? If so what dataset(s) should I grab and choose for not dick category? Or there are better solutions?  Appreciate your hints about this.",deeplearning,2022-06-07 15:52:13,21
"TLDR: Deep Learning Specialization by Andrew Ng + Book by Ian Goodfellow


Hi. As you just finished ML, go for Deep Learning Specialization by Andrew Ng on Cousera. But, keep the book Deep Learning by Ian Goodfellow by your side. Andree Ng does a great job explaining application of math in Deep Learning, however, reading the corresponding sec in that book will help u master that topic.",2,v7jnhg,"Hi, I just finished machine learning algorithms and made projects using sklearn. I have no prior knowledge of perceptrons or neurons and other jargon of deep learning and wanted to learn deep learning. Fast.ai's course was the first recommendation from one of my friends. Is it good or are there any other resources or books better than this. Please help",deeplearning,2022-06-08 00:24:15,4
Eventually you're going to want to retrain to improve performance for some edge classes you didn't have examples of before and if it's spaghetti code you'll basically have to start over,3,v79vgk,"Quick question for all those who are trying to build stuff with AI/ML  
\-Why do you care/not care about reproducible/usable code/models? i know it's a basic question but i'm trying to dive deeper and understand the underlying reasons about why it matters or doesn't matter to you. (5 whys analysis of this question basically)",deeplearning,2022-06-07 15:35:00,6
"Do you have a training dataset (i.e: labelled images of the coins)? if you're happy to share i can try to help, i dont need any compensation, i am just looking for some projects to do in my free time.",4,v6o5n2," 

I'm both a collector and a coin dealer. I look through tens of thousands of coins a week for rare dates, errors, etc. But as I get older, my eyes are not what they use to be. So it's getting somewhat difficult for me to see the key details on the coin. So I decided to make a setup that can look through coins for me. I've been greatly influenced by this machine that does everything I want, but I need something a lot smaller.

[https://youtu.be/k7okDtRRCcY](https://youtu.be/k7okDtRRCcY)

I do have a basic background in coding and how it works. But I have little experience with making an AI. I've watched many video tutorials and I now understand clearly how an AI learns. I think the best route is to use Python, TensorFlow, and open-cv. But I keep getting some kind of errors that have been a major roadblock for me.

If this is relevant. My company setup is a ryzen 9 5900x. 3080 gpu and has 64gb of ram.

I'm looking for someone who can guide me through installing and training an AI model. I will compensate for your time, either in money or in collectible coins. What I mean for collectable coins is good quality coins. Not those cheapy coins you pick up from gift shops. But actually pieces of history. I've got silver coins, I've got a ton of English coins from 1600s-1800s. You can check out my ebay store to get a idea of what I have to offer. [https://www.ebay.com/sch/uncommoncentscoins/m.html?\_nkw&\_armrs=1&\_ipg&\_from&LH\_Complete=1&LH\_Sold=1&rt=nc&\_trksid=p2046732.m1684](https://www.ebay.com/sch/uncommoncentscoins/m.html?_nkw&_armrs=1&_ipg&_from&LH_Complete=1&LH_Sold=1&rt=nc&_trksid=p2046732.m1684)",deeplearning,2022-06-06 23:12:32,11
"Don't buy a laptop for deep learning. You can buy a used gaming desktop with an rtx 3060 12gb vram, 32 GB RAM, nvme storage plus an SSD and a processor that's a couple years old for under 1k. Then just buy a used laptop that's low spec but small and comfortable to ssh into it.

Then you can upgrade the GPU to something crazy like an A100 in a year or two if you decide you need to

Edit: this is what I did, just used the 6 year old laptop I've had, and it might be more expensive but I do like not having to think about letting it run for days or transferring GB back and forth with sshfs lazily set up.",3,v745tn,,deeplearning,2022-06-07 11:43:34,1
"* Demo : [https://main-common-computer-jonathanlampkin.endpoint.ainize.ai/](https://main-common-computer-jonathanlampkin.endpoint.ainize.ai/)
* Ainize(Free deploy with GPU) : [https://ainize.ai/](https://ainize.ai/)
* AIN DAO Discord : [https://discord.gg/RCwaY6vEhA](https://discord.gg/RCwaY6vEhA)",1,v6qx9e,"&#x200B;

https://reddit.com/link/v6qx9e/video/30fg1e2h26491/player",deeplearning,2022-06-07 02:18:34,1
"This is the kind of research I love :)
Good job!",29,v61isf,"Hello! I build some sex position classifiers using state-of-the-art techniques in deep learning! The best results were achieved by combining three input streams: RGB, Skeleton, and Audio.

Basically, human action recognition is applied to the adult content domain. It presents some technical difficulties, especially due to the enormous variation in camera position (the challenge is to classify actions based on a single video).

Check it out on Github: [https://github.com/rlleshi/phar](https://github.com/rlleshi/phar)

Possible use-cases include:

1. Improving the recommender system
2. Automatic tag generator
3. Automatic timestamp generator (when does an action start and finish)
4. Filtering video content based on actions (positions)",deeplearning,2022-06-06 04:40:52,23
"Try to do more data cleaning, like removing the unwanted rows or the rows that can lead to wrong predictions, and irrelevant or duplicated rows/columns.

Since most of the classifiers you used are tree-based, try to use some neural network classifiers like ELM and MLP, if that didn't work I'd recommend working with an ensemble learning method where you can divide your data since it's not too small and feed it to multiple learning algorithms to obtain better performance than could be obtained from only one classifier.

You can use ensemble learning with both machine learning and deep learning classifiers.",2,v65it6,"I'm working on a project and the accuracy of the model is not improving. I have real-time dataset of more than 1 million entries and 22 columns. I have applied PCA, Correlation, removed the null values, removed the outliers, done the label encoding, also randomly selected the data in 3 patches. After doing the proprocessing step, I have 600k entries which are relevant, out of which I have used 240k. 
I have applied 8 ML models (Logistic Regression, Naive Bayes, KNN, J48, Random Forest, Ada Boost, Xg Boost and Boost). I have also done the hypertuning of the models and also Grid Serach. But the accuracy is still below 60.
Can anyone pls tell me how can I take the accuracy above 70?
Also, I'm planning to use the deep learning if ML fails. So any suggestions what model should I use?",deeplearning,2022-06-06 08:01:20,9
"I don't have specific knowledge of Datawigs imputer, but if it is a multivariate imputer, scaling the data usually helps if imputer is not tree-based. But they might already be  applying normalization on their own, if so I think they would mention it in the docs.",1,v5b3ns,"Since DataWig’s Simple Imputer is basically a neural network, and since these usually function best on scaled data, I was wondering whether it is sensible to scale my data before doing the imputation, or whether DataWig has been optimized to handle unscaled data. Any advice would be appreciated!",deeplearning,2022-06-05 04:05:52,2
"For gpu it's entirely dependent on what kind of workload you will be working with, so I can't help you on this one. Although personally I'd always pick up a 3080 (12GB variant) if it's available at decent price.

If you aren't doing a lot of preprocessing and cpu bottleneck is not an issue then any cheap cpu will do, see if you can find an amd ryzen 5 5600 (non X) 

Likewise, motherboard is not super important, they are mor3 concerned with cpu/ram overclocking and i/o. Any b550/x470/b450 will do, as long as the x470/b450 has an updated bios to use a ryzen 5000 cpu.

The one thing you should never cheap out is your power supply. If a fault in the power grid happens, a good psu will sacrifice itself for the whole PC whereas a cheap one might just blow up. RM750x is a good option

No comments on ram, it's a good and cheap kit

Have a look around on the secondhand market for used PC cases.

Next AMD platform and nvidia GPU are due in Q3 2022, new nvidia gpus are said to have 2x performance uplift compared to the current generation. You might want to hold off building a PC right now to either wait for people to upgrade and sell their parts at cheaper prices, or to jump on the new nvidia gpu instead.",3,v58gyg,,deeplearning,2022-06-05 00:39:45,7
"I'm assuming you built the model in python, just figure out when want to call the model and the simplest way would be just load the model and perform inference. 

Django is just a python server, you can freely call whatever python you want e.g. `import torch` etc",3,v5bmn5," I built a deep learning model that classify texts into two categories positive and negative, the negative is any text that contains racial slurs or any kind of threatening or doxing, I want to integrate it with another project it is similar to twitter, I want to categorize posts with the model to make it easier for the admin to review them but I don't have any idea how to merge these two projects together, any ideas or references on how to do that",deeplearning,2022-06-05 04:42:12,5
"It's certainly possible. Things to consider - it may be the first and the last PCI-e slots for many ATX motherboards - worth checking GPU would still fit the last slot, PSU and MB connectors wise. The last slot may also be only PCI-e 4x. As an option - to have one GPU in the MB slot and another moved to the free well ventilated space in a case with the raiser cable - I have a similar setup on one of the rigs.

A reasonably good option would be using the raiser cables and the open case, similar to the ones miners are using. You may even have a 4 GPUs rig, with well cooled and reasonably quiet GPUs (with reduced power limit so PSU can handle it).",3,v53rwd,"Hi friends,

I plan to build a dual 3090 deep learning rig but I found that most people on youtube are using watercooling. I don't want to get into the risk of water leaking and also the extra complexity and cost.

I am wondering if it is possible to do dual 3090 without watercool. For example, getting a good motherboard and inserting those two GPUs with an empty PCI-e slot in-between. Also, getting more fans for better ventilation.

Is it possible? Any previous successful experiences? Happy to learn anything from you all. Thanks.

\- Alex",deeplearning,2022-06-04 19:20:43,13
"Yes, you should be able to do it by replacing the the backbone and training the other parts again. The results may be better or worse than you expected. See: https://github.com/faustomorales/keras-ocr/issues/113",3,v58xya,"I have been studying deep learning and I observed that new architectures like CRAFT for text detection still use VGG-16 as its backbone instead of newer and more efficient models like EfficientNets, Inception, etc.   


And a follow up question: Can I change models used by those architectures depending on my needs like changing the VGG part of CRAFT to be efficientNet so that I can use it in more resource critical devices?",deeplearning,2022-06-05 01:18:27,2
"I don't think there's any way to pool GPU memory that works 100% the same as a single GPU that is twice as fast with double the memory. It would be wonderful if there was. Plenty of scenarios do let you achieve something like this result, but it's not as simple as a single flag you set or something.

As other's have pointed out you can do things to make it work, sort of, in some scenarios.

You may also want to look at stuff like [Microsoft DeepSpeed](https://github.com/microsoft/DeepSpeed)",2,v4qlqj,"I’m planning to rent a machine with two A100-SXM4-80G but I’m a bit confused on the multi-gpu setup. I hear that it can combine the VRAM so that it becomes one GPU of 160G where we can double the batch size. But when I checked out Pytorch multi-gpu code samples, the closest I can find is DataParallel, which splits the data to x amount of small batches to train on each gpu and then merge the result. Am I looking at the right example? If so, is that all it can do? Wouldn’t the merging hurt the result? I mean I’m really looking to actually have the gpu memory combined so I can use it as one GPU.",deeplearning,2022-06-04 07:58:49,7
"Code for https://arxiv.org/abs/2206.00272 found: https://github.com/huawei-noah/CV-Backbones

[Paper link](https://arxiv.org/abs/2206.00272) | [List of all code implementations](https://www.catalyzex.com/paper/arxiv:2206.00272/code)



--

To opt out from receiving code links, DM me",1,v4pjvc,"[Vision GNN: An Image is Worth Graph of Nodes](https://arxiv.org/abs/2206.00272)
Network architecture plays a key role in the deep learning-based computer vision system. The widely-used convolutional neural network and transformer treat the image as a grid or sequence structure, which is not flexible to capture irregular and complex objects. In this paper, we propose to represent the image as a graph structure and introduce a new Vision GNN (ViG) architecture to extract graph-level feature for visual tasks. We first split the image to a number of patches which are viewed as nodes, and construct a graph by connecting the nearest neighbors. Based on the graph representation of images, we build our ViG model to transform and exchange information among all the nodes. ViG consists of two basic modules: Grapher module with graph convolution for aggregating and updating graph information, and FFN module with two linear layers for node feature transformation. Both isotropic and pyramid architectures of ViG are built with different model sizes. Extensive experiments on image recognition and object detection tasks demonstrate the superiority of our ViG architecture. We hope this pioneering study of GNN on general visual tasks will provide useful inspiration and experience for future research.",deeplearning,2022-06-04 07:03:59,2
"I started watching tutorials on youtube. Then, tried to replicate their works after that I tried to modify some existing codes which I needed for my research. Eventually, along the way, I was able to learn little by little what and how the part of the network works.",1,v4hwwo,"Hi, I am looking to get started on Deep Learning and would like to know methods or learning pathways that worked for you. I was thinking of going through a [deeplearning.ai](https://deeplearning.ai) course as my first step. Any help at all is appreciated. Tell me about your learning pathways please!",deeplearning,2022-06-03 22:20:00,2
"Link to the GitHub: https://github.com/johnGettings/LIHQ  
  
Let me know if you have any questions.",2,v48feg,,deeplearning,2022-06-03 13:36:51,1
"Can you overlay different paint shades on walls and simulate different kinds of lighting? 

There are apps out there for this, but they don't show you what the colors look like with different lighting conditions. 

I'm not a CV guy, but this seems like it could be neat.",1,v4bg0r,"So I'm trying to think of some good Deep Learning projects that could be fun/interesting for home improvement.. Just trying to do some fun side projects that aren't the typical ""Attempt some Kaggle projects w/ Deep Learning""

Anyone have any good personal projects they've worked on that might be useful/interesting around the house?

Some of my own thoughts:

* Voice control IoT devices
* Homemade security w/ face detection
* Have auto-generated ""paintings""",deeplearning,2022-06-03 16:04:17,1
"probably something like arcface. why? because it manages to split milions of identities using that hypersphere approach, while being able to train it distributely. it has many supported frameworks like pytorch and onnx and is readily able to be deployed in tensorRt, tflite, openvino…",1,v4cj1k,"I am working on a university project, and I am going with face recognition. So what is the most suitable algorithm to use and why?",deeplearning,2022-06-03 17:01:49,7
How is this different that 100s of other tutorials?,3,v3nwlo,"[Face Image Generation using Convolutional Variational Autoencoder and PyTorch](https://debuggercafe.com/face-image-generation-using-convolutional-variational-autoencoder-and-pytorch/)

[https://debuggercafe.com/face-image-generation-using-convolutional-variational-autoencoder-and-pytorch/](https://debuggercafe.com/face-image-generation-using-convolutional-variational-autoencoder-and-pytorch/)

&#x200B;

https://preview.redd.it/aduqh3ih4b391.png?width=1200&format=png&auto=webp&s=7b126160486483f54005cec0c28b152d9d1df8bb",deeplearning,2022-06-02 18:14:34,1
"Looks interesting, quick search gave me these;  

https://arxiv.org/abs/2102.04306  

https://vitalab.github.io/article/2021/06/25/transUnet.html  

https://www.youtube.com/watch?v=8tBVvUXHDcA  <= use CC with English

https://www.youtube.com/watch?v=jKBJITQ8xJY",1,v3i66z,"Is there any video or notes that explains TransUnet in detail as well as the internal working of transformers in images. Or does anyone know how it works ? 
Thanks in advance",deeplearning,2022-06-02 13:32:17,1
"What would the goal be here? If b is a feature I would recommend just letting the model learn how to handle that. If its something else, it may make more sense to have different models all together and only hand the input into the correct one. Could you give a little more info as to why you want to do this?",3,v34l37,"Hi \*,

I'm working on a TensorFlow model, and I would like to have different logic in the model depending on the input. My question is if it's possible to have a ""dispatch"" layer in TensorFlow that depending on the input uses one model or another?

**Simplified example:**

Imagine your input has two fields a: int and b: Optional\[int\]. This means b can be defined or not.

If b is not None you want to execute some layers, and if it's None you want to execute some other layers.

My idea was to have a dispatch layer that depending on the input executes one branch of the model or another one. Something like the architecture in the image below:

&#x200B;

https://preview.redd.it/j54smbzqa6391.png?width=1552&format=png&auto=webp&s=9bb06a521a9b3acfcc5a85a6cc543ed9b039ea85

With this architecture, you'll have one only model (which is easier to deploy) with ""polymorphism"" depending on the input.

Thank you very much for your help :)",deeplearning,2022-06-02 02:01:59,2
"This is known as diffusion, it's currently used for state of the art media synthesis with transformers. 

Search out Disco Diffusion for image examples and colab notebooks to try.",4,v32n7b,"I am trying to build a model that take noisy images - blurred by Gaussian noise- and output the unblurred version of the image. 

I saw a lot of literature and blogs but have anybody try to build such a model before? If so, do you have any suggestions that you had most success in terms of low MSE for example and low time per epoch (relatively speaking ofc)?",deeplearning,2022-06-01 23:39:42,7
"It would be an interesting experiment. My prediction is it would lead to mode collapse right away.

Also, you have to map somehow the output image to the latent state, and there is no unique way to do that.

If you really think about it, you are just describing an autoencoder.",1,v2y5xp,"If one connects the output of an adversarial generative network to the input of the same network:

**Will it free run or will it lock up in a single state?**

To explain in more detail: If the output of a GAN is fed into the input and allowed to generate one picture, then feed the resulting picture to the input and repeat over an over, will it free run and keep generating random pictures, of random subjects, or will the first picture generated, then force the generation of the exact same picture, or possibly a closely  related picture and thus ""lock up"" in a fairly or exactly constant state?

Are there any papers on this? What is the term for this so I can find the papers?

Follow on question: If I inject some level of randomness in the latent layer (the bottle neck of the GAN) will it then not lock up? How much randomness is needed to prevent lockup?",deeplearning,2022-06-01 19:12:50,2
VGG-like,3,v3b4go,"    input_shape = (BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, CHANNELS)
    n_classes = 10
    
    model = models.Sequential([
    resize_and_rescale,
    layers.Conv2D(32, kernel_size = (3,3), activation='relu', input_shape=input_shape),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64,  kernel_size = (3,3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64,  kernel_size = (3,3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dense(n_classes, activation='softmax'),
    ])
    
    model.build(input_shape=input_shape)

&#x200B;

https://preview.redd.it/4whz966k58391.png?width=472&format=png&auto=webp&s=e5a2042ea155cd806f0059f78256975ef975fc7c",deeplearning,2022-06-02 08:14:02,4
"Wav2vec2 is definitely up there in terms of performance, and likely still SOTA on a dataset or two. Check out “papers with code” to get a good view of current leaders for a variety of tasks/datasets",1,v2ytid,"I'm trying to build ASR model for one of the low-resource languages. I've come across DeepSpeech. However, in one of the blog of wav2vec they say that they beat DeepSpeech results. Is wav2vec SOTA?",deeplearning,2022-06-01 19:46:44,1
"Yes and no: The best models nowadays are a mix of CNNs and Transformers, CNNs are very good when it comes to learning the gist of an image, while Transformers are really good at learning local features, the downside of Transformers for image classification is that they're very data hungry, some progress was made to overcome this issue.

&#x200B;

https://github.com/facebookresearch/ConvNeXt",16,v2ew58,"As far as I know the transformer architecture is the de facto standard for all NLP tasks since 2017. Since ""attention is all you need"" and the Bert model came out, everyone started using transformers, to push new SOTA in NLP tasks.

I see similar things now happening in the computer vision field as well, e.g.[https://paperswithcode.com/sota/image-classification-on-imagenet](https://paperswithcode.com/sota/image-classification-on-imagenet)[https://paperswithcode.com/sota/semantic-segmentation-on-cityscapes](https://paperswithcode.com/sota/semantic-segmentation-on-cityscapes)[https://paperswithcode.com/sota/object-detection-on-coco](https://paperswithcode.com/sota/object-detection-on-coco)All top models now use transformers in some way.

I found the paper ""AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE"" as a very influential one.

So what do you think, is this the end of CNNs? What other papers are important in the CV field, with regards to Transformers ?",deeplearning,2022-06-01 04:36:30,13
"The choice of the paper really doesnt matter much. Just use something that fascinates you. It will make it less tedious for you and youll be able to talk about the topic more elaborately.

However, as a company, I would like you to follow software engineering best practices and care about others being able to reproduce your results easily. Also, dont just create a messy repo with your implementation but try to integrate it with a popular framework, such as huggingface transformers. It will show that you are able to understand an architecture or philosophy and contribute to it. In the best case, create a pull request with your changes. If it acutally gets merged, you will be one of the contributors to that library forever.",7,v2mndc,"Hey all,

So I'm trying to land an internship in NLP it's the field I'm interested in I have a couple years of experience in ML and a background of biostatistics but I want to work more with DL in general and NLP in particular.

I talked to a couple companies and the response has been good but they all suggested that they like to see an implemented a NLP paper, I know it's subjective but if you would have to list the top 3 papers you'd like an employee to your company to have showcased in their portfolio, what would they be?",deeplearning,2022-06-01 10:41:02,4
"Code for https://arxiv.org/abs/2002.05709 found: https://github.com/sthalles/SimCLR

[Paper link](https://arxiv.org/abs/2002.05709) | [List of all code implementations](https://www.catalyzex.com/paper/arxiv:2002.05709/code)



--

To opt out from receiving code links, DM me",1,v2tc6l,"I'm currently reading the [2020 SimCLR paper](https://arxiv.org/abs/2002.05709), and was confused about the difference in performance they observe when training a linear classifier on the learned representations vs. fine-tuning the network with some amount of labeled data. If I'm reading Table 6 and Table 7 right (also see Figure B.5), the linear evaluation consistently outperforms fine-tuning. 

I find this very counterintuitive. In both instances we're starting with the representations learned during self-supervised training. The linear classifier just needs to work with whatever linear representations arose during training, whereas fine-tuning can further modify the weights of the entire network using some labeled data. The latter seems inherently more powerful, so why does it perform worse? 

I am seeing that they trained for more epochs when doing linear evaluation vs. fine-tuning (90 vs 60 epochs), perhaps that alone explains it? If so, I'm still unsure why this would be the standard approach. Any insight or clarification would be greatly appreciated, thanks in advance!",deeplearning,2022-06-01 15:26:50,3
"I guess it depends on what your model is trying to accomplish. In general, calculating frame wise differences between subsequent feature vectors can help give a simple MLP context. I don’t see how adding velocity to your feature set would hurt the performance of your model.",1,v2v6yk,"if I have displacement and x position as features to my model, would deriving velocity help with my MLP model? I believe so, but on the other hand it's literally and mathematically a derivative of displacement, so I dont know if that feature would be too similar to displacement.",deeplearning,2022-06-01 16:47:35,1
"Can you get access to the MIMIC dataset? It has an enormous wealth of information on a wide variety of clinical data. I've trained a few models on it myself. It will certainly test your data processing skills, however.",2,v2n6xj,"hi everyone, i've tried all possible sites i can in order to get the diseases name, description and their symptoms. if any of you know any good site that contains the above described details in csv format or even if its scrapable please let me know. i really need it.",deeplearning,2022-06-01 11:03:33,3
"Why would you use fewer real images if you did augmentation?

I'd do 6k +augmentation training, 2k validation and 4k test. Don't use augmentation in your validation or test data. How much and what kind of augmenting you'll have to determine based on performance",1,v2qr3m,"Have a dataset that has normal sound data, which is almost similar to each other (sounds slightly different based on the load but very similar). I want to use it to train for anomaly detection using ResNet. Because the data looks almost similar when turned into a spectrogram, which would be the best approach to get good results? 3000 normal images plus 4 augmentation techniques on that which would make 12k images or original 12k images without augmentation? Or some other approach?

I tried some techniques, but some audio sounds failed to recognize anomalies. Any ideas/approaches? Thank you!!",deeplearning,2022-06-01 13:36:13,2
Self promotion plz remove @mod,1,v27vu4,,deeplearning,2022-05-31 20:44:58,1
"Well how else would you train on a task in a non-supervised manner? If you have true/fake labels then you don't need a discriminator, but that's supervised learning. It's not really feasible for the amount of data you need to train an adversarial generative model.",4,v1rxo0,"Background: I have a dataset of essays and some classes associated with them, the dataset is initially imbalanced, and collecting more data is not an option, so I was looking for Text Generation methods, I am trying to generate text samples that are almost identical to the original samples, so provided, I add keywords and classes as inputs to the generative model,  


Is it necessary to add a discriminative model that predicts whether a sample is real or fake?",deeplearning,2022-05-31 07:46:24,1
Nah,9,v1kl40,,deeplearning,2022-05-31 00:15:08,6
"Augment data better, maybe use contrastive learning...",1,v1n3bk,"I used the gtsrb dataset  train a yolov5 model

but it can only detect traffic signs when it is close to the camera.

Any tips of how i can make it detect traffic signs at a distance?

also, sometimes it is detecting a person as a traffic sign..any idea why?",deeplearning,2022-05-31 03:23:02,1
"In theory, if there is any flaw in the design of RSA and it's possible to create new signatures without the private key, then it would be possible to create a model to predict what the signature would be on arbitrary values.  But...I VERY much doubt that any model you try and create is going to be able to do such a thing.  In the same way that, in theory, you can create a model to fully emulate a human and human intelligence (yet we are not currently capable of it), it may be possible to do this (and we aren't capable of it).",6,v16baf,"Hi guys, I was working on a cryptographic problem for which I need to make a GAN learn how to generate RSA encodings.

Example input: b'GxHqH5c3fB/TEQispLvYByl5iPmiYLFq2ZyDQqfNbOpt4UOUWOOI4ZyAd0dWHSdTBhGmf8Psa9Ivo6tEbtAu1BZKIE1m1FwGL38FO6HgmTQnpeoJTqPaadq9wdax7TF3XZJFBeqNRChIkePcEt3yComXEKA8gOY2FnlSFo8jTRY='

Example output:

b'CRMV1iuaCpuQgxctqob1CIoUbm3twa85Ahi8HKrm7gZjXS3NA5rW60/XNn5uWZ1OAUBXQAIn3e7m3s1Bs6kOV7MjQyqcf5M8KPx+TtifUbNgO8ENUboCqOXOv3/aLwaAbNvRaCucOox6sh70rWXKUzVD6jKZZ7sBWC2VDnklO7Y='

My main goal is to make the model overfit the training data as well as it should be able to generate new encodings for unseen data. Is this thing even possible? If yes what kind of GANs should I use. Also, what would be a good generator and discriminator network in this case? Any help would be appreciated. Thanks.",deeplearning,2022-05-30 11:17:20,5
"If learning unreal is really important to your agenda go ahead, but if the objective is to learn RL, I'd also recommend to take a look at petting zoo reinforcement learning environments.",1,v163hs,"Hi everyone,

I'm planning to learn Unreal Engine 5 in the summer and I would like to create a deep learning model using Unreal Engine. A project that came to my mind was to create a 3rd person shooting game which has two AI agents and they have to try and learn the best strategy to beat the other agent in the game. I think that helps me learn about Reinforcement learning as well as Unreal Engine environment but I would love to see what other project ideas you may have that have something to do with deep learning and Unreal Engine.

&#x200B;

Thank you",deeplearning,2022-05-30 11:07:14,3
"2080TI is better specially on FP16 [I got a RTX3060]
Also, RTX3060 memory bandwidth is lower even though it has extra 1 GB VRAM.

https://www.techpowerup.com/gpu-specs/geforce-rtx-2080-ti.c3305  
https://www.techpowerup.com/gpu-specs/geforce-rtx-3060.c3682  

RTX3060 = 12.74 TFLOPS 360.0 GB/s  
GTX1080Ti = 11.34 TFLOPS 484.4 GB/s  
RTX2080Ti = 13.45 TFLOPS 616.0 GB/s  [FP16 26.90 TFLOPS ]

Rough benchmark :
https://www.reddit.com/r/MachineLearning/comments/ut30ck/d_my_experience_with_running_pytorch_on_the_m1_gpu/",4,v10v9e,"Hi, I got a 2080ti 11gb (blower design :( ) and a gigabyte OC 3060 12GB for roughly the same price. I'm Wondering which to keep for my ML workstation, 

Pros 2080ti: Much higher memory bandwith, more tensor cores (though first gen.)

Pros 3060: Lower TDP, more silent, third gen CUDA and tensor cores, slighlty higher RAM  

Can you help? Need decide quick. I mainly train my own GANs. I am unsure wether the higher memory bandwith justifies the lower gen, but more, CUDA and Tensor cores.

supposedly the speedup is quite high between the core generations from what I've heard. In a ML Setting, is the 2080ti more comparable to a 3070 or should I go with the 3060 for efficiency?

Thanks in advance.",deeplearning,2022-05-30 07:01:20,10
"I am thinking that you can make your version of torch somewhat a challenging way of learning pytorch, or perhaps something to use in interviews to trick/challenge interviewees ...  also make sure to write the comments in English not French",1,v14o0l,"As an exercise for a ML course I took last semester I had to do a reimplementation of some PyTorch functions using only NumPy.

Now that the semester is over, I'd like to develop this project over the summer, but first I want to hear your criticism/suggestions about anything in the project, don't hold back, bring me down.

I will upload a demo notebook soon, I just need to translate it.

Here is the [link](https://github.com/Balocre/pyretorch), let me know what you think about it in the comments.",deeplearning,2022-05-30 10:01:15,2
Brain tumors are rare. What is the percent of positive samples in your dataset? Your model may have learned to output 0 for everything.,28,v0o3vz," I am doing a project regarding semantic segmentation. I am achieving 98 accuracies and 0 loss on the first epoch? Why is it not working?  

    import os
    from tqdm import tqdm
    import cv2
    import numpy as np
    from tensorflow.keras import layers, models, losses
    from tensorflow import keras
    from sklearn.model_selection import train_test_split
    from keras.preprocessing.image import ImageDataGenerator
    i_size = 256
    x = []
    y = []
    
    folderpath = r'C:\Users\kesch\OneDrive\Documents\MATLAB\tumorpng3'
    for i in tqdm(os.listdir(folderpath)):
        img = cv2.imread(os.path.join(folderpath, i), 0)
        img = cv2.resize(img, (i_size, i_size))
        x.append(img)
    # Loads X values
    
    folderpath1 = r'C:\Users\kesch\OneDrive\Documents\MATLAB\tumormask1'
    for i in tqdm(os.listdir(folderpath1)):
        img = cv2.imread(os.path.join(folderpath1, i), 0)
        img = cv2.resize(img, (i_size, i_size))
        y.append(img)
    # Loads Y values
    
    x = np.array(x)
    y = np.array(y)
    
    x = np.array(x)/255.
    y = np.array(y)/255.
    
    
    x_train,x_test,y_train,y_test = train_test_split(x, y, test_size=0.2,random_state=2)
    #Split Train and Test
    
    image_x = dict(rotation_range=15,
    shear_range=0.1,
    zoom_range=0.1,
    width_shift_range=0.15,
    height_shift_range=0.15,
    horizontal_flip=True)
    
    mask_y = dict(rotation_range=15,
    shear_range=0.1,
    zoom_range=0.1,
    width_shift_range=0.15,
    height_shift_range=0.15,
    horizontal_flip=True)
    
    image_data_generator = ImageDataGenerator(**image_x)
    mask_data_generator = ImageDataGenerator(**mask_y)
    
    x_train = x_train.reshape(2451, 256, 256, 1)
    y_train = y_train.reshape(2451, 256, 256, 1)
    
    image_data_generator.fit(x_train, augment=True, seed = 24)
    mask_data_generator.fit(y_train, augment=True, seed = 24)
    
    
    unet = models.Sequential()
    unet.add(layers.Conv2D(64, (3,3), activation='relu', padding='same', input_shape=(i_size, i_size, 1)))
    unet.add(layers.MaxPool2D((2,2), padding='same'))
    unet.add(layers.Conv2D(128, (3,3), activation='relu', padding='same'))
    unet.add(layers.MaxPool2D((2,2), padding='same'))
    unet.add(layers.Conv2D(256, (3,3), activation='relu', padding='same'))
    unet.add(layers.MaxPool2D((2,2), padding='same'))
    unet.add(layers.Conv2D(512, (3,3), activation='relu', padding='same'))
    unet.add(layers.MaxPool2D((2,2), padding='same'))
    unet.add(layers.Conv2D(1024, (3,3), activation='relu', padding='same'))
    unet.add(layers.Conv2D(512, (3,3), activation='relu', padding='same'))
    unet.add(layers.UpSampling2D((2,2)))
    unet.add(layers.Conv2D(256, (3,3), activation='relu', padding='same'))
    unet.add(layers.UpSampling2D((2,2)))
    unet.add(layers.Conv2D(128, (3,3), activation='relu', padding='same'))
    unet.add(layers.UpSampling2D((2,2)))
    unet.add(layers.Conv2D(64, (3,3), activation='relu', padding='same'))
    unet.add(layers.UpSampling2D((2,2)))
    
    unet.add(layers.Conv2D(1, 1, padding=""same"", activation = ""sigmoid""))
    
    unet.compile(optimizer='Adam', loss=""categorical_crossentropy"", metrics=[""accuracy""])
    
    model_history = unet.fit(x_train, y_train,
    epochs=100,
    verbose = 1,
    batch_size = 32,
    validation_data = (x_test, y_test))
    
    unet.summary()",deeplearning,2022-05-29 17:25:22,36
I can't imagine why you would do this rather than increase your batch size to use all of the VRAM and tune your data loading and preprocessing pipeline to maximize utilization.,6,v0dyas,"Hello everyone,

I was just wondering if anyone has experience using the same GPU for training two (or more) models at the same time. Does it work? Are there any drawbacks?",deeplearning,2022-05-29 08:48:07,12
"Wait for a 4090, then buy the 3090",33,v0abti,"Hi everyone, I am planning to buy a 3090 (assuming I can purchase it at MSRP) as an upgrade for my 2060 for Deep Learning / 3D modelling & game dev, but not so sure if it's worth waiting for the release of the 4090. I'm also considering the option of buying twos. Main areas of research is NLP (requirement for increase in VRAM), signal processing and CV. I have a subscription of Colab Pro but it doesn't serve the needs at the fullest and I have tried Colab Pro+ which is quite good although large data persistence / having non-interrupted runtime is deal breaker for me.

Is it worth the wait?",deeplearning,2022-05-29 05:38:36,15
Speech has high-variance such as speaker and background information which can become detrimental to the training objective. Quantization is a simple way of discarding those elements by retaining only a pool of learnable feature representations (essential aka denoised) by maximizing the entropy (harder to pick from the best) which makes up the most alike discrete (the codebook is quasi-finite) representation to the original.,2,v0llwm,"Hi!
I am currently reading a lot about wav2vec and self-supervised learning but I have a problem. I don’t understand why they say that latent speech representations are continuous and need to be discretized using some code books to obtain these so called « quantized feature » ?
Thank you in advance for your help.",deeplearning,2022-05-29 15:06:40,2
"I’m currently implementing my own convolutional VAE, here’s what I think:

1. You can omit the conv2d. My guess is that it’s used to add network depth. This codebase looks fairly usable, so you could probably run tests with/without conv2d to see performance differences.

2. A normalization scheme you can use is scaling your pixels to be centred around 0 and make them range from -1 to 1. Therefore, the tanh activation works for this purpose. Alternatively, if you scale your images to be between 0 and 1, sigmoid can be used.",1,v0g57z,"I am a beginner in VAE implementation and I am currently going through codes [here](https://github.com/AntixK/PyTorch-VAE/blob/master/models/vanilla_vae.py).

I had a small question on it and thought that someone in this subreddit might be able to help me out.

* In the self.final\_layer, why is nn.ConvTranspose2d created with both in\_channels and out\_channels as hidden\_dims\[-1\] followed later by nn.Conv2d? Can we not have out\_channels as 3 directly in nn.ConvTranspose2d and omit nn.Conv2d?
* Also, I could not understand why we have nn.Conv2d with nn.Tanh() as activation function.

Thank you!",deeplearning,2022-05-29 10:34:18,5
"As it often the case, it depends.

Two 3060s have vastly more memory than a single 3080, but fewer CUDA cores (even combined), and slightly slower memory speed. They also take up an extra slot in case you ever wanted to expand. It is also dependent on market price for each. At MSRP two 3060s and one 3080 should cost about the same, cards haven't been selling at retail for a very long time.

Take a look at your models, data size, future plans, price, etc. and then make the call.",13,uzxztu,Which is better?,deeplearning,2022-05-28 15:44:03,12
"I think this would sum it up better:

Pick a laptop in terms of things such as usability and preference; for instance, battery life, keyboard, etc. I say this because i dont believe in deep learning on laptops tbh. 

With that said, one could argue that without a gpu, one may have difficulty testing code. To that, I say to use colab.

Obviously, there will be instances where this may not apply such as if one also wants to game, but doesnt want a desktop and a laptop; however, this comment is really meant to argue against the recommendation for someone to get a laptop for dl in general.",2,v09ibs,"Hey there  👋   


I recently had to go through the process of choosing a new laptop! Since I was very thorough this time, I figured I'd write a blogpost about it, with my personal example of a deep learning laptop:  
[https://frgfm.github.io/tech-brewing/choosing-a-new-laptop](https://frgfm.github.io/tech-brewing/choosing-a-new-laptop)  


Especially for machine learning, I felt  like there is a lot of overcomplicated information for an accurate laptop selection. In many ways, it's incomplete and I have no professional qualification about hardware, but I tried to make the topic as accessible as possible!

Feedback is always welcome, I'd love to improve  😁   
Cheers!",deeplearning,2022-05-29 04:47:51,11
"This is a convolutional VAE, not technically a ""vanilla VAE"" which would be an MLP (i.e. using nn.Linear/Fully connected layers), but one of the most popular uses for VAE is image generation, so convolutional VAEs are very common.

They're using strided convolution rather than pooling. This is likely for symmetry reasons, so that the ConvTranspose layers are structurally the inverse of the Conv layers.

BN standardises the inputs to a mean of zero and standard deviation of one. Learnable gamma and beta mean that the output of BN is set to a mean of beta and a standard deviation of gamma.",3,v06xse,"I am referring to the code in the link [here](https://github.com/AntixK/PyTorch-VAE/blob/master/models/vanilla_vae.py) for VAE code. I have the following questions and confusions:

1. I wanted to understand why Convolutional neural network is used in creating VAE. Is it to understand features of the data well so that a good low dimensional summarized latent representation can be created? Are there any other reasons or details connected to it?
2. Also, I couldn't see pooling being used. Is there any specific reason as to why pooling is not used after convolutional layer?
3. I am also confused regarding when in the process is the gamma and beta learnable parameters of Batch Normalization learnt by the network. Could anyone please shed some lights on this too?

Thank you!",deeplearning,2022-05-29 01:37:47,2
"I'm not familiar with plant disease, but I don't think you would need segmentations to start with. Could you not just create a classifier that will say if the plant if the diseased or not?",1,uzp1pg,"I am using U-Net for plant disease detection.I am new to deep learning and computer vision. I need to know the use of image masking in image segmentation using U-Net, why it is needed? What kind of Masking do we need to feed to the architecture?

Currently, I'm feeding the masking images generated via open cv HSV format to detect colours from the images so that the background and foreground are detected. Is it the correct approach? Or Do we also need to Target the disease part for masking ( which is difficult I making).",deeplearning,2022-05-28 07:59:06,5
This is sick 💎,5,uyrxzg,,deeplearning,2022-05-26 23:29:11,12
"Is it a pretrained resnet and if so, what was it trained on?",3,uyjgb9,"Using Mel spectrograms for motor sounds, 6000 images each in normal and anomaly category. Used a few augmentation techniques on the training set. I am assuming the data might not be complex enough? Any light you can shed on what's going on here?",deeplearning,2022-05-26 15:36:49,22
"Please format your post, it's difficult to read your non-code text when it's put into a code cell.",4,uycerd,"The datasets I am working with correspond to individual time series signals. Each signal is unique, with differing total number of data points. here I want to simulate dataset A using dataset B.

Dataset A :

    [1.16630985]
    (15000, 1)

Dataset B :

    [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.
       0.   0.   0.   0.   4.   0.   2.   0.   5.   8.   3.   2.   0.   0.
       1.  26.  83.  98.  70.   6.   4.   0.   0.   3.   1.   1.   2.   0.
      10.   8.   8.   8.   3.   3.  82. 116. 120. 103.  12.  28.   4.   0.
       0.   0.   1.   0.   0.   1.   1.   6.   5.  17.  76.  95. 101.  90.
       7.   8.   0.   0.   0.  49. 102. 109. 104.  21.   5.   3.   0. 118.
      76.  10.   4.  18.   1.   0.   0.   0.   0.   6.   0.  11.   0.   0.
       0.   0.   1.   0.   0.   0.   0.   0.   0.   2.   0.   0.   0.   5.
       6.   0.   0.   0.   3.   0.   0.   0.   2.   0.   2.   0.   0.   0.
       0.   6.   3.   8.   0.   0.   0.   8.   4.   6.   0.   0.   4.   2.
       3.   0.   0.   0.   0.   0.   0.   0.   0.   5.   4.   1.   4.   1.
       0.   0.   0.   0.   0.   0.   1.   0.   0.   5.   9.   9.   3.   0.
       0.   0.   0.   0.   0.   3.   0.   0.   0.   0.]
    (15000, 178)

Spliting Dataset Code:

        x = SmartInsole[:,0:178]
        y = Avg[:,0]
        y = y.reshape(-1,1)
    
        scaler_x = MinMaxScaler()
        scaler_y = MinMaxScaler()
        scaler_x.fit(x)
        xscale = scaler_x.transform(x)
        scaler_y.fit(y)
        yscale = scaler_y.transform(y)
    
        X_train, X_test, y_train, y_test = train_test_split(xscale, yscale, test_size=0.25, random_state=2)

The dataset after splitting and normalized:

        [0.83974359 0.81818182 0.60264901 0.10457516 0.         0.
         0.         0.         0.         0.66878981 0.7654321  0.77439024
         0.05031447 0.18674699 0.         0.         0.         0.
         0.83892617 0.85620915 0.8590604  0.77852349 0.57236842 0.35333333
         0.         0.         0.         0.05217391 0.6835443  0.85064935
         0.72955975 0.08275862 0.         0.         0.         0.
         0.         0.73758865 0.84868421 0.76923077 0.69230769 0.53472222
         0.53571429 0.65714286 0.49450549 0.47747748 0.72592593 0.77707006
         0.86928105 0.80519481 0.31333333 0.         0.0516129  0.
         0.         0.         0.         0.39316239 0.35036496 0.07086614
         0.38392857 0.57843137 0.58181818 0.68376068 0.74100719 0.84868421
         0.81879195 0.80519481 0.14       0.         0.         0.
         0.         0.         0.83802817 0.89189189 0.88811189 0.48979592
         0.         0.         0.         0.         0.         0.33793103
         0.         0.         0.         0.         0.         0.9929078
         0.97222222 0.81118881 0.45890411 0.         0.         0.
         0.         0.63551402 0.97810219 0.95172414 0.95205479 0.88356164
         0.94630872 0.40384615 0.         0.         0.         0.97222222
         0.9862069  0.96478873 0.76510067 0.52       0.24113475 0.
         0.         0.         0.21568627 0.88970588 0.94594595 0.89864865
         0.08510638 0.37662338 0.0979021  0.         0.         0.
         0.46153846 0.92517007 0.74590164 0.48571429 0.05882353 0.19847328
         0.11428571 0.07857143 0.11510791 0.56375839 0.80794702 0.87012987
         0.81045752 0.21527778 0.         0.         0.         0.
         0.         0.         0.         0.         0.         0.
         0.         0.07042254 0.21052632 0.62745098 0.75471698 0.80503145
         0.78980892 0.         0.         0.         0.         0.
         0.         0.55357143 0.66878981 0.67272727 0.17682927 0.
         0.         0.         0.         0.         0.         0.
         0.         0.         0.         0.        ]
        [0.59662633]
        (3000, 178)
        (3000, 1)

I am working with Keras and trying to fit a resnet50 to the data just to evaluate it. Below is the my resnet model structure:

Below is identity blok:

        def identity_block(input_tensor,units):
    	""""""The identity block is the block that has no conv layer at shortcut.
    	# Arguments
    		input_tensor: input tensor
    		units:output shape
    	# Returns
    		Output tensor for the block.
    	""""""
    	x = layers.Dense(units)(input_tensor)
    	x = layers.BatchNormalization()(x)
    	x = layers.Activation('relu')(x)
    
    	x = layers.Dense(units)(x)
    	x = layers.BatchNormalization()(x)
    	x = layers.Activation('relu')(x)
    
    	x = layers.Dense(units)(x)
    	x = layers.BatchNormalization()(x)
    
    	x = layers.add([x, input_tensor])
    	x = layers.Activation('relu')(x)
    
    	return x

Below is dens\_block:

    def dens_block(input_tensor,units):
    	""""""A block that has a dense layer at shortcut.
    	# Arguments
    		input_tensor: input tensor
    		unit: output tensor shape
    	# Returns
    		Output tensor for the block.
    	""""""
    	x = layers.Dense(units)(input_tensor)
    	x = layers.BatchNormalization()(x)
    	x = layers.Activation('relu')(x)
    
    	x = layers.Dense(units)(x)
    	x = layers.BatchNormalization()(x)
    	x = layers.Activation('relu')(x)
    
    	x = layers.Dense(units)(x)
    	x = layers.BatchNormalization()(x)
    
    	shortcut = layers.Dense(units)(input_tensor)
    	shortcut = layers.BatchNormalization()(shortcut)
    
    	x = layers.add([x, shortcut])
    	x = layers.Activation('relu')(x)
    	return x

Resnet50 model:

     def ResNet50Regression():
    	Res_input = layers.Input(shape=(178,))
    	width = 16
    
    	x = dens_block(Res_input,width)
    	x = identity_block(x,width)
    	x = identity_block(x,width)
    
    	x = dens_block(x,width)
    	x = identity_block(x,width)
    	x = identity_block(x,width)
    	
    	x = dens_block(x,width)
    	x = identity_block(x,width)
    	x = identity_block(x,width)
    
    	x = layers.BatchNormalization()(x)
    	x = layers.Dense(1,activation=""linear"")(x)
    	model = models.Model(inputs=Res_input, outputs=x)
    
    	return model

Essentially, I am fitting the model to each dataset as follows:

        import datetime
        from tensorflow.keras import layers,models
    
        model = ResNet50Regression()
    
        model.compile(loss='mse', optimizer=Adam(learning_rate=0.0001), metrics=['mse'])
        model.summary()
    
        starttime = datetime.datetime.now()
    
        history = model.fit(X_train, y_train, epochs=200, batch_size=64,  verbose=2, validation_data=(X_test, y_test))
        endtime = datetime.datetime.now()

How can I get optimal prediction results from the above model, below is my results prediction now:

&#x200B;

https://preview.redd.it/cr8noqmbwu191.png?width=322&format=png&auto=webp&s=5463e5065eb506b61da6e0debb1d841d65db2551

https://preview.redd.it/ix88hqp9wu191.png?width=660&format=png&auto=webp&s=66f67bdb29b56a074f3bbd048003f875b00aa770

based on the predictions of the model above, the predictions generated are not able to predict properly. how to make prediction results correspondent  the real value",deeplearning,2022-05-26 10:10:13,7
"The way I see it, 2 options:

1. Find the datasets
2. Get a dataset with a mic w/ neutral frequency response and then apply filters mimicking other micrphones.

The neutral frequency response is absolutely key here. If you grab non-neutral mic and start applying eq profiles, you will just stack eq on top of eq and who knows what is the actual response you will get.

There is one other option - if you have a dataset on a stable mic AND you know that mic frequency response, you can eq it to neutral and then apply other mic profiles- this is not ideal, but in theory it should work well enough, provided you use gentle phase-neutral eq.

Option #3 is probably the easiest, realistically speaking, but easy to mess up getting neutral mic (Eq'ing introduces whole bunch of problems, need domain expertise to minimise those), option #2 is close to ideal, but good luck finding dataset big enough recorded on a neutral mic and option #1 is probably the ideal, but collecting that data will be hard.

For option #1 - do you care if its spoken or sang? Coz you can find stems for remixes of vocals, and those will be mic'ed differently, so with a bit of elbow grease you can get yourself a big enough dataset.

Edit: My ass didn't read about mobile - so another suggestion, is to scrap tik-tok videos off audio for the data - most tik-toks are done with phones, and if you grab many different ones, you are bound to get a good variety.",2,uyjgbc,"Hi,

Does anyone know of datasets that include recordings with multiple microphones (including a high-quality recording)?

Alternatively I could augment high-quality recordings with filters that mimic different microphone profiles. I wonder if anyone has been looking into this that I could have a look 😇

I'm mostly looking for mobile phone recordings with a baseline high-quality recording.

Thanks",deeplearning,2022-05-26 15:36:49,7
Learn more here: https://huggingface.co/blog/community-update,-1,uxo3qf,,deeplearning,2022-05-25 11:45:01,1
"Disclaimer: I'm a complete noob. 

Why do holdout from specific component datasets? Some of your datasets are probably closer to the aggregate than others. How does the aggregate model do when you test on a sample from the aggregate?

As for unseen data, if it's far from what it was mostly trained on, I would expect to see poor performance.",2,uy6kaw,"You have a combined dataset consisting of 10 component datasets collected from 10 different sources. Independent models trained separately on each component dataset perform well on hold-out examples from that dataset. However, the aggregated model trained by combining the examples from all component datasets behaves weirdly. On hold-out examples from some component datasets, the aggregated model performs better than the independent models. On others, it performs worse than the independent models. 

During deployment, you expect to see input examples from these 10 component sources but also from many other sources which the model has not been trained on. 

What approach will you take to develop a model that will generalize well to examples from the seen and also the yet-unseen sources?",deeplearning,2022-05-26 05:31:45,2
"Since you look Indian, why do you want to go to such a fuckall university in a fuckall country for tech?",3,uy2dim,"Hello,

I've been offered admission in the MSc Computer Vision Program at MBZUAI, \[Mohamed bin Zayed University of Artificial Intelligence\] UAE. The university is providing full tuition scholarship, accommodation, very good monthly stipend, health insurance, travel expenses, etc. The university was started pretty recently and has no brand value so far, but the faculties present there currently teaching and indulging in research seems to be quite good and from other good universities across the world.

I want to pursue a career in ML/AI research. I'm considering doing a PhD after this in an established and good university in USA/Canada or other European countries. If I'm not able to get into a good PhD program after my two years UAE study, I would also be happy to work there as AI Engineer and the students have told that the salary of graduates can be as good as 20k AED/month, which is quite a lot. Thus, I do not wish to give up this opportunity. Can someone who has good experience in AI/ML research or who might have some idea about the situation in UAE comment and let me know what should I do? My goal is to contribute to the best research and be able to learn from the best.

I'm currently working as SDE1 in a promising tech startup here in India which has good growth potential, but I might make more money by pursuing the MSc program at MBZUAI due to their monthly stipend.",deeplearning,2022-05-26 00:48:17,10
"Depending on how heavy your inference is, you may not need gpu. At that's most likely when you don't need to Generate something (i.e. text or image). Actually, it's even easier to scale up horizontally with cpus than gpus.",4,uy6whn,"GPU is needed for training a deep learning model. Once the model is trained, it seems to me that GPU is not needed anymore since we are just scoring new data and do not need to keep training it, so GPU is not needed anymore. Is my understanding correct?",deeplearning,2022-05-26 05:50:15,7
just the motorcyclists that aren't using a helmet,1,uy18mu,"I'm doing a thesis on identifying traffic violations using OpenCV. The way the program works is by identifying motorcyclists in traffic lights that aren't using helmets (I live in Indonesia btw) through the CCTV cameras that are in traffic lights. What would be the best approach/algorithm to use? Honestly I'm pretty new to ML and at this moment I'm planning to use the YOLO method but if there's another approach that's better please help me out!

The variables I presume will be people, cars, motorcyclists, and motorcyclists without using a helmet

For the dataset would I have to label everything above or just the motorcyclists that aren't using a helmet?

Thank you everyone",deeplearning,2022-05-25 23:26:26,2
"you have two choices:

1) convert outputs from [-1,1] to [0,1] by adding 1 and deviding by 2 (like y”=(y+1) // 2) when training and then calculating predictions by multiplying by 2 and substracting 1 (y=y_pred*2 - 1) or

2) incorporating this into BCE sum
((y+1)/2)*log((y+1)/2) + (1-(y+1)/2)*log(1-(y+1)/2)

either is fine.",2,uxn41w,"Hello.

I have a classification problem in which the output is binary, either 1 or -1.

I am training a neural network and then I am updating its weights to improve the results. 

I saw on the internet that for cases in which the output is either 1 or 0, we could use the binary cross entropy (BCE) but what about the case of different binary results? 

I hope my question was clear and thank you!",deeplearning,2022-05-25 10:59:01,6
"* Website : [https://main-image-background-changer-589hero.endpoint.ainize.ai/](https://main-image-background-changer-589hero.endpoint.ainize.ai/)
* Github : [https://github.com/589hero/image-background-changer](https://github.com/589hero/image-background-changer)
* Rembg github : [https://github.com/danielgatis/rembg](https://github.com/danielgatis/rembg)",2,uxce0s,"This project was made using rembg package that performs image segmentation with U\^2-Net.

https://reddit.com/link/uxce0s/video/4i5ny7a7wk191/player",deeplearning,2022-05-25 00:57:27,2
Remind me in a week,2,ux9my5,"Is there a good tutorial on how to implement diffusion models (ideally dall-e-2) from (almost) scratch? I see that this [github repo](https://github.com/lucidrains/DALLE2-pytorch) exists but without deep understanding of the model, it's a tad hard to follow. What I'm hoping is to implement this on a smaller scale using the Coco dataset (and generating a 32x32 image). Any thoughts?",deeplearning,2022-05-24 21:45:26,1
"Xlm Roberta model , it is good with multilingual data . I have used the same in my project for token classification, I think you can also find the dataset in hf datasets , i dealt with German & English .",2,ux84zc,"I am working on a problem where a sentence contains both English and Hindi Language.

For Example :- "" good afternoon मेरा नाम ambuje है में tcs से call कर  रहा  हूँ ""

So in this example - ambuje - <Person> and tcs <ORG>

How can I perform NER on this data where a sentence contains both the languages?

Thank you ,",deeplearning,2022-05-24 20:15:45,1
It’s almost as if understanding model behavior is important.,6,uwp1fb,,deeplearning,2022-05-24 04:53:05,2
Could be a disruptor in the ad creative space.,4,uwhvl3,,deeplearning,2022-05-23 20:41:48,7
Wrong sub 😤😤,5,uxd1sa,,deeplearning,2022-05-25 01:50:07,1
"Honestly and without malice: if your supervisor gave you this project and you **don't even know Python**, there is no chance you can get it done. Talk to them and either get (significant and local) help, or pass the project to someone else.",4,ux288a,"Sorry if I'm not posting in the right sub but I need help with my project and I'm also new to programming with python, my project was given to me by my supervisor and it's breaking me down...It's a malware detection system using RNN, DenseNet CNN, VGG\_19 CNN and Deep neural network algorithms, with particle swam Optimization and Ant Colony Optimization for feature selection while using a confusion matrix for Classification. Please can anyone help me with the steps to follow, I need guidance, please",deeplearning,2022-05-24 14:59:59,1
"👋 Hello! Thanks for asking about **handling inference results**. YOLOv5 🚀 [PyTorch Hub](https://github.com/ultralytics/yolov5/issues/36) models allow for simple model loading and inference in a pure python environment without using `detect.py`. 

### Simple Inference Example

This example loads a pretrained YOLOv5s model from PyTorch Hub as `model` and passes an image for inference. `'yolov5s'` is the YOLOv5 'small' model. For details on all available models please see the [README](https://github.com/ultralytics/yolov5#pretrained-checkpoints). Custom models can also be loaded, including custom trained PyTorch models and their [exported](https://github.com/ultralytics/yolov5/issues/251) variants, i.e. ONNX, TensorRT, TensorFlow, OpenVINO YOLOv5 models.

```python
import torch

# Model
model = torch.hub.load('ultralytics/yolov5', 'yolov5s')  # yolov5n - yolov5x6 official model
#                                            'custom', 'path/to/best.pt')  # custom model

# Images
im = 'https://ultralytics.com/images/zidane.jpg'  # or file, Path, URL, PIL, OpenCV, numpy, list

# Inference
results = model(im)

# Results
results.print()  # or .show(), .save(), .crop(), .pandas(), etc.

results.xyxy[0]  # im predictions (tensor)
results.pandas().xyxy[0]  # im predictions (pandas)
#      xmin    ymin    xmax   ymax  confidence  class    name
# 0  749.50   43.50  1148.0  704.5    0.874023      0  person
# 2  114.75  195.75  1095.0  708.0    0.624512      0  person
# 3  986.00  304.00  1028.0  420.0    0.286865     27     tie
```
<img width=""550"" src=""https://user-images.githubusercontent.com/26833433/149679662-b2021c78-59b2-4f52-b768-1e5161537c85.png"">

See [YOLOv5 PyTorch Hub Tutorial](https://github.com/ultralytics/yolov5/issues/36) for details.

Good luck 🍀 and let us know if you have any other questions!",1,uwvrg2,"I want to count the different classes of object detected and display them at the corner of the screen.

Do I need to implement a tracking algorithm (such as deepsort) for this?

How can I do this..please help",deeplearning,2022-05-24 10:10:44,1
"Default image size is 640 (P5 models) and 1280 (P6 models), but you can train at any image size you want.",1,uwu3az,,deeplearning,2022-05-24 08:57:02,2
"Are you asking about a theoretical foundation for your loss function or for help with implementing it? It sounds like you're talking about ordinal regression.

https://towardsdatascience.com/how-to-perform-ordinal-regression-classification-in-pytorch-361a2a095a99",1,uwmzc1,"Hi,

I'm making a CNN for multi-class classification with 3 classes. I would like to have a metric which finds the minimum number of ""serious errors"". What I mean by serious errors is the classification where class 1 is classified as 3 and vice versa. Do you guys have expereience with this way of making a custom loss function?",deeplearning,2022-05-24 02:40:25,3
"try start here: https://youtu.be/pDdP0TFzsoQ
and google questions, good luck!",1,uwp77e,"Hi, I am trying to create a image classifier for Pubg game guns which also takes skins into account. Can you guys suggest some resources to learn and use so that I could start up with the project and complete it.",deeplearning,2022-05-24 05:02:01,1
Because the task is not just mapping the source to a target,7,uw62lq,,deeplearning,2022-05-23 10:53:20,1
